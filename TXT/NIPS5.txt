FN Clarivate Analytics Web of Science
VR 1.0
PT S
AU Volokitin, A
   Roig, G
   Poggio, T
AF Volokitin, Anna
   Roig, Gemma
   Poggio, Tomaso
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Do Deep Neural Networks Suffer from Crowding?
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Crowding is a visual effect suffered by humans, in which an object that can be recognized in isolation can no longer be recognized when other objects, called flankers, are placed close to it. In this work, we study the effect of crowding in artificial Deep Neural Networks (DNNs) for object recognition. We analyze both deep convolutional neural networks (DCNNs) as well as an extension of DCNNs that are multi-scale and that change the receptive field size of the convolution filters with their position in the image. The latter networks, that we call eccentricity-dependent, have been proposed for modeling the feedforward path of the primate visual cortex. Our results reveal that the eccentricity-dependent model, trained on target objects in isolation, can recognize such targets in the presence of flankers, if the targets are near the center of the image, whereas DCNNs cannot. Also, for all tested networks, when trained on targets in isolation, we find that recognition accuracy of the networks decreases the closer the flankers are to the target and the more flankers there are. We find that visual similarity between the target and flankers also plays a role and that pooling in early layers of the network leads to more crowding. Additionally, we show that incorporating flankers into the images of the training set for learning the DNNs does not lead to robustness against configurations not seen at training.
C1 [Volokitin, Anna; Roig, Gemma; Poggio, Tomaso] MIT, Ctr Brains Minds & Machines, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
   [Roig, Gemma; Poggio, Tomaso] MIT, Ist Italiano Tecnol, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
   [Volokitin, Anna] Swiss Fed Inst Technol, Comp Vis Lab, Zurich, Switzerland.
   [Roig, Gemma] Singapore Univ Technol & Design, Singapore, Singapore.
RP Volokitin, A (reprint author), MIT, Ctr Brains Minds & Machines, 77 Massachusetts Ave, Cambridge, MA 02139 USA.; Volokitin, A (reprint author), Swiss Fed Inst Technol, Comp Vis Lab, Zurich, Switzerland.
EM voanna@vision.ee.ethz.ch; gemmar@mit.edu; tp@csail.mit.edu
RI Jeong, Yongwook/N-7413-2016
FU Center for Brains, Minds and Machines (CBMM) - NSF STC award [CCF
   -1231216]; Swiss Commission for Technology and Innovation (KTI)
   [2-69723-16]; SUTD SRG grant [SRG ISTD 2017 131]
FX This work was supported by the Center for Brains, Minds and Machines
   (CBMM), funded by NSF STC award CCF -1231216. A. Volokitin was also
   funded by Swiss Commission for Technology and Innovation (KTI, Grant No
   2-69723-16), and thanks Luc Van Gool for his support. G. Roig was partly
   funded by SUTD SRG grant (SRG ISTD 2017 131). We also thank Xavier Boix,
   Francis Chen and Yena Han for helpful discussions.
CR Balas B, 2009, J VISION, V9, DOI [10.1167/9.12.13, 10.1167/9.2.16]
   BANKS WP, 1977, PERCEPT PSYCHOPHYS, V22, P232, DOI 10.3758/BF03199684
   BOUMA H, 1970, NATURE, V226, P177, DOI 10.1038/226177a0
   Bulatov Y., 2011, NOTMNIST DATASET
   Chen F., 2017, AAAI SPRING S SERIES
   Cheung B., 2016, INT C LEARN REPR
   Francis G., 2016, J VISION, V16, P1114
   Freeman J, 2011, NAT NEUROSCI, V14, P1195, DOI 10.1038/nn.2889
   Goodfellow IJ, 2014, ARXIV14126572
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Keshvari S, 2016, J VISION, V16, DOI 10.1167/16.3.39
   KOOI FL, 1994, SPATIAL VISION, V8, P255, DOI 10.1163/156856894X00350
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun  Y., 1998, MNIST DATABASE HANDW
   Levi DM, 2008, VISION RES, V48, P635, DOI 10.1016/j.visres.2007.12.009
   Luo Y., 2015, ARXIV151106292
   Mnih V., 2014, ADV NEURAL INFORM PR, V3, P2204
   NAZIR TA, 1992, VISION RES, V32, P771, DOI 10.1016/0042-6989(92)90192-L
   Poggio T., 2014, ARXIV14061770
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Szegedy C., 2015, COMPUTER VISION PATT
   Whitney D, 2011, TRENDS COGN SCI, V15, P160, DOI 10.1016/j.tics.2011.02.005
   Zeiler M. D., 2014, EUR C COMP VIS, P818, DOI DOI 10.1007/978-3-319-10590-1_53
   Zhou B., 2014, ADV NEURAL INFORM PR, V27, P487, DOI DOI 10.1162/153244303322533223
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405069
DA 2019-06-15
ER

PT S
AU Wald, Y
   Globerson, A
AF Wald, Yoav
   Globerson, Amir
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Robust Conditional Probabilities
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID REGULARIZATION; INEQUALITIES; OPTIMIZATION; ALGORITHM; BOUNDS
AB Conditional probabilities are a core concept in machine learning. For example, optimal prediction of a label Y given an input X corresponds to maximizing the conditional probability of Y given X . A common approach to inference tasks is learning a model of conditional probabilities. However, these models are often based on strong assumptions (e.g., log-linear models), and hence their estimate of conditional probabilities is not robust and is highly dependent on the validity of their assumptions.
   Here we propose a framework for reasoning about conditional probabilities without assuming anything about the underlying distributions, except knowledge of their second order marginals, which can be estimated from data. We show how this setting leads to guaranteed bounds on conditional probabilities, which can be calculated efficiently in a variety of settings, including structured-prediction. Finally, we apply them to semi-supervised deep learning, obtaining results competitive with variational autoencoders.
C1 [Wald, Yoav] Hebrew Univ Jerusalem, Sch Comp Sci & Engn, Jerusalem, Israel.
   [Globerson, Amir] Tel Aviv Univ, Balvatnik Sch Comp Sci, Tel Aviv, Israel.
RP Wald, Y (reprint author), Hebrew Univ Jerusalem, Sch Comp Sci & Engn, Jerusalem, Israel.
EM yoay.wald@mail.huji.ac.il; gamir@mail.tau.ac.il
FU ISF Centers of Excellence [2180/15]; Intel Collaborative Research
   Institute for Computational Intelligence (ICRI-CI)
FX This work was supported by the ISF Centers of Excellence grant 2180/15,
   and by the Intel Collaborative Research Institute for Computational
   Intelligence (ICRI-CI).
CR AHUJA RK, 1995, NETWORKS, V25, P89, DOI 10.1002/net.3230250207
   Akhiezer N. I., 1965, CLASSICAL MOMENT PRO, V5
   Benavoli A., 2017, P 10 INT S IMPR PROB
   Bertsimas D, 2005, SIAM J OPTIMIZ, V15, P780, DOI 10.1137/S1052623401399903
   Cowell Robert G., 2006, PROBABILISTIC NETWOR
   Dudik M, 2007, J MACH LEARN RES, V8, P1217
   Eban Elad, 2014, P 31 INT C MACH LEAR, P1233
   Fromer M., 2009, ADV NEURAL INFORM PR, V22, P567
   Grandvalet Y., 2005, ADV NEURAL INFORM PR, V17, P529
   Joachims T, 1999, MACHINE LEARNING, PROCEEDINGS, P200
   Kingma D. P., 2014, ADV NEURAL INFORM PR, V4, P3581
   Koller D., 2009, PROBABILISTIC GRAPHI
   Lafferty J.D., 2001, P INT C MACH LEARN, V18, P282
   Lanckriet Gert RG, 2002, J MACHINE LEARNING R, V3, P555
   Lasserre JB, 2001, SIAM J OPTIMIZ, V11, P796, DOI 10.1137/S1052623400366802
   Livni Roi, 2012, P 15 INT C ART INT S, P722
   McClosky D., 2006, P N AM CHAPT ASS COM, P152, DOI DOI 10.3115/1220835.1220855
   Miranda E, 2007, J THEOR PROBAB, V20, P663, DOI 10.1007/s10959-007-0055-4
   Muller AC, 2014, J MACH LEARN RES, V15, P2055
   Parrilo PA, 2003, MATH PROGRAM, V96, P293, DOI 10.1007/s10107-003-0387-5
   Rasmus A., 2015, ADV NEURAL INFORM PR, P3546
   SMITH JE, 1995, OPER RES, V43, P807, DOI 10.1287/opre.43.5.807
   Smola A, 2007, LECT NOTES ARTIF INT, V4754, P13
   Song L, 2013, IEEE SIGNAL PROC MAG, V30, P98, DOI 10.1109/MSP.2013.2252713
   Sriperumbudur BK, 2011, J MACH LEARN RES, V12, P2389
   Tsoumakas G, 2011, J MACH LEARN RES, V12, P2411
   Vandenberghe L, 2007, SIAM REV, V49, P52, DOI 10.1137/S003614450440543
   Vazirani V. V., 2013, APPROXIMATION ALGORI
   Wainwright M, 2004, STAT COMPUT, V14, P143, DOI 10.1023/B:STCO.0000021412.33763.d5
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Weiss D., 2015, P 53 ANN M ASS COMP, V1, P323
   Xu H, 2009, J MACH LEARN RES, V10, P1485
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406042
DA 2019-06-15
ER

PT S
AU Wang, C
   Lu, YM
AF Wang, Chuang
   Lu, Yue M.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI The Scaling Limit of High-Dimensional Online Independent Component
   Analysis
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID DYNAMICS
AB We analyze the dynamics of an online algorithm for independent component analysis in the high-dimensional scaling limit As the ambient dimension tends to infinity, and with proper time scaling, we show that the time-varying joint empirical measure of the target feature vector and the estimates provided by the algorithm will converge weakly to a deterministic measured-valued process that can be characterized as the unique solution of a nonlinear PDE. Numerical solutions of this PDE, which involves two spatial variables and one time variable, can be efficiently obtained. These solutions provide detailed information about the performance of the ICA algorithm, as many practical performance metrics are functionals of the joint empirical measures. Numerical simulations show that our asymptotic analysis is accurate even for moderate dimensions. In addition to providing a tool for understanding the performance of the algorithm, our PDE analysis also provides useful insight. In particular, in the high-dimensional limit, the original coupled dynamics associated with the algorithm will be asymptotically "decoupled", with each coordinate independently solving a 1-D effective minimization problem via stochastic gradient descent. Exploiting this insight to design new algorithms for achieving optimal trade-offs between computational and statistical efficiency may prove an interesting line of future research.
C1 [Wang, Chuang; Lu, Yue M.] Harvard Univ, John A Paulson Sch Engn & Appl Sci, 33 Oxford St, Cambridge, MA 02138 USA.
RP Wang, C (reprint author), Harvard Univ, John A Paulson Sch Engn & Appl Sci, 33 Oxford St, Cambridge, MA 02138 USA.
EM chuangwang@seas.harvard.edu; yuelu@seas.harvard.edu
RI Jeong, Yongwook/N-7413-2016
FU US Army Research Office [W911NF-16-1-0265]; US National Science
   Foundation [CCF-1319140, CCF-1718698]
FX This work is supported by US Army Research Office under contract
   W911NF-16-1-0265 and by the US National Science Foundation under grants
   CCF-1319140 and CCF-1718698.
CR Barbier J, 2016, ADV NEURAL INFORM PR, P424
   Basalyga G, 2004, J MACH LEARN RES, V4, P1393
   Bayati M, 2011, IEEE T INFORM THEORY, V57, P764, DOI 10.1109/TIT.2010.2094817
   BIEHL M, 1994, EUROPHYS LETT, V25, P391, DOI 10.1209/0295-5075/25/5/014
   Biehl Michael, 1998, J PHYS A, V31, P97
   Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924
   Chuang Wang, 2016, 2016 IEEE Information Theory Workshop (ITW), P186, DOI 10.1109/ITW.2016.7606821
   CUGLIANDOLO LF, 1994, J PHYS A-MATH GEN, V27, P5749, DOI 10.1088/0305-4470/27/17/011
   Donoho D, 2016, PROBAB THEORY REL, V166, P935, DOI 10.1007/s00440-015-0675-z
   Ge R., 2015, J MACH LEARN RES, V40, P1
   Haykin Simon, LEAST MEAN SQUARE AD, V31
   Hyvarinen A, 1997, ADV NEUR IN, V9, P480
   JIN C, 2017, ARXIV170300887
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li Chris Junchi, 2016, ADV NEURAL INF PROCE, P4961
   Li X., 2016, ARXIV160604933
   McKean H. P, 1967, LECTURE SERIES DIFFE, P41
   MCKEAN HP, 1966, P NATL ACAD SCI USA, V56, P1907, DOI 10.1073/pnas.56.6.1907
   MELEARD S, 1987, STOCH PROC APPL, V26, P317, DOI 10.1016/0304-4149(87)90184-0
   Mitliagkas Ioannis, 2013, ADV NEURAL INF PROCE
   Netrapalli P., 2013, ADV NEURAL INFORM PR, V26, P2796
   OJA E, 1985, J MATH ANAL APPL, V106, P69, DOI 10.1016/0022-247X(85)90131-3
   Rattray M, 2002, ADV NEUR IN, V14, P495
   Rattray M, 2002, NEURAL COMPUT, V14, P421, DOI 10.1162/08997660252741185
   Roberts GO, 1997, ANN APPL PROBAB, V7, P110
   Saad D, 1997, PHYS REV LETT, V79, P2578, DOI 10.1103/PhysRevLett.79.2578
   SAAD D, 1995, PHYS REV LETT, V74, P4337, DOI 10.1103/PhysRevLett.74.4337
   Shamir O., 2013, INT C MACH LEARN, V28, P71
   Sznitman Alain-Sol, 1989, PROBAB SAINT FLOUR 1, P165
   Zhang Huishuai, 2016, ARXIV160303805
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406068
DA 2019-06-15
ER

PT S
AU Wang, D
   Ye, MW
   Xu, JH
AF Wang, Di
   Ye, Minwei
   Xu, Jinhui
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Differentially Private Empirical Risk Minimization Revisited: Faster and
   More General
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB In this paper we study the differentially private Empirical Risk Minimization (ERM) problem in different settings. For smooth (strongly) convex loss function with or without (non)-smooth regularization, we give algorithms that achieve either optimal or near optimal utility bounds with less gradient complexity compared with previous work. For ERM with smooth convex loss function in high-dimensional (p >> n) setting, we give an algorithm which achieves the upper bound with less gradient complexity than previous ones. At last, we generalize the expected excess empirical risk from convex loss functions to non-convex ones satisfying the Polyak-Lojasiewicz condition and give a tighter upper bound on the utility than the one in [34].
C1 [Wang, Di; Ye, Minwei; Xu, Jinhui] SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA.
RP Wang, D (reprint author), SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA.
EM dwang45@buffalo.edu; minweiye@buffalo.edu; jinhui@buffalo.edu
FU NSF [IIS-1422591, CCF-1422324, CCF-1716400]
FX This research was supported in part by NSF through grants IIS-1422591,
   CCF-1422324, and CCF-1716400.
CR Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318
   Agarwal Naman, 2017, P 34 INT C MACH LEAR, P32
   Allen-Zhu Z., 2016, P 33 INT C MACH LEAR
   Allen-Zhu Z., 2017, P 8 INN THEOR COMP S
   Allen-Zhu Z, 2017, ACM S THEORY COMPUT, P1200, DOI 10.1145/3055399.3055448
   Bassily R, 2014, ANN IEEE SYMP FOUND, P464, DOI 10.1109/FOCS.2014.56
   Chaudhuri K., 2009, ADV NEURAL INFORM PR, P289
   Chaudhuri K., 2012, ADV NEURAL INFORM PR, V25, P989
   Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069
   Dwork C., 2014, P 46 ANN ACM S THEOR, P11
   Dwork C, 2008, LECT NOTES COMPUT SC, V4978, P1, DOI 10.1007/978-3-540-79228-4_1
   Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265
   Dwork C, 2010, ANN IEEE SYMP FOUND, P51, DOI 10.1109/FOCS.2010.12
   Feldman D, 2009, ACM S THEORY COMPUT, P361
   Hardt Moritz, 2013, P 45 ANN ACM S THEOR, P331, DOI [DOI 10.1145/2488608.2488650, 10.1145/2488608.2488650]
   Hazan Elad, 2015, ADV NEURAL INFORM PR, P1594
   Jain P., 2012, COLT, V23
   Karimi H., 2016, JOINT EUR C MACH LEA, P795, DOI DOI 10.1007/978-3-319-46128-1_50
   Kasiviswanathan S. P., 2016, INT C MACH LEARN, P488
   Kifer D., 2012, J MACHINE LEARNING R, V1, P3
   Li GY, 2018, FOUND COMPUT MATH, V18, P1199, DOI 10.1007/s10208-017-9366-8
   Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5
   Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8
   Nitanda A, 2014, NEURAL INF PROCESS S, V27, P1574
   Polyak B. T., 1963, USSR COMP MATH MATH, V3, P864
   Prasad S, 2017, PODS'17: PROCEEDINGS OF THE 36TH ACM SIGMOD-SIGACT-SIGAI SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS, P167, DOI 10.1145/3034786.3034795
   Reddi S. J, 2016, INT C MACH LEARN, P314
   Talwar K., 2015, ADV NEURAL INFORM PR, P3025
   Talwar K., 2014, ARXIV14115417
   Thakurta A. G., 2013, ADV NEURAL INFORM PR, P2733
   Wang YQ, 2016, BMC GENET, V17, DOI 10.1186/s12863-016-0364-7
   Wu X., 2015, ARXIV151206388
   Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791
   Zhang J., 2017, P 26 INT JOINT C ART, P3922
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402075
DA 2019-06-15
ER

PT S
AU Wang, JF
   Hu, XL
AF Wang, Jianfeng
   Hu, Xiaolin
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Gated Recurrent Convolution Neural Network for OCR
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID RECOGNITION
AB Optical Character Recognition (OCR) aims to recognize text in natural images. Inspired by a recently proposed model for general image classification, Recurrent Convolution Neural Network (RCNN), we propose a new architecture named Gated RCNN (GRCNN) for solving this problem. Its critical component, Gated Recurrent Convolution Layer (GRCL), is constructed by adding a gate to the Recurrent Convolution Layer (RCL), the critical component of RCNN. The gate controls the context modulation in RCL and balances the feed-forward information and the recurrent information. In addition, an efficient Bidirectional Long Short-Term Memory (BLSTM) is built for sequence modeling. The GRCNN is combined with BLSTM to recognize text in natural images. The entire GRCNN-BLSTM model can be trained end-to-end. Experiments show that the proposed model outperforms existing methods on several benchmark datasets including the IIIT-5K, Street View Text (SVT) and ICDAR.
C1 [Wang, Jianfeng] Beijing Univ Posts & Telecommun, Beijing 100876, Peoples R China.
   [Hu, Xiaolin] Tsinghua Univ, Tsinghua Natl Lab Informat Sci & Technol TNList, Dept Comp Sci & Technol, CBICR, Beijing 100084, Peoples R China.
RP Wang, JF (reprint author), Beijing Univ Posts & Telecommun, Beijing 100876, Peoples R China.
EM jianfengwang1991@gmail.com; xlhu@tsinghua.edu.cn
RI Jeong, Yongwook/N-7413-2016
FU National Basic Research Program (973 Program) of China [2013CB329403];
   National Natural Science Foundation of China [91420201, 61332007,
   61621136008, 61620106010]
FX This work was supported in part by the National Basic Research Program
   (973 Program) of China under grant no. 2013CB329403, the National
   Natural Science Foundation of China under grant nos. 91420201, 61332007,
   61621136008 and 61620106010, and in part by a grant from Sensetime.
CR Almazan J, 2014, IEEE T PATTERN ANAL, V36, P2552, DOI 10.1109/TPAMI.2014.2339814
   Alsharif O., 2013, COMPUTER SCI
   Bissacco A., 2014, IEEE INT C COMP VIS, P785
   Cho K., 2014, COMPUTER SCI
   Gers FA, 2000, IEEE IJCNN, P189, DOI 10.1109/IJCNN.2000.861302
   Goel V, 2013, PROC INT CONF DOC, P398, DOI 10.1109/ICDAR.2013.87
   Gordo A., 1998, CVPR, P2956
   Graves A., 2006, P 23 INT C MACH LEAR, P369, DOI DOI 10.1145/1143844.1143891
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Huang G, 2017, CVPR
   HUBEL DH, 1965, J NEUROPHYSIOL, V28, P229
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Jaderberg  M., 2014, WORKSH DEEP LEARN NI
   Jaderberg M., 2014, ICLR
   Jaderberg M, 2014, LECT NOTES COMPUT SC, V8692, P512, DOI 10.1007/978-3-319-10593-2_34
   Jawahar C. V., 2014, CVPR, P2687
   Jones HE, 2001, J NEUROPHYSIOL, V86, P2011
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lee CY, 2016, PROC CVPR IEEE, P2231, DOI 10.1109/CVPR.2016.245
   Lee CY, 2014, PROC CVPR IEEE, P4050, DOI 10.1109/CVPR.2014.516
   Liang M., 2015, NIPS
   Liang M, 2015, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR.2015.7298958
   Liu XY, 2017, IEEE INT C INTELL TR
   Lucas SM, 2003, SEVENTH INTERNATIONAL CONFERENCE ON DOCUMENT ANALYSIS AND RECOGNITION, VOLS I AND II, PROCEEDINGS, P682
   Mishra A., 2013, BMVC
   Neumann L, 2012, PROC CVPR IEEE, P3538, DOI 10.1109/CVPR.2012.6248097
   NOVIKOVA T, 2012, ECCV, V7577, P752
   Rodriguez-Serrano JA, 2015, INT J COMPUT VISION, V113, P193, DOI 10.1007/s11263-014-0793-6
   Schuster M., 1997, BIDIRECTIONAL RECURR
   Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371
   Shi CZ, 2013, PROC CVPR IEEE, P2961, DOI 10.1109/CVPR.2013.381
   Socher R., 2010, NIPS
   Su B., 2014, P AS C COMP VIS, P35, DOI DOI 10.1007/978-3-319-16865-4_
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Wang J., 2016, NIPS
   Wang K, 2010, LECT NOTES COMPUT SC, V6311, P591, DOI 10.1007/978-3-642-15549-9_43
   Wang Ke, 2012, Proceedings of the 2012 Second International Conference on Intelligent System Design and Engineering Application (ISDEA), P1457, DOI 10.1109/ISdea.2012.421
   Wang T, 2012, INT C PATT RECOG, P3304
   Xie Saining, 2017, CVPR
   Yao C, 2014, PROC CVPR IEEE, P4042, DOI 10.1109/CVPR.2014.515
   Zeiler M D., 2012, COMPUTER SCI
   Zeiler  M.D., 2014, ECCV
NR 43
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400032
DA 2019-06-15
ER

PT S
AU Wang, LW
   Schwing, AG
   Lazebnik, S
AF Wang, Liwei
   Schwing, Alexander G.
   Lazebnik, Svetlana
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Diverse and Accurate Image Description Using a Variational Auto-Encoder
   with an Additive Gaussian Encoding Space
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB This paper explores image caption generation using conditional variational auto-encoders (CVAEs). Standard CVAEs with a fixed Gaussian prior yield descriptions with too little variability. Instead, we propose two models that explicitly structure the latent space around K components corresponding to different types of image content, and combine components to create priors for images that contain multiple types of content simultaneously (e.g., several kinds of objects). Our first model uses a Gaussian Mixture model (GMM) prior, while the second one defines a novel Additive Gaussian (AG) prior that linearly combines component means. We show that both models produce captions that are more diverse and more accurate than a strong LSTM baseline or a "vanilla" CVAE with a fixed Gaussian prior, with AG-CVAE showing particular promise.
C1 [Wang, Liwei; Schwing, Alexander G.; Lazebnik, Svetlana] Univ Illinois, Champaign, IL 61820 USA.
RP Wang, LW (reprint author), Univ Illinois, Champaign, IL 61820 USA.
EM lwang97@illinois.edu; aschwing@illinois.edu; slazebni@illinois.edu
RI Jeong, Yongwook/N-7413-2016
FU National Science Foundation [1563727, 1718221]; Sloan Foundation
FX This material is based upon work supported in part by the National
   Science Foundation under Grants No. 1563727 and 1718221, and by the
   Sloan Foundation. We would like to thank Jian Peng and Yang Liu for
   helpful discussions.
CR Anderson P., 2016, ECCV
   Batra D., 2012, ECCV
   Bishop C. M., 1994, MIXTURE DENSITY NETW
   Chen X, 2015, CORR, V1504, P325
   Dai Bo, 2017, ICCV
   Denkowski M., 2014, P EACL 2014 WORKSH S
   Deshpande A., 2017, CVPR
   Devlin J., 2015, ARXIV150501809
   Devlin J., 2015, ARXIV150504467
   Farhadi  A., 2010, ECCV
   Hershey J., 2007, ICASSP
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Jain U., 2017, CVPR
   Jang Eric, 2017, ICLR
   Johnson M. J., 2016, NIPS
   Kingma Diederik P, 2014, ICLR
   Kiros R., 2014, ICML
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Kuznetsova P., 2013, ACL
   Lin C. Y., 2004, TEXT SUMMARIZATION B, V8
   Liu S., 2017, ICCV
   Mao J, 2015, ICLR
   Mitchell M., 2012, P 13 C EUR CHAPT ASS, P747
   Papineni K., 2002, ACL
   Ren S., 2015, NIPS
   Shetty R., 2017, ICCV
   Simonyan K, 2014, ARXIV14091556
   Sohn K., 2015, NIPS
   Vedantam Ramakrishna, 2015, CVPR
   Vijayakumar A. K., 2016, ARXIV161002424
   Vinyals O, 2015, CVPR
   Vinyals Oriol, 2016, IEEE T PATTERN ANAL
   Wang L., 2016, CVPR
   Wang Zhuhao, 2016, IJCAI
   Xu K, 2015, ICML
   You Q., 2016, CVPR
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405081
DA 2019-06-15
ER

PT S
AU Wang, MZ
   Tang, YH
   Wang, J
   Deng, J
AF Wang, Mingzhe
   Tang, Yihe
   Wang, Jian
   Deng, Jia
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Premise Selection for Theorem Proving by Deep Graph Embedding
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We propose a deep learning-based approach to the problem of premise selection: selecting mathematical statements relevant for proving a given conjecture. We represent a higher-order logic formula as a graph that is invariant to variable renaming but still fully preserves syntactic and semantic information. We then embed the graph into a vector via a novel embedding method that preserves the information of edge ordering. Our approach achieves state-of-the-art results on the HolStep dataset, improving the classification accuracy from 83% to 90.3%.
C1 [Wang, Mingzhe; Tang, Yihe; Wang, Jian; Deng, Jia] Univ Michigan, Ann Arbor, MI 48109 USA.
RP Wang, MZ (reprint author), Univ Michigan, Ann Arbor, MI 48109 USA.
RI Jeong, Yongwook/N-7413-2016
FU National Science Foundation [1633157]
FX This work is partially supported by the National Science Foundation
   under Grant No. 1633157.
CR Alama J, 2014, J AUTOM REASONING, V52, P191, DOI 10.1007/s10817-013-9286-5
   Bridge JP, 2014, J AUTOM REASONING, V53, P141, DOI 10.1007/s10817-014-9301-5
   Church A., 1940, J SYMBOLIC LOGIC, V5, P56, DOI 10. 2307/2266170
   DEFFERRARD M., 2016, ADV NEURAL INFORM PR, P3837
   Denzinger Jorg, 1999, LEARNING PREVIOUS PR
   Dowek G., 1992, COQ PROOF ASSISTANT
   Duvenaud D. K., 2015, ADV NEURAL INFORM PR, P2224
   Farber Michael, 2016, Automated Reasoning. 8th International Joint Conference, IJCAR 2016. Proceedings: LNCS 9706, P349, DOI 10.1007/978-3-319-40229-1_24
   Goller C, 1996, IEEE IJCNN, P347, DOI 10.1109/ICNN.1996.548916
   Gonthier Georges, 2013, MACHINE CHECKED PROO, P163
   Gori M, 2005, IEEE IJCNN, P729
   Grover Aditya, 2016, KDD, V2016, P855
   Hales T, 2017, FORUM MATH PI, V5, DOI 10.1017/fmp.2017.1
   Harrington J, 2013, J TROP MED, DOI 10.1155/2013/756832
   Harrison J, 2014, HDB HIST LOGIC COMPU, V9, P135
   Harrison John, 2009, HOL LIGHT OVERVIEW T, P60
   Henaff Mikael, 2015, ARXIV150605163
   Hinton G, 2012, LECT 6A OVERVIEW MIN
   Hoder K, 2011, LECT NOTES ARTIF INT, V6803, P299, DOI 10.1007/978-3-642-22438-6_23
   Ioffe S., 2015, ARXIV150203167
   Irving G., 2016, ADV NEURAL INFORM PR, P2235
   Jain A, 2016, PROC CVPR IEEE, P5308, DOI 10.1109/CVPR.2016.573
   Jakubuv Jan, 2017, ARXIV170106532
   Kaliszyk C, 2015, LECT NOTES COMPUT SC, V9450, P88, DOI 10.1007/978-3-662-48899-7_7
   Kaliszyk Cezary, 2017, ARXIV170300426
   Kern C., 1999, ACM Transactions on Design Automation of Electronic Systems, V4, P123, DOI 10.1145/307988.307989
   Kipf T. N., 2016, ARXIV160902907
   Klein G, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P207
   Koval L, 2013, INZ MINER, P1
   Kuhlwein D, 2015, J AUTOM REASONING, V55, P91, DOI 10.1007/s10817-015-9329-1
   Leroy X, 2009, COMMUN ACM, V52, P107, DOI 10.1145/1538788.1538814
   Li Y., 2015, ARXIV151105493
   Loos Sarah, 2017, ARXIV170106972
   Mikolov T., 2013, COMPUTING RES REPOSI, V1301, P3781, DOI DOI 10.1109/TNN.2003.820440]
   Mikolov T., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951
   Misra S., 2016, P C EMP METH NAT LAN, P1775
   Naumowicz Adam, 2009, BRIEF OVERVIEW MIZAR, P67
   Niepert Mathias, 2016, P 33 ANN INT C MACH
   Perozzi B., 2014, KDD, P701, DOI DOI 10.1145/2623330.2623732
   Robinson A., 2001, HDB AUTOMATED REASON, V1
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Schulz S, 2002, AI COMMUN, V15, P111
   Schulz SA, 2000, LEARNING SEARCH CONT, V230
   Socher R., 2011, P 28 INT C MACH LEAR, P129, DOI DOI 10.1007/978-3-540-87479-9
   Socher R., 2011, ADV NEURAL INFORM PR, P801
   Suttner C., 1990, 10th International Conference on Automated Deduction Proceedings, P470
   Tai K. S., 2015, ARXIV150300075
   Tang J, 2015, P 24 INT C WORLD WID, P1067, DOI DOI 10.1145/2736277.2741093
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wenzel M., 2008, LECT NOTES COMPUT SC, V5170, P33, DOI [10.1007/978-3-540-71067-7_7, DOI 10.1007/978-3-540-71067-7_7]
   Whalen Daniel, 2016, ARXIV160802644
NR 51
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402081
DA 2019-06-15
ER

PT S
AU Wang, QS
   Chen, W
AF Wang, Qinshi
   Chen, Wei
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Improving Regret Bounds for Combinatorial Semi-Bandits with
   Probabilistically Triggered Arms and Its Applications
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We study combinatorial multi-armed bandit with probabilistically triggered arms and semi-bandit feedback (CMAB-T). We resolve a serious issue in the prior CMAB-T studies where the regret bounds contain a possibly exponentially large factor of 1/p* where p* is the minimum positive probability that an arm is triggered by any action. We address this issue by introducing a triggering probability modulated (TPM) bounded smoothness condition into the general CMAB-T framework, and show that many applications such as influence maximization bandit and combinatorial cascading bandit satisfy this TPM condition. As a result, we completely remove the factor of 1/p* from the regret bounds, achieving significantly better regret bounds for influence maximization and cascading bandits than before. Finally, we provide lower bound results showing that the factor 1/p* is unavoidable for general CMAB-T problems, suggesting that the TPM condition is crucial in removing this factor.
C1 [Wang, Qinshi] Princeton Univ, Princeton, NJ 08544 USA.
   [Chen, Wei] Microsoft Res, Beijing, Peoples R China.
RP Wang, QS (reprint author), Princeton Univ, Princeton, NJ 08544 USA.
EM qinshiw@princeton.edu; weic@microsoft.com
FU National Natural Science Foundation of China [61433014]
FX Wei Chen is partially supported by the National Natural Science
   Foundation of China (Grant No. 61433014).
CR Auer P, 2003, SIAM J COMPUT, V32, P48
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Berry D., 1985, BANDIT PROBLEMS SEQU
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Chen W., 2016, NIPS
   Chen W, 2016, J MACH LEARN RES, V17
   Chen Wei, 2013, INFORM INFLUENCE PRO
   Combes  Richard, 2015, NIPS
   Gai Yi, 2012, IEEE ACM T NETWORKIN, V20
   Gopalan Aditya, 2014, P 31 INT C MACH LEAR
   HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952
   Kempe D., 2003, P 9 ACM SIGKDD INT C, P137, DOI DOI 10.1145/956750.956769
   Kveton B., 2015, P 18 INT C ART INT S
   Kveton B., 2015, P 32 INT C MACH LEAR
   Kveton Branislav, 2015, ADV NEURAL INFORM PR
   Kveton Branislav, 2014, P 30 C UNC ART INT U
   Lagree P, 2016, ADV NEURAL INFORM PR, P1597
   Lei S., 2015, KDD
   Mitzenmacher M., 2005, PROBABILITY COMPUTIN
   ROBBINS H, 1952, B AM MATH SOC, V58, P527, DOI 10.1090/S0002-9904-1952-09620-8
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.2307/2332286
   Vaswani S, 2015, NIPS WORKSH NETW SOC
   Vaswani Sharan, 2017, P 34 INT C MACH LEAR
   Wang SH, 2016, PROCEEDINGS OF 2016 INTERNATIONAL CONFERENCE ON APPLIED SYSTEM INNOVATION (ICASI)
   Wen Zheng, 2016, CORR
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401020
DA 2019-06-15
ER

PT S
AU Wang, SN
   Shroff, N
AF Wang, Sinong
   Shroff, Ness
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI A New Alternating Direction Method for Linear Programming
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID ALGORITHMS; CONVERGENCE
AB It is well known that, for a linear program (LP) with constraint matrix A is an element of R-mxn, the Alternating Direction Method of Multiplier converges globally and linearly at a rate O((parallel to A parallel to(2)(F) + mn) log(1/epsilon)). However, such a rate is related to the problem dimension and the algorithm exhibits a slow and fluctuating "tail convergence" in practice. In this paper, we propose a new variable splitting method of LP and prove that our method has a convergence rate of O(parallel to A parallel to(2) log(1/epsilon)). The proof is based on simultaneously estimating the distance from a pair of primal dual iterates to the optimal primal and dual solution set by certain residuals. In practice, we result in a new first-order LP solver that can exploit both the sparsity and the specific structure of matrix A and a significant speedup for important problems such as basis pursuit, inverse covariance matrix estimation, L1 SVM and nonnegative matrix factorization problem compared with the current fastest LP solvers.
C1 [Wang, Sinong] Ohio State Univ, Dept ECE, Columbus, OH 43210 USA.
   [Shroff, Ness] Ohio State Univ, Dept ECE & CSE, Columbus, OH 43210 USA.
RP Wang, SN (reprint author), Ohio State Univ, Dept ECE, Columbus, OH 43210 USA.
EM wang.7691@osu.edu; shroff.11@osu.edu
RI Jeong, Yongwook/N-7413-2016
FU ONR [N00014-17-1-2417, N00014-15-1-2166]; ARO [W911NF-1-0277]; NSF
   [CNS-1719371]
FX This work is supported by ONR N00014-17-1-2417, N00014-15-1-2166, NSF
   CNS-1719371 and ARO W911NF-1-0277.
CR Allen-Zhu  Z, 2016, P 33 INT C MACH LEAR, P1110
   Bittorf V., 2012, ADV NEURAL INFORM PR, V12, P1214
   Boley D, 2013, SIAM J OPTIMIZ, V23, P2183, DOI 10.1137/120878951
   Deng W, 2016, J SCI COMPUT, V66, P889, DOI 10.1007/s10915-015-0048-x
   ECKSTEIN J, 1992, MATH PROGRAM, V55, P293, DOI 10.1007/BF01581204
   Eckstein J, 1990, ALTERNATING DIRECTIO
   Eleuterio Vania Lucia Dos Santos, 2009, THESIS
   GLOWINSKI R, 1975, REV FR AUTOMAT INFOR, V9, P41
   GULER O, 1992, J OPTIMIZ THEORY APP, V75, P445, DOI 10.1007/BF00940486
   Hoffman Alan J, 1952, J RES NBS, V49
   Hong M., 2012, MATH PROGRAM, V162, P1
   Lee YT, 2013, ANN IEEE SYMP FOUND, P147, DOI 10.1109/FOCS.2013.24
   LI W, 1994, SIAM J CONTROL OPTIM, V32, P140, DOI 10.1137/S036301299222723X
   Lin Q., 2014, ADV NEURAL INFORM PR, V27, P3059
   Lin TY, 2015, SIAM J OPTIMIZ, V25, P1478, DOI 10.1137/140971178
   Meshi O, 2011, LECT NOTES ARTIF INT, V6912, P470, DOI 10.1007/978-3-642-23783-6_30
   Nishihara Robert, 2015, P 32 INT C MACH LEAR, V32, P343
   Richtarik P, 2014, MATH PROGRAM, V144, P1, DOI 10.1007/s10107-012-0614-z
   Yang JF, 2011, SIAM J SCI COMPUT, V33, P250, DOI 10.1137/090777761
   Yen Ian En-Hsu, 2015, ADV NEURAL INFORM PR, P2368
   Yin WT, 2010, SIAM J IMAGING SCI, V3, P856, DOI 10.1137/090760350
   Yuan M, 2010, J MACH LEARN RES, V11, P2261
   Zhu J., 2003, ADV NEURAL INFORM PR, P49
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401050
DA 2019-06-15
ER

PT S
AU Wang, XQ
   Chen, H
   Cai, WD
   Shen, DG
   Huang, H
AF Wang, Xiaoqian
   Chen, Hong
   Cai, Weidong
   Shen, Dinggang
   Huang, Heng
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Regularized Modal Regression with Applications in Cognitive Impairment
   Prediction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID VARIABLE SELECTION; WHITE-MATTER; DISEASE; ROBUST; MRI
AB Linear regression models have been successfully used to function estimation and model selection in high-dimensional data analysis. However, most existing methods are built on least squares with the mean square error (MSE) criterion, which are sensitive to outliers and their performance may be degraded for heavy-tailed noise. In this paper, we go beyond this criterion by investigating the regularized modal regression from a statistical learning viewpoint. A new regularized modal regression model is proposed for estimation and variable selection, which is robust to outliers, heavy-tailed noise, and skewed noise. On the theoretical side, we establish the approximation estimate for learning the conditional mode function, the sparsity analysis for variable selection, and the robustness characterization. On the application side, we applied our model to successfully improve the cognitive impairment prediction using the Alzheimer's Disease Neuroimaging Initiative (ADNI) cohort data.
C1 [Wang, Xiaoqian; Chen, Hong; Huang, Heng] Univ Pittsburgh, Dept Elect & Comp Engn, Pittsburgh, PA 15260 USA.
   [Cai, Weidong] Univ Sydney, Sch Informat Technol, Sydney, NSW, Australia.
   [Shen, Dinggang] Univ North Carolina Chapel Hill, Dept Radiol, Chapel Hill, NC USA.
   [Shen, Dinggang] Univ North Carolina Chapel Hill, BRIC, Chapel Hill, NC USA.
RP Huang, H (reprint author), Univ Pittsburgh, Dept Elect & Comp Engn, Pittsburgh, PA 15260 USA.
EM xqwang1991@gmail.com; chenh@mail.hzau.edu.cn; tom.cai@sydney.edu.au;
   dinggang_shen@med.unc.edu; heng.huang@pitt.edu
RI Jeong, Yongwook/N-7413-2016
FU U.S. NSF-IIS [1302675]; NSF-IIS [1633753, 1344152, 1619308]; NSF-DBI
   [1356628]; NIH [AG049371]; National Natural Science Foundation of China
   (NSFC) [11671161]
FX This work was partially supported by U.S. NSF-IIS 1302675, NSF-IIS
   1344152, NSF-DBI 1356628, NSF-IIS 1619308, NSF-IIS 1633753, NIH
   AG049371. Hong Chen was partially supported by National Natural Science
   Foundation of China (NSFC) 11671161. We are grateful to the anonymous
   NIPS reviewers for the insightful comments.
CR Armitage SG, 1946, PSYCHOL MONOGR, V60, P1
   Bozzali M, 2002, J NEUROL NEUROSUR PS, V72, P742, DOI 10.1136/jnnp.72.6.742
   Chambers CD, 2004, NAT NEUROSCI, V7, P217, DOI 10.1038/nn1203
   Chen YC, 2016, ANN STAT, V44, P489, DOI 10.1214/15-AOS1373
   COLLOMB G, 1987, J STAT PLAN INFER, V15, P227, DOI 10.1016/0378-3758(86)90099-6
   Feng Y., 2017, ARXIV170205960
   Feng YL, 2015, J MACH LEARN RES, V16, P993
   FOLSTEIN MF, 1975, J PSYCHIAT RES, V12, P189, DOI 10.1016/0022-3956(75)90026-6
   He R, 2011, IEEE T PATTERN ANAL, V33, P1561, DOI 10.1109/TPAMI.2010.220
   Huang J, 2007, AM J NEURORADIOL, V28, P1943, DOI 10.3174/ajnr.A0700
   Huber P. J., 1981, ROBUST STAT
   HUBER PJ, 1984, ANN STAT, V12, P119, DOI 10.1214/aos/1176346396
   Jack CR, 2008, J MAGN RESON IMAGING, V27, P685, DOI 10.1002/jmri.21049
   Karas G, 2008, AM J NEURORADIOL, V29, P944, DOI 10.3174/ajnr.A0949
   Lichman M., 2013, UCI MACHINE LEARNING
   Moradi E, 2017, NEUROIMAGE-CLIN, V13, P415, DOI 10.1016/j.nicl.2016.12.011
   Nickel J, 2003, EPILEPSIA, V44, P1551, DOI 10.1111/j.0013-9580.2003.13603.x
   Nikolova M, 2005, SIAM J SCI COMPUT, V27, P937, DOI 10.1137/030600862
   Principe JC, 2010, INFORM SCI STAT, P1, DOI 10.1007/978-1-4419-1570-2
   Rockafellar R T, 1970, CONVEX ANAL
   SAGER TW, 1982, ANN STAT, V10, P690, DOI 10.1214/aos/1176345865
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Wang H., NEUR INF PROC SYST C, P1286
   Wang H, 2011, LECT NOTES COMPUT SC, V6893, P115, DOI 10.1007/978-3-642-23626-6_15
   Wang H, 2011, IEEE I CONF COMP VIS, P557, DOI 10.1109/ICCV.2011.6126288
   Wang X., 19 INT C MED IM COMP, P273
   Yang H, 2014, J MULTIVARIATE ANAL, V129, P227, DOI 10.1016/j.jmva.2014.04.024
   Yao WX, 2014, SCAND J STAT, V41, P656, DOI 10.1111/sjos.12054
   Yao WX, 2012, J NONPARAMETR STAT, V24, P647, DOI 10.1080/10485252.2012.678848
   Zhao WH, 2014, ANN I STAT MATH, V66, P165, DOI 10.1007/s10463-013-0410-4
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401047
DA 2019-06-15
ER

PT S
AU Wang, YC
   Ye, XJ
   Zha, HY
   Song, L
AF Wang, Yichen
   Ye, Xiaojing
   Zha, Hongyuan
   Song, Le
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Predicting User Activity Level In Point Processes With Mass Transport
   Equation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Point processes are powerful tools to model user activities and have a plethora of applications in social sciences. Predicting user activities based on point processes is a central problem. However, existing works are mostly problem specific, use heuristics, or simplify the stochastic nature of point processes. In this paper, we propose a framework that provides an efficient estimator of the probability mass function of point processes. In particular, we design a key reformulation of the prediction problem, and further derive a differential-difference equation to compute a conditional probability mass function. Our framework is applicable to general point processes and prediction tasks, and achieves superb predictive and efficiency performance in diverse real-world applications compared to the state of the art.
C1 [Wang, Yichen; Zha, Hongyuan; Song, Le] Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.
   [Ye, Xiaojing] Georgia Inst Technol, Sch Math, Atlanta, GA 30332 USA.
   [Song, Le] Ant Financial, Xihu, Peoples R China.
RP Wang, YC (reprint author), Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.
EM yichen.wang@gatech.edu; xye@gsu.edu; zha@cc.gatech.edu;
   lsong@cc.gatech.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [IIS-1218749, IIS-1639792 EAGER, CNS-1704701]; NIH BIGDATA
   [1R01GM108341]; NSF CAREER [IIS-1350983]; ONR [N00014-15-1-2340,
   DMS-1620342, CMMI-1745382, IIS-1639792, IIS-1717916]; NVIDIA; Intel
   ISTC; Amazon AWS
FX This project was supported in part by NSF IIS-1218749, NIH BIGDATA
   1R01GM108341, NSF CAREER IIS-1350983, NSF IIS-1639792 EAGER, NSF
   CNS-1704701, ONR N00014-15-1-2340, DMS-1620342, CMMI-1745382,
   IIS-1639792, IIS-1717916, NVIDIA, Intel ISTC and Amazon AWS.
CR Aalen OO, 2008, STAT BIOL HEALTH, P1
   Antoniades D., 2013, ARXIV13096001
   BLACKWELL D, 1947, ANN MATH STAT, V18, P105, DOI 10.1214/aoms/1177730497
   Bremaud P., 1981, POINT PROCESSES QUEU
   Da Fonseca J, 2014, J FUTURES MARKETS, V34, P548, DOI 10.1002/fut.21644
   Dai Hanjun, 2016, ARXIV160903675
   Dormand J. R., 1980, J COMPUT APPL MATH, V6, P19, DOI [DOI 10.1016/0771-050X(80)90013-3, 10.1016/0771-050X(80)90013-3]
   Du  N., 2013, NIPS
   Du N., 2015, ADV NEURAL INFORM PR, P3492
   Du Nan, 2012, NIPS
   Dudley R. M., 2002, REAL ANAL PROBABILIT
   Farajtabar M, 2015, P ADV NEUR INF PROC, P1954
   Gao S., 2015, WSDM
   Gelfand I, 2000, CALCULUS VARIATIONS
   HAWKES AG, 1971, BIOMETRIKA, V58, P83, DOI 10.1093/biomet/58.1.83
   He N., 2016, ARXIV160801264
   He X., 2015, P 32 INT C MACH LEAR, P871
   Kendall MG, 1938, BIOMETRIKA, V30, P81, DOI 10.1093/biomet/30.1-2.81
   Lian W., 2015, ICML, P2030
   OGATA Y, 1981, IEEE T INFORM THEORY, V27, P23, DOI 10.1109/TIT.1981.1056305
   Pan J., 2016, P 33 INT C MACH LEAR, P2244
   Pastor-Satorras R, 2015, REV MOD PHYS, V87, P925, DOI 10.1103/RevModPhys.87.925
   Tan X., 2016, UAI, P726
   Trivedi R., 2017, ICML
   Wang  Y., 2016, ICML, P2226
   Wang Y., 2017, ICML
   Wang Y., 2016, ADV NEURAL INFORM PR, P4547
   Wang Y.-X., 2017, NIPS
   Wang Yichen, 2016, ARXIV160309021
   Yang S. H., 2013, P 30 INT C MACH LEAR, V28, P1
   Yu  L., 2015, ICDM
   Zhao Q., 2015, KDD
   ZHOU K, 2013, AISTATS, V31, P641
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401066
DA 2019-06-15
ER

PT S
AU Wang, YX
   Ramanan, D
   Hebert, M
AF Wang, Yu-Xiong
   Ramanan, Deva
   Hebert, Martial
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning to Model the Tail
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We describe an approach to learning from long-tailed, imbalanced datasets that are prevalent in real-world settings. Here, the challenge is to learn accurate "few-shot" models for classes in the tail of the class distribution, for which little data is available. We cast this problem as transfer learning, where knowledge from the data-rich classes in the head of the distribution is transferred to the data-poor classes in the tail. Our key insights are as follows. First, we propose to transfer meta-knowledge about learning-to-learn from the head classes. This knowledge is encoded with a meta-network that operates on the space of model parameters, that is trained to predict many-shot model parameters from few-shot model parameters. Second, we transfer this meta-knowledge in a progressive manner, from classes in the head to the "body", and from the "body" to the tail. That is, we transfer knowledge in a gradual fashion, regularizing meta-networks for few-shot regression with those trained with more training data. This allows our final network to capture a notion of model dynamics, that predicts how model parameters are likely to change as more training data is gradually added. We demonstrate results on image classification datasets (SUN, Places, and ImageNet) tuned for the long-tailed setting, that significantly outperform common heuristics, such as data resampling or reweighting.
C1 [Wang, Yu-Xiong; Ramanan, Deva; Hebert, Martial] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA.
RP Wang, YX (reprint author), Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA.
EM yuxiongw@cs.cmu.edu; dramanan@cs.cmu.edu; hebert@cs.cmu.edu
RI Jeong, Yongwook/N-7413-2016
FU ONR MURI [N000141612007]; U.S. Army Research Laboratory (ARL) under the
   Collaborative Technology Alliance Program [W911NF-10-2-0016]; National
   Science Foundation (NSF) [IIS-1618903]; Google; Facebook
FX We thank Liangyan Gui, Olga Russakovsky, Yao-Hung Hubert Tsai, and
   Ruslan Salakhutdinov for valuable and insightful discussions. This work
   was supported in part by ONR MURI N000141612007 and U.S. Army Research
   Laboratory (ARL) under the Collaborative Technology Alliance Program,
   Cooperative Agreement W911NF-10-2-0016. DR was supported in part by the
   National Science Foundation (NSF) under grant number IIS-1618903,
   Google, and Facebook. We also thank NVIDIA for donating GPUs and AWS
   Cloud Credits for Research program.
CR Agrawal P., 2014, ECCV
   Ba J. Lei, 2015, ICCV
   Bengio S., 2015, ICMI
   Bertinetto L., 2016, NIPS
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Donahue J., 2014, ICML
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Finn C., 2017, ICML
   Fu YW, 2018, IEEE SIGNAL PROC MAG, V35, P112, DOI 10.1109/MSP.2017.2763441
   George D, 2017, SCIENCE, V358, DOI 10.1126/science.aag2612
   Gomez S., 2016, NIPS
   Ha D., 2017, ICLR
   Hariharan Bharath, 2017, ICCV
   He HB, 2009, IEEE T KNOWL DATA EN, V21, P1263, DOI 10.1109/TKDE.2008.239
   He K., 2016, ECCV
   He K., 2016, CVPR
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Huang C., 2016, CVPR
   Huh M., 2016, NIPS WORKSH
   Jia Y., 2014, ACM MM
   Koch G., 2015, ICML WORKSH
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Krizhevsky A., 2012, NIPS
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Li K., 2017, ICLR
   Li Z., 2016, ECCV
   Lin T.-Y., 2014, ECCV
   Munkhdalai T., 2017, ICML
   Noh H., 2016, CVPR
   Ouyang W., 2016, CVPR
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Ravi S., 2017, ICLR
   Rebuffi S.-A., 2017, NIPS
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Santoro A., 2016, ICML
   Schmidhuber J, 1997, MACH LEARN, V28, P105, DOI 10.1023/A:1007383707642
   SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P131, DOI 10.1162/neco.1992.4.1.131
   SCHMIDHUBER J, 1987, THESIS
   Schmidhuber J., 1993, IEEE INT C NEUR NETW
   Shen L., 2016, ECCV
   Simonyan Karen, 2015, ICLR
   Singh A, 2016, AAAI
   Sinha A., 2017, ICLR
   Snell J., 2017, NIPS
   Socher R., 2013, NIPS
   Sun C., 2017, ICCV
   Szegedy C., 2015, CVPR
   Thrun S., 2012, LEARNING LEARN
   Triantafillou E., 2017, NIPS
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Van Horn G., 2017, ARXIV170901450
   Vinyals O., 2016, NIPS
   Wang Y., 2017, CVPR
   Wang Y.-X., 2015, CVPR
   Wang Yu- Xiong, 2016, ECCV
   Wang Yunhe, 2016, NIPS
   Xiao JX, 2016, INT J COMPUT VISION, V119, P3, DOI 10.1007/s11263-014-0748-y
   Yosinski J., 2014, NIPS
   Zhong Q., 2016, CVPR WORKSH
   Zhou B., 2017, TPAMI
   Zhu X., 2014, CVPR
   Zhu XX, 2016, INT J COMPUT VISION, V119, P76, DOI 10.1007/s11263-015-0812-2
NR 64
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649407012
DA 2019-06-15
ER

PT S
AU Wang, Y
   Chen, W
   Liu, YT
   Ma, ZM
   Liu, TY
AF Wang, Yue
   Chen, Wei
   Liu, Yuting
   Ma, Zhi-Ming
   Liu, Tie-Yan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Finite Sample Analysis of the GTD Policy Evaluation Algorithms in Markov
   Setting
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB In reinforcement learning (RL), one of the key components is policy evaluation, which aims to estimate the value function (i.e., expected long-term accumulated reward) of a policy. With a good policy evaluation method, the RL algorithms will estimate the value function more accurately and find a better policy. When the state space is large or continuous Gradient-based Temporal Difference(GTD) policy evaluation algorithms with linear function approximation are widely used. Considering that the collection of the evaluation data is both time and reward consuming, a clear understanding of the finite sample performance of the policy evaluation algorithms is very important to reinforcement learning. Under the assumption that data are i.i.d. generated, previous work provided the finite sample analysis of the GTD algorithms with constant step size by converting them into convex-concave saddle point problems. However, it is well-known that, the data are generated from Markov processes rather than i.i.d. in RL problems.. In this paper, in the realistic Markov setting, we derive the finite sample bounds for the general convex-concave saddle point problems, and hence for the GTD algorithms. We have the following discussions based on our bounds. (1) With variants of step size, GTD algorithms converge. (2) The convergence rate is determined by the step size, with the mixing time of the Markov process as the coefficient. The faster the Markov processes mix, the faster the convergence. (3) We explain that the experience replay trick is effective by improving the mixing property of the Markov process. To the best of our knowledge, our analysis is the first to provide finite sample bounds for the GTD algorithms in Markov setting.
C1 [Wang, Yue; Liu, Yuting] Beijing Jiaotong Univ, Sch Sci, Beijing, Peoples R China.
   [Chen, Wei; Liu, Tie-Yan] Microsoft Res, Beijing, Peoples R China.
   [Ma, Zhi-Ming] Chinese Acad Sci, Acad Math & Syst Sci, Beijing, Peoples R China.
   [Wang, Yue] Microsoft Res Asia, Beijing, Peoples R China.
RP Wang, Y (reprint author), Beijing Jiaotong Univ, Sch Sci, Beijing, Peoples R China.
EM 11271012@bjtu.edu.cn; wche@microsoft.com; ytliu@bjtu.edu.cn;
   mazm@amt.ac.cn; Tie-Yan.Liu@microsoft.com
RI Jeong, Yongwook/N-7413-2016
FU A Foundation for the Author of National Excellent Doctoral Dissertation
   of RP China [FANEDD 201312]; National Center for Mathematics and
   Interdisciplinary Sciences of CAS
FX This work was supported by A Foundation for the Author of National
   Excellent Doctoral Dissertation of RP China (FANEDD 201312) and National
   Center for Mathematics and Interdisciplinary Sciences of CAS.
CR Bahdanau Dzmitry, 2016, ARXIV160707086
   Dann C, 2014, J MACH LEARN RES, V15, P809
   Duchi JC, 2012, SIAM J OPTIMIZ, V22, P1549, DOI 10.1137/110836043
   Durrett R, 2016, SPRINGER TEXTS STAT, P95, DOI 10.1007/978-3-319-45614-0_2
   Kober J, 2013, INT J ROBOT RES, V32, P1238, DOI 10.1177/0278364913495721
   Lazaric A, 2012, J MACH LEARN RES, V13, P3041
   Levin D. A., 2009, MARKOV CHAINS MIXING
   LIN L, 1993, THESIS
   Liu B., 2015, UAI, P504
   Maei H., 2011, THESIS
   Maei H. R., 2009, ADV NEURAL INFORM PR, P1204
   Meyn S. P., 2012, MARKOV CHAINS STOCHA
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Nemirovski A, 2004, SIAM J OPTIMIZ, V15, P229, DOI 10.1137/S1052623403425629
   Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277
   Silver  D., 2014, ICML, P387
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Sutton R., 2009, INT C MACH LEARN, P993, DOI DOI 10.1145/1553374.1553501
   Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479
   Sutton R. S., 2009, ADV NEURAL INFORM PR, P1609
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Tagorti Manel, 2015, P 32 INT C MACH LEAR, P1521
   Tsitsiklis JN, 1997, IEEE T AUTOMAT CONTR, V42, P674, DOI 10.1109/9.580874
   Yu H., 2015, P 28 C LEARN THEOR, P1724
NR 24
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405057
DA 2019-06-15
ER

PT S
AU Wang, Y
   Solus, L
   Yang, KD
   Uhler, C
AF Wang, Yuhao
   Solus, Liam
   Yang, Karren Dai
   Uhler, Caroline
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Permutation-based Causal Inference Algorithms with Interventions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID MARKOV EQUIVALENCE CLASSES; NETWORKS
AB Learning directed acyclic graphs using both observational and interventional data is now a fundamentally important problem due to recent technological developments in genomics that generate such single-cell gene expression data at a very large scale. In order to utilize this data for learning gene regulatory networks, efficient and reliable causal inference algorithms are needed that can make use of both observational and interventional data. In this paper, we present two algorithms of this type and prove that both are consistent under the faithfulness assumption. These algorithms are interventional adaptations of the Greedy SP algorithm and are the first algorithms using both observational and interventional data with consistency guarantees. Moreover, these algorithms have the advantage that they are nonparametric, which makes them useful also for analyzing non-Gaussian data. In this paper, we present these two algorithms and their consistency guarantees, and we analyze their performance on simulated data, protein signaling data, and single-cell gene expression data.
C1 [Wang, Yuhao; Uhler, Caroline] MIT, Lab Informat & Decis Syst, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
   [Wang, Yuhao; Yang, Karren Dai; Uhler, Caroline] MIT, Inst Data Syst & Soc, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
   [Solus, Liam] KTH Royal Inst Technol, Dept Math, Stockholm, Sweden.
   [Yang, Karren Dai] MIT, Broad Inst MIT & Harvard, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
RP Wang, Y (reprint author), MIT, Lab Informat & Decis Syst, 77 Massachusetts Ave, Cambridge, MA 02139 USA.; Wang, Y (reprint author), MIT, Inst Data Syst & Soc, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
EM yuhaow@mit.edu; solus@kth.se; karren@mit.edu; cuhler@mit.edu
RI Jeong, Yongwook/N-7413-2016
FU DARPA [W911NF-16-1-0551]; ONR [N00014-17-1-2147]; NSF Mathematical
   Sciences Postdoctoral Research Fellowship [DMS - 1606407]; MIT
   Department of Biological Engineering; NSF [1651995]
FX Yuhao Wang was supported by DARPA (W911NF-16-1-0551) and ONR
   (N00014-17-1-2147). Liam Solus was supported by an NSF Mathematical
   Sciences Postdoctoral Research Fellowship (DMS - 1606407). Karren Yang
   was supported by the MIT Department of Biological Engineering. Caroline
   Uhler was partially supported by DARPA (W911NF-16-1-0551), NSF (1651995)
   and ONR (N00014-17-1-2147). We thank Dr. Sofia Triantafillou from the
   University of Crete for helping us run COmbINE.
CR Andersson SA, 1997, ANN STAT, V25, P505
   Chickering D. M., 2003, Journal of Machine Learning Research, V3, P507, DOI 10.1162/153244303321897717
   Chickering D. M., 1995, P 11 C UNC ART INT
   Dixit A, 2016, CELL, V167, P1853, DOI 10.1016/j.cell.2016.11.038
   Friedman N, 2000, J COMPUT BIOL, V7, P601, DOI 10.1089/106652700750050961
   Fukumizu K., 2008, ADV NEURAL INFORM PR
   Garber M, 2012, MOL CELL, V47, P810, DOI 10.1016/j.molcel.2012.07.030
   Hauser A, 2015, J R STAT SOC B, V77, P291, DOI 10.1111/rssb.12071
   Hauser A, 2012, J MACH LEARN RES, V13, P2409
   Hyttinen A., 2014, CONSTRAINT BASED CAU
   Lauritzen SL, 1996, GRAPHICAL MODELS
   MAATHUIS M, 2015, ARXIV150702608
   Macosko EZ, 2015, CELL, V161, P1202, DOI 10.1016/j.cell.2015.05.002
   Magliacane S., 2016, ADV NEURAL INFORM PR
   Meek C., 1997, THESIS
   Meinshausen N, 2016, P NATL ACAD SCI USA, V113, P7361, DOI 10.1073/pnas.1510493113
   Pearl J, 1988, PROBABILISTIC REASON
   Pearl J., 2000, CAUSALITY MODELS REA
   Rau A, 2013, BMC SYST BIOL, V7, DOI 10.1186/1752-0509-7-111
   Robins JM, 2000, EPIDEMIOLOGY, V11, P550, DOI 10.1097/00001648-200009000-00011
   Sachs K, 2005, SCIENCE, V308, P523, DOI 10.1126/science.1105809
   Solus L., 2017, ARXIV170203530
   Spirtes P., 2001, CAUSATION PREDICTION
   Tillman R. E., 2009, ADV NEURAL INFORM PR
   Triantafillou S, 2015, J MACH LEARN RES, V16, P2147
   Tsamardinos I, 2006, MACH LEARN, V65, P31, DOI 10.1007/s10994-006-6889-7
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405087
DA 2019-06-15
ER

PT S
AU Wang, YB
   Long, MS
   Wang, JM
   Gao, ZF
   Yu, PS
AF Wang, Yunbo
   Long, Mingsheng
   Wang, Jianmin
   Gao, Zhifeng
   Yu, Philip S.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI PredRNN: Recurrent Neural Networks for Predictive Learning using
   Spatiotemporal LSTMs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB The predictive learning of spatiotemporal sequences aims to generate future images by learning from the historical frames, where spatial appearances and temporal variations are two crucial structures. This paper models these structures by presenting a predictive recurrent neural network (PredRNN). This architecture is enlightened by the idea that spatiotemporal predictive learning should memorize both spatial appearances and temporal variations in a unified memory pool. Concretely, memory states are no longer constrained inside each LSTM unit. Instead, they are allowed to zigzag in two directions: across stacked RNN layers vertically and through all RNN states horizontally. The core of this network is a new Spatiotemporal LSTM (ST-LSTM) unit that extracts and memorizes spatial and temporal representations simultaneously. PredRNN achieves the state-of-the-art prediction performance on three video prediction datasets and is a more general framework, that can be easily extended to other predictive learning tasks by integrating with other architectures.
C1 [Wang, Yunbo; Long, Mingsheng; Wang, Jianmin; Gao, Zhifeng; Yu, Philip S.] Tsinghua Univ, Sch Software, Beijing, Peoples R China.
RP Long, MS (reprint author), Tsinghua Univ, Sch Software, Beijing, Peoples R China.
EM wangyb15@mails.tsinghua.edu.cn; mingsheng@tsinghua.edu.cn;
   jimwang@tsinghua.edu.cn; gzf16@mails.tsinghua.edu.cn; psyu@uic.edu
FU National Key R&D Program of China [2016YFB1000701]; National Natural
   Science Foundation of China [61772299, 61325008, 61502265, 61672313];
   TNList Fund
FX This work was supported by the National Key R&D Program of China
   (2016YFB1000701), National Natural Science Foundation of China
   (61772299, 61325008, 61502265, 61672313) and TNList Fund.
CR Abadi M., 2016, ARXIV160304467
   Cho K., 2014, ARXIV14091259
   De Brabandere  B., 2016, NIPS
   Denton E. L., 2015, ADV NEURAL INFORM PR, P1486
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Finn Chelsea, 2016, NIPS
   Goodfellow I. J., 2014, ADV NEURAL INFORM PR, V3, P2672
   Graves A., 2014, ICML, P1764, DOI DOI 10.1145/1143844.1143891
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Kalchbrenner  N., 2017, ICML
   Kingma D. P., 2015, ICLR
   Lotter W., 2017, INT C LEARN REPR ICL
   Mathieu M., 2016, ICLR
   NG JYH, 2015, CVPR, P4694
   Oh J., 2015, ADV NEURAL INFORM PR, P2863
   Patraucean  V., 2016, ICLR WORKSH
   Ranzato M, 2014, ARXIV14126604
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Shi X., 2015, ADV NEURAL INFORM PR, V9199, P802
   Simonyan K., 2014, ADV NEURAL INFORM PR, P568, DOI DOI 10.1109/ICCVW.2017.368
   Srivastava N, 2015, ICML
   Sutskever I, 2011, P 28 INT C MACH LEAR, P1017
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   van den Oord A., 2016, ADV NEURAL INFORM PR, P4790
   Villegas R., 2017, INT C LEARN REPR ICL
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
NR 28
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400084
DA 2019-06-15
ER

PT S
AU Wang, ZY
   Merel, J
   Reed, S
   Wayne, G
   de Freitas, N
   Heess, N
AF Wang, Ziyu
   Merel, Josh
   Reed, Scott
   Wayne, Greg
   de Freitas, Nando
   Heess, Nicolas
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Robust Imitation of Diverse Behaviors
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Deep generative models have recently shown great promise in imitation learning for motor control. Given enough data, even supervised approaches can do one-shot imitation learning; however, they are vulnerable to cascading failures when the agent trajectory diverges from the demonstrations. Compared to purely supervised methods, Generative Adversarial Imitation Learning (GAIL) can learn more robust controllers from fewer demonstrations, but is inherently mode-seeking and more difficult to train. In this paper, we show how to combine the favourable aspects of these two approaches. The base of our model is a new type of variational autoencoder on demonstration trajectories that learns semantic policy embeddings. We show that these embeddings can be learned on a 9 DoF Jaco robot arm in reaching tasks, and then smoothly interpolated with a resulting smooth interpolation of reaching behavior. Leveraging these policy representations, we develop a new version of GAIL that (1) is much more robust than the purely-supervised controller, especially with few demonstrations, and (2) avoids mode collapse, capturing many diverse behaviors when GAIL on its own does not. We demonstrate our approach on learning diverse gaits from demonstration on a 2D biped and a 62 DoF 3D humanoid in the MuJoCo physics environment.
C1 [Wang, Ziyu; Merel, Josh; Reed, Scott; Wayne, Greg; de Freitas, Nando; Heess, Nicolas] DeepMind, London, England.
RP Wang, ZY (reprint author), DeepMind, London, England.
EM ziyu@google.com; jsmerel@google.com; reedscot@google.com;
   gregwayne@google.com; nandodefreitas@google.com; heess@google.com
RI Jeong, Yongwook/N-7413-2016
CR Abbeel P., 2017, ARXIV170301703
   Argall BD, 2009, ROBOT AUTON SYST, V57, P469, DOI 10.1016/j.robot.2008.10.024
   Arjovsky M., 2017, ARXIV170107875
   Baram N., 2016, ARXIV161202179
   Berthelot D., 2017, ARXIV170310717
   Billard A., 2008, SPRINGER HDB ROBOTIC, P1371, DOI DOI 10.1007/978-3-540-30301-5_60
   Duan Yan, 2017, ARXIV170307326
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Goodfellow Ian, 2016, ARXIV170100160
   Hausman K., 2017, ARXIV170510479
   Hjelm RD, 2017, ARXIV170208431
   Ho J., 2016, ADV NEURAL INFORM PR, P4565
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hussein A, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3054912
   Kalchbrenner N., 2016, NIPS
   Kingma D.P., 2013, ARXIV13126114
   Kuefler Alex, 2017, ARXIV170106699
   Li Yunzhu, 2017, ARXIV170308840
   Lillicrap T P, 2015, ARXIV150902971
   Mao Xudong, 2016, ARXIV161104076
   Merel J., 2017, ARXIV170702201
   Mirza M., 2014, ARXIV14111784, DOI DOI 10.1029/2009WR008312
   Muico U., 2009, SIGGRAPH
   Peng XB, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275014
   Pomerleau DA, 1991, NEURAL COMPUT, V3, P88, DOI 10.1162/neco.1991.3.1.88
   Qi G.-J., 2017, ARXIV170106264
   Rezende D. J., 2014, ICML
   Rosca M, 2017, ARXIV170604987
   Ross S., 2010, AISTATS
   Ross Stephane, 2011, AISTATS
   Rusu AA, 2015, ARXIV151106295
   Schmidhuber J., 2005, ARTIFICIAL NEURAL NE, P753
   Schulman John, 2015, ICML
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Sharon D, 2005, IEEE INT CONF ROBOT, P2387
   Silver D., 2014, ICML
   Sok K. W., 2007, SIMULATING BIPED BEH
   Theis L., 2015, ARXIV151101844
   Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109
   Van Den Oord A., 2016, ARXIV160903499
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang Ruohan, 2017, ARXIV170403817
   Yin K., 2007, SIGGRAPH
NR 43
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405039
DA 2019-06-15
ER

PT S
AU Watters, N
   Tacchetti, A
   Weber, T
   Pascanu, R
   Battaglia, P
   Zoran, D
AF Watters, Nicholas
   Tacchetti, Andrea
   Weber, Theophane
   Pascanu, Razvan
   Battaglia, Peter
   Zoran, Daniel
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Visual Interaction Networks: Learning a Physics Simulator from Video
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB From just a glance, humans can make rich predictions about the future of a wide range of physical systems. On the other hand, modern approaches from engineering, robotics, and graphics are often restricted to narrow domains or require information about the underlying state. We introduce the Visual Interaction Network, a general-purpose model for learning the dynamics of a physical system from raw visual observations. Our model consists of a perceptual front-end based on convolutional neural networks and a dynamics predictor based on interaction networks. Through joint training, the perceptual front-end learns to parse a dynamic visual scene into a set of factored latent object representations. The dynamics predictor learns to roll these states forward in time by computing their interactions, producing a predicted physical trajectory of arbitrary length. We found that from just six input video frames the Visual Interaction Network can generate accurate future trajectories of hundreds of time steps on a wide range of physical systems. Our model can also be applied to scenes with invisible objects, inferring their future states from their effects on the visible objects, and can implicitly infer the unknown mass of objects. This work opens new opportunities for model-based decision-making and planning from raw sensory observations in complex physical environments.
C1 [Watters, Nicholas; Tacchetti, Andrea; Weber, Theophane; Pascanu, Razvan; Battaglia, Peter; Zoran, Daniel] DeepMind, London, England.
RP Watters, N (reprint author), DeepMind, London, England.
EM nwatters@google.com; atacchet@google.com; theophane@google.com;
   razp@google.com; peterbattaglia@google.com; danielzoran@google.com
RI Jeong, Yongwook/N-7413-2016
CR Agrawal Pulkit, 2016, ARXIV160607419
   Battaglia P., 2016, ADV NEURAL INFORM PR, P4502
   Battaglia PW, 2013, P NATL ACAD SCI USA, V110, P18327, DOI 10.1073/pnas.1306572110
   Bhat KS, 2002, LECT NOTES COMPUT SC, V2350, P551
   Bhattacharyya Apratim, 2016, ARXIV161108841
   Brubaker MA, 2009, IEEE I CONF COMP VIS, P2389, DOI 10.1109/ICCV.2009.5459407
   Chang Michael B, 2016, ARXIV161200341
   Ehrhardt S., 2017, ARXIV170300247
   Fragkiadaki  K., 2015, ARXIV151107404
   Gerstenberg Tobias, 2012, P 34 CIT
   Gerstenberg Tobias, 2014, COGSCI
   Grzeszczuk R., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P9
   Hamrick JB, 2016, COGNITION, V157, P61, DOI 10.1016/j.cognition.2016.08.012
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Ladicky L, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818129
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lerer  A., 2016, ARXIV160301312
   Li  W., 2016, ARXIV160400066
   Mottaghi R, 2016, PROC CVPR IEEE, P3521, DOI 10.1109/CVPR.2016.383
   Mottaghi R, 2016, LECT NOTES COMPUT SC, V9908, P269, DOI 10.1007/978-3-319-46493-0_17
   Spelke ES, 2007, DEVELOPMENTAL SCI, V10, P89, DOI 10.1111/j.1467-7687.2007.00569.x
   Stewart Russell, 2016, ARXIV160905566
   Winograd Terry, 1971, PROCEDURES REPRESENT
   Winston Patrick H, 1970, LEARNING STRUCTURAL
   Wu J., 2015, ADV NEURAL INFORM PR, P127, DOI DOI 10.1007/978-3-319-26532-2_15
   Wu Jiajun, 2016, PSYCHOL SCI, V13, P89
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404059
DA 2019-06-15
ER

PT S
AU Wei, CY
   Hong, YT
   Lu, CJ
AF Wei, Chen-Yu
   Hong, Yi-Te
   Lu, Chi-Jen
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Online Reinforcement Learning in Stochastic Games
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We study online reinforcement learning in average-reward stochastic games (SGs). An SG models a two-player zero-sum game in a Markov environment, where state transitions and one-step payoffs are determined simultaneously by a learner and an adversary. We propose the UCSG algorithm that achieves a sublinear regret compared to the game value when competing with an arbitrary opponent. This result improves previous ones under the same setting. The regret bound has a dependency on the diameter, which is an intrinsic value related to the mixing property of SGs. If we let the opponent play an optimistic best response to the learner, UCSG finds an epsilon-maximin stationary policy with a sample complexity of (O) over tilde (poly(1/epsilon)), where epsilon is the gap to the best policy.
C1 [Wei, Chen-Yu; Hong, Yi-Te; Lu, Chi-Jen] Acad Sinica, Inst Informat Sci, Taipei, Taiwan.
RP Wei, CY (reprint author), Acad Sinica, Inst Informat Sci, Taipei, Taiwan.
EM bahh723@iis.sinica.edu.tw; ted0504@iis.sinica.edu.tw;
   cjlu@iis.sinica.edu.tw
RI Jeong, Yongwook/N-7413-2016
CR Abbasi Yasin, 2013, ADV NEURAL INFORM PR
   Auer Peter, 2007, ADV NEURAL INFORM PR
   Bartlett Peter L, 2009, P C UNC ART INT
   Bowling Michael, 2001, INT JOINT C ART INT
   Brafman R. I., 2002, J MACHINE LEARNING R
   Bubeck Sebastien, 2012, C LEARN THEOR
   Cho Grace E, 2000, LINEAR ALGEBRA ITS A
   Conitzer V., 2007, MACHINE LEARNING
   Dann Christoph, 2015, ADV NEURAL INFORM PR
   Dick Travis, 2014, P INT C MACH LEARN
   Even-Dar Eyal, 2009, MATH OPERATIONS RES
   Federgruen Awi, 1978, ADV APPL PROBABILITY
   Garivier A., 2016, C LEARN THEOR, P1028
   Hordijk Arie, 1974, MC TRACTS
   Hunter Jeffrey J, 2005, LINEAR ALGEBRA ITS A
   Hunter Jeffrey J, 1982, LINEAR ALGEBRA ITS A
   Iyengar GN, 2005, MATH OPER RES, V30, P257, DOI 10.1287/moor.1040.0129
   Jaakkola Tommi, 1994, NEURAL COMPUTATION
   Jaksch T., 2010, J MACHINE LEARNING R
   Kakade S. M., 2003, THESIS
   Lagoudakis Michail G, 2002, P C UNC ART INT
   Lattimore T., 2012, INT C ALG LEARN THEO
   Lim SH, 2016, MATH OPER RES, V41, P1325, DOI 10.1287/moor.2016.0779
   Littman M., 1994, P INT C MACH LEARN
   Maurer A, 2009, C LEARN THEOR
   Mertens J- F, 1981, INT J GAME THEORY
   Neu Gergely, 2010, ADV NEURAL INFORM PR
   Neu Gergely, 2012, AISTATS
   Nilim A, 2005, OPER RES, V53, P780, DOI 10.1287/opre.1050.0216
   Perolat Julien, 2015, P INT C MACH LEARN
   Sambariya DK, 2015, 2015 2nd International Conference on Recent Advances in Engineering & Computational Sciences (RAECS)
   Shapley Lloyd S, 1953, P NATL ACAD SCI
   Szepesvari Csaba, 1996, P INT C MACH LEARN
   Van der Wal J, 1980, INT J GAME THEORY
   Yu Jia Yuan, 2009, P C DEC CONTR
NR 35
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405007
DA 2019-06-15
ER

PT S
AU Wei, YT
   Yang, F
   Wainwright, MJ
AF Wei, Yuting
   Yang, Fanny
   Wainwright, Martin J.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Early stopping for kernel boosting algorithms: A general analysis with
   localized complexities
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID GRADIENT; REGRESSION; PREDICTION
AB Early stopping of iterative algorithms is a widely-used form of regularization in statistics, commonly used in conjunction with boosting and related gradient type algorithms. Although consistency results have been established in some settings, such estimators are less well-understood than their analogues based on penalized regularization. In this paper, for a relatively broad class of loss functions and boosting algorithms (including L-2 -boost, LogitBoost and AdaBoost, among others), we exhibit a direct connection between the performance of a stopped iterate and the localized Gaussian complexity of the associated function class. This connection allows us to show that local fixed point analysis of Gaussian or Rademacher complexities, now standard in the analysis of penalized estimators, can be used to derive optimal stopping rules. We derive such stopping rules in detail for various kernel classes, and illustrate the correspondence of our theory with practice for Sobolev kernel classes.
C1 [Wei, Yuting; Wainwright, Martin J.] Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA.
   [Yang, Fanny; Wainwright, Martin J.] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
RP Yang, F (reprint author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
EM ytwei@berkeley.edu; fanny-yang@berkeley.edu; wainwrig@berkeley.edu
RI Jeong, Yongwook/N-7413-2016
FU DOD Advanced Research Projects Agency [W911NF-16-1-0552]; National
   Science Foundation [NSF-DMS-1612948]; Office of Naval Research
   [DOD-ONR-N00014]
FX This work was partially supported by DOD Advanced Research Projects
   Agency W911NF-16-1-0552, National Science Foundation grant
   NSF-DMS-1612948, and Office of Naval Research Grant DOD-ONR-N00014.
CR ANDERSSEN RS, 1981, J AUST MATH SOC B, V22, P488, DOI 10.1017/S0334270000002824
   Bartlett PL, 2007, J MACH LEARN RES, V8, P2347
   Bartlett PL, 2005, ANN STAT, V33, P1497, DOI 10.1214/009053605000000282
   Berlinet A, 2004, REPRODUCING KERNEL H
   Breiman L, 1999, NEURAL COMPUT, V11, P1493, DOI 10.1162/089976699300016106
   Breiman L, 1998, ANN STAT, V26, P801
   Buhlmann P, 2007, STAT SCI, V22, P477, DOI 10.1214/07-STS242
   Buhlmann P, 2003, J AM STAT ASSOC, V98, P324, DOI 10.1198/016214503000125
   Camoriano Raffaello, 2016, P 19 INT C ART INT S, P1403
   Caponetto A., 2006, 265AI CBCL MIT
   Caponneto A., 2006, 264AI CBCL MIT
   Caruana R, 2001, ADV NEUR IN, V13, P402
   De Vito E, 2010, FOUND COMPUT MATH, V10, P455, DOI 10.1007/s10208-010-9064-2
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223
   Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451
   Gu C., 2002, SPR S STAT
   GyOrfi L., 2002, SPRINGER SERIES STAT
   Jiang WX, 2004, ANN STAT, V32, P13
   KIMELDORF G, 1971, J MATH ANAL APPL, V33, P82, DOI 10.1016/0022-247X(71)90184-3
   Koltchinskii V, 2006, ANN STAT, V34, P2593, DOI 10.1214/009053606000001019
   Ledoux M., 1991, PROBABILITY BANACH S
   Ledoux M., 2001, MATH SURVEYS MONOGRA
   Mason L, 2000, ADV NEUR IN, V12, P512
   Mendelson S., 2002, Computational Learning Theory. 15th Annual Conference on Computational Learning Theory, COLT 2002. Proceedings (Lecture Notes in Artificial Intelligence Vol.2375), P29
   Prechelt L, 1998, LECT NOTES COMPUT SC, V1524, P55
   Raskutti G, 2014, J MACH LEARN RES, V15, P335
   Rosasco Lorenzo, 2015, ADV NEURAL INFORM PR, P1630
   Schapire RE, 2003, LECT NOTES STAT, V171, P149
   SCHAPIRE RE, 1990, MACH LEARN, V5, P197, DOI 10.1007/BF00116037
   Scholkopf B., 2002, LEARNING KERNELS
   STRAND ON, 1974, SIAM J NUMER ANAL, V11, P798, DOI 10.1137/0711066
   VAN DE GEER S. A., 2000, EMPIRICAL PROCESSES
   van der Vaart A., 1996, WEAK CONVERGENCE EMP
   Wahba G., 1990, CBMS NSF REGIONAL C
   Wahba G., 1987, INVERSE ILL POSED PR, P37
   Wainwright Martin J., 2017, HIGH DIMENSIONAL STA
   Yang Y., 2017, ANN STAT
   Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2
   Zhang T, 2005, ANN STAT, V33, P1538, DOI 10.1214/009053605000000255
NR 40
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406014
DA 2019-06-15
ER

PT S
AU Welleck, S
   Mao, JL
   Cho, K
   Zhang, Z
AF Welleck, Sean
   Mao, Jialin
   Cho, Kyunghyun
   Zhang, Zheng
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Saliency-based Sequential Image Attention with Multiset Prediction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID VISUAL-ATTENTION
AB Humans process visual scenes selectively and sequentially using attention. Central to models of human visual attention is the saliency map. We propose a hierarchical visual architecture that operates on a saliency map and uses a novel attention mechanism to sequentially focus on salient regions and take additional glimpses within those regions. The architecture is motivated by human visual attention, and is used for multi-label image classification on a novel multiset task, demonstrating that it achieves high precision and recall while localizing objects with its attention. Unlike conventional multi-label image classification models, the model supports multiset prediction due to a reinforcement-learning based training process that allows for arbitrary label permutation and multiple instances per label.
C1 [Welleck, Sean; Mao, Jialin; Cho, Kyunghyun; Zhang, Zheng] NYU, New York, NY 10003 USA.
RP Welleck, S (reprint author), NYU, New York, NY 10003 USA.
EM wellecks@nyu.edu; jialin.mao@nyu.edu; kyunghyun.cho@nyu.edu; zz@nyu.edu
RI Jeong, Yongwook/N-7413-2016
FU NYU Global Seed Funding <Model-Free Object Tracking with Recurrent
   Neural Networks>; STCSM [17JC1404100/1]; Huawei HIPP Open 2017
FX This work was partly supported by the NYU Global Seed Funding
   <Model-Free Object Tracking with Recurrent Neural Networks>, STCSM
   17JC1404100/1, and Huawei HIPP Open 2017.
CR Aslam M, 2017, IEEE PAC RIM CONF CO
   Awh E, 2000, J EXP PSYCHOL HUMAN, V26, P834, DOI 10.1037/0096-1523.26.2.834
   Ba J., 2014, MULTIPLE OBJECT RECO, V1412, P7755
   Bellver M., 2016, ARXIV161103718
   Caicedo JC, 2015, IEEE I CONF COMP VIS, P2488, DOI 10.1109/ICCV.2015.286
   Carrasco M, 2011, VISION RES, V51, P1484, DOI 10.1016/j.visres.2011.04.012
   Carrasco M, 2002, J VISION, V2, DOI 10.1167/2.6.4
   Cavanagh P, 2005, TRENDS COGN SCI, V9, P349, DOI 10.1016/j.tics.2005.05.009
   Chen ZQ, 2016, IEEE ICC
   Cheung Brian, 2016, ARXIV161109430
   Cornia M., 2016, ARXIV161109571
   Cornia Marcella, 2017, ARXIV170608474
   Dayan P., 1993, ADV NEURAL INFORMATI, V5, P271
   Fecteau JH, 2006, TRENDS COGN SCI, V10, P382, DOI 10.1016/j.tics.2006.06.011
   Fischer J, 2009, CURR BIOL, V19, P1356, DOI 10.1016/j.cub.2009.06.059
   Graves A, 2013, ARXIV13080850
   Hendricks Lisa Anne, 2016, EUR C ECCV
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jaderberg M., 2015, ARXIV150602025
   Karayev S., 2012, ADV NEURAL INFORM PR, V25, P890
   Klein RM, 2000, TRENDS COGN SCI, V4, P138, DOI 10.1016/S1364-6613(00)01452-2
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Kowler E, 2011, VISION RES, V51, P1457, DOI 10.1016/j.visres.2010.12.014
   Kulkarni T. D., 2016, ADV NEURAL INFORM PR, P3675
   Lamme VAF, 2000, TRENDS NEUROSCI, V23, P571, DOI 10.1016/S0166-2236(00)01657-X
   Larochelle H., 2010, P ADV NEUR INF PROC, P1243
   Lei BA Jimmy, 2015, NIPS, P2593
   Li Y., 2017, IEEE C COMP VIS PATT
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Liu Xiao, 2016, ARXIV160506217
   Mathe S., 2013, ADV NEURAL INFORM PR, P1923
   Mathe S, 2016, PROC CVPR IEEE, P2894, DOI 10.1109/CVPR.2016.316
   McMains SA, 2004, NEURON, V42, P677, DOI 10.1016/S0896-6273(04)00263-6
   Mnih V., 2014, ADV NEURAL INFORM PR, V3, P2204
   Netzer Yuval, READING DIGITS NATUR
   Qi W, 2016, IEEE IC COMP COM NET
   Schulman  J., 2015, ADV NEURAL INFORM PR, P3528
   Semenov S., 2016, P IEEE C ANT MEAS AP, P1
   Seo P.H., 2016, ARXIV160602393
   Simonyan K., 2013, 13126034 ARXIV
   Tavakoli Hamed R., 2017, ARXIV170407402
   Veale R, 2017, PHILOS T R SOC B, V372, DOI 10.1098/rstb.2016.0113
   Vezhnevets Alexander Sasha, 2017, ARXIV170301161
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Xu K, 2015, ARXIV150203044
   Yu F., 2015, ARXIV151107122
   Zagoruyko S., 2016, ARXIV161203928
NR 47
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405025
DA 2019-06-15
ER

PT S
AU Wen, Z
   Kveton, B
   Valko, M
   Vaswani, S
AF Wen, Zheng
   Kveton, Branislav
   Valko, Michal
   Vaswani, Sharan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Online Influence Maximization under Independent Cascade Model with
   Semi-Bandit Feedback
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We study the online influence maximization problem in social networks under the independent cascade model. Specifically, we aim to learn the set of "best influencers" in a social network online while repeatedly interacting with it. We address the challenges of (i) combinatorial action space, since the number of feasible influencer sets grows exponentially with the maximum number of influencers, and (ii) limited feedback, since only the influenced portion of the network is observed. Under a stochastic semi-bandit feedback, we propose and analyze IMLinUCB, a computationally efficient UCB-based algorithm. Our bounds on the cumulative regret are polynomial in all quantities of interest, achieve near-optimal dependence on the number of interactions and reflect the topology of the network and the activation probabilities of its edges, thereby giving insights on the problem complexity. To the best of our knowledge, these are the first such results. Our experiments show that in several representative graph topologies, the regret of IMLinUCB scales as suggested by our upper bounds. IMLinUCB permits linear generalization and thus is both statistically and computationally suitable for large-scale problems. Our experiments also show that IMLinUCB with linear generalization can lead to low regret in real-world online influence maximization.
C1 [Wen, Zheng; Kveton, Branislav] Adobe Res, San Jose, CA 95110 USA.
   [Valko, Michal] INRIA Lille Nord Europe, SequeL Team, Lille, France.
   [Vaswani, Sharan] Univ British Columbia, Vancouver, BC, Canada.
RP Wen, Z (reprint author), Adobe Res, San Jose, CA 95110 USA.
EM zwen@adobe.com; kveton@adobe.com; michal.valko@inria.fr;
   sharanv@cs.ubc.ca
RI Jeong, Yongwook/N-7413-2016
FU French Ministry of Higher Education and Research; Nord-Pas-de-Calais
   Regional Council; French National Research Agency project ExTra-Learn
   [ANR-14-CE24-0010-01]; French National Research Agency project BoB
   [ANR-16-CE23-0003]
FX The research presented was supported by French Ministry of Higher
   Education and Research, Nord-Pas-de-Calais Regional Council and French
   National Research Agency projects ExTra-Learn (n.ANR-14-CE24-0010-01)
   and BoB (n.ANR-16-CE23-0003). We would also like to thank Dr. Wei Chen
   and Mr. Qinshi Wang for pointing out a mistake in an earlier version of
   this paper.
CR Abbasi-Yadkori Yasin, 2011, NEURAL INFORM PROCES
   Bao Yixin, 2016, INT S QUAL SERV APR
   Barbieri N, 2013, KNOWL INF SYST, V37, P555, DOI 10.1007/s10115-013-0646-6
   Bnaya Z., 2013, HUMAN, V2, P84
   Carpentier Alexandra, 2016, INT C ART INT STAT
   Chen W, 2016, J MACH LEARN RES, V17
   Chen Wei, 2010, KNOWLEDGE DISCOVERY
   Chen Wei, 2013, INT C MACH LEARN
   Dani Varsha, 2008, C LEARN THEOR
   Easley D., 2010, NETWORKS CROWDS MARK
   Fang Meng, 2014, INT C KNOWL DISC DAT
   Farajtabar Mehrdad, 2016, NEURAL INFORM PROCES
   Goyal A., 2010, P 3 ACM INT C WEB SE, P241, DOI DOI 10.1145/1718487.1718518
   Grover Aditya, 2016, KNOWLEDGE DISCOVERY
   Kempe D., 2003, P 9 ACM SIGKDD INT C, P137, DOI DOI 10.1145/956750.956769
   Kveton B., 2015, P 18 INT C ART INT S
   Kveton B., 2015, P 32 INT C MACH LEAR
   Kveton B, 2015, ADV NEURAL INFORM PR, P1450
   Lagree Paul, 2017, INT C DAT MIN
   Lei Siyu, 2015, KNOWLEDGE DISCOVERY
   Li Yanhua, 2013, ACM INT C WEB SEARCH
   Netrapalli Praneeth, 2012, Performance Evaluation Review, V40, P211, DOI 10.1145/2318857.2254783
   Rodriguez M Gomez, 2012, INT C MACH LEARN
   Saito K, 2008, LECT NOTES ARTIF INT, V5179, P67, DOI 10.1007/978-3-540-85567-5_9
   Singla Adish, 2015, INT JOINT C ART INT
   Tang Y., 2014, INFLUENCE MAXIMIZATI
   Valko Michal, 2016, BANDITS GRAPHS STRUC
   Vaswani Sharan, 2015, NIPS WORKSH NETW SOC, V2015
   Vaswani Sharan, 2016, TECHNICAL REPORT
   Vaswani Sharan, 2017, INT C MACH LEARN
   Wen Zheng, 2015, INT C MACH LEARN
   Zhang Y, 2017, NEURAL PLAST, DOI 10.1155/2017/9382797
   Zong Shi, 2016, UNCERTAINTY ARTIFICI
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403009
DA 2019-06-15
ER

PT S
AU Wilson, AC
   Roelofs, R
   Stern, M
   Srebro, N
   Recht, B
AF Wilson, Ashia C.
   Roelofs, Rebecca
   Stern, Mitchell
   Srebro, Nathan
   Recht, Benjamin
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI The Marginal Value of Adaptive Gradient Methods in Machine Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We show that for simple overparameterized problems, adaptive methods often find drastically different solutions than gradient descent (GD) or stochastic gradient descent (SGD). We construct an illustrative binary classification problem where the data is linearly separable, GD and SGD achieve zero test error, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to half. We additionally study the empirical generalization capability of adaptive methods on several state-of-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.
C1 [Wilson, Ashia C.; Roelofs, Rebecca; Stern, Mitchell; Recht, Benjamin] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Srebro, Nathan] Toyota Technol Inst, Chicago, IL USA.
RP Wilson, AC (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM ashia@berkeley.edu; roelofs@berkeley.edu; mitchell@berkeley.edu;
   nati@ttic.edu; brecht@berkeley.edu
RI Jeong, Yongwook/N-7413-2016
FU DOE [AC02-05CH11231]; NSF Graduate Research Fellowships; NSF
   [IIS-13-02662, IIS-15-46500, CCF-1359814]; Inter ICRI-RI award; Google
   Faculty Award; ONR [N00014-14-1-0024, N00014-17-1-2191]; DARPA
   Fundamental Limits of Learning (Fun LoL) Program; Sloan Research
   Fellowship
FX The authors would like to thank Pieter Abbeel, Moritz Hardt, Tomer
   Koren, Sergey Levine, Henry Milner, Yoram Singer, and Shivaram
   Venkataraman for many helpful comments and suggestions. RR is generously
   supported by DOE award AC02-05CH11231. MS and AW are supported by NSF
   Graduate Research Fellowships. NS is partially supported by
   NSF-IIS-13-02662 and NSF-IIS-15-46500, an Inter ICRI-RI award and a
   Google Faculty Award. BR is generously supported by NSF award
   CCF-1359814, ONR awards N00014-14-1-0024 and N00014-17-1-2191, the DARPA
   Fundamental Limits of Learning (Fun LoL) Program, a Sloan Research
   Fellowship, and a Google Faculty Award.
CR Choe Do Kook, 2016, P C EMP METH NAT LAN, P2331
   Cross James, 2016, P 2016 C EMP METH NA, P1
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1, DOI 10.1162/neco.1997.9.1.1
   Isola  P., 2016, ARXIV161107004
   Karparthy Andrej, PEEK TRENDS MACHINE
   Keskar N. S., 2017, INT C LEARN REPR ICL
   Kingma  D.P., 2015, INT C LEARN REPR ICL
   Lillicrap T. P., 2016, INT C LEARN REPR ICL
   Ma Siyuan, 2017, ARXIV170310622
   Marcus M.P., 1993, COMPUTATIONAL LINGUI, V19, P313, DOI DOI 10.1080/07494460903404410
   McMahan H. B., 2010, P 23 ANN C LEARN THE
   Mnih V., 2016, INT C MACH LEARN ICM
   Neyshabur Behnam, 2015, INT C LEARN REPR ICL
   Neyshabur Behnam, 2015, NEURAL INFORM PROCES
   Raginsky  M., 2017, ARXIV170203849
   Recht Benjamin, 2016, P INT C MACH LEARN I
   Reed Scott, 2016, P INT C MACH LEARN I
   Sutskever Ilya, 2013, P INT C MACH LEARN I
   Szegedy C, 2016, P IEEE C COMP VIS PA
   TIELEMAN T, 2012, COURSERA NEURAL NETW
   Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2
NR 22
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404022
DA 2019-06-15
ER

PT S
AU Wu, AQ
   Roy, NA
   Keeley, S
   Pillow, JW
AF Wu, Anqi
   Roy, Nicholas A.
   Keeley, Stephen
   Pillow, Jonathan W.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Gaussian process based nonlinear latent structure discovery in
   multivariate spike train data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB A large body of recent work focuses on methods for extracting low-dimensional latent structure from multi-neuron spike train data. Most such methods employ either linear latent dynamics or linear mappings from latent space to log spike rates. Here we propose a doubly nonlinear latent variable model that can identify low-dimensional structure underlying apparently high-dimensional spike train data. We introduce the Poisson Gaussian-Process Latent Variable Model (P-GPLVM), which consists of Poisson spiking observations and two underlying Gaussian processes-one governing a temporal latent variable and another governing a set of nonlinear tuning curves. The use of nonlinear tuning curves enables discovery of low-dimensional latent structure even when spike responses exhibit high linear dimensionality (e.g., as found in hippocampal place cell codes). To learn the model from data, we introduce the decoupled Laplace approximation, a fast approximate inference method that allows us to efficiently optimize the latent path while marginalizing over tuning curves. We show that this method outperforms previous Laplace-approximation-based inference methods in both the speed of convergence and accuracy. We apply the model to spike trains recorded from hippocampal place cells and show that it compares favorably to a variety of previous methods for latent structure discovery, including variational auto-encoder (VAE) based methods that parametrize the nonlinear mapping from latent space to spike rates with a deep neural network.
C1 [Wu, Anqi; Roy, Nicholas A.; Keeley, Stephen; Pillow, Jonathan W.] Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA.
RP Wu, AQ (reprint author), Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA.
RI Jeong, Yongwook/N-7413-2016
CR Archer E., 2015, ARXIV151107367
   Archer E. W., 2014, ADV NEURAL INFORM PR, V27, P343
   Buesing L, 2012, ADV NEURAL INFORM PR, V25, P1682
   Cunningham JP, 2014, NAT NEUROSCI, V17, P1500, DOI 10.1038/nn.3776
   Damianou AC, 2014, ARXIV14092287
   Gao Y., 2016, ADV NEURAL INFORM PR, V29, P163
   Hafting T, 2005, NATURE, V436, P801, DOI 10.1038/nature03721
   Kao JC, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms8759
   Karlsson M, 2005, SIMULTANEOUS EXTRACE, DOI [10.6080/K0NK3BZJ, DOI 10.6080/K0NK3BZJ]
   Lawrence ND, 2004, ADV NEUR IN, V16, P329
   Linderman SW, 2016, J NEUROSCI METH, V263, P36, DOI 10.1016/j.jneumeth.2016.01.022
   Macke J. H., 2011, ADV NEURAL INF PROCE, V24, P1350
   Macke JH, 2015, ADV STATE SPACE METH, P137
   Nam Hooram, 2015, THESIS, P8
   Paninski L, 2010, J COMPUT NEUROSCI, V29, P107, DOI 10.1007/s10827-009-0179-x
   Pfau D., 2013, ADV NEURAL INF PROCE, V26, P2391
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Sussillo David, 2016, ARXIV160806315
   Yu B. M., 2009, ADV NEURAL INFORM PR, P1881, DOI [10.1152/jn.90941.2008, DOI 10.1152/JN.90941]
   Zhao Yuan, 2016, ARXIV160403053
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403055
DA 2019-06-15
ER

PT S
AU Wu, G
   Say, B
   Sanner, S
AF Wu, Ga
   Say, Buser
   Sanner, Scott
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Scalable Planning with Tensorflow for Hybrid Nonlinear Domains
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Given recent deep learning results that demonstrate the ability to effectively optimize high-dimensional non-convex functions with gradient descent optimization on GPUs, we ask in this paper whether symbolic gradient optimization tools such as Tensorflow can be effective for planning in hybrid (mixed discrete and continuous) nonlinear domains with high dimensional state and action spaces? To this end, we demonstrate that hybrid planning with Tensorflow and RMSProp gradient descent is competitive with mixed integer linear program (MILP) based optimization on piecewise linear planning domains (where we can compute optimal solutions) and substantially outperforms state-of-the-art interior point methods for nonlinear planning domains. Furthermore, we remark that Tensorflow is highly scalable, converging to a strong plan on a large-scale concurrent domain with a total of 576,000 continuous action parameters distributed over a horizon of 96 time steps and 100 parallel instances in only 4 minutes. We provide a number of insights that clarify such strong performance including observations that despite long horizons, RMSProp avoids both the vanishing and exploding gradient problems. Together these results suggest a new frontier for highly scalable planning in nonlinear hybrid domains by leveraging GPUs and the power of recent advances in gradient descent with highly optimized toolkits like Tensorflow.
C1 [Wu, Ga; Say, Buser; Sanner, Scott] Univ Toronto, Dept Mech & Ind Engn, Toronto, ON, Canada.
RP Wu, G (reprint author), Univ Toronto, Dept Mech & Ind Engn, Toronto, ON, Canada.
EM wuga@mie.utoronto.ca; bsay@mie.utoronto.ca; ssanner@mie.utoronto.ca
RI Jeong, Yongwook/N-7413-2016
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Agarwal Y., 2010, P 2 ACM WORKSH EMB S, P1, DOI DOI 10.1145/1878431.1878433
   Balduzzi David, 2016, ARXIV161102345
   Bryce Daniel, 2015, P 29 AAAI C ART INT, P3247
   Cashmore M., 2016, P 26 INT C AUT PLANN, P79
   Coles A, 2013, J ARTIF INTELL RES, V46, P343, DOI 10.1613/jair.3788
   Coulom R, 2007, LECT NOTES COMPUT SC, V4630, P72
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Erickson V, 2009, P 1 ACM WORKSH EMB S, P19, DOI DOI 10.1145/1810279.1810284
   Faulwasser T, 2009, LECT NOTES CONTR INF, V384, P335
   Ivankovic F, 2014, P 24 INT C AUT PLANN, P145
   Keller Thomas, 2013, P 23 INT C AUT PLANN
   Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282
   Linnainmaa Seppo, 1970, THESIS, P6
   Lohr Johannes, 2012, P 22 INT C AUT PLANN
   Mnih V., 2013, NIPS DEEP LEARN WORK
   Piotrowski Wiktor Mateusz, 2016, P 30 AAAI C ART INT, P4254
   Rumelhart D. E., COGNITIVE MODELING, V5, P1
   Say Buser, 2017, P 26 INT JOINT C ART, P750
   SCALA E, 2016, ECAI, V285, P655, DOI DOI 10.3233/978-1-61499-672-9-655
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Sutton R., 1998, INTRO REINFORCEMENT
   Szepesvari C., 2010, ALGORITHMS REINFORCE
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26
   WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698
   Weinstein Ari, 2012, P 22 INT C AUT PLANN
   YEH WWG, 1985, WATER RESOUR RES, V21, P1797, DOI 10.1029/WR021i012p01797
   Zeiler M.D., 2012, ARXIV12125701
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406034
DA 2019-06-15
ER

PT S
AU Wu, JJ
   Lu, E
   Kohli, P
   Freeman, WT
   Tenenbaum, JB
AF Wu, Jiajun
   Lu, Erika
   Kohli, Pushmeet
   Freeman, William T.
   Tenenbaum, Joshua B.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning to See Physics via Visual De-animation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We introduce a paradigm for understanding physical scenes without human annotations. At the core of our system is a physical world representation that is first recovered by a perception module and then utilized by physics and graphics engines. During training, the perception module and the generative models learn by visual de-animation - interpreting and reconstructing the visual information stream. During testing, the system first recovers the physical world state, and then uses the generative models for reasoning and future prediction.
   Even more so than forward simulation, inverting a physics or graphics engine is a computationally hard problem; we overcome this challenge by using a convolutional inversion network. Our system quickly recognizes the physical world state from appearance and motion cues, and has the flexibility to incorporate both differentiable and non-differentiable physics and graphics engines. We evaluate our system on both synthetic and real datasets involving multiple physical scenes, and demonstrate that our system performs well on both physical state estimation and reasoning problems. We further show that the knowledge learned on the synthetic dataset generalizes to constrained real images.
C1 [Wu, Jiajun; Freeman, William T.; Tenenbaum, Joshua B.] MIT CSAIL, Cambridge, MA 02139 USA.
   [Lu, Erika] Univ Oxford, Oxford, England.
   [Kohli, Pushmeet] DeepMind, London, England.
   [Freeman, William T.] Google Res, Mountain View, CA USA.
RP Wu, JJ (reprint author), MIT CSAIL, Cambridge, MA 02139 USA.
RI Jeong, Yongwook/N-7413-2016
FU NSF [1212849, 1447476]; ONR MURI [N00014-16-1-2007]; Center for Brain,
   Minds and Machines (NSF) [1231216]; Toyota Research Institute; Shell;
   MIT Advanced Undergraduate Research Opportunities Program (SuperUROP);
   Samsung
FX We thank Michael Chang, Donglai Wei, and Joseph Lim for helpful
   discussions. This work is supported by NSF #1212849 and #1447476, ONR
   MURI N00014-16-1-2007, the Center for Brain, Minds and Machines (NSF
   #1231216), Toyota Research Institute, Samsung, Shell, and the MIT
   Advanced Undergraduate Research Opportunities Program (SuperUROP).
CR Agrawal P., 2016, NIPS
   [Anonymous], 2010, BULLET PHYS ENGINE
   Ba  Jimmy, 2015, ICLR
   Bai JM, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185562
   Battaglia PW, 2013, P NATL ACAD SCI USA, V110, P18327, DOI 10.1073/pnas.1306572110
   Battaglia Peter W., 2016, NIPS
   Brubaker MA, 2010, INT J COMPUT VISION, V87, P140, DOI 10.1007/s11263-009-0274-5
   Chang Michael B, 2017, ICLR
   Collobert  R., 2011, BIGLEARN
   Denil Misha, 2017, ICLR
   Ehrhardt S., 2017, ARXIV170300247
   Finn Chelsea, 2016, NIPS
   Fragkiadaki Katerina, 2016, ICLR
   Gupta A., 2010, ECCV
   Hamrick JB, 2017, ICLR
   He Kaiming, 2015, CVPR, V6, P9
   Hinton G. E., 2016, NIPS
   HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831
   Huang Jonathan, 2015, ICLR WORKSH
   Jia ZY, 2015, IEEE T PATTERN ANAL, V37, P905, DOI 10.1109/TPAMI.2014.2359435
   Kitani KM, 2017, COMPUT VIS PATT REC, P273, DOI 10.1016/B978-0-12-809276-7.00014-X
   Kulkarni Tejas D, 2015, CVPR
   Kulkarni Tejas D, 2015, NIPS
   Kyriazis Nikolaos, 2013, CVPR
   Lerer Adam, 2016, ICML
   Li Wenbin, 2017, ICRA
   Mathieu M., 2016, ICLR
   Mnih A., 2016, ICML
   Mottaghi Roozbeh, 2016, ECCV
   Mottaghi Roozbeh, 2016, CVPR
   Pinto L., 2016, ECCV
   Ranjan A., 2017, CVPR
   Rezende D., 2016, NIPS
   Salzmann Mathieu, 2011, ICCV
   Shao Tianjia, 2014, ACM TOG, V33
   Szegedy C., 2015, CVPR
   Vondrak M, 2013, IEEE T PATTERN ANAL, V35, P52, DOI 10.1109/TPAMI.2012.61
   Walker  Jacob, 2015, ICCV, P2
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Wu Jiajun, 2016, BMVC
   Wu  Jiajun, 2017, CVPR
   Wu  Jiajun, 2015, NIPS, P2
   Xue  T., 2016, NIPS
   Yuille A, 2006, TRENDS COGN SCI, V10, P301, DOI 10.1016/j.tics.2006.05.002
   Zhang Renqiao, 2016, COGSCI
   Zheng B, 2015, INT J COMPUT VISION, V112, P221, DOI 10.1007/s11263-014-0795-4
   Zhu SC, 2006, FOUND TRENDS COMPUT, V2, P259, DOI 10.1561/0600000018
NR 47
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400015
DA 2019-06-15
ER

PT S
AU Wu, JJ
   Wang, YF
   Xue, TF
   Sun, XY
   Freeman, WT
   Tenenbaum, JB
AF Wu, Jiajun
   Wang, Yifan
   Xue, Tianfan
   Sun, Xingyuan
   Freeman, William T.
   Tenenbaum, Joshua B.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI MarrNet: 3D Shape Reconstruction via 2.5D Sketches
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB 3D object reconstruction from a single image is a highly under-determined problem, requiring strong prior knowledge of plausible 3D shapes. This introduces challenges for learning-based approaches, as 3D object annotations are scarce in real images. Previous work chose to train on synthetic data with ground truth 3D information, but suffered from domain adaptation when tested on real data.
   In this work, we propose MarrNet, an end-to-end trainable model that sequentially estimates 2.5D sketches and 3D object shape. Our disentangled, two-step formulation has three advantages. First, compared to full 3D shape, 2.5D sketches are much easier to be recovered from a 2D image; models that recover 2.5D sketches are also more likely to transfer from synthetic to real data. Second, for 3D reconstruction from 2.5D sketches, systems can learn purely from synthetic data. This is because we can easily render realistic 2.5D sketches without modeling object appearance variations in real images, including lighting, texture, etc. This further relieves the domain adaptation problem. Third, we derive differentiable projective functions from 3D shape to 2.5D sketches; the framework is therefore end-to-end trainable on real images, requiring no human annotations. Our model achieves state-of-the-art performance on 3D shape reconstruction.
C1 [Wu, Jiajun; Xue, Tianfan; Freeman, William T.; Tenenbaum, Joshua B.] MIT CSAIL, Cambridge, MA 02139 USA.
   [Wang, Yifan] ShanghaiTech Univ, Shanghai, Peoples R China.
   [Sun, Xingyuan] Shanghai Jiao Tong Univ, Shanghai, Peoples R China.
   [Freeman, William T.] Google Res, Mountain View, CA USA.
RP Wu, JJ (reprint author), MIT CSAIL, Cambridge, MA 02139 USA.
RI Jeong, Yongwook/N-7413-2016
FU NSF [1212849, 1447476]; ONR MURI [N00014-16-1-2007]; Center for Brain,
   Minds and Machines (NSF) [1231216]; Toyota Research Institute; Shell;
   Samsung
FX We thank Shubham Tulsiani for sharing the DRC results, and Chengkai
   Zhang for the help on shape visualization. This work is supported by NSF
   #1212849 and #1447476, ONR MURI N00014-16-1-2007, the Center for Brain,
   Minds and Machines (NSF #1231216), Toyota Research Institute, Samsung,
   and Shell.
CR Bansal Aayush, 2016, CVPR, P2
   Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712
   Barrow HG, 1978, COMPUTER VISION SYST
   Bell S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601206
   Chang A.X., 2015, ARXIV151203012
   Chen W., 2016, NIPS
   Choy C. B., 2016, ECCV
   Collobert R, 2011, BIGLEARN NIPS WORKSH
   Dai A., 2017, CVPR
   Eigen D., 2015, ICCV
   Firman Michael, 2016, CVPR
   Girdhar Rohit, 2016, ECCV
   He  K., 2015, CVPR
   Hoiem D, 2005, ACM T GRAPHIC, V24, P577, DOI 10.1145/1073204.1073232
   Horn B., 1989, SHAPE SHADING
   Izadi Shahram, 2011, UIST
   Jakob W., 2010, MITSUBA RENDERER
   Janner Michael, 2017, NIPS
   Kanazawa A., 2016, CVPR
   Kar Abhishek, 2015, CVPR
   Kemelmacher-Shlizerman I, 2011, IEEE T PATTERN ANAL, V33, P394, DOI 10.1109/TPAMI.2010.63
   Lim J., 2013, ICCV
   LOWE DG, 1987, ARTIF INTELL, V31, P355, DOI 10.1016/0004-3702(87)90070-1
   Marr D, 1982, VISION COMPUTATIONAL
   McCormac John, 2017, ICCV
   Rezende D., 2016, NIPS
   Rock Jason, 2015, CVPR
   Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132
   Shi Jian, 2017, CVPR
   Silberman  N., 2012, ECCV
   Soltani A. A., 2017, CVPR
   Song Shuran, 2017, CVPR
   Tappen M. F., 2003, NIPS
   Tulsiani S., 2017, CVPR
   Wang Xiaolong, 2015, CVPR
   Weiss Yair, 2001, ICCV
   Wu J, 2016, NIPS
   Wu Jiajun, 2016, ECCV
   Xiang  Yu, 2014, WACV
   Xiao J., 2010, CVPR
   Yan X., 2016, NIPS
   Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284
NR 42
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400052
DA 2019-06-15
ER

PT S
AU Wu, J
   Poloczek, M
   Wilson, AG
   Frazier, PI
AF Wu, Jian
   Poloczek, Matthias
   Wilson, Andrew Gordon
   Frazier, Peter, I
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Bayesian Optimization with Gradients
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID GLOBAL OPTIMIZATION
AB Bayesian optimization has been successful at global optimization of expensiveto-evaluate multimodal objective functions. However, unlike most optimization methods, Bayesian optimization typically does not use derivative information. In this paper we show how Bayesian optimization can exploit derivative information to find good solutions with fewer objective function evaluations. In particular, we develop a novel Bayesian optimization algorithm, the derivative-enabled knowledge gradient (d-KG), which is one-step Bayes-optimal, asymptotically consistent, and provides greater one-step value of information than in the derivative-free setting. d-KG accommodates noisy and incomplete derivative information, comes in both sequential and batch forms, and can optionally reduce the computational cost of inference through automatically selected retention of a single directional derivative. We also compute the d-KG acquisition function and its gradient using a novel fast discretization-free technique. We show d-KG provides state-of-the-art performance compared to a wide range of optimization procedures with and without gradients, on benchmarks including logistic regression, deep learning, kernel learning, and k-nearest neighbors.
C1 [Wu, Jian; Wilson, Andrew Gordon; Frazier, Peter, I] Cornell Univ, Ithaca, NY 14853 USA.
   [Poloczek, Matthias] Univ Arizona, Tucson, AZ 85721 USA.
RP Wu, J (reprint author), Cornell Univ, Ithaca, NY 14853 USA.
RI Jeong, Yongwook/N-7413-2016
FU NSF [IIS-1563887, CAREER CMMI-1254298, CMMI-1536895, IIS-1247696]; AFOSR
   [FA9550-12-1-0200, FA9550-15-1-0038, FA9550-16-1-0046]
FX Wilson was partially supported by NSF IIS-1563887. Frazier, Poloczek,
   and Wu were partially supported by NSF CAREER CMMI-1254298, NSF
   CMMI-1536895, NSF IIS-1247696, AFOSR FA9550-12-1-0200, AFOSR
   FA9550-15-1-0038, and AFOSR FA9550-16-1-0046.
CR Ahmed M. O., 2016, NIPS BAYESOPT
   Bingham D., 2015, OPTIMIZATION TEST PR
   Brochu E., 2010, ARXIV10122599
   Contal Emile, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8188, P225, DOI 10.1007/978-3-642-40988-2_15
   Desautels T, 2014, J MACH LEARN RES, V15, P3873
   Foreman-Mackey D, 2013, PUBL ASTRON SOC PAC, V125, P306, DOI 10.1086/670067
   Forrester A., 2008, ENG DESIGN VIA SURRO
   Frazier P, 2009, INFORMS J COMPUT, V21, P599, DOI 10.1287/ijoc.1080.0314
   Gardner J.R., 2014, ICML, P937
   Gelbart M. A., 2014, P 30 C UNC ART INT, P250
   Gonzalez  J., 2016, P 19 INT C ART INT S, P648
   Harold J., 2003, STOCHASTIC APPROXIMA
   Hernandez-Lobato J M, 2014, ADV NEURAL INFORM PR, P918
   Huang D, 2006, J GLOBAL OPTIM, V34, P441, DOI 10.1007/s10898-005-2454-3
   Jameson A, 1999, J AIRCRAFT, V36, P36, DOI 10.2514/2.2412
   Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147
   Kathuria  T., 2016, ADV NEURAL INFORM PR, P4206
   Koistinen O. - P., 2016, NANOSYSTEMS PHYS CHE, V7
   LeCun  Y., 1998, MNIST DATABASE HANDW
   LECUYER P, 1995, MANAGE SCI, V41, P738, DOI 10.1287/mnsc.41.4.738
   Lizotte D. J., 2008, THESIS
   Maclaurin D., 2015, P 32 INT C MACH LEAR, P2113
   Marmin  S., 2016, ARXIV160902700
   Milgrom P, 2002, ECONOMETRICA, V70, P583, DOI 10.1111/1468-0262.00296
   N. T.. L. Commission, 2016, NYC TRIP REC DAT
   OSBORNE M. A., 2009, 3 INT C LEARN INT OP, P1
   Pedregosa  F., 2016, P 33 INT C MACH LEAR, P737
   Picheny V, 2013, TECHNOMETRICS, V55, P2, DOI 10.1080/00401706.2012.707580
   Plessix RE, 2006, GEOPHYS J INT, V167, P495, DOI 10.1111/j.1365-246X.2006.02978.x
   Poloczek M., 2017, ADV NEURAL INFORM PR
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Scott W, 2011, SIAM J OPTIMIZ, V21, P996, DOI 10.1137/100801275
   Shah A., 2015, ADV NEURAL INFORM PR, P3312
   Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218
   Snoek J., 2012, ADV NEURAL INFORM PR, V2012, P2951
   Snyman JA, 2005, APPL OPTIM, V97, P1, DOI 10.1007/b105200
   Srinivas N., 2010, ICML, P1015, DOI DOI 10.1109/TIT.2011.2182033
   Swersky K., 2013, ADV NEURAL INFORM PR, P2004
   Wang J., 2016, ARXIV160205149
   Wilson A., 2013, P 30 INT C MACH LEAR, P1067
   Wilson A., 2015, INT C MACH LEARN, P1775
   Wilson A. G., 2015, ARXIV151101870
   Wilson A. G, 2016, P 19 INT C ART INT S, P370
   Wu  J., 2016, ADV NEURAL INFORM PR, P3126
NR 44
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405034
DA 2019-06-15
ER

PT S
AU Wu, X
   Guo, RQ
   Suresh, AT
   Kumar, S
   Holtmann-Rice, D
   Simcha, D
   Yu, FX
AF Wu, Xiang
   Guo, Ruiqi
   Suresh, Ananda Theertha
   Kumar, Sanjiv
   Holtmann-Rice, Dan
   Simcha, David
   Yu, Felix X.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Multiscale Quantization for Fast Similarity Search
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID PRODUCT QUANTIZATION; TREES
AB We propose a multiscale quantization approach for fast similarity search on large, high-dimensional datasets. The key insight of the approach is that quantization methods, in particular product quantization, perform poorly when there is large variance in the norms of the data points. This is a common scenario for real-world datasets, especially when doing product quantization of residuals obtained from coarse vector quantization. To address this issue, we propose a multiscale formulation where we learn a separate scalar quantizer of the residual norm scales. All parameters are learned jointly in a stochastic gradient descent framework to minimize the overall quantization error. We provide theoretical motivation for the proposed technique and conduct comprehensive experiments on two large-scale public datasets, demonstrating substantial improvements in recall over existing state-of-the-art methods.
C1 [Wu, Xiang; Guo, Ruiqi; Suresh, Ananda Theertha; Kumar, Sanjiv; Holtmann-Rice, Dan; Simcha, David; Yu, Felix X.] Google Res, New York, NY 11021 USA.
RP Wu, X (reprint author), Google Res, New York, NY 11021 USA.
EM wuxiang@google.com; guorq@google.com; theertha@google.com;
   sanjivk@google.com; dhr@google.com; dsimcha@google.com;
   felixyu@google.com
RI Jeong, Yongwook/N-7413-2016
CR Andoni A., 2015, ADV NEURAL INFORM PR, P1225
   Andre F, 2015, PROC VLDB ENDOW, V9, P288
   Babenko A, 2016, PROC CVPR IEEE, P2055, DOI 10.1109/CVPR.2016.226
   Babenko A, 2015, PROC CVPR IEEE, P4240, DOI 10.1109/CVPR.2015.7299052
   Babenko A, 2014, PROC CVPR IEEE, P931, DOI 10.1109/CVPR.2014.124
   Babenko A, 2012, PROC CVPR IEEE, P3069, DOI 10.1109/CVPR.2012.6248038
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Cayley A., 1846, J REINE ANGEW MATH, V32, P119
   Dasgupta S, 2008, ACM S THEORY COMPUT, P537
   Douze M, 2016, LECT NOTES COMPUT SC, V9906, P785, DOI 10.1007/978-3-319-46475-6_48
   Ge TZ, 2014, IEEE T PATTERN ANAL, V36, P744, DOI 10.1109/TPAMI.2013.240
   Gersho A., 2012, VECTOR QUANTIZATION, V159
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Gray R. M., 1984, IEEE ASSP Magazine, V1, P4, DOI 10.1109/MASSP.1984.1162229
   Guo Ruiqi, 2016, J MACHINE LEARNING R, P482
   Harwood B, 2016, PROC CVPR IEEE, P5713, DOI 10.1109/CVPR.2016.616
   He KM, 2013, PROC CVPR IEEE, P2938, DOI 10.1109/CVPR.2013.378
   Heo JP, 2012, PROC CVPR IEEE, P2957, DOI 10.1109/CVPR.2012.6248024
   Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876
   Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Johnson J., 2017, ARXIV170208734
   Kalantidis Y, 2014, PROC CVPR IEEE, P2329, DOI 10.1109/CVPR.2014.298
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kulis B., 2009, ADV NEURAL INFORM PR, P1042
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Martinez J, 2016, LECT NOTES COMPUT SC, V9906, P137, DOI 10.1007/978-3-319-46475-6_9
   Martinez Julieta, 2014, CORR
   Matsui Y, 2015, IEEE I CONF COMP VIS, P1940, DOI 10.1109/ICCV.2015.225
   Muja M, 2014, IEEE T PATTERN ANAL, V36, P2227, DOI 10.1109/TPAMI.2014.2321376
   Norouzi M, 2014, IEEE T PATTERN ANAL, V36, P1107, DOI 10.1109/TPAMI.2013.231
   Norouzi M, 2013, PROC CVPR IEEE, P3017, DOI 10.1109/CVPR.2013.388
   Shrivastava A, 2014, NIPS, V27, P2321
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Wang J., 2014, ARXIV14082927
   Wang J, 2016, P IEEE, V104, P34, DOI 10.1109/JPROC.2015.2487976
   Weiss Y., 2009, ADV NEURAL INFORM PR, P1753
   Xu T, 2015, INT CONF IMAG VIS
   Zhang QF, 2014, INT CONF MACH LEARN, P807, DOI 10.1109/ICMLC.2014.7009713
   Zhang T, 2015, PROC CVPR IEEE, P4548, DOI 10.1109/CVPR.2015.7299085
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405080
DA 2019-06-15
ER

PT S
AU Wu, YB
   Lan, M
   Sun, SL
   Zhang, Q
   Huang, XJ
AF Wu, Yuanbin
   Lan, Man
   Sun, Shiliang
   Zhang, Qi
   Huang, Xuanjing
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI A Learning Error Analysis for Structured Prediction with Approximate
   Inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB In this work, we try to understand the differences between exact and approximate inference algorithms in structured prediction. We compare the estimation and approximation error of both underestimate (e.g., greedy search) and overestimate (e.g., linear relaxation of integer programming) models. The result shows that, from the perspective of learning errors, performances of approximate inference could be as good as exact inference. The error analyses also suggest a new margin for existing learning algorithms Empirical evaluations on text classification, sequential labelling and dependency parsing witness the success of approximate inference and the benefit of the proposed margin.
C1 [Wu, Yuanbin; Lan, Man; Sun, Shiliang] East China Normal Univ, Sch Comp Sci & Software Engn, Shanghai, Peoples R China.
   [Wu, Yuanbin; Lan, Man] Shanghai Key Lab Multidimens Informat Proc, Shanghai, Peoples R China.
   [Zhang, Qi; Huang, Xuanjing] Fudan Univ, Sch Comp Sci, Shanghai, Peoples R China.
RP Wu, YB (reprint author), East China Normal Univ, Sch Comp Sci & Software Engn, Shanghai, Peoples R China.; Wu, YB (reprint author), Shanghai Key Lab Multidimens Informat Proc, Shanghai, Peoples R China.
EM ybwu@cs.ecnu.edu.cn; mlan@cs.ecnu.edu.cn; slsun@cs.ecnu.edu.cn;
   qz@fudan.edu.cn; xjhuang@fudan.edu.cn
RI Jeong, Yongwook/N-7413-2016
FU NSFC [61402175, 61532011]; STCSM [15ZR1410700]; Shanghai Key Laboratory
   of Trustworthy Computing [07dz22304201604]; Microsoft Research Asia
   Collaborative Research Program
FX The authors wish to thank all reviewers for their helpful comments and
   suggestions. The corresponding authors are Man Lan and Shiliang Sun.
   This research is (partially) supported by NSFC (61402175, 61532011),
   STCSM (15ZR1410700) and Shanghai Key Laboratory of Trustworthy Computing
   (07dz22304201604). Yuanbin Wu is supported by a Microsoft Research Asia
   Collaborative Research Program.
CR Besag Julian, 1986, J ROYAL STAT SOC B, V48, P48
   Catoni Olivier, 2007, LECT NOTES MONOGRAPH, V56
   Chandrasekaran Venkat, 2013, P NATL ACAD SCI, V110
   Chang Kai-Wei, 2015, P AAAI, P2525
   Collins M, 2002, PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P1
   Collins Michael, 2001, P 7 INT WORKSH PARS
   Cortes C., 2016, NIPS, P2514
   Crammer K, 2006, J MACH LEARN RES, V7, P551
   Daniely Amit, 2012, NIPS, P494
   Eisner Jason M., 1996, P COLING
   Emerson T., 2005, P 4 SIGHAN WORKSH CH, P123
   Finley  T., 2008, P 25 INT C MACH LEAR, P304, DOI DOI 10.1145/1390156.1390195
   Germain P., 2009, P 26 ANN INT C MACH, P353
   Globerson Amir, 2015, P 32 INT C MACH LEAR, P2181
   Honorio Jean, 2016, P UAI
   Huang L., 2012, 2012 C N AM CHAPT AS, P142
   Kulesza A., 2007, ADV NEURAL INFORM PR, P785
   Kundu Gourab, 2013, P ACL, P905
   Langford  John, 2002, NIPS, P423
   London  Ben, 2013, P 30 INT C MACH LEAR, P828
   Martins Andre F. T., 2009, P ICML, P713
   McAllester DA, 1999, MACH LEARN, V37, P355, DOI 10.1023/A:1007618624809
   McAllester DA, 2003, MACH LEARN, V51, P5, DOI 10.1023/A:1021840411064
   McAllester David, 2007, GEN BOUNDS CONSISTEN
   McAllester David A., 2011, NIPS, P2205
   Meshi O., 2010, P 27 INT C MACH LEAR, P783
   Meshi O., 2016, P ICML
   Nivre Joakim, 2007, P CONLL SHAR TASK SE, V2007, P915
   Pereira Fernando, 2006, P EACL
   RENEGAR J, 1994, MATH PROGRAM, V65, P73, DOI 10.1007/BF01581690
   RENEGAR J, 1995, SIAM J OPTIMIZ, V5, P506, DOI 10.1137/0805026
   Samdani Rajhans, 2012, P ICML
   Seeger Matthias, 2002, JMLR, V3, P233
   Sontag David, 2010, NIPS, P2181
   Taskar B., 2003, P NEUR INF PROC SYST, P25
   Tjong Erik F., 2000, P CONLL LLL
   Wang Zhuoran, 2009, P 12 INT C ART INT S, P599
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406020
DA 2019-06-15
ER

PT S
AU Wu, YH
   Mansimov, E
   Liao, S
   Grosse, R
   Ba, J
AF Wu, Yuhuai
   Mansimov, Elman
   Liao, Shun
   Grosse, Roger
   Ba, Jimmy
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Scalable trust-region method for deep reinforcement learning using
   Kronecker-factored approximation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB In this work, we propose to apply trust region optimization to deep reinforcement learning using a recently proposed Kronecker-factored approximation to the curvature. We extend the framework of natural policy gradient and propose to optimize both the actor and the critic using Kronecker-factored approximate curvature (K-FAC) with trust region; hence we call our method Actor Critic using Kronecker-Factored Trust Region (ACKTR). To the best of our knowledge, this is the first scalable trust region natural gradient method for actor-critic methods. It is also the method that learns non-trivial tasks in continuous control as well as discrete control policies directly from raw pixel inputs. We tested our approach across discrete domains in Atari games as well as continuous domains in the Mu-JoCo environment. With the proposed methods, we are able to achieve higher rewards and a 2-to 3-fold improvement in sample efficiency on average, compared to previous state-of-the-art on-policy actor-critic methods. Code is available at https://github.com/openai/baselines.
C1 [Wu, Yuhuai; Liao, Shun; Grosse, Roger; Ba, Jimmy] Univ Toronto, Vector Inst, Toronto, ON, Canada.
   [Mansimov, Elman] NYU, New York, NY 10003 USA.
RP Wu, YH (reprint author), Univ Toronto, Vector Inst, Toronto, ON, Canada.
EM ywu@cs.toronto.edu; mansimov@cs.nyu.edu; sliao3@cs.toronto.edu;
   rgrosse@cs.toronto.edu; jimmy@psi.utoronto.ca
RI Jeong, Yongwook/N-7413-2016
CR Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746
   Ba Jimmy, 2017, ICLR
   Bagnell J Andrew, 2003, IJCAI
   Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912
   Brockman G, 2016, ARXIV160601540
   Grosse Roger, 2016, ICML
   Gu S., 2017, ICLR
   Heess N., 2017, ARXIV170702286
   Jaderberg  Max, 2017, ICLR
   Kakade S. M., 2002, ADV NEURAL INFORM PR
   Kingma D. P., 2015, ICLR
   Lillicrap T. P., 2016, ICLR
   Martens J, 2014, ARXIV14121193
   Martens J., 2010, ICML 10
   Martens James, 2015, ICML
   Mnih V., 2016, ICML
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Peters J, 2008, NEUROCOMPUTING, V71, P1180, DOI 10.1016/j.neucom.2007.11.026
   Schraudolph N. N., 2002, NEURAL COMPUTATION
   Schulman J., 2016, P INT C LEARN REPR I
   Schulman  J., 2017, ARXIV170706347
   Schulman J., 2015, P 32 INT C MACH LEAR
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Sutton R., 2000, ADV NEURAL INFORM PR, V12
   Todorov E., 2012, IEEE RSJ INT C INT R
   Wang Z., 2016, ICLR
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
NR 28
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405035
DA 2019-06-15
ER

PT S
AU Xia, F
   Zhang, MJ
   Zou, J
   Tse, D
AF Xia, Fei
   Zhang, Martin J.
   Zou, James
   Tse, David
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI NeuralFDR: Learning Discovery Thresholds from Hypothesis Features
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB As datasets grow richer, an important challenge is to leverage the full features in the data to maximize the number of useful discoveries while controlling for false positives. We address this problem in the context of multiple hypotheses testing, where for each hypothesis, we observe a p-value along with a set of features specific to that hypothesis. For example, in genetic association studies, each hypothesis tests the correlation between a variant and the trait. We have a rich set of features for each variant (e.g. its location, conservation, epigenetics etc.) which could inform how likely the variant is to have a true association. However popular empirically-validated testing approaches, such as Benjamini-Hochberg's procedure (BH) and independent hypothesis weighting (IHW), either ignore these features or assume that the features are categorical or uni-variate. We propose a new algorithm, NeuralFDR, which automatically learns a discovery threshold as a function of all the hypothesis features. We parametrize the discovery threshold as a neural network, which enables flexible handling of multi-dimensional discrete and continuous features as well as efficient end-to-end optimization. We prove that NeuralFDR has strong false discovery rate (FDR) guarantees, and show that it makes substantially more discoveries in synthetic and real datasets. Moreover, we demonstrate that the learned discovery threshold is directly interpretable.
C1 [Xia, Fei; Zhang, Martin J.; Zou, James; Tse, David] Stanford Univ, Stanford, CA 94305 USA.
RP Xia, F (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM feixia@stanford.edu; jinye@stanford.edu; jamesz@stanford.edu;
   dntse@stanford.edu
RI Jeong, Yongwook/N-7413-2016
CR Ardlie KG, 2015, SCIENCE, V348, P648, DOI 10.1126/science.1262110
   Arias-Castro E, 2017, ELECTRON J STAT, V11, P1983, DOI 10.1214/17-EJS1277
   Arjovsky M., 2017, ARXIV170107875
   Benjamini Y, 1997, SCAND J STAT, V24, P407, DOI 10.1111/1467-9469.00072
   BENJAMINI Y, 1995, J R STAT SOC B, V57, P289
   Boca Simina M, 2015, BIORXIV
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   DUNN OJ, 1961, J AM STAT ASSOC, V56, P52, DOI 10.2307/2282330
   Efron B, 2008, ANN APPL STAT, V2, P197, DOI 10.1214/07-AOAS141
   Genovese CR, 2006, BIOMETRIKA, V93, P509, DOI 10.1093/biomet/93.3.509
   Himes BE, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0099625
   HOLM S, 1979, SCAND J STAT, V6, P65
   Hu JX, 2010, J AM STAT ASSOC, V105, P1215, DOI 10.1198/jasa.2010.tm09329
   Ignatiadis N, 2016, NAT METHODS, V13, P577, DOI [10.1038/NMETH.3885, 10.1038/nmeth.3885]
   Ignatiadis Nikolaos, 2017, ARXIV170105179
   Lei L., 2016, P INT C MACH LEARN, P2924
   Lei L., 2017, ARXIV171002776
   Lei Lihua, 2016, ARXIV160906035
   Li Ang, 2016, ARXIV160607926
   Love MI, 2014, GENOME BIOL, V15, DOI 10.1186/s13059-014-0550-8
   Storey JD, 2004, J ROY STAT SOC B, V66, P187, DOI 10.1111/j.1467-9868.2004.00439.x
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401056
DA 2019-06-15
ER

PT S
AU Xia, YC
   Tian, F
   Wu, LJ
   Lin, JX
   Qin, T
   Yu, NH
   Liu, TY
AF Xia, Yingce
   Tian, Fei
   Wu, Lijun
   Lin, Jianxin
   Qin, Tao
   Yu, Nenghai
   Liu, Tie-Yan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Deliberation Networks: Sequence Generation Beyond One-Pass Decoding
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB The encoder-decoder framework has achieved promising progress for many sequence generation tasks, including machine translation, text summarization, dialog system, image captioning, etc. Such a framework adopts an one-pass forward process while decoding and generating a sequence, but lacks the deliberation process: A generated sequence is directly used as final output without further polishing. However, deliberation is a common behavior in human's daily life like reading news and writing papers/articles/books. In this work, we introduce the deliberation process into the encoder-decoder framework and propose deliberation networks for sequence generation. A deliberation network has two levels of decoders, where the first-pass decoder generates a raw sequence and the second-pass decoder polishes and refines the raw sentence with deliberation. Since the second-pass deliberation decoder has global information about what the sequence to be generated might be, it has the potential to generate a better sequence by looking into future words in the raw sentence. Experiments on neural machine translation and text summarization demonstrate the effectiveness of the proposed deliberation networks. On the WMT 2014 English-to-French translation task, our model establishes a new state-of-the-art BLEU score of 41.5.
C1 [Xia, Yingce; Lin, Jianxin; Yu, Nenghai] Univ Sci & Technol China, Hefei, Anhui, Peoples R China.
   [Tian, Fei; Qin, Tao; Liu, Tie-Yan] Microsoft Res, Beijing, Peoples R China.
   [Wu, Lijun] Sun Yat Sen Univ, Guangzhou, Guangdong, Peoples R China.
RP Xia, YC (reprint author), Univ Sci & Technol China, Hefei, Anhui, Peoples R China.
EM yingce.xia@gmail.com; fetia@microsoft.com; wulijun3@mail2.sysu.edu.cn;
   linjx@mail.ustc.edu.cn; taoqin@microsoft.com; ynh@ustc.edu.cn;
   tie-yan.liu@microsoft.com
FU National Natural Science Foundation of China [61371192]
FX The authors would like to thank Yang Fan and Kaitao Song for
   implementing the deep neural machine translation basic model. This work
   is partially supported by the National Natural Science Foundation of
   China (Grant No. 61371192).
CR Bahdanau D., 2015, INT C LEARN REPR
   Chatterjee R., 2016, P 1 C MACH TRANSL SH, V2
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Graff D., 2003, LINGUISTIC DATA CONS
   Grangier D., 2017, ICML
   He D., 2017, 31 ANN C NEUR INF PR
   He K., 2016, CVPR
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Jean S., 2015, ANN M ASS COMP LING
   Kingma D. P., 2014, ARXIV14126980
   Li J, 2016, ARXIV161108562
   Lin C. Y., 2004, TEXT SUMMARIZATION B, V8
   Niehues J., 2016, COLING
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311
   Ranzato M., 2015, ARXIV151106732
   Rush A.M., 2015, P 2015 C EMP METH NA, P379, DOI DOI 10.18653/V1/D15-1044
   Rush A. M., 2015, ACL
   Sennrich R., 2016, ANN M ASS COMP LING
   Shen S., 2016, ANN M ASS COMP LING
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   T. D. Team, 2016, ARXIV160502688
   Tu Z., 2017, AAAI
   Vaswani A., 2017, NIPS
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wiseman Sam, 2016, ACL
   Wu L., 2017, P 26 INT JOINT C ART, P3098
   Wu Y., 2016, ARXIV160908144
   Xia Y., 2017, INT C MACH LEARN, P3789
   Xia Y., 2017, P 26 INT JOINT C ART, P3112
   Xia  Y., 2016, ADV NEURAL INFORM PR, P820
   Xia Y., 2017, EUR C MACH LEARN PRI
   Xu K, 2015, ICML
   Yang Z., 2016, ADV NEURAL INFORM PR, P2361
   Zeiler M.D., 2012, ARXIV12125701
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401079
DA 2019-06-15
ER

PT S
AU Xie, QZ
   Dai, ZH
   Du, YL
   Hovy, E
   Neubig, G
AF Xie, Qizhe
   Dai, Zihang
   Du, Yulun
   Hovy, Eduard
   Neubig, Graham
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Controllable Invariance through Adversarial Feature Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Learning meaningful representations that maintain the content necessary for a particular task while filtering away detrimental variations is a problem of great interest in machine learning. In this paper, we tackle the problem of learning representations invariant to a specific factor or trait of data. The representation learning process is formulated as an adversarial minimax game. We analyze the optimal equilibrium of such a game and find that it amounts to maximizing the uncertainty of inferring the detrimental factor given the representation while maximizing the certainty of making task-specific predictions. On three benchmark tasks, namely fair and bias-free classification, language-independent generation, and lighting-independent image classification, we show that the proposed framework induces an invariant representation, and leads to better generalization evidenced by the improved performance.
C1 [Xie, Qizhe; Dai, Zihang; Du, Yulun; Hovy, Eduard; Neubig, Graham] Carnegie Mellon Univ, Language Technol Inst, Pittsburgh, PA 15213 USA.
RP Xie, QZ (reprint author), Carnegie Mellon Univ, Language Technol Inst, Pittsburgh, PA 15213 USA.
EM qizhex@cs.cmu.edu; dzihang@cs.cmu.edu; yulund@cs.cmu.edu;
   hovy@cs.cmu.edu; gneubig@cs.cmu.edu
RI Jeong, Yongwook/N-7413-2016
FU DARPA [FA8750-12-2-0342]
FX We thank Shi Feng, Di Wang and Zhilin Yang for insightful discussions.
   This research was supported in part by DARPA grant FA8750-12-2-0342
   funded under the DEFT program.
CR Bandanau D., 2015, ICLR
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bousmalis  Konstantinos, 2016, NIPS
   Cettolo Mauro, 2012, P 16 C EUR ASS MACH, V261, P268
   Chen X., 2016, ARXIV160601614
   Chen Xi, 2016, NIPS
   Frank A., 2010, UCI MACHINE LEARNING
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ganin  Yaroslav, 2015, ICML
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Goodfellow I., 2014, NIPS
   Gybenko G, 1989, MATH CONTROL SIGNAL, V2, P303, DOI DOI 10.1007/BF02551274
   Hinton G., 2008, JMLR
   Hochreiter S., 1997, NEURAL COMPUTATION
   Ioffe S., 2015, ARXIV150203167
   Johnson M, 2016, ARXIV161104558
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kingma Diederik P, 2014, ICLR
   Klein G., 2017, ARXIV E PRINTS
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li Yujia, 2014, ARXIV14125244
   Louizos Christos, 2016, ICLR
   Louppe  G., 2016, ARXIV161101046
   Papineni K., 2002, ACL
   Rezende D. J., 2014, ICML
   Sennrich R., 2016, ACL
   Tenenbaum J. B., 1997, NIPS
   Tzeng  E., 2017, ARXIV170205464
   Xu R., 2017, ACL
   Zellinger Werner, 2017, ICLR
   Zemel R., 2013, ICML
   Zhang Chiyuan, 2017, ICLR
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400056
DA 2019-06-15
ER

PT S
AU Xu, AL
   Raginsky, M
AF Xu, Aolin
   Raginsky, Maxim
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Information-theoretic analysis of generalization capability of learning
   algorithms
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID STABILITY
AB We derive upper bounds on the generalization error of a learning algorithm in terms of the mutual information between its input and output. The bounds provide an information-theoretic understanding of generalization in learning problems, and give theoretical guidelines for striking the right balance between data fit and generalization by controlling the input-output mutual information. We propose a number of methods for this purpose, among which are algorithms that regularize the ERM algorithm with relative entropy or with random noise. Our work extends and leads to nontrivial improvements on the recent results of Russo and Zou.
C1 [Xu, Aolin; Raginsky, Maxim] Univ Illinois, Dept Elect & Comp Engn, 1406 W Green St, Urbana, IL 61801 USA.
   [Xu, Aolin; Raginsky, Maxim] Univ Illinois, Coordinated Sci Lab, 1101 W Springfield Ave, Urbana, IL 61801 USA.
RP Xu, AL (reprint author), Univ Illinois, Dept Elect & Comp Engn, 1406 W Green St, Urbana, IL 61801 USA.; Xu, AL (reprint author), Univ Illinois, Coordinated Sci Lab, 1101 W Springfield Ave, Urbana, IL 61801 USA.
EM aolinxu2@illinois.edu; maxim@illinois.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF CAREER award [CCF-1254041]; Center for Science of Information
   (CSoI); NSF Science and Technology Center [CCF-0939370]
FX This work was supported in part by the NSF CAREER award CCF-1254041 and
   in part by the Center for Science of Information (CSoI), an NSF Science
   and Technology Center, under grant agreement CCF-0939370.
CR Alabdulmohsin I., 2017, 20 INT C ART INT STA
   Alabdulmohsin I., 2015, 28 ANN C NEUR INF PR
   Bassily  Raef, 2016, P 48 ANN ACM S THEOR
   Boucheron S., 2013, CONCENTRATION INEQUA
   Boucheron Stephane, 2005, ESAIM-PROBAB STAT, V9, P323, DOI DOI 10.1051/ps:2005018
   Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704
   Buescher KL, 1996, IEEE T AUTOMAT CONTR, V41, P545, DOI 10.1109/9.489275
   Devroye L., 1996, PROBABILISTIC THEORY
   Dwork C., 2015, P 47 ACM S THEOR COM
   Dwork C., 2015, 28 ANN C NEUR INF PR
   Dwork Cynthia, 2014, FDN TRENDS THEORETIC, V9
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   McSherry F., 2007, P 48 ANN IEEE S FDN
   Polyanskiy Y., 2012, LECT NOTES ECE563 UI
   Raginsky M, 2016, 2016 IEEE INFORMATION THEORY WORKSHOP (ITW)
   Raginsky M, 2016, IEEE T INFORM THEORY, V62, P3355, DOI 10.1109/TIT.2016.2549542
   Russo D., 2016, MUCH DOES YOUR DATA
   Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN
   Shalev-Shwartz S, 2010, J MACH LEARN RES, V11, P2635
   Verdu S., 1996, Problems of Information Transmission, V32, P86
   Wang Y.-X., 2016, P INT C PRIV STAT DA
   Zhang C, 2017, INT C LEARN REPR ICL
   Zhang T, 2006, IEEE T INFORM THEORY, V52, P1307, DOI 10.1109/TIT.2005.864439
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402056
DA 2019-06-15
ER

PT S
AU Xu, D
   Ouyang, WL
   Alameda-Pineda, X
   Ricci, E
   Wang, XG
   Sebe, N
AF Xu, Dan
   Ouyang, Wanli
   Alameda-Pineda, Xavier
   Ricci, Elisa
   Wang, Xiaogang
   Sebe, Nicu
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning Deep Structured Multi-Scale Features using Attention-Gated CRFs
   for Contour Prediction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Recent works have shown that exploiting multi-scale representations deeply learned via convolutional neural networks (CNN) is of tremendous importance for accurate contour detection. This paper presents a novel approach for predicting contours which advances the state of the art in two fundamental aspects, i.e. multi-scale feature generation and fusion. Different from previous works directly considering multi-scale feature maps obtained from the inner layers of a primary CNN architecture, we introduce a hierarchical deep model which produces more rich and complementary representations. Furthermore, to refine and robustly fuse the representations learned at different scales, the novel Attention-Gated Conditional Random Fields (AG-CRFs) are proposed. The experiments ran on two publicly available datasets (BSDS500 and NYUDv2) demonstrate the effectiveness of the latent AG-CRF model and of the overall hierarchical framework.
C1 [Xu, Dan; Sebe, Nicu] Univ Trento, Trento, Italy.
   [Ouyang, Wanli] Univ Sydney, Sydney, NSW, Australia.
   [Alameda-Pineda, Xavier] INRIA, Percept Grp, Rennes, France.
   [Ricci, Elisa] Univ Perugia, Perugia, Italy.
   [Wang, Xiaogang] Chinese Univ Hong Kong, Hong Kong, Peoples R China.
RP Xu, D (reprint author), Univ Trento, Trento, Italy.
EM dan.xu@unitn.it; wanli.ouyang@sydney.edu.au;
   xavier.alameda-pineda@inria.fr; elisa.ricci@unipg.it;
   xgwang@ee.cuhk.edu.hk; niculae.sebe@unitn.it
RI Jeong, Yongwook/N-7413-2016
CR Arbelaez P., 2011, TPAMI, V33
   Bertasius G., 2015, CVPR
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chorowski J. K., 2015, NIPS
   Chu X., 2016, NIPS
   Chung J., 2014, CORR
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Deng J., 2009, CVPR
   Dollar P, 2013, ICCV
   Dollar P, 2015, IEEE T PATTERN ANAL, V37, P1558, DOI 10.1109/TPAMI.2014.2377715
   Felzenszwalb P. F., 2004, IJCV, V59
   Gers FA, 2003, J MACH LEARN RES, V3, P115, DOI 10.1162/153244303768966139
   Gupta  S., 2013, CVPR
   Gupta S., 2014, ECCV
   Hallman S., 2015, CVPR
   Hariharan B., 2015, CVPR
   He  K., 2015, ARXIV151203385
   Jia Y., 2014, ACM MM
   Kokkinos I., 2015, ARXIV151107386
   Krizhevsky A., 2012, NIPS
   Li G., 2015, CVPR
   Lim J.J., 2013, CVPR
   Liu Y., 2016, ARXIV161202103
   Maninis K. K., 2016, ECCV
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   Minka T., 2009, NIPS
   Mnih V., 2014, ADV NEURAL INFORM PR, V3, P2204
   Pont-Tuset J., 2016, TPAMI
   Ren X., 2008, ECCV
   Ren Z., 2013, CVPR
   Shen W., 2015, CVPR
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Silberman  N., 2012, ECCV
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Tang Y., 2010, NIPS WORKSH TRANSF L
   Winn J., 2012, AISTATS
   Xiao  T., 2015, CVPR
   Xie  S., 2015, ICCV
   Xu D., 2017, CVPR
   Xu K, 2015, ICML
   Yang Jimei, 2016, OBJECT CONTOUR DETEC
   Yang S., 2015, ICCV
   Yu F., 2015, ARXIV151107122
   Zeng X., 2016, ARXIV161002579
   Zhang Z., 2016, CVPR
   Zhao Q., 2015, BMVC
NR 46
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404004
DA 2019-06-15
ER

PT S
AU Xu, HT
   Zha, HY
AF Xu, Hongteng
   Zha, Hongyuan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI A Dirichlet Mixture Model of Hawkes Processes for Event Sequence
   Clustering
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID IDENTIFICATION
AB How to cluster event sequences generated via different point processes is an interesting and important problem in statistical machine learning. To solve this problem, we propose and discuss an effective model-based clustering method based on a novel Dirichlet mixture model of a special but significant type of point processes-Hawkes process. The proposed model generates the event sequences with different clusters from the Hawkes processes with different parameters, and uses a Dirichlet distribution as the prior distribution of the clusters. We prove the identifiability of our mixture model and propose an effective variational Bayesian inference algorithm to learn our model. An adaptive inner iteration allocation strategy is designed to accelerate the convergence of our algorithm. Moreover, we investigate the sample complexity and the computational complexity of our learning algorithm in depth. Experiments on both synthetic and real-world data show that the clustering method based on our model can learn structural triggering patterns hidden in asynchronous event sequences robustly and achieve superior performance on clustering purity and consistency compared to existing methods.
C1 [Xu, Hongteng] Georgia Inst Technol, Sch ECE, Atlanta, GA 30332 USA.
   [Zha, Hongyuan] Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.
RP Xu, HT (reprint author), Georgia Inst Technol, Sch ECE, Atlanta, GA 30332 USA.
EM hongtengxu313@gmail.com; zha@cc.gatech.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [IIS-1639792, IIS-1717916, CMMI-1745382]
FX This work is supported in part by NSF IIS-1639792, IIS-1717916, and
   CMMI-1745382.
CR Bacry E, 2012, EUR PHYS J B, V85, DOI 10.1140/epjb/e2012-21005-8
   Blei DM, 2006, BAYESIAN ANAL, V1, P121, DOI 10.1214/06-BA104
   Blundell C., 2012, NIPS
   Daley DJ, 2007, INTRO THEORY POINT P, VII
   Du N., 2015, KDD
   Du Nan, 2012, NIPS
   Eichler M., 2016, J TIME SERIES ANAL
   Farajtabar M., 2014, NIPS
   Gorur D, 2010, J COMPUT SCI TECH-CH, V25, P653, DOI [10.1007/s11390-010-1051-1, 10.1007/s11390-010-9355-8]
   Golub GH, 2000, LINEAR ALGEBRA APPL, V309, P289, DOI 10.1016/S0024-3795(99)00204-9
   Green PJ, 1995, BIOMETRIKA, V82, P711, DOI 10.1093/biomet/82.4.711
   Han F., 2013, ICML
   HAWKES AG, 1971, BIOMETRIKA, V58, P83, DOI 10.1093/biomet/58.1.83
   Kim D., 2008, THESIS
   Lemonnier Remi, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8725, P161, DOI 10.1007/978-3-662-44851-9_11
   Lewis E., 2011, J NONPARAMETRIC STAT
   Li L., 2013, CIKM
   Lian W, 2015, ICML
   Liao TW, 2005, PATTERN RECOGN, V38, P1857, DOI 10.1016/j.patcog.2005.01.025
   Luo D., 2015, IJCAI
   Luo DX, 2016, IEEE T KNOWL DATA EN, V28, P1518, DOI 10.1109/TKDE.2016.2522426
   Luo DX, 2014, IEEE T BROADCAST, V60, P61, DOI 10.1109/TBC.2013.2295894
   Maharaj EA, 2000, J CLASSIF, V17, P297, DOI 10.1007/s003570000023
   Manning C. D., 2008, INTRO INFORM RETRIEV, V1
   Maugis C, 2009, BIOMETRICS, V65, P701, DOI 10.1111/j.1541-0420.2008.01160.x
   Meijer E, 2008, J CLASSIF, V25, P113, DOI 10.1007/s00357-008-9008-6
   Ogunnaike B.A., 1994, PROCESS DYNAMICS MOD
   Rasmussen C. E., 1999, NIPS
   Rasmussen JG, 2013, METHODOL COMPUT APPL, V15, P623, DOI 10.1007/s11009-011-9272-5
   Reynaud-Bouret P, 2010, ANN STAT, V38, P2781, DOI 10.1214/10-AOS806
   ROTHENBERG TJ, 1971, ECONOMETRICA, V39, P577, DOI 10.2307/1913267
   Saeed M, 2002, COMPUT CARDIOL, V29, P641, DOI 10.1109/CIC.2002.1166854
   Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218
   Simma A., 2010, UAI
   Snoek  Jasper, 2012, NIPS
   Socher R., 2011, AISTATS
   Teh Y. W., 2006, NIPS
   Tibshirani R, 2005, J COMPUT GRAPH STAT, V14, P511, DOI 10.1198/106186005X59243
   Van Wijk J. J., 1999, IEEE S INF VIS
   Von Luxburg U., 2010, CLUSTERING STABILITY
   Xu H., 2016, ICML
   Xu H., 2017, ICML
   Xu Haifeng, 2015, IJCAI
   Xu HT, 2017, IEEE T KNOWL DATA EN, V29, P157, DOI 10.1109/TKDE.2016.2618925
   Xu Y., 2016, BIOMETRICS
   YAKOWITZ SJ, 1968, ANN MATH STAT, V39, P209, DOI 10.1214/aoms/1177698520
   Yang SH, 2013, ICML
   Zhang ZH, 2004, STAT COMPUT, V14, P343, DOI 10.1023/B:STCO.0000039484.36470.41
   Zhao Q., 2015, KDD
   Zhou K., 2013, ICML
   Zhou K., 2013, AISTATS
NR 51
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401038
DA 2019-06-15
ER

PT S
AU Xu, JS
   Chi, EC
   Lange, K
AF Xu, Jason
   Chi, Eric C.
   Lange, Kenneth
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Generalized Linear Model Regression under Distance-to-set Penalties
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID VARIABLE SELECTION; SPLIT FEASIBILITY; OPTIMIZATION; MATRIX;
   REGULARIZATION; MAJORIZATION; LIKELIHOOD
AB Estimation in generalized linear models (GLM) is complicated by the presence of constraints. One can handle constraints by maximizing a penalized log-likelihood. Penalties such as the lasso are effective in high dimensions, but often lead to unwanted shrinkage. This paper explores instead penalizing the squared distance to constraint sets. Distance penalties are more flexible than algebraic and regularization penalties, and avoid the drawback of shrinkage. To optimize distance penalized objectives, we make use of the majorization-minimization principle. Resulting algorithms constructed within this framework are amenable to acceleration and come with global convergence guarantees. Applications to shape constraints, sparse regression, and rank-restricted matrix regression on synthetic and real data showcase strong empirical performance, even under non-convex constraints.
C1 [Xu, Jason; Lange, Kenneth] Univ Calif Los Angeles, Los Angeles, CA 90095 USA.
   [Chi, Eric C.] North Carolina State Univ, Raleigh, NC 27695 USA.
RP Xu, JS (reprint author), Univ Calif Los Angeles, Los Angeles, CA 90095 USA.
EM jqxu@ucla.edu; eric_chi@ncsu.edu; klange@ucla.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF MSPRF [1606177]
FX We would like to thank Hua Zhou for helpful discussions about matrix
   regression and the EEG data. JX was supported by NSF MSPRF #1606177.
CR Barlow R, 1972, STAT INFERENCE ORDER
   Becker M P, 1997, Stat Methods Med Res, V6, P38, DOI 10.1191/096228097677258219
   Bertsimas D, 2016, ANN STAT, V44, P813, DOI 10.1214/15-AOS1388
   Bregman L. M., 1967, USSR COMP MATH MATH, V7, P200, DOI DOI 10.1016/0041-5553(67)90040-7
   Byrne C, 2001, ANN OPER RES, V105, P77, DOI 10.1023/A:1013349430987
   Censor Y, 2005, INVERSE PROBL, V21, P2071, DOI 10.1088/0266-5611/21/6/017
   Censor Y, 1994, NUMER ALGORITHMS, V8, P221, DOI DOI 10.1007/BF02142692
   Chi EC, 2014, MATH PROGRAM, V146, P409, DOI 10.1007/s10107-013-0697-1
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Fan JQ, 2010, STAT SINICA, V20, P101
   Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273
   Friedman J, 2010, J STAT SOFTW, V33, P1
   Gaines B. R., 2016, ARXIV161101511
   Golub G. H., 2012, MATRIX COMPUTATIONS, V3
   Hastie T, 2017, ARXIV170708692
   Hung H, 2013, BIOSTATISTICS, V14, P189, DOI 10.1093/biostatistics/kxs023
   Jones P., 2016, TRENDS COMPENDIUM DA
   Lange K, 2000, J COMPUT GRAPH STAT, V9, P1, DOI 10.2307/1390605
   Lange K., 2015, ARXIV150707598
   Lange K., 2016, MM OPTIMIZATION ALGO
   Li B, 2010, ANN STAT, V38, P1094, DOI 10.1214/09-AOS737
   Mairal J, 2015, SIAM J OPTIMIZ, V25, P829, DOI 10.1137/140957639
   MAZUMDER R., 2017, ARXIV170803288
   McCullagh P, 1989, GEN LINEAR MODELS, V37
   Meinshausen N, 2007, COMPUT STAT DATA AN, V52, P374, DOI 10.1016/j.csda.2006.12.019
   More J. J., 1978, Proceedings of the Biennial Conference on numerical analysis, P105
   Park MY, 2007, J ROY STAT SOC B, V69, P659, DOI 10.1111/j.1467-9868.2007.00607.x
   Polson NG, 2015, STAT SCI, V30, P559, DOI 10.1214/15-STS530
   Savalle P., 2012, P 29 INT C MACH LEAR, P1351
   Su W., 2017, ANN STAT, V45
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Wu WB, 2001, BIOMETRIKA, V88, P793, DOI 10.1093/biomet/88.3.793
   Xu J., 2017, ARXIV161205614
   Zhang CH, 2010, ANN STAT, V38, P894, DOI 10.1214/09-AOS729
   ZHANG XL, 1995, BRAIN RES BULL, V38, P531, DOI 10.1016/0361-9230(95)02023-5
   Zhou H, 2014, J R STAT SOC B, V76, P463, DOI 10.1111/rssb.12031
   Zhou H, 2011, STAT COMPUT, V21, P261, DOI 10.1007/s11222-009-9166-3
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401041
DA 2019-06-15
ER

PT S
AU Xu, P
   Ma, J
   Gu, QQ
AF Xu, Pan
   Ma, Jian
   Gu, Quanquan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Speeding Up Latent Variable Gaussian Graphical Model Estimation via
   Nonconvex Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID COVARIANCE ESTIMATION; MATRIX COMPLETION; SELECTION
AB We study the estimation of the latent variable Gaussian graphical model (LVGGM), where the precision matrix is the superposition of a sparse matrix and a low-rank matrix. In order to speed up the estimation of the sparse plus low-rank components, we propose a sparsity constrained maximum likelihood estimator based on matrix factorization, and an efficient alternating gradient descent algorithm with hard thresholding to solve it. Our algorithm is orders of magnitude faster than the convex relaxation based methods for LVGGM. In addition, we prove that our algorithm is guaranteed to linearly converge to the unknown sparse and low-rank components up to the optimal statistical precision. Experiments on both synthetic and genomic data demonstrate the superiority of our algorithm over the state-of-the-art algorithms and corroborate our theory.
C1 [Xu, Pan; Gu, Quanquan] Univ Virginia, Dept Comp Sci, Charlottesville, VA 22904 USA.
   [Ma, Jian] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
RP Xu, P (reprint author), Univ Virginia, Dept Comp Sci, Charlottesville, VA 22904 USA.
EM px3ds@virginia.edu; jianma@cs.cmu.edu; qg5w@virginia.edu
FU National Science Foundation [IIS-1652539, IIS-1717205, IIS-1717206]
FX We would like to thank the anonymous reviewers for their helpful
   comments. This research was sponsored in part by the National Science
   Foundation IIS-1652539, IIS-1717205 and IIS-1717206. The views and
   conclusions contained in this paper are those of the authors and should
   not be interpreted as representing any funding agencies.
CR Agarwal A, 2012, ANN STAT, V40, P1171, DOI 10.1214/12-AOS1000
   Balakrishnan S., 2014, ARXIV14082156
   Bhojanapalli Srinadh, 2015, DROPPING CONVEXITY F
   Blumensath T, 2009, APPL COMPUT HARMON A, V27, P265, DOI 10.1016/j.acha.2009.04.002
   Cai T Tony, 2012, BIOMETRIKA
   Cai T, 2011, J AM STAT ASSOC, V106, P594, DOI 10.1198/jasa.2011.tm10155
   Candes E, 2012, COMMUN ACM, V55, P111, DOI 10.1145/2184319.2184343
   Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Chandrasekaran V, 2010, 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P1610, DOI 10.1109/ALLERTON.2010.5707106
   Chandrasekaran V, 2011, SIAM J OPTIMIZ, V21, P572, DOI 10.1137/090761793
   Chen Y., 2015, ARXIV150903025
   Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045
   Golub G. H., 2012, MATRIX COMPUTATIONS, V3
   Gu Q., 2016, P 19 INT C ART INT S, P600
   Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806
   Hardt M, 2014, ANN IEEE SYMP FOUND, P651, DOI 10.1109/FOCS.2014.75
   Huang JZ, 2006, BIOMETRIKA, V93, P85, DOI 10.1093/biomet/93.1.85
   Jain P., 2013, P 45 ANN ACM S THEOR, P665, DOI DOI 10.1145/2488608.2488693
   Lauritzen S. L., 1996, GRAPHICAL MODELS, V17
   Li Xingguo, 2016, STOCHASTIC VARIANCE
   Liu H, 2009, J MACH LEARN RES, V10, P2295
   Ma SQ, 2013, NEURAL COMPUT, V25, P2172, DOI 10.1162/NECO_a_00379
   Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281
   Meng Zhaoshi, 2014, ARXIV14062721
   Negahban S., 2009, ADV NEURAL INFORM PR, P1348
   Negahban S, 2012, J MACH LEARN RES, V13, P1665
   Nesterov Y., 2013, INTRO LECT CONVEX OP, V87
   Ravikumar P, 2011, ELECTRON J STAT, V5, P935, DOI 10.1214/11-EJS631
   Rothman AJ, 2008, ELECTRON J STAT, V2, P494, DOI 10.1214/08-EJS176
   Tu S., 2015, ARXIV150703566
   Vershynin  Roman, 2010, ARXIV10113027
   Wang CJ, 2010, SIAM J OPTIMIZ, V20, P2994, DOI 10.1137/090772514
   Wang Lingxiao, 2016, ARXIV161005275
   Wang Lingxiao, 2017, INT C MACH LEARN, P3617
   Wang Lingxiao, 2016, P 19 INT C ART INT S, P177
   Wang Z., 2014, ARXIV14128729
   Xu Pan, 2016, ARXIV161209297
   Xu Pan, 2017, ARTIF INTELL, P923
   Xu  Pan, 2016, ADV NEURAL INFORM PR, P1064
   Yi Xinyang, 2016, ARXIV160507784
   Yin JX, 2011, ANN APPL STAT, V5, P2630, DOI 10.1214/11-AOAS494
   Yuan XT, 2014, IEEE T INFORM THEORY, V60, P1673, DOI 10.1109/TIT.2013.2296784
   Zhang Xiao, 2017, ARXIV170206525
   Zhao Tuo, 2015, Adv Neural Inf Process Syst, V28, P559
   Zheng Q., 2015, ADV NEURAL INFORM PR, P109
NR 45
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401093
DA 2019-06-15
ER

PT S
AU Xu, Y
   Lin, QH
   Yang, TB
AF Xu, Yi
   Lin, Qihang
   Yang, Tianbao
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Adaptive SVRG Methods under Error Bound Conditions with Unknown Growth
   Parameter
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID CONVERGENCE
AB Error bound, an inherent property of an optimization problem, has recently revived in the development of algorithms with improved global convergence without strong convexity. The most studied error bound is the quadratic error bound, which generalizes strong convexity and is satisfied by a large family of machine learning problems. Quadratic error bound have been leveraged to achieve linear convergence in many first-order methods including the stochastic variance reduced gradient (SVRG) method, which is one of the most important stochastic optimization methods in machine learning. However, the studies along this direction face the critical issue that the algorithms must depend on an unknown growth parameter (a generalization of strong convexity modulus) in the error bound. This parameter is difficult to estimate exactly and the algorithms choosing this parameter heuristically do not have theoretical convergence guarantee. To address this issue, we propose novel SVRG methods that automatically search for this unknown parameter on the fly of optimization while still obtain almost the same convergence rate as when this parameter is known. We also analyze the convergence property of SVRG methods under Holderian error bound, which generalizes the quadratic error bound.
C1 [Xu, Yi; Yang, Tianbao] Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA.
   [Lin, Qihang] Univ Iowa, Dept Management Sci, Iowa City, IA 52242 USA.
RP Xu, Y (reprint author), Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA.
EM yi-xu@uiowa.edu; qihang-lin@uiowa.edu; tianbao-yan@uiowa.edu
RI Jeong, Yongwook/N-7413-2016
FU National Science Foundation [IIS-1463988, IIS-1545995]
FX We thank the anonymous reviewers for their helpful comments. Y. Xu and
   T. Yang are partially supported by National Science Foundation
   (IIS-1463988, IIS-1545995).
CR Allen-Zhu Z., 2017, P 49 ANN ACM S THEOR
   Bolte Jerome, 2015, CORR
   Defazio A., 2014, ADV NEURAL INFORM PR, P1646
   Drusvyatskiy D., 2016, ARXIV160206661
   Gong P., 2014, CORR
   Hou K., 2013, ADV NEURAL INFORM PR, P710
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Karimi H., 2016, JOINT EUR C MACH LEA, P795, DOI DOI 10.1007/978-3-319-46128-1_50
   Li GY, 2013, MATH PROGRAM, V137, P37, DOI 10.1007/s10107-011-0481-z
   Lin Q., 2014, ICML, P73
   Liu J., 2016, CORR
   Liu J, 2015, SIAM J OPTIMIZ, V25, P351, DOI 10.1137/140961134
   Liu M., 2017, CORR
   LUO ZQ, 1992, SIAM J CONTROL OPTIM, V30, P408, DOI 10.1137/0330025
   LUO ZQ, 1992, J OPTIMIZ THEORY APP, V72, P7, DOI 10.1007/BF00939948
   Ma C., 2015, CORR
   Murata T., 2017, CORR
   Necoara I., 2015, CORR
   Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5
   Nguyen L., 2017, CORR
   NYQUIST H, 1983, COMMUN STAT-THEOR M, V12, P2511, DOI 10.1080/03610928308828618
   Rockafellar RT, 1970, PRINCETON MATH SERIE
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791
   Xu Y., 2016, ADV NEURAL INFORM PR, P1208
   Xu Yi, 2017, P 34 INT C MACH LEAR, P3821
   Yang Tianbao, 2016, CORR
   Zeyuan Allen-Zhu, 2016, INT C MACH LEARN, P1080
   Zhang Hui, 2016, CORR
   Zhang Hui, 2013, ARXIV13034645
   Zhi-Quan Luo, 1993, Annals of Operations Research, V46-47, P157
   Zhou Z., 2015, P 32 INT C MACH LEAR, P1501
   Zhou Z., 2015, ARXIV151203518
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403034
DA 2019-06-15
ER

PT S
AU Xu, Y
   Liu, MR
   Lin, QH
   Yang, TB
AF Xu, Yi
   Liu, Mingrui
   Lin, Qihang
   Yang, Tianbao
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI ADMM without a Fixed Penalty Parameter: Faster Convergence with New
   Adaptive Penalization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID ALTERNATING DIRECTION METHOD; OPTIMIZATION; ALGORITHMS
AB Alternating direction method of multipliers (ADMM) has received tremendous interest for solving numerous problems in machine learning, statistics and signal processing. However, it is known that the performance of ADMM and many of its variants is very sensitive to the penalty parameter of a quadratic penalty applied to the equality constraints. Although several approaches have been proposed for dynamically changing this parameter during the course of optimization, they do not yield theoretical improvement in the convergence rate and are not directly applicable to stochastic ADMM. In this paper, we develop a new ADMM and its linearized variant with a new adaptive scheme to update the penalty parameter. Our methods can be applied under both deterministic and stochastic optimization settings for structured non-smooth objective function. The novelty of the proposed scheme lies at that it is adaptive to a local sharpness property of the objective function, which marks the key difference from previous adaptive scheme that adjusts the penalty parameter per-iteration based on certain conditions on iterates. On theoretical side, given the local sharpness characterized by an exponent theta is an element of (0; 1], we show that the proposed ADMM enjoys an improved iteration complexity of (O) over tilde (1/epsilon(1-theta))(1) in the deterministic setting and an iteration complexity of (O) over tilde (1/epsilon(2(1-theta))) in the stochastic setting without smoothness and strong convexity assumptions. The complexity in either setting improves that of the standard ADMM which only uses a fixed penalty parameter. On the practical side, we demonstrate that the proposed algorithms converge comparably to, if not much faster than, ADMM with a fine-tuned fixed penalty parameter.
C1 [Xu, Yi; Liu, Mingrui; Yang, Tianbao] Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA.
   [Lin, Qihang] Univ Iowa, Dept Management Sci, Iowa City, IA 52242 USA.
RP Xu, Y (reprint author), Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA.
EM yi-xu@uiowa.edu; mingrui-liu@uiowa.edu; qihang-lin@uiowa.edu;
   tianbao-yang@uiowa.edu
FU National Science Foundation [IIS-1463988, IIS-1545995]
FX We thank the anonymous reviewers for their helpful comments. Y. Xu, M.
   Liu and T. Yang are partially supported by National Science Foundation
   (IIS-1463988, IIS-1545995). Y. Xu would like to thank Yan Yan for useful
   discussions on the low-rank representation experiments.
CR Bolte Jerome, 2015, CORR
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Deng W, 2016, J SCI COMPUT, V66, P889, DOI 10.1007/s10915-015-0048-x
   Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045
   Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2061, DOI 10.1137/110848876
   Goldstein T, 2014, SIAM J IMAGING SCI, V7, P1588, DOI 10.1137/120896219
   Hazan E., 2011, P 24 ANN C LEARN THE, P421
   He BS, 2015, NUMER MATH, V130, P567, DOI 10.1007/s00211-014-0673-6
   He BS, 2012, SIAM J NUMER ANAL, V50, P700, DOI 10.1137/110836936
   He BS, 2000, J OPTIMIZ THEORY APP, V106, P337, DOI 10.1023/A:1004603514434
   Hong MY, 2017, MATH PROGRAM, V162, P165, DOI 10.1007/s10107-016-1034-2
   Kim S, 2009, BIOINFORMATICS, V25, pI204, DOI 10.1093/bioinformatics/btp218
   Kurdyka K, 1998, ANN I FOURIER, V48, P769, DOI 10.5802/aif.1638
   Li GY, 2013, MATH PROGRAM, V137, P37, DOI 10.1007/s10107-011-0481-z
   Lin Zhouchen, 2011, P ADV NEUR INF PROC, P612
   Liu G., 2010, P INT C MACH LEARN, V27, P663
   Liu M., 2017, CORR
   Luo ZQ, 2000, APPL OPTIM, V33, P383
   Monteiro RDC, 2013, SIAM J OPTIMIZ, V23, P475, DOI 10.1137/110849468
   Necoara I., 2015, CORR
   Ouyang H., 2013, P INT C MACH LEARN, P80
   Ouyang YY, 2015, SIAM J IMAGING SCI, V8, P644, DOI 10.1137/14095697X
   Suzuki T., 2013, P 30 INT C MACH LEAR, V28, P392
   Suzuki T., 2014, P 31 INT C MACH LEAR, V32, P736
   Tibshirani R, 2005, J R STAT SOC B, V67, P91, DOI 10.1111/j.1467-9868.2005.00490.x
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Tibshirani RJ, 2011, ANN STAT, V39, P1335, DOI 10.1214/11-AOS878
   Xu Y., 2016, ADV NEURAL INFORM PR, P1208
   Xu Yi, 2017, P 34 INT C MACH LEAR, P3821
   Xu Z., 2016, CORR
   Yang Tianbao, 2016, CORR
   Zhang XG, 2016, I C OPT COMMUN NETW
   Zhang XQ, 2011, J SCI COMPUT, V46, P20, DOI 10.1007/s10915-010-9408-8
   Zhang XQ, 2010, SIAM J IMAGING SCI, V3, P253, DOI 10.1137/090746379
   Zhao P., 2015, P 32 INT C MACH LEAR, P69
   Zheng S., 2016, 25 INT JOINT C ART I
   Zhong W., 2014, P 31 INT C MACH LEAR, P46
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401030
DA 2019-06-15
ER

PT S
AU Xu, YC
   Zhang, HY
   Miller, K
   Singh, A
   Dubrawski, A
AF Xu, Yichong
   Zhang, Hongyang
   Miller, Kyle
   Singh, Aarti
   Dubrawski, Artur
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Noise-Tolerant Interactive Learning Using Pairwise Comparisons
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID BOUNDS
AB We study the problem of interactively learning a binary classifier using noisy labeling and pairwise comparison oracles, where the comparison oracle answers which one in the given two instances is more likely to be positive. Learning from such oracles has multiple applications where obtaining direct labels is harder but pairwise comparisons are easier, and the algorithm can leverage both types of oracles. In this paper, we attempt to characterize how the access to an easier comparison oracle helps in improving the label and total query complexity. We show that the comparison oracle reduces the learning problem to that of learning a threshold function. We then present an algorithm that interactively queries the label and comparison oracles and we characterize its query complexity under Tsybakov and adversarial noise conditions for the comparison and labeling oracles. Our lower bounds show that our label and total query complexity is almost optimal.
C1 [Xu, Yichong; Zhang, Hongyang; Singh, Aarti] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.
   [Miller, Kyle; Dubrawski, Artur] Carnegie Mellon Univ, Anton Lab, Pittsburgh, PA 15213 USA.
RP Xu, YC (reprint author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.
EM yichongx@cs.cmu.edu; hongyanz@cs.cmu.edu; mille856@andrew.cmu.edu;
   aarti@cs.cmu.edu; awd@cs.cmu.edu
RI Jeong, Yongwook/N-7413-2016
FU AFRL [FA8750-17-2-0212]
FX This research is supported in part by AFRL grant FA8750-17-2-0212. We
   thank Chicheng Zhang for insightful ideas on improving results in [6]
   using Rademacher complexity.
CR Agarwal S, 2005, LECT NOTES COMPUT SC, V3559, P32, DOI 10.1007/11503415_3
   Agarwal S, 2009, J MACH LEARN RES, V10, P441
   Ailon N., 2007, ARXIV07102889
   Attenberg J, 2010, LECT NOTES ARTIF INT, V6321, P40, DOI 10.1007/978-3-642-15880-3_9
   Awasthi  P., 2016, ANN C LEARN THEOR, P152
   Awasthi P, 2017, J ACM, V63, DOI 10.1145/3006384
   Balcan M.-F., 2016, ARXIV160509227
   Balcan M.-F., 2012, COLT, P20
   Balcan M. - F., 2013, 26 ANN C LEARN THEOR, P288
   Balcan M. - F., 2016, ADV NEURAL INFORM PR, P2955
   Balcan M.-F., 2006, ICML, P65, DOI DOI 10.1145/1143844.1143853]
   Balcan MF, 2007, LECT NOTES COMPUT SC, V4539, P35, DOI 10.1007/978-3-540-72927-3_5
   Beygelzimer A., 2016, ADV NEURAL INFORM PR, P3342
   Boucheron Stephane, 2005, ESAIM-PROBAB STAT, V9, P323, DOI DOI 10.1051/ps:2005018
   Castro RM, 2008, IEEE T INFORM THEORY, V54, P2339, DOI 10.1109/TIT.2008.920189
   Dekel O, 2012, J MACH LEARN RES, V13, P2655
   Furnkranz J, 2010, PREFERENCE LEARNING, P65, DOI 10.1007/978-3-642-14125-6_4
   Hanneke S., 2014, THEORY ACTIVE LEARNI
   Hanneke S., 2009, COLT
   Hanneke Steve, 2012, ARXIV12073772
   Heckel Reinhard, 2016, ARXIV160608842
   Jamieson K. G., 2011, P NEUR INF PROC SYST, P2240
   Kane D. M., 2017, ARXIV170403564
   Krishnamurthy A., 2015, THESIS
   Lovasz L, 2007, RANDOM STRUCT ALGOR, V30, P307, DOI 10.1002/rsa.20135
   Maji S, 2014, INT J COMPUT VISION, V108, P82, DOI 10.1007/s11263-014-0716-6
   Sabato Sivan, 2016, ANN C LEARN THEOR, P1419
   Shah N. B., 2014, ARXIV14066618
   Stewart N, 2005, PSYCHOL REV, V112, P881, DOI 10.1037/0033-295X.112.4.881
   Tsybakov AB, 2004, ANN STAT, V32, P135
   Wah C, 2014, PROC CVPR IEEE, P859, DOI 10.1109/CVPR.2014.115
   Yan S., 2017, ARXIV170205581
   Yang L., 2009, CMUML09113
   Zhang Chicheng, 2014, ADV NEURAL INFORM PR, P442
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402047
DA 2019-06-15
ER

PT S
AU Xu, Z
   Modayil, J
   van Hasselt, H
   Barreto, A
   Silver, D
   Schaul, T
AF Xu, Zhongwen
   Modayil, Joseph
   van Hasselt, Hado
   Barreto, Andre
   Silver, David
   Schaul, Tom
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Natural Value Approximators: Learning when to Trust Past Estimates
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Neural networks have a smooth initial inductive bias, such that small changes in input do not lead to large changes in output. However, in reinforcement learning domains with sparse rewards, value functions have non-smooth structure with a characteristic asymmetric discontinuity whenever rewards arrive. We propose a mechanism that learns an interpolation between a direct value estimate and a projected value estimate computed from the encountered reward and the previous estimate. This reduces the need to learn about discontinuities, and thus improves the value function approximation. Furthermore, as the interpolation is learned and state-dependent, our method can deal with heterogeneous observability. We demonstrate that this one change leads to significant improvements on multiple Atari games, when applied to the state-of-the-art A3C algorithm.
C1 [Xu, Zhongwen; Modayil, Joseph; van Hasselt, Hado; Barreto, Andre; Silver, David; Schaul, Tom] DeepMind, London, England.
RP Xu, Z (reprint author), DeepMind, London, England.
EM zhongwen@google.com; modayil@google.com; hado@google.com;
   andrebarreto@google.com; davidsilver@google.com; schaul@google.com
CR Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912
   Bellman R., 1957, TECHNICAL REPORT
   Gyorfi L, 2002, DISTRIBUTION FREE TH
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Kingma Diederik P, 2014, ICLR
   LIN LJ, 1992, MACH LEARN, V8, P293, DOI 10.1023/A:1022628806385
   Littman ML, 2002, ADV NEUR IN, V14, P1555
   Mnih  V., 2016, INT C MACH LEARN, V48, P1928
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479
   Sutton R. S., 1984, THESIS
   Sutton R. S., 1998, REINFORCEMENT LEARNI, V1
   Van Hasselt H., 2016, AAAI, P2094
   van Hasselt Hado, 2015, ABS150804582 CORR
   Wang Z., 2016, SER P MACHINE LEARNI, P1995
   Watkins C. J. C. H., 1989, THESIS
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402017
DA 2019-06-15
ER

PT S
AU Yan, SB
   Zhang, CC
AF Yan, Songbai
   Zhang, Chicheng
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Revisiting Perceptron: Efficient and Label-Optimal Learning of
   Halfspaces
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID CONVERGENCE; COMPLEXITY
AB It has been a long-standing problem to efficiently learn a halfspace using as few labels as possible in the presence of noise. In this work, we propose an efficient Perceptron-based algorithm for actively learning homogeneous halfspaces under the uniform distribution over the unit sphere. Under the bounded noise condition [49], where each label is flipped with probability at most eta < 1/2, our algorithm achieves a near-optimal label complexity of <(O)over tilde>(d/(1-2 eta)(2) ln 1/epsilon)(2) in time (O) over tilde (d(2)/epsilon(1-2 eta)(3)). Under the adversarial noise condition [6, 45, 42], where at most a (Omega) over tilde (epsilon) fraction of labels can be flipped, our algorithm achieves a near-optimal label complexity of (O) over tilde (d ln 1/epsilon) in time (O) over tilde (d(2)/epsilon) Furthermore, we show that our active learning algorithm can be converted to an efficient passive learning algorithm that has near-optimal sample complexities with respect to epsilon and d.
C1 [Yan, Songbai; Zhang, Chicheng] Univ Calif San Diego, La Jolla, CA 92093 USA.
   [Zhang, Chicheng] Microsoft Res, New York, NY USA.
RP Yan, SB (reprint author), Univ Calif San Diego, La Jolla, CA 92093 USA.
EM yansongbai@ucsd.edu; chicheng.zhang@microsoft.com
FU NSF [IIS-1167157, 1162581]
FX The authors thank Kamalika Chaudhuri for help and support, Hongyang
   Zhang for thought-provoking initial conversations, Jiapeng Zhang for
   helpful discussions, and the anonymous reviewers for their insightful
   feedback. Much of this work is supported by NSF IIS-1167157 and 1162581.
CR Agarwal A., 2013, JMLR P, P1220
   Ailon N, 2014, J MACH LEARN RES, V15, P885
   Angluin D., 1988, Machine Learning, V2, P343, DOI 10.1007/BF00116829
   Anthony  M., 2009, NEURAL NETWORK LEARN
   Arora S., 1993, Proceedings. 34th Annual Symposium on Foundations of Computer Science (Cat. No.93CH3368-8), P724, DOI 10.1109/SFCS.1993.366815
   Awasthi P., 2015, C LEARNING THEORY CO, P167
   Awasthi P., 2014, STOC, P449
   Awasthi Pranjal, 2016, P 28 C LEARN THEOR C
   Balcan M. - F., 2007, COLT
   Balcan M.-F., 2013, COLT
   Balcan M.-F. F., 2013, ADV NEURAL INFORM PR, P1295
   Balcan MF, 2010, MACH LEARN, V80, P111, DOI 10.1007/s10994-010-5174-y
   Balcan MF, 2009, J COMPUT SYST SCI, V75, P78, DOI 10.1016/j.jcss.2008.07.003
   Balcan Maria- Florina, 2017, ARXIV170307758
   Beygelzimer A., 2010, NIPS
   Beygelzimer Alina, 2009, 26 INT C MACH LEARN
   Blum A, 1998, ALGORITHMICA, V22, P35, DOI 10.1007/PL00013833
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Cesa-Bianchi N., 2009, P 26 ANN INT C MACH, P121
   Chen Lin, 2017, 31 AAAI C ART INT
   COHN D, 1994, MACH LEARN, V15, P201, DOI 10.1023/A:1022673506211
   Cristianini N., 2000, INTRO SUPPORT VECTOR
   Daniely Amit, 2015, ARXIV150505800
   Dasgupta S, 2005, LECT NOTES COMPUT SC, V3559, P249, DOI 10.1007/11503415_17
   Dasgupta S., 2017, ICML, P3444
   Dasgupta S., 2005, NIPS
   Dasgupta S, 2011, THEOR COMPUT SCI, V412, P1767, DOI 10.1016/j.tcs.2010.12.054
   Dasgupta Sanjoy, 2007, ADV NEURAL INFORM PR, V20
   Dekel O, 2012, J MACH LEARN RES, V13, P2655
   DUNAGAN J., 2004, P 36 ANN ACM S THEOR, P315
   Feldman V, 2006, ANN IEEE SYMP FOUND, P563
   Freund Y, 1997, MACH LEARN, V28, P133, DOI 10.1023/A:1007330508534
   Guillory A., 2009, ARTIF INTELL, P201
   Guruswami V, 2009, SIAM J COMPUT, V39, P742, DOI 10.1137/070685798
   Hanneke S, 2009, THESIS
   Hanneke S., 2007, ICML
   Hanneke S, 2015, LECT NOTES ARTIF INT, V9355, P149, DOI 10.1007/978-3-319-24486-0_10
   Hanneke S, 2014, FOUND TRENDS MACH LE, V7, DOI 10.1561/2200000037
   Hanneke S, 2011, ANN STAT, V39, P333, DOI 10.1214/10-AOS843
   Hanneke Steve, 2012, ARXIV12073772
   Hsu Daniel, 2010, THESIS
   Huang Tzu- Kuo, 2015, CORR
   Kalai AT, 2008, SIAM J COMPUT, V37, P1777, DOI 10.1137/060649057
   KEARNS M, 1993, SIAM J COMPUT, V22, P807, DOI 10.1137/0222052
   Klivans A., 2014, APPROX RANDOM 2014, V28, P793
   Klivans AR, 2009, J MACH LEARN RES, V10, P2715
   Koltchinskii V., 2010, JMLR
   KULKARNI SR, 1993, MACH LEARN, V11, P23, DOI 10.1023/A:1022627018023
   LONG PM, 1995, IEEE T NEURAL NETWOR, V6, P1556, DOI 10.1109/72.471352
   Massart P, 2006, ANN STAT, V34, P2326, DOI 10.1214/009053606000000786
   Monteleoni C, 2006, LECT NOTES ARTIF INT, V4005, P650, DOI 10.1007/11776420_47
   MOTZKIN TS, 1954, CAN J MATH, V6, P393, DOI 10.4153/CJM-1954-038-x
   Orabona F., 2011, P 28 INT C MACH LEAR, P433
   Raginsky Maxim, 2011, ADV NEURAL INFORM PR, P1026
   Settles B., 2010, ACTIVE LEARNING LIT, V52, P11, DOI DOI 10.1111/J.1467-7687.2012.01135.X
   Singh A, 2016, AAAI
   TONG S, 2001, J MACHINE LEARNING R, V2, P45, DOI DOI 10.1162/153244302760185243
   VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025
   Wang LW, 2011, J MACH LEARN RES, V12, P2269
   Zhang Chicheng, 2014, ADV NEURAL INFORM PR, P442
   Zhang  Y., 2017, COLT, P1980
NR 61
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401010
DA 2019-06-15
ER

PT S
AU Yang, F
   Yang, ZL
   Cohen, WW
AF Yang, Fan
   Yang, Zhilin
   Cohen, William W.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Differentiable Learning of Logical Rules for Knowledge Base Reasoning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We study the problem of learning probabilistic first-order logical rules for knowledge base reasoning. This learning problem is difficult because it requires learning the parameters in a continuous space as well as the structure in a discrete space. We propose a framework, Neural Logic Programming, that combines the parameter and structure learning of first-order logical rules in an end-to-end differentiable model. This approach is inspired by a recently-developed differentiable logic called TensorLog [5], where inference tasks can be compiled into sequences of differentiable operations. We design a neural controller system that learns to compose these operations. Empirically, our method outperforms prior work on multiple knowledge base benchmark datasets, including Freebase and WikiMovies.
C1 [Yang, Fan; Yang, Zhilin; Cohen, William W.] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
RP Yang, F (reprint author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
EM fanyang1@cs.cmu.edu; zhiliny@cs.cmu.edu; wcoher@cs.cmu.edu
FU NSF [IIS1250956]; Google Research
FX This work was funded by NSF under IIS1250956 and by Google Research.
CR Andreas J., 2016, P NAACL HLT, P1545
   Bollacker K., 2008, P 2008 ACM SIGMOD IN, P1247, DOI DOI 10.1145/1376616.1376746
   Bordes A., 2014, ARXIV14063676
   Bordes A., 2013, ADV NEURAL INFORM PR, P2787
   Cohen W., 2016, ARXIV160506523
   Denham W. W., 1973, THESIS
   Getoor L, 2007, INTRO STAT RELATIONA
   Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Kilgarriff A, 2000, WORDNET ELECT LEXICA
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kok S., 2007, P 24 INT C MACH LEAR, V227, P433
   Lao  N., 2011, P C EMP METH NAT LAN, P529
   Lao N, 2010, MACH LEARN, V81, P53, DOI 10.1007/s10994-010-5205-8
   Miller A, 2016, ARXIV160603126
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Muggleton S.H., 1996, ADV INDUCTIVE LOGIC, P254
   Muggleton Stephen, 1992, INDUCTIVE LOGIC PROG, V38
   Neelakantan Arvind, 2016, ARXIV161108945
   Neelakantan Arvind, 2015, ARXIV151104834
   Richardson M, 2006, MACH LEARN, V62, P107, DOI 10.1007/s10994-006-5833-1
   Shen Y., 2016, ARXIV161104642
   Socher R., 2013, ADV NEURAL INFORM PR, P926
   Toutanova K, 2015, P 3 WORKSH CONT VECT, P57
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang William Yang, 2014, CIKM 2014
   Weston J., 2014, ARXIV14103916
   William YangWang, 2013, P 22 ACM INT C INF K, P2129
   Yang B., 2015, ICLR
NR 29
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402036
DA 2019-06-15
ER

PT S
AU Yang, F
   Ramdas, A
   Jamieson, K
   Wainwright, M
AF Yang, Fanny
   Ramdas, Aaditya
   Jamieson, Kevin
   Wainwright, Martin
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI A framework for Multi-A(rmed)/B(andit) Testing with Online FDR Control
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We propose an alternative framework to existing setups for controlling false alarms when multiple A/B tests are run over time. This setup arises in many practical applications, e.g. when pharmaceutical companies test new treatment options against control pills for different diseases, or when internet companies test their default webpages versus various alternatives over time. Our framework proposes to replace a sequence of A/B tests by a sequence of best-arm MAB instances, which can be continuously monitored by the data scientist. When interleaving the MAB tests with an online false discovery rate (FDR) algorithm, we can obtain the best of both worlds: low sample complexity and any time online FDR control. Our main contributions are: (i) to propose reasonable definitions of a null hypothesis for MAB instances; (ii) to demonstrate how one can derive an always-valid sequential p-value that allows continuous monitoring of each MAB test; and (iii) to show that using rejection thresholds of online-FDR algorithms as the confidence levels for the MAB algorithms results in both sample-optimality, high power and low FDR at any point in time. We run extensive simulations to verify our claims, and also report results on real data collected from the New Yorker Cartoon Caption contest.
C1 [Yang, Fanny] Univ Calif Berkeley, Dept EECS, Berkeley, CA 94720 USA.
   [Ramdas, Aaditya; Wainwright, Martin] Univ Calif Berkeley, Dept EECS & Stat, Berkeley, CA USA.
   [Jamieson, Kevin] Univ Washington, Allen Sch CSE, Seattle, WA 98195 USA.
RP Yang, F (reprint author), Univ Calif Berkeley, Dept EECS, Berkeley, CA 94720 USA.
EM fanny-yang@berkeley.edu; ramdas@berkeley.edu;
   jamieson@cs.washington.edu; wainwrig@berkeley.edu
RI Jeong, Yongwook/N-7413-2016
FU Office of Naval Research MURI [DOD-002888]; Air Force Office of
   Scientific Research Grant [AFOSR-FA9550-14-1-001]; National Science
   Foundation [CIF-31712-23800, DMS-1309356]
FX This work was partially supported by Office of Naval Research MURI grant
   DOD-002888, Air Force Office of Scientific Research Grant
   AFOSR-FA9550-14-1-001, and National Science Foundation Grants
   CIF-31712-23800 and DMS-1309356.
CR Aharoni E, 2014, J R STAT SOC B, V76, P771, DOI 10.1111/rssb.12048
   Balsubramani A., 2016, P 32 C UNC ART INT, P42
   BENJAMINI Y, 1995, J R STAT SOC B, V57, P289
   Foster DP, 2008, J R STAT SOC B, V70, P429, DOI 10.1111/j.1467-9868.2007.00643.x
   Jamieson K, 2014, INF SCI SYST CISS 20, P1
   Jamieson K. G., 2014, P 27 C LEARN THEOR, V35, P423
   Javanmard A., 2017, ANN STAT
   Javanmard A., 2015, ARXIV150206197
   Johari Ramesh, 2015, ARXIV151204922
   Kalyanakrishnan S., 2012, P 29 INT C MACH LEAR, P655
   Kaufmann E., 2015, J MACHINE LEARNING R
   Ramdas A., 2017, ADV NEURAL INFORM PR
   Simchowitz M., 2017, ARXIV170205186
   Villar SS, 2015, STAT SCI, V30, P199, DOI 10.1214/14-STS504
NR 14
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406004
DA 2019-06-15
ER

PT S
AU Yang, G
   Schoenholz, SS
AF Yang, Greg
   Schoenholz, Samuel S.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Mean Field Residual Networks: On the Edge of Chaos
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We study randomly initialized residual networks using mean field theory and the theory of difference equations. Classical feedforward neural networks, such as those with tanh activations, exhibit exponential behavior on the average when propagating inputs forward or gradients backward. The exponential forward dynamics causes rapid collapsing of the input space geometry, while the exponential backward dynamics causes drastic vanishing or exploding gradients. We show, in contrast, that by adding skip connections, the network will, depending on the nonlinearity, adopt subexponential forward and backward dynamics, and in many cases in fact polynomial. The exponents of these polynomials are obtained through analytic methods and proved and verified empirically to be correct. In terms of the "edge of chaos" hypothesis, these subexponential and polynomial laws allow residual networks to "hover over the boundary between stability and chaos," thus preserving the geometry of the input space and the gradient information flow. In our experiments, for each activation function we study here, we initialize residual networks with different hyperparameters and train them on MNIST. Remarkably, our initialization time theory can accurately predict test time performance of these networks, by tracking either the expected amount of gradient explosion or the expected squared distance between the images of two input vectors. Importantly, we show, theoretically as well as empirically, that common initializations such as the Xavier or the He schemes are not optimal for residual networks, because the optimal initialization variances depend on the depth. Finally, we have made mathematical contributions by deriving several new identities for the kernels of powers of ReLU functions by relating them to the zeroth Bessel function of the second kind.
C1 [Yang, Greg] Microsoft Res AI, Redmond, WA 98052 USA.
   [Schoenholz, Samuel S.] Google Brain, Mountain View, CA USA.
   [Yang, Greg] Harvard Univ, Cambridge, MA 02138 USA.
RP Yang, G (reprint author), Microsoft Res AI, Redmond, WA 98052 USA.
EM gregyan@microsoft.com; schsam@google.com
CR Bertschinger N, 2004, NEURAL COMPUT, V16, P1413, DOI 10.1162/089976604323057443
   Cho Y., 2009, ADV NEURAL INFORM PR, P342, DOI DOI 10.1021/ed028p10
   Daniely Amit, 2016, NIPS, P2253
   Glorot X., 2010, JLMR P TRACK, P249, DOI DOI 10.1.1/207.2059
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276
   Poole B, 2016, ADV NEURAL INFORM PR, V29, P3360
   Raghu Maithra, 2016, ARXIV160605336CSSTAT
   Schoenholz Samuel S., 2017, DEEP INFORM PROPAGAT
NR 11
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402089
DA 2019-06-15
ER

PT S
AU Yang, YQ
   Grover, P
   Kar, S
AF Yang, Yaoqing
   Grover, Pulkit
   Kar, Soummya
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Coded Distributed Computing for Inverse Problems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Computationally intensive distributed and parallel computing is often bottlenecked by a small set of slow workers known as stragglers. In this paper, we utilize the emerging idea of "coded computation" to design a novel error-correcting-code inspired technique for solving linear inverse problems under specific iterative methods in a parallelized implementation affected by stragglers. Example machine-learning applications include inverse problems such as personalized PageRank and sampling on graphs. We provably show that our coded-computation technique can reduce the mean-squared error under a computational deadline constraint. In fact, the ratio of mean-squared error of replication-based and coded techniques diverges to infinity as the deadline increases. Our experiments for personalized PageRank performed on real systems and real social networks show that this ratio can be as large as 10(4). Further, unlike coded-computation techniques proposed thus far, our strategy combines outputs of all workers, including the stragglers, to produce more accurate estimates at the computational deadline. This also ensures that the accuracy degrades "gracefully" in the event that the number of stragglers is large.
C1 [Yang, Yaoqing; Grover, Pulkit; Kar, Soummya] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
RP Yang, YQ (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM yyaoqing@andrew.cmu.edu; pgrover@andrew.cmu.edu; soummyak@andrew.cmu.edu
RI Jeong, Yongwook/N-7413-2016
CR Chen S, 2016, IEEE GLOB COMM CONF
   Chen SH, 2015, IEEE T SIGNAL PROCES, V63, P6510, DOI 10.1109/TSP.2015.2469645
   Da Wang, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P7, DOI 10.1145/2847220.2847223
   Da Wang, 2014, ACM SIGMETRICS Performance Evaluation Review, V42, P599, DOI 10.1145/2591971.2592042
   Dean J, 2013, COMMUN ACM, V56, P74, DOI 10.1145/2408776.2408794
   Dimakis AG, 2010, IEEE T INFORM THEORY, V56, P4539, DOI 10.1109/TIT.2010.2054295
   Dutta Sanghamitra, 2016, ADV NEURAL INFORM PR, P2092
   Ferdinand NS, 2016, ANN ALLERTON CONF, P954, DOI 10.1109/ALLERTON.2016.7852337
   Golub G. H., 2012, MATRIX COMPUTATIONS, V3
   Haikin M, 2016, IEEE INT SYMP INFO, P2074, DOI 10.1109/ISIT.2016.7541664
   Haveliwala T. H., 2002, P 11 INT C WORLD WID, P517, DOI DOI 10.1145/511446.511513
   HUANG KH, 1984, IEEE T COMPUT, V33, P518, DOI 10.1109/TC.1984.1676475
   Joshi G, 2014, IEEE J SEL AREA COMM, V32, P989, DOI 10.1109/JSAC.2014.140518
   Lee K, 2017, IEEE INT SYMP INFO, P2413, DOI 10.1109/ISIT.2017.8006962
   Lee K, 2017, IEEE INT SYMP INFO, P2418, DOI 10.1109/ISIT.2017.8006963
   Lee K, 2016, IEEE INT SYMP INFO, P1143, DOI 10.1109/ISIT.2016.7541478
   Leskovec J., 2012, ADV NEURAL INFORM PR, P539
   Li S., 2016, P 53 ACM EDAC IEEE D, P1
   Li S, 2017, IEEE COMMUN MAG, V55, P34, DOI 10.1109/MCOM.2017.1600894
   Li SZ, 2015, ANN ALLERTON CONF, P964, DOI 10.1109/ALLERTON.2015.7447112
   Longbo Huang, 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P2766, DOI 10.1109/ISIT.2012.6284026
   Maddah-Ali MA, 2015, IEEE ACM T NETWORK, V23, P1029, DOI 10.1109/TNET.2014.2317316
   Mood A. M., 1974, INTRO THEORY STAT
   Narang SK, 2013, IEEE GLOB CONF SIG, P491, DOI 10.1109/GlobalSIP.2013.6736922
   Page  L, 1999, TECHNICAL REPORT
   Reisizadeh A, 2017, IEEE INT SYMP INFO, P2408, DOI 10.1109/ISIT.2017.8006961
   SAAD Y., 2003, ITERATIVE METHODS SP
   Sandryhaila A, 2013, IEEE T SIGNAL PROCES, V61, P1644, DOI 10.1109/TSP.2013.2238935
   Sathiamoorthy M, 2013, PROC VLDB ENDOW, V6, P325, DOI 10.14778/2535573.2488339
   Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192
   Tandon R., 2016, GRADIENT CODING
   Wang XH, 2015, IEEE T SIGNAL PROCES, V63, P2432, DOI 10.1109/TSP.2015.2411217
   Yang Y., 2017, IEEE T INFORM THEORY
   Zhang H., 2013, J APPL MATH
   Zhang RW, 2017, NEURAL PLAST, DOI 10.1155/2017/6809745
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400068
DA 2019-06-15
ER

PT S
AU Yang, Y
   Etesami, J
   He, N
   Kiyavash, N
AF Yang, Yingxiang
   Etesami, Jalal
   He, Niao
   Kiyavash, Negar
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Online Learning for Multivariate Hawkes Processes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID STABILITY
AB We develop a nonparametric and online learning algorithm that estimates the triggering functions of a multivariate Hawkes process (MHP). The approach we take approximates the triggering function f(i,j) (t) by functions in a reproducing kernel Hilbert space (RKHS), and maximizes a time-discretized version of the log-likelihood, with Tikhonov regularization. Theoretically, our algorithm achieves an O (log T) regret bound. Numerical results show that our algorithm offers a competing performance to that of the nonparametric batch learning algorithm, with a run time comparable to parametric online learning algorithms.
C1 [Yang, Yingxiang; Kiyavash, Negar] Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA.
   [Etesami, Jalal; He, Niao; Kiyavash, Negar] Univ Illinois, Dept Ind & Enterprise Syst Engn, Urbana, IL 61801 USA.
RP Yang, Y (reprint author), Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA.
EM yyang172@illinois.edu; etesami2@illinois.edu; niaohe@illinois.edu;
   kiyavash@illinois.edu
RI Jeong, Yongwook/N-7413-2016
FU MURI grant ARMY [W911NF-15-1-0479]; ONR [W911NF-15-1-0479]
FX This work was supported in part by MURI grant ARMY W911NF-15-1-0479 and
   ONR grant W911NF-15-1-0479.
CR Bacry E, 2012, EUR PHYS J B, V85, DOI 10.1140/epjb/e2012-21005-8
   Bacry E., 2015, GEN ERROR BOUND SPAR
   Bacry E, 2015, MARK MICROSTRUCT LIQ, V1, DOI 10.1142/S2382626615500057
   Bacry E, 2016, IEEE T INFORM THEORY, V62, P2184, DOI 10.1109/TIT.2016.2533397
   Bagnell J Andrew, 2015, LEARNING POSITIVE FU
   BHOJANAPALLI S, 2016, P C LEARN THEOR, P530
   Bochnak Jacek, 2013, REAL ALGEBRAIC GEOME, V36
   Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704
   Bremaud P, 1996, ANN PROBAB, V24, P1563
   Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924
   Eichler M, 2017, J TIME SER ANAL, V38, P225, DOI 10.1111/jtsa.12213
   Etesami J, 2014, P AMER CONTR CONF, P2563, DOI 10.1109/ACC.2014.6859362
   Etesami Jalal, 2016, C UNC ART INT
   Flaxman Seth, 2017, INT C ART INT STAT
   Hall EC, 2016, IEEE T INFORM THEORY, V62, P4327, DOI 10.1109/TIT.2016.2568202
   HAWKES AG, 1971, BIOMETRIKA, V58, P83, DOI 10.1093/biomet/58.1.83
   Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013
   Hongteng X., 2016, P 33 INT C MACH LEAR, V48, P1717
   Kim S, 2014, P IEEE, V102, P683, DOI 10.1109/JPROC.2014.2307888
   Kivinen J, 2004, IEEE T SIGNAL PROCES, V52, P2165, DOI 10.1109/TSP.2004.830991
   Krumin M, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00147
   Leskovec J, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P497
   Liniger T. J., 2009, THESIS
   OZAKI T, 1979, ANN I STAT MATH, V31, P145, DOI 10.1007/BF02480272
   Reynaud-Bouret P, 2010, ANN STAT, V38, P2781, DOI 10.1214/10-AOS806
   Scholkopf B, 2001, LECT NOTES ARTIF INT, V2111, P416
   Vandenberghe L, 1996, SIAM REV, V38, P49, DOI 10.1137/1038003
   Yang S. H., 2013, P 30 INT C MACH LEAR, V28, P1
   Zhou Ke, 2013, P 30 INT C MACH LEAR, V28, P1301
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405002
DA 2019-06-15
ER

PT S
AU Yang, ZR
   Balasubramanian, K
   Wang, ZR
   Liu, H
AF Yang, Zhuoran
   Balasubramanian, Krishna
   Wang, Zhaoran
   Liu, Han
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Learning Non-Gaussian Multi-Index Model via Second-Order Stein's Method
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID SLICED INVERSE REGRESSION; DIMENSION REDUCTION; SIGNAL RECOVERY; SPARSE;
   CONVERGENCE; RATES
AB We consider estimating the parametric components of semiparametric multi-index models in high dimensions. To bypass the requirements of Gaussianity or elliptical symmetry of covariates in existing methods, we propose to leverage a second-order Stein's method with score function-based corrections. We prove that our estimator achieves a near-optimal statistical rate of convergence even when the score function or the response variable is heavy-tailed. To establish the key concentration results, we develop a data-driven truncation argument that may be of independent interest. We supplement our theoretical findings with simulations.
C1 [Yang, Zhuoran; Balasubramanian, Krishna] Princeton Univ, Princeton, NJ 08544 USA.
   [Wang, Zhaoran; Liu, Han] Tencent AI Lab, Bellevue, WA USA.
   [Wang, Zhaoran; Liu, Han] Northwestern Univ, Evanston, IL 60208 USA.
RP Yang, ZR (reprint author), Princeton Univ, Princeton, NJ 08544 USA.
EM zy6@princeton.edu; kb18@princeton.edu; zhaoranwang@gmail.com;
   hanliu.cmu@gmail.com
RI Jeong, Yongwook/N-7413-2016
CR Ai Albert, 2014, LINEAR ALGEBRA ITS A
   Boucheron S., 2013, CONCENTRATION INEQUA
   Boufounos PT, 2008, 2008 42ND ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS, VOLS 1-3, P16, DOI 10.1109/CISS.2008.4558487
   Cai TT, 2016, ANN STAT, V44, P2221, DOI 10.1214/16-AOS1443
   Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924
   Candes EJ, 2013, COMMUN PUR APPL MATH, V66, P1241, DOI 10.1002/cpa.21432
   Chen X, 2010, ANN STAT, V38, P3696, DOI 10.1214/10-AOS826
   Davenport MA, 2014, INF INFERENCE, V3, P189, DOI 10.1093/imaiai/iau006
   Fan JQ, 2011, ANNU REV ECON, V3, P291, DOI 10.1146/annurev-economics-061109-080451
   Goldstein L., 2016, ARXIV160901025
   Gross David, 2015, J FOURIER ANAL APPL
   Horowitz J. L., 2009, SEMIPARAMETRIC NONPA, V12
   Janzamin Majid, 2015, ARXIV150608473
   Janzamin Majid, 2014, ARXIV14122863
   Jiang B, 2014, ANN STAT, V42, P1751, DOI 10.1214/14-AOS1233
   Krauthgamer R, 2015, ANN STAT, V43, P1300, DOI 10.1214/15-AOS1310
   Lecue G, 2015, ELECTRON J PROBAB, V20, P1, DOI 10.1214/EJP.v20-3525
   LI KC, 1992, J AM STAT ASSOC, V87, P1025, DOI 10.2307/2290640
   LI KC, 1991, J AM STAT ASSOC, V86, P316, DOI 10.2307/2290563
   LI KC, 1989, ANN STAT, V17, P1009, DOI 10.1214/aos/1176347254
   Li XD, 2013, SIAM J MATH ANAL, V45, P3019, DOI 10.1137/120893707
   Lin Qian, 2015, ARXIV150703895
   Lin Qian, 2017, ARXIV170106009
   Mendelson Shahar, 2014, C LEARN THEOR, P25
   Neykov Matey, 2016, ADV NEURAL INFORM PR, P4089
   Neykov Matey, 2016, J MACHINE LEARNING R, V17, P1
   Oliveira Roberto Imbuzeiro, 2013, ARXIV13122903
   Plan Y, 2016, IEEE T INFORM THEORY, V62, P1528, DOI 10.1109/TIT.2016.2517008
   Stein Charles, 2004, STEINS METHOD
   Sun J., 2016, ARXIV160206664
   Tan K. M., 2016, ARXIV160408697
   Tan Kean Ming, 2016, CONVEX FORMULA UNPUB
   Vu V.Q., 2013, ADV NEURAL INFORM PR, P2670
   Wang TY, 2016, ANN STAT, V44, P1896, DOI 10.1214/15-AOS1369
   Wang Z., 2015, ARXIV151208861
   Wang Zhaoran, 2014, Adv Neural Inf Process Syst, V2014, P3383
   Wang ZR, 2014, ANN STAT, V42, P2164, DOI 10.1214/14-AOS1238
   Yang Zhuoran, 2015, INT C MACH LEARN
   Yi Xinyang, 2015, Adv Neural Inf Process Syst, V28, P1549
   Zhu LX, 2006, J AM STAT ASSOC, V101, P630, DOI 10.1198/016214505000001285
NR 40
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406017
DA 2019-06-15
ER

PT S
AU Yao, SR
   Huang, B
AF Yao, Sirui
   Huang, Bert
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Beyond Parity: Fairness Objectives for Collaborative Filtering
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We study fairness in collaborative-filtering recommender systems, which are sensitive to discrimination that exists in historical data. Biased data can lead collaborative-filtering methods to make unfair predictions for users from minority groups. We identify the insufficiency of existing fairness metrics and propose four new metrics that address different forms of unfairness. These fairness metrics can be optimized by adding fairness terms to the learning objective. Experiments on synthetic and real data show that our new metrics can better measure fairness than the baseline, and that the fairness objectives effectively help reduce unfairness.
C1 [Yao, Sirui] Virginia Tech, Dept Comp Sci, Blacksburg, VA 24061 USA.
RP Yao, SR (reprint author), Virginia Tech, Dept Comp Sci, Blacksburg, VA 24061 USA.
EM ysirui@vt.edu; bhuang@vt.edu
RI Jeong, Yongwook/N-7413-2016
CR Beede D. N., 2011, WOMEN STEM GENDER GA
   Beutel A, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P203, DOI 10.1145/3038912.3052713
   Broad S., 2014, ASS SUPPORTING COMPU, P29
   Chausson O., 2010, WHO WATCHES WHAT ASS
   Dascalu MI, 2016, BEHAV INFORM TECHNOL, V35, P290, DOI 10.1080/0144929X.2015.1128977
   DAYMONT TN, 1984, J HUM RESOUR, V19, P408, DOI 10.2307/145880
   Ekstrand Michael D., 2010, Foundations and Trends in Human-Computer Interaction, V4, P81, DOI 10.1561/1100000009
   Griffith AL, 2010, ECON EDUC REV, V29, P911, DOI 10.1016/j.econedurev.2010.06.010
   Hardt M., 2016, ADV NEURAL INFORM PR, P3315
   Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872
   Holland PW, 1976, SOCIOL METHODOL, V7, P1, DOI [10.2307/270703, DOI 10.2307/270703]
   HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732
   Kamishima T., 2011, 2011 IEEE International Conference on Data Mining Workshops, P643, DOI 10.1109/ICDMW.2011.83
   Kamishima T., 2012, DECISIONS RECSYS, P8
   Kamishima  T., 2014, RECSYS POSTERS
   Kamishima T., 2013, DECISIONS RECSYS, P1
   Kamishima T, 2016, INT CONF DAT MIN WOR, P860, DOI [10.1109/ICDMW.2016.23, 10.1109/ICDMW.2016.0127]
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Lum K., 2016, ARXIV161008077
   Marlin B., 2012, ARXIV12065267
   Marlin B. M., 2009, P 3 ACM C REC SYST R, P5, DOI DOI 10.1145/1639714.1639717
   Nguyen TN, 2010, PROCEDIA COMPUT SCI, V1, P2811, DOI 10.1016/j.procs.2010.08.006
   Pedreschi D., 2008, P 14 ACM SIGKDD INT, P560, DOI DOI 10.1145/1401890.1401959
   Sacin C. V., 2009, ED DATA MINING
   Sahebi S., 2015, 9 ACM C REC SYST REC, P131, DOI DOI 10.1145/2792838.2800188
   Smith E, 2011, BRIT EDUC RES J, V37, P993, DOI 10.1080/01411926.2010.515019
   Zafar Muhammad Bilal, 2017, ARXIV150705259
   Zemel R., 2013, JMLR P, P325
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402095
DA 2019-06-15
ER

PT S
AU Ye, NY
   Zhu, ZX
   Mantiuk, RK
AF Ye, Nanyang
   Zhu, Zhanxing
   Mantiuk, Rafal K.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Langevin Dynamics with Continuous Tempering for Training Deep Neural
   Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Minimizing non-convex and high-dimensional objective functions is challenging, especially when training modern deep neural networks. In this paper, a novel approach is proposed which divides the training process into two consecutive phases to obtain better generalization performance: Bayesian sampling and stochastic optimization. The first phase is to explore the energy landscape and to capture the 'fat" modes; and the second one is to fine-tune the parameter learned from the first phase. In the Bayesian learning phase, we apply continuous tempering and stochastic approximation into the Langevin dynamics to create an efficient and effective sampler, in which the temperature is adjusted automatically according to the designed "temperature dynamics". These strategies can overcome the challenge of early trapping into bad local minima and have achieved remarkable improvements in various types of neural networks as shown in our theoretical analysis and empirical experiments.
C1 [Ye, Nanyang; Mantiuk, Rafal K.] Univ Cambridge, Cambridge, England.
   [Zhu, Zhanxing] Peking Univ, Ctr Data Sci, BIBDR, Beijing, Peoples R China.
RP Ye, NY (reprint author), Univ Cambridge, Cambridge, England.
EM yn272@cam.ac.uk; zhanxing.zhu@pku.edu.cn; rafal.mantiuk@cl.cam.ac.uk
RI Jeong, Yongwook/N-7413-2016
CR Chen C., 2016, AISTATS
   Chen C., 2015, NIPS
   Chen T., 2014, P 31 INT C MACH LEAR, P1683
   Choromanska Anna, 2015, AISTATS
   Dauphin Yann N., 2014, NIPS
   GEMAN S, 1986, SIAM J CONTROL OPTIM, V24, P1031, DOI 10.1137/0324060
   Glorot X., 2010, JLMR P TRACK, P249, DOI DOI 10.1.1/207.2059
   Gobbo G, 2015, PHYS REV E, V91, DOI 10.1103/PhysRevE.91.061301
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   INGBER L, 1993, MATH COMPUT MODEL, V18, P29, DOI 10.1016/0895-7177(93)90204-C
   Karpathy A., 2015, CORR
   Kawaguchi K., 2016, ADV NEURAL INFORM PR, V29, P586
   Keskar N. S., 2016, ARXIV160904836
   Kingma D. P., 2014, ARXIV14126980
   KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671
   Laio A, 2002, P NATL ACAD SCI USA, V99, P12562, DOI 10.1073/pnas.202427399
   Lenner N, 2016, J CHEM THEORY COMPUT, V12, P486, DOI 10.1021/acs.jctc.5b00751
   Mattingly JC, 2010, SIAM J NUMER ANAL, V48, P552, DOI 10.1137/090770527
   Neelakantan Arvind, 2015, ARXIV151106807
   Rapaport D. C., 2004, ART MOL DYNAMICS SIM
   Saxe  A., 2014, ICLR
   Sutskever  I., 2013, ICML
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Welling M., 2011, ICML
   Zhang S., 2015, NIPS
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400059
DA 2019-06-15
ER

PT S
AU Yi, JF
   Hsieh, CJ
   Varshney, KR
   Zhang, LJ
   Li, Y
AF Yi, Jinfeng
   Hsieh, Cho-Jui
   Varshney, Kush R.
   Zhang, Lijun
   Li, Yao
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Scalable Demand-Aware Recommendation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Recommendation for e-commerce with a mix of durable and nondurable goods has characteristics that distinguish it from the well-studied media recommendation problem. The demand for items is a combined effect of form utility and time utility, i.e., a product must both be intrinsically appealing to a consumer and the time must be right for purchase. In particular for durable goods, time utility is a function of inter-purchase duration within product category because consumers are unlikely to purchase two items in the same category in close temporal succession. Moreover, purchase data, in contrast to rating data, is implicit with non-purchases not necessarily indicating dislike. Together, these issues give rise to the positive-unlabeled demand-aware recommendation problem that we pose via joint low-rank tensor completion and product category inter-purchase duration vector estimation. We further relax this problem and propose a highly scalable alternating minimization approach with which we can solve problems with millions of users and millions of items in a single thread. We also show superior prediction accuracies on multiple real-world datasets.
C1 [Yi, Jinfeng; Varshney, Kush R.] IBM Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA.
   [Hsieh, Cho-Jui; Li, Yao] Univ Calif Davis, Davis, CA 95616 USA.
   [Zhang, Lijun] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing, Jiangsu, Peoples R China.
   [Yi, Jinfeng] Tencent AI Lab, Bellevue, WA 98004 USA.
RP Yi, JF (reprint author), IBM Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA.; Yi, JF (reprint author), Tencent AI Lab, Bellevue, WA 98004 USA.
EM jinfengyi.ustc@gmail.com; chohsieh@ucdavis.edu; krvarshn@us.ibm.com;
   zhanglj@lamda.nju.edu.cn; yaoli@ucdavis.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [IIS-1719097]; Nvidia; TACC
FX Cho-Jui Hsieh and Yao Li acknowledge the support of NSF IIS-1719097,
   TACC and Nvidia.
CR Adomavicius G, 2011, RECOMMENDER SYSTEMS HANDBOOK, P217, DOI 10.1007/978-0-387-85820-3_7
   Baltrunas L., 2009, WORKSH CONT AW REC S
   Bing L, 2003, THIRD IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P179
   Bobadilla J, 2012, KNOWL-BASED SYST, V26, P225, DOI 10.1016/j.knosys.2011.07.021
   Campos PG, 2014, USER MODEL USER-ADAP, V24, P67, DOI 10.1007/s11257-012-9136-x
   Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   CHATFIELD C, 1973, J AM STAT ASSOC, V68, P828, DOI 10.2307/2284508
   Chi EC, 2012, SIAM J MATRIX ANAL A, V33, P1272, DOI 10.1137/110859063
   Du Nan, 2015, NIPS, P3474
   du Plessis M. C., 2014, NIPS, P703
   Gandy S, 2011, INVERSE PROBL, V27, DOI 10.1088/0266-5611/27/2/025010
   Grippo L, 2000, OPER RES LETT, V26, P127, DOI 10.1016/S0167-6377(99)00074-7
   Hsieh C. J., 2015, P 32 INT C MACH LEAR, P2445
   Hsieh C. J., 2014, ICML
   Hu YF, 2008, IEEE DATA MINING, P263, DOI 10.1109/ICDM.2008.22
   Jain P., 2014, ADV NEURAL INFORM PR, P1431
   Koren Y, 2010, COMMUN ACM, V53, P89, DOI 10.1145/1721654.1721677
   Lee Dokyun, 2014, P INT C INF SYST AUC
   Li B, 2011, P 22 INT JOINT C ART, P2293
   Liu J, 2013, IEEE T PATTERN ANAL, V35, P208, DOI 10.1109/TPAMI.2012.39
   Mnih A., 2008, P INT C MACH LEARN, V25, P880, DOI DOI 10.1145/1390156.1390267
   Narita A, 2011, LECT NOTES ARTIF INT, V6912, P501, DOI 10.1007/978-3-642-23783-6_32
   Rendle S., 2009, P 25 C UNC ART INT, P452, DOI DOI 10.1145/1772690.1772773
   Rennie J. D. M., 2005, P 22 INT C MACH LEAR, P713, DOI [10.1145/1102351.1102441, DOI 10.1145/1102351.1102441]
   Scott C, 2012, ELECTRON J STAT, V6, P958, DOI 10.1214/12-EJS699
   Sexton Robert L., 2013, EXPLORING EC
   STEINER RL, 1976, J MARKETING, V40, P2, DOI 10.2307/1249988
   Sun JZ, 2014, IEEE T SIGNAL PROCES, V62, P3499, DOI 10.1109/TSP.2014.2326618
   Wang Y., 2015, P 21 ACM SIGKDD INT, P1265, DOI DOI 10.1145/2783258.2783395
   Xiong L, 2010, P SIAM INT C DAT MIN, P211, DOI DOI 10.1137/1.9781611972801.19
   Yi J., 2013, 1 AAAI C HUM COMP CR
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402045
DA 2019-06-15
ER

PT S
AU You, S
   Ding, D
   Canini, K
   Pfeifer, J
   Gupta, MR
AF You, Seungil
   Ding, David
   Canini, Kevin
   Pfeifer, Jan
   Gupta, Maya R.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Deep Lattice Networks and Partial Monotonic Functions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We propose learning deep models that are monotonic with respect to a user-specified set of inputs by alternating layers of linear embeddings, ensembles of lattices, and calibrators (piecewise linear functions), with appropriate constraints for monotonicity, and jointly training the resulting network. We implement the layers and projections with new computational graph nodes in TensorFlow and use the Adam optimizer and batched stochastic gradients. Experiments on benchmark and real-world datasets show that six-layer monotonic deep lattice networks achieve state-of-the art performance for classification and regression with monotonicity guarantees.
C1 [You, Seungil; Ding, David; Canini, Kevin; Pfeifer, Jan; Gupta, Maya R.] Google Res, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA.
RP You, S (reprint author), Google Res, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA.
EM siyou@google.com; dwding@google.com; canini@google.com;
   janpf@google.com; mayagupta@google.com
RI Jeong, Yongwook/N-7413-2016
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   ARCHER NP, 1993, DECISION SCI, V24, P60, DOI 10.1111/j.1540-5915.1993.tb00462.x
   Armstrong W. W., 1996, HDB NEURAL COMPUTATI
   AYER M, 1955, ANN MATH STAT, V26, P641, DOI 10.1214/aoms/1177728423
   Blake C., 1998, UCI REPOSITORY MACHI
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Canini K., 2016, ADV NEURAL INFORM PR
   Daniels H, 2010, IEEE T NEURAL NETWOR, V21, P906, DOI 10.1109/TNN.2010.2044803
   Dugas C., 2009, J MACHINE LEARNING R
   Garcia E. K., 2009, ADV NEURAL INFORM PR
   Groeneboom P., 2014, NONPARAMETRIC ESTIMA
   Gupta M. R., 2016, J MACHINE LEARNING R, V17, P1
   Howard A., 2007, ADV NEURAL INFORM PR
   Kay H, 2000, AICHE J, V46, P2426, DOI 10.1002/aic.690461211
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Lovasz L., 1983, MATH PROGRAMMING STA, P235, DOI DOI 10.1007/978-3-642-68874-4_10
   Minin A, 2010, NEURAL NETWORKS, V23, P471, DOI 10.1016/j.neunet.2009.09.002
   Sill J., 1998, ADV NEURAL INFORM PR
   Wang S., 1994, NEURAL COMPUT APPL, V2, P160
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403005
DA 2019-06-15
ER

PT S
AU Yousefnezhad, M
   Zhang, DQ
AF Yousefnezhad, Muhammad
   Zhang, Daoqiang
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Deep Hyperalignment
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB This paper proposes Deep Hyperalignment (DHA) as a regularized, deep extension, scalable Hyperalignment (HA) method, which is well-suited for applying functional alignment to fMRI datasets with nonlinearity, high-dimensionality (broad ROI), and a large number of subjects. Unlink previous methods, DHA is not limited by a restricted fixed kernel function. Further, it uses a parametric approach, rank-m Singular Value Decomposition (SVD), and stochastic gradient descent for optimization. Therefore, DHA has a suitable time complexity for large datasets, and DHA does not require the training data when it computes the functional alignment for a new subject. Experimental studies on multi-subject fMRI analysis confirm that the DHA method achieves superior performance to other state-of-the-art HA algorithms.
C1 [Yousefnezhad, Muhammad; Zhang, Daoqiang] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing, Jiangsu, Peoples R China.
RP Yousefnezhad, M (reprint author), Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing, Jiangsu, Peoples R China.
EM myousefnezhad@nuaa.edu.cn; dgzhang@nuaa.edu.cn
RI Jeong, Yongwook/N-7413-2016
FU National Natural Science Foundation of China [61422204, 61473149,
   61732006]; NUAA Fundamental Research Funds [NE2013105]
FX This work was supported in part by the National Natural Science
   Foundation of China (61422204, 61473149, and 61732006), and NUAA
   Fundamental Research Funds (NE2013105).
CR Andrew G., 2012, 30 INT C MACH LEARN, P1247
   Benton A., 2017, 5 INT C LEARN REPR I
   Brand M, 2002, LECT NOTES COMPUT SC, V2350, P707
   Chen P. -C., 2014, 2014 POW SYST COMP C, P1
   Chen P.-H., 2015, ADV NEURAL INFORM PR, V28, P460
   Chen P. H., 2016, 29 WORKSH REPR LEARN
   Duncan KJ, 2009, NEUROIMAGE, V46, P1018, DOI 10.1016/j.neuroimage.2009.03.014
   Guntupalli J. S., 2016, CEREBRAL CORTEX
   Haxby JV, 2014, ANNU REV NEUROSCI, V37, P435, DOI 10.1146/annurev-neuro-062012-170325
   Langs G., 2010, 23 ADV NEURAL INFORM
   Lorbert A., 2012, ADV NEUR INF PROC SY, P1790
   Rastogi P., 2015, P ANN C N AM CHAPT A, P556, DOI DOI 10.3115/V1/N15-1058
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Smola AJ, 2004, STAT COMPUT, V14, P199, DOI 10.1023/B:STCO.0000035301.49549.88
   Tom SM, 2007, SCIENCE, V315, P515, DOI 10.1126/science.1134239
   Wakeman DG, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.1
   Walz JM, 2013, J NEUROSCI, V33, P19212, DOI 10.1523/JNEUROSCI.2649-13.2013
   Wang W., 53 ANN ALL C COMM CO, P688
   Xu H, 2012, 2012 IEEE STATISTICAL SIGNAL PROCESSING WORKSHOP (SSP), P229, DOI 10.1109/SSP.2012.6319668
   Yousefnezhad M., 2017, 34 AAAI C ART INT AA, P59
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401062
DA 2019-06-15
ER

PT S
AU Yu, HZ
   Li, TX
   Varshney, LR
AF Yu, Haizi
   Li, Tianxi
   Varshney, Lay R.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Probabilistic Rule Realization and Selection
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID EXPERT-SYSTEM; REGRESSION
AB A Abstraction and realization are bilateral processes that are key in deriving intelligence and creativity. In many domains, the two processes are approached through rules: high-level principles that reveal invariances within similar yet diverse examples. Under a probabilistic setting for discrete input spaces, we focus on the rule realization problem which generates input sample distributions that follow the given rules. More ambitiously, we go beyond a mechanical realization that takes whatever is given, but instead ask for proactively selecting reasonable rules to realize. This goal is demanding in practice, since the initial rule set may not always be consistent and thus intelligent compromises are needed. We formulate both rule realization and selection as two strongly connected components within a single and symmetric bi-convex problem, and derive an efficient algorithm that works at large scale. Taking music compositional rules as the main example throughout the paper, we demonstrate our model's efficiency in not only music realization (composition) but also music interpretation and understanding (analysis).
C1 [Yu, Haizi] Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA.
   [Li, Tianxi] Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA.
   [Varshney, Lay R.] Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA.
RP Yu, HZ (reprint author), Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA.
EM haiziyu7@illinois.edu; tianxili@umich.edu; varshney@illinois.edu
RI Jeong, Yongwook/N-7413-2016
FU IBM-Illinois Center for Cognitive Computing Systems Research (C3SR), as
   part of the IBM Cognitive Horizons Network
FX Supported in part by the IBM-Illinois Center for Cognitive Computing
   Systems Research (C3SR), a research collaboration as part of the IBM
   Cognitive Horizons Network.
CR Barry A., 1997, VISUAL INTELLIGENCE
   Bengio Yoshua, 2013, Statistical Language and Speech Processing. First International Conference, SLSP 2013. Proceedings: LNCS 7978, P1, DOI 10.1007/978-3-642-39593-2_1
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   COPE D, 1987, COMPUT MUSIC J, V11, P30, DOI 10.2307/3680238
   EBCIOGLU K, 1988, COMPUT MUSIC J, V12, P43, DOI 10.2307/3680335
   Efron B, 2004, ANN STAT, V32, P407
   Friedman J., 2010, NOTE GROUP LASSO SPA
   Fux J. J, 1725, GRADUS AD PARNASSUM
   Haase K., 1987, 293 MIT AI LAB
   Hastie T, 2007, ELECTRON J STAT, V1, P1, DOI 10.1214/07-EJS004
   Hong M., 2012, MATH PROGRAM, V162, P1
   JACKSON DD, 1972, GEOPHYS J ROY ASTR S, V28, P97, DOI 10.1111/j.1365-246X.1972.tb06115.x
   Lewin Kurt, 1951, FIELD THEORY SOCIAL
   Pierce J. R., 1949, MM4915029 BEL TEL LA
   Schenker H., 1922, KONTRAPUNKT
   Skorstad J., 1988, P 10 ANN C COGN SCI, P419
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Tibshirani R, 2012, J R STAT SOC B, V74, P245, DOI 10.1111/j.1467-9868.2011.01004.x
   Wang  Jie, 2014, ADV NEURAL INFORM PR, P2132
   Wang W., 2013, ARXIV13091541CSLG
   Yu H., 2017, P 5 INT C LEARN REPR
   Yu H, 2016, IEEE INT WORKS MACH
   Yu H, 2016, 2016 FIRST INTERNATIONAL WORKSHOP ON SENSING, PROCESSING AND LEARNING FOR INTELLIGENT MACHINES (SPLINE)
   Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401058
DA 2019-06-15
ER

PT S
AU Yu, H
   Neely, MJ
   Wei, XH
AF Yu, Hao
   Neely, Michael J.
   Wei, Xiaohan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Online Convex Optimization with Stochastic Constraints
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID GRADIENT; REGRET; BOUNDS; TIME
AB This paper considers online convex optimization (OCO) with stochastic constraints, which generalizes Zinkevich's OCO over a known simple fixed set by introducing multiple stochastic functional constraints that are i.i.d. generated at each round and are disclosed to the decision maker only after the decision is made. This formulation arises naturally when decisions are restricted by stochastic environments or deterministic environments with noisy observations. It also includes many important problems as special case, such as OCO with long term constraints, stochastic constrained convex optimization, and deterministic constrained convex optimization. To solve this problem, this paper proposes a new algorithm that achieves O(root T) expected regret and constraint violations and O(root T log(T)) high probability regret and constraint violations. Experiments on a real-world data center scheduling problem further verify the performance of the new algorithm.
C1 [Yu, Hao; Neely, Michael J.; Wei, Xiaohan] Univ Southern Calif, Dept Elect Engn, Los Angeles, CA 90007 USA.
RP Yu, H (reprint author), Univ Southern Calif, Dept Elect Engn, Los Angeles, CA 90007 USA.
EM yuhao@usc.edu; mjneely@usc.edu; xiaohanw@usc.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [CCF-1718477]
FX This work is supported in part by grant NSF CCF-1718477.
CR Bartlett Peter L, 2008, P C LEARN THEOR COLT
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   CesaBianchi N, 1996, IEEE T NEURAL NETWOR, V7, P604, DOI 10.1109/72.501719
   Cotter Andrew, 2015, P C LEARN THEOR COLT
   Doob J. L., 1953, STOCHASTIC PROCESSES
   Durrett R., 2010, PROBABILITY THEORY E
   Gandhi A, 2012, 2012 INTERNATIONAL GREEN COMPUTING CONFERENCE (IGCC)
   Gordon Geoffrey J, 1999, P C LEARN THEOR COLT
   HAJEK B, 1982, ADV APPL PROBAB, V14, P502, DOI 10.2307/1426671
   Hao Yu, 2016, ARXIV160402218
   Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013
   Hazan E, 2007, MACH LEARN, V69, P169, DOI 10.1007/s10994-007-5016-8
   Jenatton Rodolphe, 2016, P INT C MACH LEARN I
   Kivinen J, 1997, INFORM COMPUT, V132, P1, DOI 10.1006/inco.1996.2612
   Lan G., 2016, ARXIV160403887
   Mahdavi  M., 2013, ADV NEURAL INFORM PR
   Mahdavi M, 2012, J MACH LEARN RES, V13, P2503
   Mannor S, 2009, J MACH LEARN RES, V10, P569
   Nedic A, 2009, J OPTIMIZ THEORY APP, V142, P205, DOI 10.1007/s10957-009-9522-7
   Neely MJ, 2016, IEEE ACM T NETWORK, V24, P2223, DOI 10.1109/TNET.2015.2449323
   Nesterov Y., 2004, INTRO LECT CONVEX OP
   Qureshi A., 2009, ACM SIGCOMM
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Tao T, 2015, ANN PROBAB, V43, P782, DOI 10.1214/13-AOP876
   Tse David, 2005, FUNDAMENTALS WIRELES
   Vu VH, 2002, RANDOM STRUCT ALGOR, V20, P262, DOI 10.1002/rsa.10032
   Yu H, 2017, SIAM J OPTIMIZ, V27, P759, DOI 10.1137/16M1059011
   Zinkevich Martin, 2003, P INT C MACH LEARN I
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401045
DA 2019-06-15
ER

PT S
AU Yu, HF
   Hsieh, CJ
   Lei, Q
   Dhillon, IS
AF Yu, Hsiang-Fu
   Hsieh, Cho-Jui
   Lei, Qi
   Dhillon, Inderjit S.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI A Greedy Approach for Budgeted Maximum Inner Product Search
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Maximum Inner Product Search (MIPS) is an important task in many machine learning applications such as the prediction phase of low-rank matrix factorization models and deep learning models. Recently, there has been substantial research on how to perform MIPS in sub-linear time, but most of the existing work does not have the flexibility to control the trade-off between search efficiency and search quality. In this paper, we study the important problem of MIPS with a computational budget. By carefully studying the problem structure of MIPS, we develop a novel Greedy-MIPS algorithm, which can handle budgeted MIPS by design. While simple and intuitive, Greedy-MIPS yields surprisingly superior performance compared to state-of-the-art approaches. As a specific example, on a candidate set containing half a million vectors of dimension 200, Greedy-MIPS runs 200x faster than the naive approach while yielding search results with the top-5 precision greater than 75%.
C1 [Yu, Hsiang-Fu] Amazon Inc, Seattle, WA 98109 USA.
   [Hsieh, Cho-Jui] Univ Calif Davis, Davis, CA 95616 USA.
   [Yu, Hsiang-Fu; Lei, Qi; Dhillon, Inderjit S.] Univ Texas Austin, Austin, TX 78712 USA.
RP Yu, HF (reprint author), Amazon Inc, Seattle, WA 98109 USA.
EM rofuyu@cs.utexas.edu; chohsieh@ucdavis.edu; leiqi@ices.utexas.edu;
   inderjit@cs.utexas.edu
RI Jeong, Yongwook/N-7413-2016
FU NSF [CCF-1320746, IIS-1546452, CCF-1564000, RI-1719097]
FX This research was supported by NSF grants CCF-1320746, IIS-1546452 and
   CCF-1564000. CJH was supported by NSF grant RI-1719097.
CR Auvolat Alex, 2016, ARXIV150705910
   Bachrach Y., 2014, RECSYS, P257, DOI 10.1145/2645710.2645741
   Ballard Grey, 2015, P IEEE INT C DAT MIN
   Chin Wei-Sheng, 2015, P PAC AS C KNOWL DIS
   Cohen E, 1997, PROCEEDINGS OF THE EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P682
   Covington  P., 2016, P 10 ACM C REC SYST
   Dror G, 2011, P 2011 INT C KDD CUP, V18, P3
   Knuth Donald E., 1998, ART CMOPUTER PROGRAM, V3
   Koenigstein N., 2012, CIKM, P535
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Mussmann Stephen, 2016, P 33 INT C INT C MAC
   Neyshabur B., 2015, ICML, P1926
   Ram P., 2012, P 18 ACM SIGKDD INT, P931
   Shrivastava A, 2014, NIPS, V27, P2321
   Shrivastava Anshumali, 2015, P 31 C UNC ART INT U, P812
   Weston J, 2010, MACH LEARN, V81, P21, DOI 10.1007/s10994-010-5198-3
   Yu H., 2014, P 31 INT C MACH LEAR, P593
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405052
DA 2019-06-15
ER

PT S
AU Yu, Q
   Maddah-Ali, MA
   Avestimehr, AS
AF Yu, Qian
   Maddah-Ali, Mohammad Ali
   Avestimehr, A. Salman
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Polynomial Codes: an Optimal Design for High-Dimensional Coded Matrix
   Multiplication
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We consider a large-scale matrix multiplication problem where the computation is carried out using a distributed system with a master node and multiple worker nodes, where each worker can store parts of the input matrices. We propose a computation strategy that leverages ideas from coding theory to design intermediate computations at the worker nodes, in order to optimally deal with straggling workers. The proposed strategy, named as polynomial codes, achieves the optimum recovery threshold, defined as the minimum number of workers that the master needs to wait for in order to compute the output. This is the first code that achieves the optimal utilization of redundancy for tolerating stragglers or failures in distributed matrix multiplication. Furthermore, by leveraging the algebraic structure of polynomial codes, we can map the reconstruction problem of the final output to a polynomial interpolation problem, which can be solved efficiently. Polynomial codes provide order-wise improvement over the state of the art in terms of recovery threshold, and are also optimal in terms of several other metrics including computation latency and communication load. Moreover, we extend this code to distributed convolution and show its order-wise optimality.
C1 [Yu, Qian; Avestimehr, A. Salman] Univ Southern Calif, Dept Elect Engn, Los Angeles, CA 90089 USA.
   [Maddah-Ali, Mohammad Ali] Nokia Bell Labs, Holmdel, NJ USA.
RP Yu, Q (reprint author), Univ Southern Calif, Dept Elect Engn, Los Angeles, CA 90089 USA.
RI Avestimehr, Amir Salman/O-7864-2019; Jeong, Yongwook/N-7413-2016
FU NSF [CCF-1408639, NETS-1419632]; ONR [N000141612189]; NSA grant; Defense
   Advanced Research Projects Agency (DARPA) [HR001117C0053]
FX This work is in part supported by NSF grants CCF-1408639, NETS-1419632,
   ONR award N000141612189, NSA grant, and a research gift from Intel. This
   material is based upon work supported by Defense Advanced Research
   Projects Agency (DARPA) under Contract No. HR001117C0053. The views,
   opinions, and/or findings expressed are those of the author(s) and
   should not be interpreted as representing the official views or policies
   of the Department of Defense or the U.S. Government.
CR Baktir S., 2006, P 44 ANN SE REG C AC, P549
   Dean J, 2013, COMMUN ACM, V56, P74, DOI 10.1145/2408776.2408794
   Dean Jeffrey, 2004, 6 USENIX S OP SYST D
   Didier F., 1886, ARXIV09011886
   Dutta  S., 2017, ARXIV170503875
   HUANG KH, 1984, IEEE T COMPUT, V33, P518, DOI 10.1109/TC.1984.1676475
   JOU JY, 1986, P IEEE, V74, P732
   Kedlaya KS, 2011, SIAM J COMPUT, V40, P1767, DOI 10.1137/08073408X
   Lee K., 2015, ARXIV151202673
   Lee K, 2017, IEEE INT SYMP INFO, P2418, DOI 10.1109/ISIT.2017.8006963
   Li S., 2016, ARXIV160901690
   Li SS, 2015, PROCEDIA ENGINEER, V119, P53, DOI 10.1016/j.proeng.2015.08.853
   Li SZ, 2018, IEEE T INFORM THEORY, V64, P109, DOI 10.1109/TIT.2017.2756959
   Reisizadehmobarakeh A., 2017, ARXIV170105973
   Roth R. M., 2006, INTRO CODING THEORY
   SINGLETON RC, 1964, IEEE T INFORM THEORY, V10, P116, DOI 10.1109/TIT.1964.1053661
   Tandon R, 2016, ARXIV161203301
   Wan FY, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P160, DOI 10.1109/ICPHM.2017.7998322
   Yu Q., 2018, ARXIV180107487
   Zaharia M, 2010, HOTCLOUD, P10, DOI DOI 10.HTTP://DL.ACM.0RG/CITATI0N.CFM?
   Zaharia M., 2008, OSDI, P7
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404046
DA 2019-06-15
ER

PT S
AU Yurochkin, M
   Nguyen, X
   Vasiloglou, N
AF Yurochkin, Mikhail
   Nguyen, XuanLong
   Vasiloglou, Nikolaos
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Multi-way Interacting Regression via Factorization Machines
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID VARIABLE SELECTION
AB We propose a Bayesian regression method that accounts for multi-way interactions of arbitrary orders among the predictor variables. Our model makes use of a factorization mechanism for representing the regression coefficients of interactions among the predictors, while the interaction selection is guided by a prior distribution on random hypergraphs, a construction which generalizes the Finite Feature Model. We present a posterior inference algorithm based on Gibbs sampling, and establish posterior consistency of our regression model. Our method is evaluated with extensive experiments on simulated data and demonstrated to be able to identify meaningful interactions in applications in genetics and retail demand forecasting.(1)
C1 [Yurochkin, Mikhail; Nguyen, XuanLong] Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA.
   [Vasiloglou, Nikolaos] LogicBlox, Atlanta, GA USA.
RP Yurochkin, M (reprint author), Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA.
EM moonfolk@umich.edu; xuanlong@umich.edu;
   nikolaos.vasiloglou@logicblox.com
RI Jeong, Yongwook/N-7413-2016
FU NSF CAREER [DMS-1351362]; NSF [CNS-1409303]; Margaret and Herman Sokol
   Faculty Award
FX This research is supported in part by grants NSF CAREER DMS-1351362, NSF
   CNS-1409303, a research gift from Adobe Research and a Margaret and
   Herman Sokol Faculty Award.
CR Ai CR, 2003, ECON LETT, V80, P123, DOI 10.1016/S0165-1765(03)00032-6
   Brambor T, 2006, POLIT ANAL, V14, P63, DOI 10.1093/pan/mpi014
   Cheng C, 2014, P 8 ACM C REC SYST, P265
   Cordell HJ, 2009, NAT REV GENET, V10, P392, DOI 10.1038/nrg2579
   Cristianini N., 2000, INTRO SUPPORT VECTOR
   Fan JQ, 2010, STAT SINICA, V20, P101
   Freudenthaler Christoph, 2011, BAYESIAN FACTORIZATI
   Ghosal S, 1999, ANN STAT, V27, P143
   Griffiths T., 2005, NEURAL INFORM PROCES, P475
   Griffiths TL, 2011, J MACH LEARN RES, V12, P1185
   Harshman RA, 1970, FDN PARAFAC PROCEDUR
   Himmelstein DS, 2011, BIODATA MIN, V4, DOI 10.1186/1756-0381-4-21
   Nguyen TV, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P63, DOI 10.1145/2600428.2609623
   Rendle Steffen, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P995, DOI 10.1109/ICDM.2010.127
   Rendle S., 2011, P 34 INT ACM SIGIR C, P635, DOI DOI 10.1145/2009916.2010002
   Templeton A. R., 2000, EPISTASIS EVOLUTIONA, P41
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Zhu J, 2004, ADV NEUR IN, V16, P49
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402063
DA 2019-06-15
ER

PT S
AU Yurochkin, M
   Guha, A
   Nguyen, X
AF Yurochkin, Mikhail
   Guha, Aritra
   Nguyen, XuanLong
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Conic Scan-and-Cover algorithms for nonparametric topic modeling
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We propose new algorithms for topic modeling when the number of topics is unknown. Our approach relies on an analysis of the concentration of mass and angular geometry of the topic simplex, a convex polytope constructed by taking the convex hull of vertices representing the latent topics. Our algorithms are shown in practice to have accuracy comparable to a Gibbs sampler in terms of topic estimation, which requires the number of topics be given. Moreover, they are one of the fastest among several state of the art parametric techniques.(1) Statistical consistency of our estimator is established under some conditions.
C1 [Yurochkin, Mikhail; Guha, Aritra; Nguyen, XuanLong] Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA.
RP Yurochkin, M (reprint author), Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA.
EM moonfolk@umich.edu; aritra@umich.edu; xuanlong@umich.edu
RI Jeong, Yongwook/N-7413-2016
FU Margaret and Herman Sokol Faculty Award;  [NSF CAREER DMS-1351362]; 
   [NSF CNS-1409303]
FX This research is supported in part by grants NSF CAREER DMS-1351362, NSF
   CNS-1409303, a research gift from Adobe Research and a Margaret and
   Herman Sokol Faculty Award.
CR Anandkumar  A., 2012, NIPS
   Arora S., 2012, ARXIV12124777
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Hsu Wei-Shou, 2016, ADV NEURAL INFORM PR, P4529
   Nguyen XL, 2015, BERNOULLI, V21, P618, DOI 10.3150/13-BEJ582
   Pritchard JK, 2000, GENETICS, V155, P945
   Tang J., 2014, P 31 INT C MACH LEAR, V32, P190
   Teh Yee Whye, 2006, J AM STAT ASS, V101
   Xu W., 2003, P 26 SIGIR TOR ON CA, P267, DOI DOI 10.1145/860435.860485
   Yurochkin M., 2016, ADV NEURAL INFORM PR, P2505
NR 13
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403091
DA 2019-06-15
ER

PT S
AU Zafar, MB
   Valera, I
   Rodriguez, MG
   Gummadi, KP
   Weller, A
AF Zafar, Muhammad Bilal
   Valera, Isabel
   Rodriguez, Manuel Gomez
   Gummadi, Krishna P.
   Weller, Adrian
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI From Parity to Preference-based Notions of Fairness in Classification
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB The adoption of automated, data-driven decision making in an ever expanding range of applications has raised concerns about its potential unfairness towards certain social groups. In this context, a number of recent studies have focused on defining, detecting, and removing unfairness from data-driven decision systems. However, the existing notions of fairness, based on parity (equality) in treatment or outcomes for different social groups, tend to be quite stringent, limiting the overall decision making accuracy. In this paper, we draw inspiration from the fair-division and envy-freeness literature in economics and game theory and propose preference-based notions of fairness-given the choice between various sets of decision treatments or outcomes, any group of users would collectively prefer its treatment or outcomes, regardless of the (dis)parity as compared to the other groups. Then, we introduce tractable proxies to design margin-based classifiers that satisfy these preference-based notions of fairness. Finally, we experiment with a variety of synthetic and real-world datasets and show that preference-based fairness allows for greater decision accuracy than parity-based fairness.
C1 [Zafar, Muhammad Bilal; Rodriguez, Manuel Gomez; Gummadi, Krishna P.] MPI SWS, Saarbrucken, Germany.
   [Valera, Isabel] MPI IS, Stuttgart, Germany.
   [Weller, Adrian] Univ Cambridge, Cambridge, England.
   [Weller, Adrian] Alan Turing Inst, London, England.
RP Zafar, MB (reprint author), MPI SWS, Saarbrucken, Germany.
EM mzafar@mpi-sws.org; isabel.valera@tue.mpg.de; manuelgr@mpi-sws.org;
   gummadi@mpi-sws.org; aw665@cam.ac.uk
RI Jeong, Yongwook/N-7413-2016
FU Alan Turing Institute under EPSRC grant [EP/N510129/1]; Leverhulme Trust
   via the CFI
FX AW acknowledges support by the Alan Turing Institute under EPSRC grant
   EP/N510129/1, and by the Leverhulme Trust via the CFI.
CR Altman Andrew, 2016, STANFORD ENCY PHILOS
   [Anonymous], 1996, ADULT DATA
   [Anonymous], 2017, STOP QUESTION FRISK
   Barocas Solon, 2016, CALIFORNIA LAW REV
   Berliant M., 1992, J MATH EC
   Bishop C. M., 2006, PATTERN RECOGNITION
   Calders Toon, 2010, DATA MINING KNOWLEDG
   Chapelle O., 2007, NEURAL COMPUTATION
   Chouldechova Alexandra, 2016, ARXIV161007524
   Corbett-Davies Sam, 2017, KDD
   Duan JC, 2012, SPR HBK COMPU STAT, P1, DOI 10.1007/978-3-642-17254-0
   Dwork Cynthia, 2017, ARXIV170706613
   Dwork Cynthia, 2012, ITCSC
   Feldman Michael, 2015, KDD
   Friedler Sorelle A, 2016, ARXIV160907236
   Goel R. S. Sharad, 2015, ANN APPL STAT
   Goh Gabriel, 2016, NIPS
   Hardt M., 2016, NIPS
   Joseph M., 2016, NIPS
   Kamiran Faisal, 2010, BENELEARN
   Kamishima Toshihiro, 2011, PADM
   Kleinberg Jon, 2017, ITCS
   Luong Binh Thanh, 2011, KDD
   Munoz Cecilia, 2016, BIG DATA REPORT ALGO
   Nash Jr J. F, 1950, ECONOMETRICA J ECONO
   NYCLU, 2017, STOP AND FRISK DAT
   Pedreschi Dino, 2008, KDD
   Shen Xinyue, 2016, ARXIV160402639
   Varian H. R., 1974, J EC THEORY
   Zafar Muhammad Bilal, 2017, WWW
   Zafar Muhammad Bilal, 2017, AISTATS
   Zemel R., 2013, ICML
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400022
DA 2019-06-15
ER

PT S
AU Zaheer, M
   Kottur, S
   Ravanbhakhsh, S
   Poczos, B
   Salakhutdinov, R
   Smola, AJ
AF Zaheer, Manzil
   Kottur, Satwik
   Ravanbhakhsh, Siamak
   Poczos, Barnabas
   Salakhutdinov, Ruslan
   Smola, Alexander J.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Deep Sets
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We study the problem of designing models for machine learning tasks defined on sets. In contrast to traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets that are invariant to permutations. Such problems are widespread, ranging from estimation of population statistics [1], to anomaly detection in piezometer data of embankment dams [2], to cosmology [3, 4]. Our main theorem characterizes the permutation invariant functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We also derive the necessary and sufficient conditions for permutation equivariance in deep models. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.
C1 [Zaheer, Manzil; Kottur, Satwik; Ravanbhakhsh, Siamak; Poczos, Barnabas; Salakhutdinov, Ruslan; Smola, Alexander J.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Zaheer, Manzil; Smola, Alexander J.] Amazon Web Serv, Seattle, WA 98108 USA.
RP Zaheer, M (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.; Zaheer, M (reprint author), Amazon Web Serv, Seattle, WA 98108 USA.
EM manzilz@cs.cmu.edu; skottur@cs.cmu.edu; mravanba@cs.cmu.edu;
   bapoczos@cs.cmu.edu; rsalakhu@cs.cmu.edu; smola@cs.cmu.edu
CR Anandkumar A., 2012, ARXIV12107559, P3
   Binney J., 1998, PR S ASTROP
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Bourbaki Nicolas, 1990, ELEMENTS MATH THEORI, V1, P15
   Brock Andrew, 2016, ARXIV160804236, p[5, 26]
   Chang Angel X, 2015, ARXIV151203012, P5
   Chang Michael B, 2016, ARXIV161200341, P3
   Chen M., 2013, P 30 INT C MACH LEAR, P1274
   Chen X., 2014, ADV NEURAL INFORM PR, P1709
   Clevert Djork-Arne, 2015, ARXIV151107289, P27
   Cohen T. S., 2016, ARXIV160207576
   Connolly AJ, 1995, ASTROPH9508100 ARXIV, p[5, 25]
   Curgus B, 2006, EXPO MATH, V24, P81, DOI 10.1016/j.exmath.2005.07.001
   Faber FA, 2016, PHYS REV LETT, V117, DOI 10.1103/PhysRevLett.117.135502
   Feng SL, 2004, PROC CVPR IEEE, P1002
   Gens Robert, 2014, ADV NEURAL INFORM PR, P2537
   Ghahramani Zoubin, 2005, NIPS, V2, p[22, 6, 7, 20, 21, 22]
   Grubinger Michael, 2007, THESIS, P23
   Guillaumin M, 2009, IEEE I CONF COMP VIS, P309, DOI 10.1109/ICCV.2009.5459266
   Guttenberg Nicholas, 2016, ARXIV161204530, P3
   Hartford Jason S, 2016, ADV NEURAL INFORM PR, p[2424, 3]
   Jung I., 2015, ADV ENG INFORM, P1
   Khesin Boris A, 2014, ARNOLD SWIMMING TIDE, V86, P15
   King DR, 2014, ADV MERGERS ACQUIS, V13, P25, DOI 10.1108/S1479-361X20140000013000
   Lin HW, 2004, COMPUT AIDED DESIGN, V36, P1, DOI 10.1016/S0010-4485(03)00064-2
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740
   Liu Ziwei, 2015, P INT C COMP VIS ICC, P8
   Loosli G., 2007, LARGE SCALE KERNEL M, p[301, 6]
   Lopez-Paz David, 2016, ARXIV160508179, P3
   Makadia A, 2008, LECT NOTES COMPUT SC, V5304, P316, DOI 10.1007/978-3-540-88690-7_24
   Marsden Jerrold E, 1993, ELEMENTARY CLASSICAL, P15
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   MICCHELLI CA, 1986, CONSTR APPROX, V2, P11, DOI 10.1007/BF01893414
   Mikolov T., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951
   Muandet K., 2013, P 30 INT C MACH LEAR
   Muandet K., 2012, P 26 ANN C NEUR INF, p[1, 3]
   Ntampaka M., 2016, ASTROPHYS J, P1
   Oliva J, 2013, INFECTIOUS FOREST DISEASES, P1, DOI 10.1079/9781780640402.0001
   Poczos B., 2013, JMLR WORKSHOP C P, P1
   Poczos B., 2012, SUPPORT DISTRIBUTION, p[3, 4]
   Pritchard JK, 2000, GENETICS, V155, P945
   Ravanbakhsh M., 2016, INT C MACH LEARN ICM, P1
   Ravanbakhsh Siamak, 2017, ARXIV170208389, P3
   Ravanbakhsh Siamak, 2016, P 33 INT C MACH LEAR, P5
   Rozo E, 2014, ASTROPHYS J, V783, DOI 10.1088/0004-637X/783/2/.0
   Rusu R.B, 2011, IEEE INT C ROB AUT I
   Shi BG, 2015, IEEE SIGNAL PROC LET, V22, P2339, DOI 10.1109/LSP.2015.2480802
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Sukhbaatar S, 2016, ADV NEURAL INFORM PR, V29, P2244
   Szabo Z., 2016, J MACHINE LEARNING R
   Taskar B, 2004, ADV NEUR IN, V16, P25
   Vinyals Oriol, 2015, ARXIV151106391, P3
   von Ahn Luis, 2004, P SIGCHI C HUM FACT, P319, DOI DOI 10.1145/985692.985733
   Wu Jiajun, 2016, ARXIV161007584, p[5, 26]
   WU ZR, 2015, PROC CVPR IEEE, P1912, DOI DOI 10.1109/CVPR.2015.7298801
NR 55
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403045
DA 2019-06-15
ER

PT S
AU Zanca, D
   Gori, M
AF Zanca, Dario
   Gori, Marco
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Variational Laws of Visual Attention for Dynamic Scenes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Computational models of visual attention are at the crossroad of disciplines like cognitive science, computational neuroscience, and computer vision. This paper proposes a model of attentional scanpath that is based on the principle that there are foundational laws that drive the emergence of visual attention. We devise variational laws of the eye-movement that rely on a generalized view of the Least Action Principle in physics. The potential energy captures details as well as peripheral visual features, while the kinetic energy corresponds with the classic interpretation in analytic mechanics. In addition, the Lagrangian contains a brightness invariance term, which characterizes significantly the scanpath trajectories. We obtain differential equations of visual attention as the stationary point of the generalized action, and we propose an algorithm to estimate the model parameters. Finally, we report experimental results to validate the model in tasks of saliency detection.
C1 [Zanca, Dario] Univ Florence, DINFO, Florence, Italy.
   [Zanca, Dario; Gori, Marco] Univ Siena, DIISM, Siena, Italy.
RP Zanca, D (reprint author), Univ Florence, DINFO, Florence, Italy.; Zanca, D (reprint author), Univ Siena, DIISM, Siena, Italy.
EM dario.zanca@unifi.it; marco@diism.unisi.it
RI Jeong, Yongwook/N-7413-2016
CR Borji A., 2015, ARXIV150503581
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Bruce N., 2007, J VIS, V7
   Bylinskii Z., MIT SALIENCY BENCHMA
   Bylinskii Zoya, 2016, ARXIV160403605
   Connor CE, 2004, CURR BIOL, V14, pR850, DOI 10.1016/j.cub.2004.09.041
   Cornia  M., 2016, PREDICTING HUMAN EYE
   Garcia-Diaz A, 2012, J VISION, V12, DOI 10.1167/12.6.17
   Gelfand I. M., 1993, CALCULUS VARIATION
   Hadizadeh H., 2012, IEEE T IMAGE PROCESS
   HAINLINE L, 1984, VISION RES, V24, P1771, DOI 10.1016/0042-6989(84)90008-7
   Harel J., SALIENCY IMPLEMENTAT
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Itti L, 2009, VISION RES, V49, P1295, DOI 10.1016/j.visres.2008.09.007
   Jones E., 2001, SCIPY OPEN SOURCE SC
   Judd T., 2012, BENCHMARK COMPUTATIO
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Kruthiventi S. S. S, 2015, ARXIV151002927
   Le Meur O, 2015, VISION RES, V116, P152, DOI 10.1016/j.visres.2014.12.026
   Maggini M, 2016, LECT NOTES COMPUT SC, V10037, P321, DOI 10.1007/978-3-319-49130-1_24
   McMains S, 2011, J NEUROSCI, V31, P587, DOI 10.1523/JNEUROSCI.3766-10.2011
   Ponpandi SD, 2013, PROC INT CONF RECON
   Tatler BW, 2005, VISION RES, V45, P643, DOI 10.1016/j.visres.2004.09.017
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   Vig E., 2014, IEEE C COMP VIS PATT
   Xu M., 2017, IEEE T IMAGE PROCESS
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649403086
DA 2019-06-15
ER

PT S
AU Zhang, LP
   Tang, K
   Yao, X
AF Zhang, Liangpeng
   Tang, Ke
   Yao, Xin
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Log-normality and Skewness of Estimated State/Action Values in
   Reinforcement Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID APPROXIMATION
AB Under/overestimation of state/action values are harmful for reinforcement learning agents. In this paper, we show that a state/action value estimated using the Bellman equation can be decomposed to a weighted sum of path-wise values that follow log-normal distributions. Since log-normal distributions are skewed, the distribution of estimated state/action values can also be skewed, leading to an imbalanced likelihood of under/overestimation. The degree of such imbalance can vary greatly among actions and policies within a single problem instance, making the agent prone to select actions/policies that have inferior expected return and higher likelihood of overestimation. We present a comprehensive analysis to such skewness, examine its factors and impacts through both theoretical and empirical results, and discuss the possible ways to reduce its undesirable effects.
C1 [Zhang, Liangpeng; Tang, Ke] Univ Sci & Technol China, Sch Comp Sci & Technol, Hefei, Anhui, Peoples R China.
   [Zhang, Liangpeng; Yao, Xin] Univ Birmingham, Birmingham, W Midlands, England.
   [Tang, Ke; Yao, Xin] Southern Univ Sci & Technol, Shenzhen Key Lab Computat Intelligence, Dept Comp Sci & Engn, Shenzhen, Peoples R China.
RP Zhang, LP (reprint author), Univ Sci & Technol China, Sch Comp Sci & Technol, Hefei, Anhui, Peoples R China.; Zhang, LP (reprint author), Univ Birmingham, Birmingham, W Midlands, England.
EM lxz472@cs.bham.ac.uk; tangk3@sustc.edu.cn; xiny@sustc.edu.cn
RI Jeong, Yongwook/N-7413-2016
FU Ministry of Science and Technology of China [2017YFB1003102]; National
   Natural Science Foundation of China [61672478, 61329302]; Science and
   Technology Innovation Committee Foundation of Shenzhen
   [ZDSYS201703031748284]; EPSRC [J017515/1]; Royal Society Newton Advanced
   Fellowship [NA150123]
FX This paper was supported by Ministry of Science and Technology of China
   (Grant No. 2017YFB1003102), the National Natural Science Foundation of
   China (Grant Nos. 61672478 and 61329302), the Science and Technology
   Innovation Committee Foundation of Shenzhen (Grant No.
   ZDSYS201703031748284), EPSRC (Grant No. J017515/1), and in part by the
   Royal Society Newton Advanced Fellowship (Reference No. NA150123).
CR Asmuth J., 2008, P 23 AAAI C ART INT, P604
   Beaulieu NC, 2004, IEEE T VEH TECHNOL, V53, P479, DOI 10.1109/TVT.2004.823494
   Bellemare M. G., 2016, AAAI, P1476
   Bertsekas DP, 2012, MATH OPER RES, V37, P66, DOI 10.1287/moor.1110.0532
   Browne CB, 2012, IEEE T COMP INTEL AI, V4, P1, DOI 10.1109/TCIAIG.2012.2186810
   Casella G., 2002, STAT INFERENCE
   D'Eramo C, 2017, P 31 AAAI C ART INT, P1840
   DAYAN P, 1992, MACH LEARN, V8, P341, DOI 10.1007/BF00992701
   Doane DP, 2011, J STAT EDUC, V19, DOI 10.1080/10691898.2011.11889611
   Hasselt H. V., 2010, ADV NEURAL INFORM PR, P2613
   Hotelling H, 1932, ANN MATH STAT, V3, P141, DOI 10.1214/aoms/1177732911
   Jiang N, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P1181
   Kakade S, 2002, ADV NEUR IN, V14, P1531
   Kocsis Levente, 2006, EUR C MACH LEARN
   Lee D, 2013, IEEE SYMP ADAPT DYNA, P93, DOI 10.1109/ADPRL.2013.6614994
   Littman M. L., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P394
   Mnih  V., 2016, INT C MACH LEARN, V48, P1928
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278
   OEHLERT GW, 1992, AM STAT, V46, P27, DOI 10.2307/2684406
   Puterman M. L., 1994, MARKOV DECISION PROC
   Schulman  J., 2015, INT C MACH LEARN, P1889
   Sutton R., 1998, INTRO REINFORCEMENT
   Szepesvari C., 1997, NEURAL INFORM PROCES, V10, P1064
   Szepesvri C., 2010, SYNTHESIS LECT ARTIF, V4, P1, DOI DOI 10.2200/S00268ED1V01Y201005AIM009
   Thrun S, 1993, P 1993 CONN MOD SUMM
   TSITSIKLIS JN, 1994, MACH LEARN, V16, P185, DOI 10.1007/BF00993306
   Van Hasselt H., 2016, AAAI, P2094
   van Seijen H, 2016, J MACH LEARN RES, V17
   Wagner P, 2014, NEURAL NETWORKS, V52, P43, DOI 10.1016/j.neunet.2014.01.002
   WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698
   Zhang LP, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P4033
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401081
DA 2019-06-15
ER

PT S
AU Zhang, LJ
   Yang, TB
   Yi, JF
   Jin, R
   Zhou, ZH
AF Zhang, Lijun
   Yang, Tianbao
   Yi, Jinfeng
   Jin, Rong
   Zhou, Zhi-Hua
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Improved Dynamic Regret for Non-degenerate Functions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Recently, there has been a growing research interest in the analysis of dynamic regret, which measures the performance of an online learner against a sequence of local minimizers. By exploiting the strong convexity, previous studies have shown that the dynamic regret can be upper bounded by the path-length of the comparator sequence. In this paper, we illustrate that the dynamic regret can be further improved by allowing the learner to query the gradient of the function multiple times, and meanwhile the strong convexity can be weakened to other non-degenerate conditions. Specifically, we introduce the squared path-length, which could be much smaller than the path-length, as a new regularity of the comparator sequence. When multiple gradients are accessible to the learner, we first demonstrate that the dynamic regret of strongly convex functions can be upper bounded by the minimum of the path-length and the squared path-length. We then extend our theoretical guarantee to functions that are semi-strongly convex or self-concordant. To the best of our knowledge, this is the first time that semi-strong convexity and self-concordance are utilized to tighten the dynamic regret.
C1 [Zhang, Lijun; Zhou, Zhi-Hua] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing, Jiangsu, Peoples R China.
   [Yang, Tianbao] Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA.
   [Yi, Jinfeng] IBM Thomas J Watson Res Ctr, AI Fdn Lab, Yorktown Hts, NY USA.
   [Jin, Rong] Alibaba Grp, Seattle, WA USA.
RP Zhang, LJ (reprint author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing, Jiangsu, Peoples R China.
EM zhanglj@lamda.nju.edu.cn; tianbao-yang@uiowa.edu; jinfengyi@tencent.com;
   jinrong.jr@alibaba-inc.com; zhouzh@lamda.nju.edu.cn
FU NSFC [61603177, 61333014]; JiangsuSF [BK20160658]; NSF [IIS-1545995];
   Collaborative Innovation Center of Novel Software Technology and
   Industrialization; YESS [2017QNRC001]
FX This work was partially supported by the NSFC (61603177, 61333014),
   JiangsuSF (BK20160658), YESS (2017QNRC001), NSF (IIS-1545995), and the
   Collaborative Innovation Center of Novel Software Technology and
   Industrialization. Jinfeng Yi is now at Tencent AI Lab, Bellevue, WA,
   USA.
CR Abernethy  J., 2008, P 21 ANN C LEARN THE
   Abernethy Jacob, 2008, P 21 ANN C LEARN THE, P263
   Besbes O, 2015, OPER RES, V63, P1227, DOI 10.1287/opre.2015.1408
   Boyd S., 2004, CONVEX OPTIMIZATION
   Buchbinder N., 2012, P 25 ANN C LEARN THE
   Cesa-Bianchi Nicolo, 2012, ADV NEURAL INFORM PR, P980
   Chiang C.-K., 2012, P 25 ANN C LEARN THE
   Daniely A., 2015, P 32 INT C MACH LEAR
   Gong Pinghua, 2014, ARXIV14061102
   Hall E. C., 2013, P INT C MACH LEARN, P579
   Hazan E., 2007, EL C COMP COMPL, V88
   Hazan E., 2011, P 24 ANN C LEARN THE, P421
   Hazan E, 2007, MACH LEARN, V69, P169, DOI 10.1007/s10994-007-5016-8
   Herbster M, 1998, MACH LEARN, V32, P151, DOI 10.1023/A:1007424614876
   Jadbabaie Ali, 2015, P 18 INT C ART INT S
   Mokhtari A., 2016, ARXIV160304954
   Necoara  Ion, 2015, ARXIV150406298
   Nemirovski A, 2004, LECT NOTES
   Nesterov Y., 2004, INTRO LECT CONVEX OP, V87
   Rakhlin S., 2013, ADV NEURAL INFORM PR, P3066
   Shalev-Shwartz S., 2007, P 24 INT C MACH LEAR, P807, DOI DOI 10.1145/1273496.1273598
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Wang PW, 2014, J MACH LEARN RES, V15, P1523
   Yang T., 2016, P 33 INT C MACH LEAR
   Zinkevich Martin, 2003, P 20 INT C MACH LEAR, P928
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400070
DA 2019-06-15
ER

PT S
AU Zhang, YZ
   Shen, DH
   Wang, GY
   Gan, Z
   Henao, R
   Carin, L
AF Zhang, Yizhe
   Shen, Dinghan
   Wang, Guoyin
   Gan, Zhe
   Henao, Ricardo
   Carin, Lawrence
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Deconvolutional Paragraph Representation Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Learning latent representations from long text sequences is an important first step in many natural language processing applications. Recurrent Neural Networks (RNNs) have become a cornerstone for this challenging task. However, the quality of sentences during RNN-based decoding (reconstruction) decreases with the length of the text. We propose a sequence-to-sequence, purely convolutional and deconvolutional autoencoding framework that is free of the above issue, while also being computationally efficient. The proposed method is simple, easy to implement and can be leveraged as a building block for many applications. We show empirically that compared to RNNs, our framework is better at reconstructing and correcting long paragraphs. Quantitative evaluation on semi-supervised text classification and summarization tasks demonstrate the potential for better utilization of long unlabeled text data.
C1 [Zhang, Yizhe; Shen, Dinghan; Wang, Guoyin; Gan, Zhe; Henao, Ricardo; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA.
RP Zhang, YZ (reprint author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA.
RI Jeong, Yongwook/N-7413-2016
FU ARO; DARPA; DOE; NGA; ONR
FX This research was supported in part by ARO, DARPA, DOE, NGA and ONR.
CR Bahdanau Dzmitry, 2016, ACTOR CRITIC ALGORIT
   Bandanau D., 2015, ICLR
   Bengio S., 2015, NIPS
   Bowman S. R., 2015, GENERATING SENTENCES
   Chetlur S., 2014, CUDNN EFFICIENT PRIM
   Chiswell Ian, 2007, MATH LOGIC, V3
   Cho  K., 2014, EMNLP
   Chung J, 2014, EMPIRICAL EVALUATION
   Collobert R., 2011, JMLR
   Dai A. M., 2015, NIPS
   Dauphin YN, 2016, LANGUAGE MODELING GA
   Dieng Adji B, 2016, ICLR
   Gan Z., 2017, ARXIV170906548
   Gan Z., 2017, EMNLP
   Gehring  J., 2017, CONVOLUTIONAL SEQUEN
   Gulrajani Ishaan, 2016, PIXELVAE LATENT VARI
   Gumbel E., 1954, STAT THEORY EXTREME
   Hochreiter S., 2001, GRADIENT FLOW RECURR
   Hochreiter S., 1997, NEURAL COMPUTATION
   Hu B., 2014, NIPS
   Huszar Ferenc, 2015, NOT TRAIN YOUR GENER
   Johnson Rie, 2015, NAACL HLT
   Johnson Rie, 2016, SUPERVISED SEMISUPER
   Kalchbrenner N., 2014, ACL
   Kalchbrenner  N., 2016, NEURAL MACHINE TRANS
   Kim Yoon, 2014, EMNLP
   Kingma D. P., 2014, NIPS
   Le Q. V., 2014, ICML
   Li J., 2015, ACL
   Li J., 2017, ARXIV170106547
   Li Jiwei, 2016, DEEP REINFORCEMENT L
   Lin Chin- Yew, 2004, ACL WORKSH
   Meng Fandong, 2015, ACL
   Mikolov T., 2013, NIPS
   Mikolov Tomas, 2010, INTERSPEECH
   Miyato  T., 2017, ICLR
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Nallapati Ramesh, 2016, CONLL
   Narayan  S., 2017, NEURAL EXTRACTIVE SU
   Papineni K., 2002, ACL
   Pu Y., 2016, NIPS
   Pu Y., 2016, P 19 INT C ART INT S, P741
   Pu Yunchen, 2015, ARXIV150404054
   Radford A., 2015, UNSUPERVISED REPRESE
   Rush A. M., 2015, EMNLP
   Semeniuta Stanislau, 2017, HYBRID CONVOLUTIONAL
   Simonyan Karen, 2015, ICLR
   Socher Richard, 2011, EMNLP
   Springenberg J. T., 2014, STRIVING SIMPLICITY
   Sutskever  I., 2014, NIPS
   van den Oord A., 2016, ADV NEURAL INFORM PR, P4790
   Wen T. - H., 2015, SEMANTICALLY CONDITI
   Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270
   Wong Kam- Fai, 2008, ICCL
   Woodard JP, 1982, WORKSH STAND SPEECH
   Yang  Z., 2016, NAACL
   Yang Zichao, 2017, IMPROVED VARIATIONAL
   Zhang X., 2015, ADV NEURAL INFORM PR, V28, P649
   Zhang Y., 2017, ICML
NR 59
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649404024
DA 2019-06-15
ER

PT S
AU Zhang, ZT
   Li, QJ
   Huang, ZJ
   Wu, JJ
   Tenenbaum, JB
   Freeman, WT
AF Zhang, Zhoutong
   Li, Qiujia
   Huang, Zhengjia
   Wu, Jiajun
   Tenenbaum, Joshua B.
   Freeman, William T.
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Shape and Material from Sound
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Hearing an object falling onto the ground, humans can recover rich information including its rough shape, material, and falling height. In this paper, we build machines to approximate such competency. We first mimic human knowledge of the physical world by building an efficient, physics-based simulation engine. Then, we present an analysis-by-synthesis approach to infer properties of the falling object. We further accelerate the process by learning a mapping from a sound wave to object properties, and using the predicted values to initialize the inference. This mapping can be viewed as an approximation of human commonsense learned from past experience. Our model performs well on both synthetic audio clips and real recordings without requiring any annotated data. We conduct behavior studies to compare human responses with ours on estimating object shape, material, and falling height from sound. Our model achieves near-human performance.
C1 [Zhang, Zhoutong; Wu, Jiajun; Tenenbaum, Joshua B.; Freeman, William T.] MIT, Cambridge, MA 02139 USA.
   [Li, Qiujia] Univ Cambridge, Cambridge, England.
   [Huang, Zhengjia] ShanghaiTech Univ, Shanghai, Peoples R China.
   [Freeman, William T.] Google Res, Mountain View, CA USA.
RP Zhang, ZT (reprint author), MIT, Cambridge, MA 02139 USA.
RI Jeong, Yongwook/N-7413-2016
FU NSF [1212849, 1447476]; ONR MURI [N00014-16-1-2007]; Toyota Research
   Institute; Samsung; Shell; Center for Brain, Minds and Machines (NSF STC
   award) [CCF-1231216]
FX The authors would like to thank Changxi Zheng, Eitan Grinspun, and Josh
   H. McDermott for helpful discussions. This work is supported by NSF
   #1212849 and #1447476, ONR MURI N00014-16-1-2007, Toyota Research
   Institute, Samsung, Shell, and the Center for Brain, Minds and Machines
   (NSF STC award CCF-1231216).
CR Aytar Yusuf, 2016, NIPS
   Battaglia PW, 2013, P NATL ACAD SCI USA, V110, P18327, DOI 10.1073/pnas.1306572110
   Battaglia Peter W., 2016, NIPS
   Bever TG, 2010, BIOLINGUISTICS, V4, P174
   Bonneel N, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360623
   Chang Michael B, 2017, ICLR
   Collobert R, 2011, BIGLEARN NIPS WORKSH
   Coumans Erwin, 2010, BULLET PHYS ENGINE
   DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889
   Fontana F., 2003, SOUNDING OBJECT
   James DL, 2006, ACM T GRAPHIC, V25, P987, DOI 10.1145/1141911.1141983
   Klatzky RL, 2000, PRESENCE-TELEOP VIRT, V9, P399, DOI 10.1162/105474600566907
   Kunkler-Peck AJ, 2000, J EXP PSYCHOL HUMAN, V26, P279, DOI 10.1037/0096-1523.26.1.279
   McDermott JH, 2013, NAT NEUROSCI, V16, P493, DOI 10.1038/nn.3347
   O'Brien James F, 2002, SCA
   O'Brien James F, 2001, SIGGRAPH
   Owens A., 2016, CVPR
   Owens A., 2016, ECCV
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Sanborn AN, 2013, PSYCHOL REV, V120, P411, DOI 10.1037/a0031912
   Siegel Max, 2014, COGSCI
   van den Doel K, 1998, PRESENCE-TELEOP VIRT, V7, P382, DOI 10.1162/105474698565794
   Wu Jiajun, 2017, NIPS
   Wu Jiajun, 2016, BMVC
   Wu  Jiajun, 2015, NIPS, P2
   Yuille A, 2006, TRENDS COGN SCI, V10, P301, DOI 10.1016/j.tics.2006.05.002
   Zhang Z., 2017, ICCV
   Zhu SC, 2006, FOUND TRENDS COMPUT, V2, P259, DOI 10.1561/0600000018
   Zwicker E., 2013, PSYCHOACOUSTICS FACT, V22
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401031
DA 2019-06-15
ER

PT S
AU Zhang, ZM
   Brand, M
AF Zhang, Ziming
   Brand, Matthew
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Convergent Block Coordinate Descent for Training Tikhonov Regularized
   Deep Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID OPTIMIZATION
AB By lifting the ReLU function into a higher dimensional space, we develop a smooth multi-convex formulation for training feed-forward deep neural networks (DNNs). This allows us to develop a block coordinate descent (BCD) training algorithm consisting of a sequence of numerically well-behaved convex optimizations. Using ideas from proximal point methods in convex analysis, we prove that this BCD algorithm will converge globally to a stationary point with R-linear convergence rate of order one. In experiments with the MNIST database, DNNs trained with this BCD algorithm consistently yielded better test-set error rates than identical DNN architectures trained via all the stochastic gradient descent (SGD) variants in the Caffe toolbox.
C1 [Zhang, Ziming; Brand, Matthew] MERL, Cambridge, MA 02139 USA.
RP Zhang, ZM (reprint author), MERL, Cambridge, MA 02139 USA.
EM zzhang@merl.com; brand@merl.com
CR Baldassarre L., ADV TOPICS MACHINE 2
   Baldassi C, 2015, PHYS REV LETT, V115, DOI 10.1103/PhysRevLett.115.128101
   Bauschke HH, 1996, SIAM REV, V38, P367, DOI 10.1137/S0036144593251710
   Bengio Y, 2014, ARXIV14077906
   Bottou Leon, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P421, DOI 10.1007/978-3-642-35289-8_25
   Bottou  L., 2016, ARXIV160604838
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   CHAUDHARI P., 2017, ARXIV170404932
   Chaudhari Pratik, 2016, ARXIV161101838
   Choromanska Anna, 2015, AISTATS
   Dauphin Y. N., 2014, ADV NEURAL INFORM PR, P2933
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Eriksson K., 2003, APPL MATH BODY SOUL, VI-III
   Gal Y., 2015, ADV APPR BAYES INF W
   Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811
   Glorot X., 2010, JLMR P TRACK, P249, DOI DOI 10.1.1/207.2059
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hochreiter S., 2001, FIELD GUIDE DYNAMICA
   Ioffe S., 2015, ARXIV150203167
   Jia Y., 2014, P 22 ACM INT C MULT, P675, DOI DOI 10.1145/2647868.2654889
   Kingma D. P., 2015, ADV NEURAL INFORM PR, P2575
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   KROGH A, 1992, ADV NEUR IN, V4, P950
   Lanckriet Gert R, 2009, P ADV NEUR INF PROC, P1759
   LeCun  Y., 1998, MNIST DATABASE HANDW
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Nesterov Y., 1994, SIAM
   Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001
   Nishihara Robert, 2015, P 32 INT C MACH LEAR, V32, P343
   Nocedal J., 1999, NUMERICAL OPTIMIZATI
   Razaviyayn M., 2014, ADV NEURAL INFORM PR, P1440
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sutskever I., 2013, P 30 INT C MACH LEAR, V28, P1139
   Taylor G., 2016, ICML
   Tieleman T., 2012, COURSERA NEURAL NETW, V4
   Wang Y., 2015, ARXIV151106324
   Willoughby RA, 1979, SIAM REV, V21, P266, DOI [10.1137/1021044, DOI 10.1137/1021044]
   XU Y., 2014, ARXIV14101386
   Xu YY, 2013, SIAM J IMAGING SCI, V6, P1758, DOI 10.1137/120887795
   Zeiler M.D., 2012, ARXIV12125701
   Zhang Z., 2016, CVPR
NR 41
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649401073
DA 2019-06-15
ER

PT S
AU Zhao, H
   Gordon, G
AF Zhao, Han
   Gordon, Geoff
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Linear Time Computation of Moments in Sum-Product Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Bayesian online algorithms for Sum-Product Networks (SPNs) need to update their posterior distribution after seeing one single additional instance. To do so, they must compute moments of the model parameters under this distribution. The best existing method for computing such moments scales quadratically in the size of the SPN, although it scales linearly for trees. This unfortunate scaling makes Bayesian online algorithms prohibitively expensive, except for small or tree-structured SPNs. We propose an optimal linear-time algorithm that works even when the SPN is a general directed acyclic graph (DAG), which significantly broadens the applicability of Bayesian online algorithms for SPNs. There are three key ingredients in the design and analysis of our algorithm: 1). For each edge in the graph, we construct a linear time reduction from the moment computation problem to a joint inference problem in SPNs. 2). Using the property that each SPN computes a multilinear polynomial, we give an efficient procedure for polynomial evaluation by differentiation without expanding the network that may contain exponentially many monomials. 3). We propose a dynamic programming method to further reduce the computation of the moments of all the edges in the graph from quadratic to linear. We demonstrate the usefulness of our linear time algorithm by applying it to develop a linear time assume density filter (ADF) for SPNs.
C1 [Zhao, Han; Gordon, Geoff] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.
RP Zhao, H (reprint author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.
EM han.zhao@cs.cmu.edu; ggordon@cs.cmu.edu
RI Jeong, Yongwook/N-7413-2016
FU ONR [N000141512365]
FX HZ thanks Pascal Poupart for providing insightful comments. HZ and GG
   are supported in part by ONR award N000141512365.
CR Boutilier C, 1996, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P115
   Csiszar I, 2003, IEEE T INFORM THEORY, V49, P1474, DOI 10.1109/TIT.2003.810633
   Darwiche A, 2003, J ACM, V50, P280, DOI 10.1145/765568.765570
   Dennis A., 2015, INT JOINT C ART INT, V24
   Gens R., 2013, P 30 INT C MACH LEAR, V28, P873
   Gens R., 2012, NIPS, P3248
   Jaini Priyank, 2016, P 8 INT C PROB GRAPH, P228
   Park JD, 2004, ARTIF INTELL, V156, P197, DOI 10.1016/j.artint.2003.04.004
   Peharz R., 2015, AISTATS
   Peharz R, 2017, IEEE T PATTERN ANAL, V39, P2030, DOI 10.1109/TPAMI.2016.2618381
   Poon H., 2011, P 12 C UNC ART INT, P2551
   Rashwan Abdullah, 2016, P 19 INT C ART INT S, P1469
   Rooshenas A., 2014, ICML
   SORENSON HW, 1968, INT J CONTROL, V8, P33, DOI 10.1080/00207176808905650
   Valiant L. G., 1979, Theoretical Computer Science, V8, P189, DOI 10.1016/0304-3975(79)90044-6
   Zhao H., 2015, ICML
   Zhao H., 2016, NIPS
   Zhao Han, 2016, ICML
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406092
DA 2019-06-15
ER

PT S
AU Zhao, J
   Xiong, L
   Jayashree, K
   Li, JS
   Zhao, F
   Wang, ZC
   Pranata, S
   Shen, SM
   Yan, SC
   Feng, JS
AF Zhao, Jian
   Xiong, Lin
   Jayashree, Karlekar
   Li, Jianshu
   Zhao, Fang
   Wang, Zhecan
   Pranata, Sugiri
   Shen, Shengmei
   Yan, Shuicheng
   Feng, Jiashi
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Dual-Agent GANs for Photorealistic and Identity Preserving Profile Face
   Synthesis
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB Synthesizing realistic profile faces is promising for more efficiently training deep pose-invariant models for large-scale unconstrained face recognition, by populating samples with extreme poses and avoiding tedious annotations. However, learning from synthetic faces may not achieve the desired performance due to the discrepancy between distributions of the synthetic and real face images. To narrow this gap, we propose a Dual-Agent Generative Adversarial Network (DA-GAN) model, which can improve the realism of a face simulator's output using unlabeled real faces, while preserving the identity information during the realism refinement. The dual agents are specifically designed for distinguishing real v.s. fake and identities simultaneously. In particular, we employ an off-the-shelf 3D face model as a simulator to generate profile face images with varying poses. DA-GAN leverages a fully convolutional network as the generator to generate high-resolution images and an auto-encoder as the discriminator with the dual agents. Besides the novel architecture, we make several key modifications to the standard GAN to preserve pose and texture, preserve identity and stabilize training process: (i) a pose perception loss; (ii) an identity perception loss; (iii) an adversarial loss with a boundary equilibrium regularization term. Experimental results show that DA-GAN not only presents compelling perceptual results but also significantly outperforms state-of-the-arts on the large-scale and challenging NIST IJB-A unconstrained face recognition benchmark. In addition, the proposed DA-GAN is also promising as a new approach for solving generic transfer learning problems more effectively. DA-GAN is the foundation of our submissions to NIST IJB-A 2017 face recognition competitions, where we won the 1st places on the tracks of verification and identification.
C1 [Zhao, Jian; Li, Jianshu; Zhao, Fang; Yan, Shuicheng; Feng, Jiashi] Natl Univ Singapore, Singapore, Singapore.
   [Zhao, Jian] Natl Univ Def Technol, Changsha, Hunan, Peoples R China.
   [Xiong, Lin; Jayashree, Karlekar; Pranata, Sugiri; Shen, Shengmei] Panasonic R&D Ctr Singapore, Singapore, Singapore.
   [Wang, Zhecan] Franklin W Olin Coll Engn, Needham, MA USA.
   [Yan, Shuicheng] Qihoo 360 AI Inst, Beijing, Peoples R China.
RP Zhao, J (reprint author), Natl Univ Singapore, Singapore, Singapore.; Zhao, J (reprint author), Natl Univ Def Technol, Changsha, Hunan, Peoples R China.
EM zhaojian90@u.nus.edu; lin.xiong@sg.panasonic.com;
   karlekar.jayashree@sg.panasonic.com; jianshu@u.nus.edu;
   elezhf@u.nus.edu; zhecan.wang@students.olin.edu;
   sugiri.pranata@sg.panasonic.com; shengmei.shen@sg.panasonic.com;
   eleyans@u.nus.edu; elefjia@u.nus.edu
RI Jeong, Yongwook/N-7413-2016
FU China Scholarship Council (CSC) [201503170248]; National University of
   Singapore [R-263-000-C08-133]; Ministry of Education of Singapore AcRF
   Tier One grant [R-263-000-C21-112]; NUS IDS grant [R-263-000-C67-646]
FX The work of Jian Zhao was partially supported by China Scholarship
   Council (CSC) grant 201503170248.; The work of Jiashi Feng was partially
   supported by National University of Singapore startup grant
   R-263-000-C08-133, Ministry of Education of Singapore AcRF Tier One
   grant R-263-000-C21-112 and NUS IDS grant R-263-000-C67-646.
CR AbdAlmageed W., 2016, 2016 IEEE WINT C APP, P1
   Berthelot D., 2017, ARXIV170310717
   Chen J, 2015, IEEE ICC, P1801, DOI 10.1109/ICC.2015.7248586
   Chen JK, 2018, IEEE T AFFECT COMPUT, V9, P38, DOI [10.1109/ICIPRM.2016.7528763, 10.1109/ICCE-TW.2016.7520909, 10.1109/TAFFC.2016.2593719]
   Chen X., 2016, ADV NEURAL INFORM PR, P2172
   Chowdhury Animesh R., 2016, 2016 IEEE International Conference on Plasma Science (ICOPS), DOI 10.1109/PLASMA.2016.7534285
   Crosswhite N., 2016, ARXIV160303958
   Gong K., 2017, ARXIV170305446
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Hassner T., 2016, P IEEE C COMP VIS PA, P59
   HE KM, 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Huang R., 2017, ARXIV170404086
   Kingma D.P., 2013, ARXIV13126114
   Klare BF, 2015, PROC CVPR IEEE, P1931, DOI 10.1109/CVPR.2015.7298803
   Li JH, 2016, P IEEE RAS-EMBS INT, P1068, DOI 10.1109/BIOROB.2016.7523773
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Masi I, 2016, ARXIV160307057
   Masi I, 2016, PROC CVPR IEEE, P4838, DOI 10.1109/CVPR.2016.523
   Mirza M., 2014, ARXIV14111784, DOI DOI 10.1029/2009WR008312
   Odena A., 2016, ARXIV161009585
   Parkhi O M, 2015, P BR MACH VIS, P6
   Ranjan R, 2016, ARXIV161100851
   Ranjan R., 2017, ARXIV170309507
   Rezende D. J, 2014, ARXIV14014082
   Sankaranarayanan S., 2016, BIOM THEOR APPL SYST, P1, DOI DOI 10.1145/2910674.2910680
   Shrivastava A., 2016, ARXIV161207828
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang D., 2015, ARXIV150707242
   Xiao S., 2016, P ACM C MULT ACM MM, P691
   Xiao ST, 2016, LECT NOTES COMPUT SC, V9905, P57, DOI 10.1007/978-3-319-46448-0_4
   Xie S., 2016, ARXIV161105431
   Yang J., 2016, ARXIV160305474
   Zhu Xiaodan, 2015, 2015 11 IEEE INT C W, P1
NR 34
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649400007
DA 2019-06-15
ER

PT S
AU Zhou, ZY
   Mertikopoulos, P
   Bambos, N
   Glynn, P
   Tomlin, C
AF Zhou, Zhengyuan
   Mertikopoulos, Panayotis
   Bambos, Nicholas
   Glynn, Peter
   Tomlin, Claire
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Countering Feedback Delays in Multi-Agent Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID ONLINE; ALGORITHMS
AB We consider a model of game-theoretic learning based on online mirror descent (OMD) with asynchronous and delayed feedback information. Instead of focusing on specific games, we consider a broad class of continuous games defined by the general equilibrium stability notion, which we call lambda-variational stability. Our first contribution is that, in this class of games, the actual sequence of play induced by OMD-based learning converges to Nash equilibria provided that the feedback delays faced by the players are synchronous and bounded. Subsequently, to tackle fully decentralized, asynchronous environments with (possibly) unbounded delays between actions and feedback, we propose a variant of OMD which we call delayed mirror descent (DMD), and which relies on the repeated leveraging of past information. With this modification, the algorithm converges to Nash equilibria with no feedback synchronicity assumptions and even when the delays grow superlinearly relative to the horizon of play.
C1 [Zhou, Zhengyuan; Bambos, Nicholas; Glynn, Peter] Stanford Univ, Stanford, CA 94305 USA.
   [Mertikopoulos, Panayotis] Univ Grenoble Alpes, CNRS, Inria, LIG, Grenoble, France.
   [Tomlin, Claire] Univ Calif Berkeley, Berkeley, CA 94720 USA.
RP Zhou, ZY (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM zyzhou@stanford.edu; panayotis.mertikopoulos@imag.fr;
   bambos@stanford.edu; glynn@stanford.edu; tomlin@eecs.berkeley.edu
RI Jeong, Yongwook/N-7413-2016
FU Stanford Graduate Fellowship; Huawei Innovation Research Program ULTRON;
   ANR JCJC project ORACLESS [ANR-16-CE33-0004-01]; NSF CPS: FORCES grant
   [CNS-1239166]
FX Zhengyuan Zhou is supported by Stanford Graduate Fellowship and he would
   like to thank Walid Krichene and Alex Bayen for stimulating discussions
   (and their charismatic research style) that have firmly planted the
   initial seeds for this work. Panayotis Mertikopoulos gratefully
   acknowledges financial support from the Huawei Innovation Research
   Program ULTRON and the ANR JCJC project ORACLESS (grant no.
   ANR-16-CE33-0004-01). Claire Tomlin is supported in part by the NSF CPS:
   FORCES grant (CNS-1239166).
CR Arora S, 2012, THEORY COMPUT, V8, P121, DOI DOI 10.4086/TOC.2012.V008A006
   BALANDAT M., 2016, NIPS 16
   Blum A, 1998, LECT NOTES COMPUT SC, V1442, P306
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   COHEN  J., 2017, NIPS 17
   Desautels T, 2014, J MACH LEARN RES, V15, P3873
   HAZAN E., 2016, FDN TRENDSR OPTIMIZA
   Joulani  P., 2013, P 30 INT C MACH LEAR, P1453
   Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016
   Krichene S, 2015, ANN ALLERTON CONF, P480, DOI 10.1109/ALLERTON.2015.7447043
   Krichene W, 2015, SIAM J CONTROL OPTIM, V53, P1056, DOI 10.1137/140980685
   Lam Kiet, 2016, 2016 ACM/IEEE 7th International Conference on Cyber-Physical Systems (ICCPS), P1, DOI 10.1109/ICCPS.2016.7479108
   LITTLESTONE N, 1994, INFORM COMPUT, V108, P212, DOI 10.1006/inco.1994.1009
   MEHTA R., 2015, ITCS 15
   MERTIKOPOULOS P., 2016, LEARNING GAMES CONTI
   MERTIKOPOULOS P., SODA 18
   Nemirovski A, 1983, PROBLEM COMPLEXITY M
   Nesterov Y, 2009, MATH PROGRAM, V120, P221, DOI 10.1007/s10107-007-0149-x
   PALAIOPANOS G., 2017, NIPS 17
   QUANRUD  K., 2015, ADV NEURAL INFORM PR, P1270
   Rockafellar R. T., 2009, VARIATIONAL ANAL, V317
   Shalev-Shwartz S., 2007, ADV NEURAL INFORM PR, V19, P1265
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Viossat Y, 2013, J ECON THEORY, V148, P825, DOI 10.1016/j.jet.2012.07.003
   Xiao L, 2010, J MACH LEARN RES, V11, P2543
   Zhou ZY, 2016, LECT NOTES COMPUT SC, V9996, P114, DOI 10.1007/978-3-319-47413-7_7
   Zhou ZY, 2016, P AMER CONTR CONF, P3802, DOI 10.1109/ACC.2016.7525505
   Zhu MH, 2016, AUTOMATICA, V63, P82, DOI 10.1016/j.automatica.2015.10.012
   Zinkevich Martin, 2003, P 20 INT C MACH LEAR, P928
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406024
DA 2019-06-15
ER

PT S
AU Zhou, ZY
   Mertikopoulos, P
   Bambos, N
   Boyd, S
   Glynn, P
AF Zhou, Zhengyuan
   Mertikopoulos, Panayotis
   Bambos, Nicholas
   Boyd, Stephen
   Glynn, Peter
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Stochastic Mirror Descent in Variationally Coherent Optimization
   Problems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB In this paper, we examine a class of non-convex stochastic optimization problems which we call variationally coherent, and which properly includes pseudo-/quasi-convex and star-convex optimization problems. To solve such problems, we focus on the widely used stochastic mirror descent (SMD) family of algorithms (which contains stochastic gradient descent as a special case), and we show that the last iterate of SMD converges to the problem's solution set with probability 1. This result contributes to the landscape of non-convex stochastic optimization by clarifying that neither pseudo-/quasi-convexity nor star-convexity is essential for (almost sure) global convergence; rather, variational coherence, a much weaker requirement, suffices. Characterization of convergence rates for the subclass of strongly variationally coherent optimization problems as well as simulation results are also presented.
C1 [Zhou, Zhengyuan; Bambos, Nicholas; Boyd, Stephen; Glynn, Peter] Stanford Univ, Stanford, CA 94305 USA.
   [Mertikopoulos, Panayotis] Univ Grenoble Alpes, CNRS, INRIA, LIG, Grenoble, France.
RP Zhou, ZY (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM zyzhou@stanford.edu; panayotis.mertikopoulos@imag.fr;
   bambos@stanford.edu; boyd@stanford.edu; glynn@stanford.edu
RI Jeong, Yongwook/N-7413-2016
FU Stanford Graduate Fellowship; Huawei Innovation Research Program ULTRON;
   ANR JCJC project ORACLESS [ANR-16-CE33-0004-01]
FX Zhengyuan Zhou is supported by Stanford Graduate Fellowship and would
   like to thank Yinyu Ye and Jose Blanchet for constructive discussions
   and feedback. Panayotis Mertikopoulos gratefully acknowledges financial
   support from the Huawei Innovation Research Program ULTRON and the ANR
   JCJC project ORACLESS (grant no. ANR-16-CE33-0004-01).
CR Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6
   Benaim M., 1996, Journal of Dynamics and Differential Equations, V8, P141, DOI 10.1007/BF02218617
   Benaim M, 1999, LECT NOTES MATH, V1709, P1
   Borkar V. S., 2008, STOCHASTIC APPROXIMA
   Boyd S., 2004, CONVEX OPTIMIZATION
   Cesa-Bianchi N., 2012, NIPS P, V25, P989
   Duchi JC, 2012, SIAM J OPTIMIZ, V22, P1549, DOI 10.1137/110836043
   Facchinei F, 2003, SPRINGER SERIES OPER
   Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811
   Juditsky A., 2011, STOCHASTIC SYSTEMS, V1, P17, DOI DOI 10.1214/10-SSY011
   KRICHENE W., 2015, NIPS 15
   KUSHNER H., 2013, STOCHASTIC MODELLING
   MERTIKOPOULOS P., 2016, LEARNING GAMES CONTI
   Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277
   Nemirovski A, 1983, PROBLEM COMPLEXITY M
   Nesterov Y, 2009, MATH PROGRAM, V120, P221, DOI 10.1007/s10107-007-0149-x
   Nesterov Y, 2007, MATH PROGRAM, V109, P319, DOI 10.1007/s10107-006-0034-z
   Rockafellar R. T., 1998, SERIES COMPREHENSIVE, V317
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Su W., 2014, ADV NEURAL INFORM PR, V27, P2510
   Wibisono A, 2016, P NATL ACAD SCI USA, V113, pE7351, DOI 10.1073/pnas.1614734113
   Xiao L, 2010, J MACH LEARN RES, V11, P2543
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649407013
DA 2019-06-15
ER

PT S
AU Zhuang, CX
   Kubilius, J
   Hartmann, M
   Yamins, D
AF Zhuang, Chengxu
   Kubilius, Jonas
   Hartmann, Mitra
   Yamins, Daniel
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Toward Goal-Driven Neural Network Models for the Rodent
   Whisker-Trigeminal System
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID RAT BARREL CORTEX; OBJECT LOCALIZATION; LAYER 2/3; RESPONSES; VIBRISSA
AB In large part, rodents "see" the world through their whiskers, a powerful tactile sense enabled by a series of brain areas that form the whisker-trigeminal system. Raw sensory data arrives in the form of mechanical input to the exquisitely sensitive, actively-controllable whisker array, and is processed through a sequence of neural circuits, eventually arriving in cortical regions that communicate with decision-making and memory areas. Although a long history of experimental studies has characterized many aspects of these processing stages, the computational operations of the whisker-trigeminal system remain largely unknown. In the present work, we take a goal-driven deep neural network (DNN) approach to modeling these computations. First, we construct a biophysically-realistic model of the rat whisker array. We then generate a large dataset of whisker sweeps across a wide variety of 3D objects in highly-varying poses, angles, and speeds. Next, we train DNNs from several distinct architectural families to solve a shape recognition task in this dataset. Each architectural family represents a structurally-distinct hypothesis for processing in the whisker-trigeminal system, corresponding to different ways in which spatial and temporal information can be integrated. We find that most networks perform poorly on the challenging shape recognition task, but that specific architectures from several families can achieve reasonable performance levels. Finally, we show that Representational Dissimilarity Matrices (RDMs), a tool for comparing population codes between neural systems, can separate these higher-performing networks with data of a type that could plausibly be collected in a neurophysiological or imaging experiment. Our results are a proof-of-concept that DNN models of the whisker-trigeminal system are potentially within reach.
C1 [Zhuang, Chengxu] Stanford Univ, Dept Psychol, Stanford, CA 94305 USA.
   [Kubilius, Jonas] MIT, Dept Brain & Cognit Sci, E25-618, Cambridge, MA 02139 USA.
   [Kubilius, Jonas] Katholieke Univ Leuven, Brain & Cognit, Leuven, Belgium.
   [Hartmann, Mitra] Northwestern Univ, Dept Biomed Engn, Evanston, IL 60208 USA.
   [Hartmann, Mitra] Northwestern Univ, Dept Mech Engn, Evanston, IL 60208 USA.
   [Yamins, Daniel] Stanford Univ, Stanford Neurosci Inst, Dept Psychol, Stanford, CA 94305 USA.
   [Yamins, Daniel] Stanford Univ, Stanford Neurosci Inst, Dept Comp Sci, Stanford, CA 94305 USA.
RP Zhuang, CX (reprint author), Stanford Univ, Dept Psychol, Stanford, CA 94305 USA.
EM chengxuz@stanford.edu; qbilius@mit.edu; hartmann@northwestern.edu;
   yamins@stanford.edu
RI Jeong, Yongwook/N-7413-2016
FU James S. McDonnell Foundation Award [220020469]; NSF Robust Intelligence
   grant [1703161]; European Union's Horizon 2020 research and innovation
   programme [705498]; NSF [IOS-0846088, IOS-1558068]
FX This project has sponsored in part by hardware donation from the NVIDIA
   Corporation, a James S. McDonnell Foundation Award (No. 220020469) and
   an NSF Robust Intelligence grant (No. 1703161) to DLKY, the European
   Union's Horizon 2020 research and innovation programme (No. 705498) to
   JK, and NSF awards (IOS-0846088 and IOS-1558068) to MJZH.
CR Arabzadeh E, 2005, PLOS BIOL, V3, P155, DOI 10.1371/journal.pbio.0030017
   ARMSTRONGJAMES M, 1992, J NEUROPHYSIOL, V68, P1345
   Berglund J., 2013, P 12 PYTH SCI C, DOI DOI 10.1088/1749-4699/8/1/014008
   Bosman LWJ, 2011, FRONT INTEGR NEUROSC, V5, DOI 10.3389/fnint.2011.00053
   Cadieu CF, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003963
   Chang A X, 2015, SHAPENET INFORM RICH
   Cho K., 2014, ARXIV14091259
   Deschenes M., 2009, SCHOLARPEDIA, V4, P7454, DOI DOI 10.4249/SCHOLARPEDIA.7454
   Diamond ME, 2008, NAT REV NEUROSCI, V9, P601, DOI 10.1038/nrn2411
   Ebara S, 2002, J COMP NEUROL, V449, P103, DOI 10.1002/cne.10277
   Felleman DJ, 1991, CEREB CORTEX, V1, P1, DOI 10.1093/cercor/1.1.1
   GOODALE MA, 1992, TRENDS NEUROSCI, V15, P20, DOI 10.1016/0166-2236(92)90344-8
   Hartmann M. J. Z., 2015, SCHOLARPEDIA, V10, P6636, DOI [10. 4249/ scholarpedia. 6636, DOI 10.4249/SCH0LARPEDIA.6636]
   Hobbs JA, 2016, FRONT BEHAV NEUROSCI, V9, DOI 10.3389/fnbeh.2015.00356
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1, DOI 10.1162/neco.1997.9.1.1
   Huet LA, 2016, IEEE T HAPTICS, V9, P158, DOI 10.1109/TOH.2016.2522432
   Inui K, 2004, CEREB CORTEX, V14, P851, DOI 10.1093/cercor/bhh043
   Iwamura Y, 1998, CURR OPIN NEUROBIOL, V8, P522, DOI 10.1016/S0959-4388(98)80041-X
   Kell A, 2015, SOC NEUROSCIENCE
   Kerr JND, 2007, J NEUROSCI, V27, P13316, DOI 10.1523/JNEUROSCI.2210-07.2007
   Khaligh-Razavi SM, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003915
   Knutsen PM, 2006, J NEUROSCI, V26, P8451, DOI 10.1523/JNEUROSCI.1516-06.2006
   Kriegeskorte N, 2008, FRONT SYST NEUROSCI, V2, DOI 10.3389/neuro.06.004.2008
   Moore JD, 2015, PLOS BIOL, V13, DOI 10.1371/journal.pbio.1002253
   O'Connor DH, 2010, NEURON, V67, P1048, DOI 10.1016/j.neuron.2010.08.026
   Petersen CCH, 2003, J NEUROSCI, V23, P1298
   PONS TP, 1987, SCIENCE, V237, P417, DOI 10.1126/science.3603028
   Purves Dale, 2001, NEUROSCIENCE, P3
   Quist BW, 2014, J NEUROSCI, V34, P9828, DOI 10.1523/JNEUROSCI.1707-12.2014
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Towal RB, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1001120
   Von Heimendahl M, 2007, PLOS BIOL, V5, P2696, DOI 10.1371/journal.pbio.0050305
   Yamins DLK, 2016, NAT NEUROSCI, V19, P356, DOI 10.1038/nn.4244
   Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111
   Yu CX, 2006, PLOS BIOL, V4, P819, DOI 10.1371/journal.pbio.0040124
NR 35
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649402059
DA 2019-06-15
ER

PT S
AU Zhuang, HL
   Wang, C
   Wang, YF
AF Zhuang, Honglei
   Wang, Chi
   Wang, Yifan
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI Identifying Outlier Arms in Multi-Armed Bandit
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
ID APPROXIMATE
AB We study a novel problem lying at the intersection of two areas: multi-armed bandit and outlier detection. Multi-armed bandit is a useful tool to model the process of incrementally collecting data for multiple objects in a decision space. Outlier detection is a powerful method to narrow down the attention to a few objects after the data for them are collected. However, no one has studied how to detect outlier objects while incrementally collecting data for them, which is necessary when data collection is expensive. We formalize this problem as identifying outlier arms in a multi-armed bandit. We propose two sampling strategies with theoretical guarantee, and analyze their sampling efficiency. Our experimental results on both synthetic and real data show that our solution saves 70-99% of data collection cost from baseline while having nearly perfect accuracy.
C1 [Zhuang, Honglei] Univ Illinois, Champaign, IL 61820 USA.
   [Zhuang, Honglei; Wang, Chi] Microsoft Res, Redmond, WA USA.
   [Wang, Yifan] Tsinghua Univ, Beijing, Peoples R China.
RP Zhuang, HL (reprint author), Univ Illinois, Champaign, IL 61820 USA.
EM hzhuang3@illinos.edu; wang.chi@microsoft.com;
   yifan-wa16@mails.tsinghua.edu.cn
RI Jeong, Yongwook/N-7413-2016
FU U.S. Army Research Lab [W911NF-09-2-0053]; National Science Foundation
   [IIS 16-18481, IIS 17-04532, IIS-17-41317]; NIGMS through funds -
   trans-NIH Big Data to Knowledge (BD2K) initiative [1U54GM114838]
FX Part of this work was done while the first author was an intern at
   Microsoft Research. The first author was sponsored in part by the U.S.
   Army Research Lab. under Cooperative Agreement No. W911NF-09-2-0053
   (NSCTA), National Science Foundation IIS 16-18481, IIS 17-04532, and
   IIS-17-41317, and grant 1U54GM114838 awarded by NIGMS through funds
   provided by the trans-NIH Big Data to Knowledge (BD2K) initiative
   (www.bd2k.nih.gov). The views and conclusions contained in this document
   are those of the author(s) and should not be interpreted as representing
   the official policies of the U.S. Army Research Laboratory or the U.S.
   Government. The U.S. Government is authorized to reproduce and
   distribute reprints for Government purposes notwithstanding any
   copyright notation hereon.
CR Abe N., 2006, P 12 ACM SIGKDD INT, P504, DOI DOI 10.1145/1150402.1150459
   Aggarwal C. C., 2008, P SIAM INT C DAT MIN, P483
   Agresti A, 1998, AM STAT, V52, P119, DOI 10.2307/2685469
   Audibert  J.-Y., 2010, COLT
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Bubeck S., 2013, P 30 INT C MACH LEAR, V28, P258
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Bubeck S, 2011, THEOR COMPUT SCI, V412, P1832, DOI 10.1016/j.tcs.2010.12.059
   Carpentier  A., 2014, ADV NEURAL INFORM PR, P1089
   Chandola V, 2009, ACM COMPUT SURV, V41, DOI 10.1145/1541880.1541882
   Chen L., 2015, ARXIV151103774
   Chen S., 2014, ADV NEURAL INFORM PR, V26, P379
   Donmez P, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P259
   Even-Dar E, 2006, J MACH LEARN RES, V7, P1079
   Gabillon V., 2016, AISTATS, P1004
   Gentile C, 2014, ICML, P757
   Hodge VJ, 2004, ARTIF INTELL REV, V22, P85, DOI 10.1007/s10462-004-4304-y
   Jiang B, 2011, PROC INT CONF DATA, P422, DOI 10.1109/ICDE.2011.5767850
   Kalyanakrishnan S., 2012, P 29 INT C MACH LEAR, P655
   Kollios G, 2003, IEEE T KNOWL DATA EN, V15, P1170, DOI 10.1109/TKDE.2003.1232271
   Korda N., 2016, J MACH LEARN RES, P1301
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Lazaric A., 2012, ADV NEURAL INFORM PR, V25, P3212
   Li L., 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758
   Liu HF, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA), P726, DOI 10.1109/BigData.2016.7840665
   Locatelli Andrea, 2016, ICML, P1690
   ROBBINS H, 1952, B AM MATH SOC, V58, P527, DOI 10.1090/S0002-9904-1952-09620-8
   Sugiyama M., 2013, ADV NEURAL INFORM PR, P467
   Wu M., 2006, 12 ACM SIGKDD INT C, P767
   Zimek A., 2013, P 19 ACM SIGKDD INT, P428, DOI DOI 10.1145/2487575.2487676
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649405028
DA 2019-06-15
ER

PT S
AU Zung, J
   Tartavull, I
   Lee, K
   Seung, HS
AF Zung, Jonathan
   Tartavull, Ignacio
   Lee, Kisuk
   Seung, H. Sebastian
BE Guyon, I
   Luxburg, UV
   Bengio, S
   Wallach, H
   Fergus, R
   Vishwanathan, S
   Garnett, R
TI An Error Detection and Correction Framework for Connectomics
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 31st Conference on Neural Information Processing Systems (NIPS)
CY 2017
CL Long Beach, CA
AB We define and study error detection and correction tasks that are useful for 3D reconstruction of neurons from electron microscopic imagery, and for image segmentation more generally. Both tasks take as input the raw image and a binary mask representing a candidate object. For the error detection task, the desired output is a map of split and merge errors in the object. For the error correction task, the desired output is the true object. We call this object mask pruning, because the candidate object mask is assumed to be a superset of the true object. We train multiscale 3D convolutional networks to perform both tasks. We find that the error-detecting net can achieve high accuracy. The accuracy of the error-correcting net is enhanced if its input object mask is "advice" (union of erroneous objects) from the error-detecting net.
C1 [Zung, Jonathan; Tartavull, Ignacio; Lee, Kisuk; Seung, H. Sebastian] Princeton Univ, Princeton, NJ 08544 USA.
   [Lee, Kisuk] MIT, Cambridge, MA 02139 USA.
RP Zung, J (reprint author), Princeton Univ, Princeton, NJ 08544 USA.
EM jzung@princeton.edu; tartavull@princeton.edu; kisukiee@mit.edu;
   sseung@princeton.edu
RI Jeong, Yongwook/N-7413-2016
FU Mathers Foundation; Samsung Scholarship; Intelligence Advanced Research
   Projects Activity (IARPA) via Department of Interior/ Interior Business
   Center (DoI/IBC) [D16PC0005]; Amazon
FX We acknowledge NVIDIA Corporation for providing us with early access to
   Titan X Pascal GPU used in this research, and Amazon for assistance
   through an AWS Research Grant. This research was supported by the
   Mathers Foundation, the Samsung Scholarship and the Intelligence
   Advanced Research Projects Activity (IARPA) via Department of Interior/
   Interior Business Center (DoI/IBC) contract number D16PC0005. The U.S.
   Government is authorized to reproduce and distribute reprints for
   Governmental purposes notwithstanding any copyright annotation thereon.
   Disclaimer: The views and conclusions contained herein are those of the
   authors and should not be interpreted as necessarily representing the
   official policies or endorsements, either expressed or implied, of
   IARPA, DoI/IBC, or the U.S. Government.
CR Beier T, 2017, NAT METHODS, V14, P101, DOI 10.1038/nmeth.4151
   Berger Daniel, VAST LITE
   De Brabandere Bert, 2017, CORR
   Fathi Alireza, 2017, CORR
   Fourure D., 2017, CORR
   Funke Jan, 2017, ARXIV170902974
   Gao Huang, 2017, CORR
   Harley A. W., 2015, CORR
   He K., 2015, CORR
   Jain Viren, 2011, NIPS
   Januszewski M., 2016, FLOOD FILLING NETWOR
   Januszewski Michal, 2017, BIORXIV
   Juan Nunez-Iglesias, 2014, GRAPH BASED ACTIVE L
   Kasthuri N, 2015, CELL, V162, P648, DOI 10.1016/j.cell.2015.06.054
   Lee K., 2017, CORR
   Luc P., 2016, CORR
   Meila M, 2007, J MULTIVARIATE ANAL, V98, P873, DOI 10.1016/j.jmva.2006.11.013
   Meirovitch Yaron, 2016, MULTIPASS APPROACH L
   Mirza M., 2014, ARXIV14111784, DOI DOI 10.1029/2009WR008312
   Nunez-Iglesias J, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0071715
   Pinheiro P.O., 2015, ADV NEURAL INFORM PR, P1990
   Ren Mengye, 2016, CORR
   Rolnick David, 2017, CORR
   Romera-Paredes Bernardino, 2015, CORR
   Ronneberger O., 2015, U NET CONVOLUTIONAL
   Saxena S., 2016, CORR
   Schmidt H, 2017, NATURE, V549, P469, DOI 10.1038/nature24005
   Turaga S C, 2010, CONVOLUTIONAL NETWOR
   Wang S, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON NANOSCALE ARCHITECTURE, P1
   WHITE JG, 1986, PHILOS T R SOC B, V314, P1, DOI 10.1098/rstb.1986.0056
   Zeng T, 2017, BIOINFORMATICS, V33, P2555, DOI 10.1093/bioinformatics/btx188
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2017
VL 30
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BL5ST
UT WOS:000452649406085
DA 2019-06-15
ER

PT S
AU Abbe, E
   Sandon, C
AF Abbe, Emmanuel
   Sandon, Colin
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Achieving the KS threshold in the general stochastic block model with
   linearized acyclic belief propagation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID ALGORITHMS
AB The stochastic block model (SBM) has long been studied in machine learning and network science as a canonical model for clustering and community detection. In the recent years, new developments have demonstrated the presence of threshold phenomena for this model, which have set new challenges for algorithms. For the detection problem in symmetric SBMs, Decelle et al. conjectured that the so-called Kesten-Stigum (KS) threshold can be achieved efficiently. This was proved for two communities, but remained open for three and more communities. We prove this conjecture here, obtaining a general result that applies to arbitrary SBMs with linear size communities. The developed algorithm is a linearized acyclic belief propagation (ABP) algorithm, which mitigates the effects of cycles while provably achieving the KS threshold in O(n ln n) time. This extends prior methods by achieving universally the KS threshold while reducing or preserving the computational complexity. ABP is also connected to a power iteration method on a generalized nonbacktracking operator, formalizing the spectral-message passing interplay described in Krzakala et al., and extending results from Bordenave et al.
C1 [Abbe, Emmanuel] Princeton Univ, Appl & Computat Math & EE Dept, Princeton, NJ 08544 USA.
   [Sandon, Colin] Princeton Univ, Dept Math, Princeton, NJ 08544 USA.
RP Abbe, E (reprint author), Princeton Univ, Appl & Computat Math & EE Dept, Princeton, NJ 08544 USA.
EM eabbe@princeton.edu; sandon@princeton.edu
FU NSF CAREER Award [CCF-1552131]; ARO [W911NF-16-1-0051]
FX This research was supported by NSF CAREER Award CCF-1552131 and ARO
   grant W911NF-16-1-0051.
CR Banks J., 2016, ARXIV160102658
   Bhattacharyya S., 2014, ARXIV14013915
   Bickel PJ, 2009, P NATL ACAD SCI USA, V106, P21068, DOI 10.1073/pnas.0907096106
   Bollobas B, 2007, RANDOM STRUCT ALGOR, V31, P3, DOI 10.1002/rsa.20168
   Boppana R. B., 1987, 28th Annual Symposium on Foundations of Computer Science (Cat. No.87CH2471-1), P280, DOI 10.1109/SFCS.1987.22
   Bordenave C, 2015, ANN IEEE SYMP FOUND, P1347, DOI 10.1109/FOCS.2015.86
   BUI TN, 1987, COMBINATORICA, V7, P171, DOI 10.1007/BF02579448
   Coja-Oghlan A, 2010, COMB PROBAB COMPUT, V19, P227, DOI 10.1017/S0963548309990514
   Decelle A, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.066106
   Donoho DL, 2009, P NATL ACAD SCI USA, V106, P18914, DOI 10.1073/pnas.0909892106
   Gao C., 2015, ARXIV E PRINTS
   Guedon O, 2016, PROBAB THEORY REL, V165, P1025, DOI 10.1007/s00440-015-0659-z
   HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7
   Karrer B, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.016107
   Krzakala F, 2013, P NATL ACAD SCI USA, V110, P20935, DOI 10.1073/pnas.1312486110
   Massone L, 2014, SPRINGER SER ADV MAN, P1, DOI 10.1007/978-94-007-7593-0_1
   McSherry F, 2001, ANN IEEE SYMP FOUND, P529, DOI 10.1109/SFCS.2001.959929
   Montanari A, 2016, STOC'16: PROCEEDINGS OF THE 48TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P814, DOI 10.1145/2897518.2897548
   Mossel E., 2014, PROOF BLOCK MODEL TH
   Mossel E, 2015, PROBAB THEORY REL, V162, P431, DOI 10.1007/s00440-014-0576-6
   Murphy K., 1999, P UAI 99, P467
   Sandon C, 2015, ARXIV151209080
   Vu V., 2014, ARXIV14043918
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703001
DA 2019-06-15
ER

PT S
AU Abernethy, J
   Amin, K
   Zhu, RH
AF Abernethy, Jacob
   Amin, Kareem
   Zhu, Ruihao
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Threshold Bandit, With and Without Censored Feedback
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We consider the Threshold Bandit setting, a variant of the classical multi-armed bandit problem in which the reward on each round depends on a piece of side information known as a threshold value. The learner selects one of K actions (arms), this action generates a random sample from a fixed distribution, and the action then receives a unit payoff in the event that this sample exceeds the threshold value. We consider two versions of this problem, the uncensored and censored case, that determine whether the sample is always observed or only when the threshold is not met. Using new tools to understand the popular UCB algorithm, we show that the uncensored case is essentially no more difficult than the classical multi-armed bandit setting. Finally we show that the censored case exhibits more challenges, but we give guarantees in the event that the sequence of threshold values is generated optimistically.
C1 [Abernethy, Jacob; Amin, Kareem] Univ Michigan, Dept Comp Sci, Ann Arbor, MI 48109 USA.
   [Zhu, Ruihao] MIT, AeroAstro&CSAIL, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
RP Abernethy, J (reprint author), Univ Michigan, Dept Comp Sci, Ann Arbor, MI 48109 USA.
EM jabernet@umich.edu; amkareem@umich.edu; rzhu@mit.edu
CR Abernethy JD, 2012, IEEE T INFORM THEORY, V58, P4164, DOI 10.1109/TIT.2012.2192096
   Abernethy Jacob D, 2015, ADV NEURAL INFORM PR, P2188
   Agarwal A., 2010, P 13 INT C ART INT S, P9
   Amin Kareem, 2012, ARXIV12104847
   Audibert J.-Y., 2009, COLT, P217
   Auer P, 2003, SIAM J COMPUT, V32, P48
   Auer P., 2003, Journal of Machine Learning Research, V3, P397, DOI 10.1162/153244303321897663
   Auer P., 2002, J MACHINE LEARNING, V3, P235, DOI DOI 10.1023/A:1013689704352
   Auer P, 2010, PERIOD MATH HUNG, V61, P55, DOI 10.1007/s10998-010-3055-6
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Dvoretzky A., 1956, ANN MATH STAT
   FOLDES A, 1981, ANN STAT, V9, P122, DOI 10.1214/aos/1176345337
   Ganchev Kuzman, 2010, UAI
   Gittins J., 2011, MULTIARMED BANDIT AL
   Huh W. T., 2009, ADAPTIVE DATA DRIVEN
   Kaplan E. L., 1958, JASA
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Neu G, 2013, LECT NOTES ARTIF INT, V8139, P234
   Peterson A. V., 1983, ENCY STAT SCI
   ROBBINS H, 1952, B AM MATH SOC, V58, P527, DOI 10.1090/S0002-9904-1952-09620-8
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701006
DA 2019-06-15
ER

PT S
AU Abuzaid, F
   Bradley, J
   Liang, F
   Feng, A
   Yang, L
   Zaharia, M
   Talwalkar, A
AF Abuzaid, Firas
   Bradley, Joseph
   Liang, Feynman
   Feng, Andrew
   Yang, Lee
   Zaharia, Matei
   Talwalkar, Ameet
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Yggdrasil: An Optimized System for Training Deep Decision Trees at Scale
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Deep distributed decision trees and tree ensembles have grown in importance due to the need to model increasingly large datasets. However, PLANET, the standard distributed tree learning algorithm implemented in systems such as XGBOOST and Spark MLLIB, scales poorly as data dimensionality and tree depths grow. We present YGGDRASIL, a new distributed tree learning method that outperforms existing methods by up to 24x. Unlike PLANET, YGGDRASIL is based on vertical partitioning of the data (i.e., partitioning by feature), along with a set of optimized data structures to reduce the CPU and communication costs of training. YGGDRASIL (1) trains directly on compressed data for compressible features and labels; (2) introduces efficient data structures for training on uncompressed data; and (3) minimizes communication between nodes by using sparse bitvectors. Moreover, while PLANET approximates split points through feature binning, YGGDRASIL does not require binning, and we analytically characterize the impact of this approximation. We evaluate YGGDRASIL against the MNIST 8M dataset and a high-dimensional dataset at Yahoo; for both, YGGDRASIL is faster by up to an order of magnitude.
C1 [Abuzaid, Firas; Zaharia, Matei] MIL CSAIL, Cambridge, MA 02139 USA.
   [Bradley, Joseph] Databricks, San Francisco, CA USA.
   [Liang, Feynman] Univ Cambridge, Cambridge, England.
   [Feng, Andrew; Yang, Lee] Yahoo, Sunnyvale, CA USA.
   [Talwalkar, Ameet] Univ Calif Los Angeles, Los Angeles, CA 90024 USA.
RP Abuzaid, F (reprint author), MIL CSAIL, Cambridge, MA 02139 USA.
CR Ben-Haim Y, 2010, J MACH LEARN RES, V11, P849
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Breiman L., 1984, CLASSIFICATION REGRE
   Caragea Doina, 2004, Int J Hybrid Intell Syst, V1, P80
   Chambi S., 2015, SOFTWARE PRACTICE EX
   Chen T., 2016, ARXIV160302754
   Cortes  C., 2014, ICML
   FAYYAD UM, 1992, MACH LEARN, V8, P87, DOI 10.1023/A:1022638503176
   Lamb A, 2012, PROC VLDB ENDOW, V5, P1790, DOI 10.14778/2367502.2367518
   Meng X., 2015, ARXIV150506807
   Panda B, 2009, INT C VER LARG DAT B
   SAFAVIAN SR, 1991, IEEE T SYST MAN CYB, V21, P660, DOI 10.1109/21.97458
   Stonebraker M., 2005, VLDB, P553, DOI DOI 10.1007/BF02443652
   Svore K. M., 2011, SCALING MACHINE LEAR, P2
   Ye J., 2009, P 18 ACM C INF KNOWL, P2061, DOI DOI 10.1145/1645953.1646301
NR 15
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703002
DA 2019-06-15
ER

PT S
AU Advani, M
   Ganguli, S
AF Advani, Madhu
   Ganguli, Surya
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI An equivalence between high dimensional Bayes optimal inference and
   M-estimation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID SELECTION
AB When recovering an unknown signal from noisy measurements, the computational difficulty of performing optimal Bayesian MMSE (minimum mean squared error) inference often necessitates the use of maximum a posteriori (MAP) inference, a special case of regularized M-estimation, as a surrogate. However, MAP is suboptimal in high dimensions, when the number of unknown signal components is similar to the number of measurements. In this work we demonstrate, when the signal distribution and the likelihood function associated with the noise are both log-concave, that optimal MMSE performance is asymptotically achievable via another M-estimation procedure. This procedure involves minimizing convex loss and regularizer functions that are nonlinearly smoothed versions of the widely applied MAP optimization problem. Our findings provide a new heuristic derivation and interpretation for recent optimal M-estimators found in the setting of linear measurements and additive noise, and further extend these results to nonlinear measurements with non-additive noise. We numerically demonstrate superior performance of our optimal M-estimators relative to MAP. Overall, at the heart of our work is the revelation of a remarkable equivalence between two seemingly very different computational problems: namely that of high dimensional Bayesian integration underlying MMSE inference, and high dimensional convex optimization underlying M-estimation. In essence we show that the former difficult integral may be computed by solving the latter, simpler optimization problem.
C1 [Advani, Madhu; Ganguli, Surya] Stanford Univ, Dept Appl Phys, Stanford, CA 94305 USA.
RP Advani, M (reprint author), Stanford Univ, Dept Appl Phys, Stanford, CA 94305 USA.
EM msadvani@stanford.edu; sganguli@stanford.edu
FU Stanford MBC; SGF; Burroughs Wellcome foundation; Simons foundation;
   Sloan foundation; McKnight foundation; McDonnell foundation; Office of
   Naval Research
FX The authors would like to thank Lenka Zdeborova and Stephen Boyd for
   useful discussions and also Chris Stock and Ben Poole for comments on
   the manuscript. M.A. thanks the Stanford MBC and SGF for support. S.G.
   thanks the Burroughs Wellcome, Simons, Sloan, McKnight, and McDonnell
   foundations, and the Office of Naval Research for support.
CR Advani M, 2016, PHYS REV X, V6, DOI 10.1103/PhysRevX.6.031034
   Bayati M, 2015, ANN APPL PROBAB, V25, P753, DOI 10.1214/14-AAP1010
   Bayati M, 2011, IEEE T INFORM THEORY, V57, P764, DOI 10.1109/TIT.2010.2094817
   Bean D, 2013, P NATL ACAD SCI USA, V110, P14563, DOI 10.1073/pnas.1307845110
   Bruckstein AM, 2009, SIAM REV, V51, P34, DOI 10.1137/060657704
   Candes EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731
   DONOHO D., 2013, PROBABILITY THEORY R, P1, DOI DOI 10.1007/S00440-015-0675-Z
   Donoho DL, 2009, P NATL ACAD SCI USA, V106, P18914, DOI 10.1073/pnas.0909892106
   Guo D, 2007, INF THEOR 2007 ISIT
   Huber P. J., 2009, ROBUST STAT
   Javanmard A., 2013, INFORM INFERENCE
   Kabashima Y, 2016, IEEE T INFORM THEORY, V62, P4228, DOI 10.1109/TIT.2016.2556702
   Krzakala F, 2012, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2012/08/P08009
   Parikh N., 2013, FDN TRENDS OPTIM, V1, P123, DOI DOI 10.1561/2400000003
   Rangan S., 2011, Proceedings of the 2011 IEEE International Symposium on Information Theory - ISIT, P2168, DOI 10.1109/ISIT.2011.6033942
   Rangan S, 2010, INF SCI SYST CISS 20
   Thrampoulidis C, 2015, ANN ALLERTON CONF, P410, DOI 10.1109/ALLERTON.2015.7447033
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Wang CC, 2006, P 44 ANN ALL C COMM, P926
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973705013
DA 2019-06-15
ER

PT S
AU Agrawal, S
   Devanur, NR
AF Agrawal, Shipra
   Devanur, Nikhil R.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Linear Contextual Bandits with Knapsacks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We consider the linear contextual bandit problem with resource consumption, in addition to reward generation. In each round, the outcome of pulling an arm is a reward as well as a vector of resource consumptions. The expected values of these outcomes depend linearly on the context of that arm. The budget/capacity constraints require that the total consumption doesn't exceed the budget for each resource. The objective is once again to maximize the total reward. This problem turns out to be a common generalization of classic linear contextual bandits (linContextual) [8, 11, 1], bandits with knapsacks (BwK) [3, 9], and the online stochastic packing problem (OSPP) [4, 14]. We present algorithms with near-optimal regret bounds for this problem. Our bounds compare favorably to results on the unstructured version of the problem [5, 10] where the relation between the contexts and the outcomes could be arbitrary, but the algorithm only competes against a fixed set of policies accessible through an optimization oracle. We combine techniques from the work on linContextual, BwK and OSPP in a nontrivial manner while also tackling new difficulties that are not present in any of these special cases.
C1 [Agrawal, Shipra] Columbia Univ, New York, NY 10027 USA.
   [Devanur, Nikhil R.] Microsoft Res, Redmond, WA USA.
RP Agrawal, S (reprint author), Columbia Univ, New York, NY 10027 USA.
EM sa3305@columbia.edu; nikdev@microsoft.com
CR Abbasi-Yadkori Y., 2012, NIPS
   Agarwal A., 2014, ICML 2014
   Agrawal S., 2014, P 15 ACM C EC COMP E
   Agrawal S., 2016, COLT
   Agrawal S, 2014, OPER RES, V62, P876, DOI 10.1287/opre.2014.1289
   Agrawal Shipra, 2015, P 26 ANN ACM SIAM S, P1405, DOI DOI 10.1137/1.9781611973730.93
   Arora S, 2012, THEORY COMPUT, V8, P121, DOI DOI 10.4086/TOC.2012.V008A006
   Auer P., 2003, J MACH LEARN RES, V3
   Badanidiyuru A., 2014, P 27 ANN C LEARN THE, P1109
   Badanidiyuru A, 2013, ANN IEEE SYMP FOUND, P207, DOI 10.1109/FOCS.2013.30
   Chu W., 2011, AISTATS
   Dani V., 2008, COLT
   Devanur N. R., 2011, EC
   Devanur N. R., 2009, EC
   Feldman J., 2010, P 18 ANN EUR C ALG 1
   Guruswami V, 2009, SIAM J COMPUT, V39, P742, DOI 10.1137/070685798
   Zheng L, 2015, PROCEEDINGS OF THE 22ND INTERNATIONAL CONGRESS ON SOUND AND VIBRATION
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702039
DA 2019-06-15
ER

PT S
AU Ahn, S
   Chertkov, M
   Shin, J
AF Ahn, Sungsoo
   Chertkov, Michael
   Shin, Jinwoo
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Synthesis of MCMC and Belief Propagation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID TIME; ALGORITHMS; BETHE
AB Markov Chain Monte Carlo (MCMC) and Belief Propagation (BP) are the most popular algorithms for computational inference in Graphical Models (GM). In principle, MCMC is an exact probabilistic method which, however, often suffers from exponentially slow mixing. In contrast, BP is a deterministic method, which is typically fast, empirically very successful, however in general lacking control of accuracy over loopy graphs. In this paper, we introduce MCMC algorithms correcting the approximation error of BP, i.e., we provide a way to compensate for BP errors via a consecutive BP-aware MCMC. Our framework is based on the Loop Calculus approach which allows to express the BP error as a sum of weighted generalized loops. Although the full series is computationally intractable, it is known that a truncated series, summing up all 2-regular loops, is computable in polynomial-time for planar pair-wise binary GMs and it also provides a highly accurate approximation empirically. Motivated by this, we first propose a polynomial-time approximation MCMC scheme for the truncated series of general (non-planar) pair-wise binary models. Our main idea here is to use the Worm algorithm, known to provide fast mixing in other (related) problems, and then design an appropriate rejection scheme to sample 2-regular loops. Furthermore, we also design an efficient rejection-free MCMC scheme for approximating the full series. The main novelty underlying our design is in utilizing the concept of cycle basis, which provides an efficient decomposition of the generalized loops. In essence, the proposed MCMC schemes run on transformed GM built upon the non-trivial BP solution, and our experiments show that this synthesis of BP and MCMC outperforms both direct MCMC and bare BP schemes.
C1 [Ahn, Sungsoo; Shin, Jinwoo] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea.
   [Chertkov, Michael] Los Alamos Natl Lab, Theoret Div, T-4, Los Alamos, NM 87545 USA.
   [Chertkov, Michael] Los Alamos Natl Lab, Ctr Nonlinear Studies, Los Alamos, NM 87545 USA.
   [Chertkov, Michael] Skolkovo Inst Sci & Technol, Moscow 143026, Russia.
RP Ahn, S (reprint author), Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea.
EM sungsoo.ahn@kaist.ac.kr; chertkov@lan1.gov; jinwoos@kaist.ac.kr
FU National Research Council of Science & Technology (NST) grant by the
   Korea government (MSIP) [CRC-15-05-ETRI]; U.S. Department of Energy's
   Office of Electricity as part of the DOE Grid Modernization Initiative
FX This work was supported by the National Research Council of Science &
   Technology (NST) grant by the Korea government (MSIP) (No.
   CRC-15-05-ETRI), and funding from the U.S. Department of Energy's Office
   of Electricity as part of the DOE Grid Modernization Initiative.
CR Andrieu C, 2003, MACH LEARN, V50, P5, DOI 10.1023/A:1020281327116
   Bang J., 2008, DIGRAPHS THEORY ALGO
   Baxter R. J., 2007, EXACTLY SOLVED MODEL
   Chandrasekaran V., 2008, ASS UNCERTAINTY ARTI
   Chertkov M, 2006, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2006/06/P06009
   Chertkov M, 2008, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2008/05/P05003
   Collevecchio A., 2015, ARXIV150903201
   Dyer M, 2002, SIAM J COMPUT, V31, P1527, DOI 10.1137/S0097539701383844
   Freeman WT, 2000, INT J COMPUT VISION, V40, P25, DOI 10.1023/A:1026501619075
   GALLAGER RG, 1962, IRE T INFORM THEOR, V8, P21, DOI 10.1109/TIT.1962.1057683
   Gomez V, 2010, J MACH LEARN RES, V11, P1273
   HORTON JD, 1987, SIAM J COMPUT, V16, P358, DOI 10.1137/0216026
   JERRUM M, 1993, SIAM J COMPUT, V22, P1087, DOI 10.1137/0222066
   Jordan M. I., 1998, SPRINGER SCI BUSINES, V89
   Kasteleyn P. W., 2009, CLASSIC PAPERS COMBI, P281
   KIRKPATRICK S, 1984, J STAT PHYS, V34, P975, DOI 10.1007/BF01009452
   Kramers HA, 1941, PHYS REV, V60, P263, DOI 10.1103/PhysRev.60.263
   Kschischang FR, 1998, IEEE J SEL AREA COMM, V16, P219, DOI 10.1109/49.661110
   Nicholas R., 2012, ADV NEURAL INFORM PR
   Pearl J., 2014, MORGAN KAUFMANN
   Pearl J., 1982, ASS ADV ARTIFICIAL I
   Prokof'ev N, 2001, PHYS REV LETT, V87, part. no., DOI 10.1103/PhysRevLett.87.160601
   Schweinsberg J, 2002, RANDOM STRUCT ALGOR, V20, P59, DOI 10.1002/rsa.10000
   Shin J, 2014, IEEE T INFORM THEORY, V60, P3959, DOI 10.1109/TIT.2014.2317487
   Teh Y. W., 2001, P 18 C UNC ART INT, P493
   Yuille AL, 2002, NEURAL COMPUT, V14, P1691, DOI 10.1162/08997660260028674
NR 26
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702065
DA 2019-06-15
ER

PT S
AU Alaa, AM
   van der Schaar, M
AF Alaa, Ahmed M.
   van der Schaar, Mihaela
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Balancing Suspense and Surprise: Timely Decision Making with Endogenous
   Information Acquisition
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB [We develop a Bayesian model for decision-making under time pressure with endogenous information acquisition. In our model, the decision-maker decides when to observe (costly) information by sampling an underlying continuous-time stochastic process (time series) that conveys information about the potential occurrence/non-occurrence of an adverse event which will terminate the decision-making process. In her attempt to predict the occurrence of the adverse event, the decision-maker follows a policy that determines when to acquire information from the time series (continuation), and when to stop acquiring information and make a final prediction (stopping). We show that the optimal policy has a "rendezvous" structure, i.e. a structure in which whenever a new information sample is gathered from the time series, the optimal "date" for acquiring the next sample becomes computable. The optimal interval between two information samples balances a trade-off between the decision maker's "surprise", i.e. the drift in her posterior belief after observing new information, and "suspense", i.e. the probability that the adverse event occurs in the time interval between two information samples. Moreover, we characterize the continuation and stopping regions in the decision-maker's state-space, and show that they depend not only on the decision-maker's beliefs, but also on the "context", i.e. the current realization of the time series.
C1 [Alaa, Ahmed M.; van der Schaar, Mihaela] Univ Calif Los Angeles, Elect Engn Dept, Los Angeles, CA 90095 USA.
RP Alaa, AM (reprint author), Univ Calif Los Angeles, Elect Engn Dept, Los Angeles, CA 90095 USA.
FU ONR; NSF [ECCS 1462245]
FX This work was supported by the ONR and the NSF (Grant number: ECCS
   1462245).
CR Banerjee T, 2012, SEQUENTIAL ANAL, V31, P40, DOI 10.1080/07474946.2012.651981
   Bertsekas D. P., 1978, STOCHASTIC OPTIMAL C, V23
   Bogacz R, 2006, PSYCHOL REV, V113, P700, DOI 10.1037/0033-295X.113.4.700
   Bortfeld T, 2015, INFORMS J COMPUT, V27, P788, DOI 10.1287/ijoc.2015.0659
   Chalfin DB, 2007, CRIT CARE MED, V35, P1477, DOI 10.1097/01.CCM.0000266585.74905.5A
   Dayanik S, 2013, SIAM J CONTROL OPTIM, V51, P2922, DOI 10.1137/100818005
   Drugowitsch J., 2014, ADV NEURAL INFORM PR, P748
   Drugowitsch J, 2012, J NEUROSCI, V32, P3612, DOI 10.1523/JNEUROSCI.4010-11.2012
   Ely J, 2015, J POLIT ECON, V123, P215, DOI 10.1086/677350
   Frazier P., 2007, ADV NEURAL INFORM PR, P465
   Itti L., 2005, ADV NEURAL INFORM PR, V19, P547
   Khalvati K., 2015, ADV NEURAL INFORM PR, P2404
   Peskir G., 2006, LEC MATH
   Schulam P, 2015, ADV NEURAL INFORM PR, P748
   Shapiro S, 1998, INT J EPIDEMIOL, V27, P735, DOI 10.1093/ije/27.5.735
   Shenoy P, 2012, ADV NEURAL INFORM PR, P2123
   Shiryaev AN, 2007, OPTIMAL STOPPING RUL, V8
   Shreve S. E, 2004, STOCHASTIC CALCULUS, V11
   Shvartsman M., 2015, ADV NEURAL INF PROCE, P2476
   Simen P, 2011, FRONT INTEGR NEUROSC, V5, DOI 10.3389/fnint.2011.00028
   Wald A., 1973, SEQUENTIAL ANAL
   Wiens J., 2012, ADV NEURAL INFORM PR, P467
   Yu AJ, 2009, J EXP PSYCHOL HUMAN, V35, P700, DOI 10.1037/a0013553
   Yu J. A., 2013, ADV NEURAL INFORM PR, P2607
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700026
DA 2019-06-15
ER

PT S
AU Alemi, AA
   Chollet, F
   Een, N
   Irving, G
   Szegedy, C
   Urban, J
AF Alemi, Alexander A.
   Chollet, Francois
   Een, Niklas
   Irving, Geoffrey
   Szegedy, Christian
   Urban, Josef
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI DeepMath - Deep Sequence Models for Premise Selection
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID FORMAL VERIFICATION; MIZAR
AB We study the effectiveness of neural sequence models for premise selection in automated theorem proving, one of the main bottlenecks in the formalization of mathematics. We propose a two stage approach for this task that yields good results for the premise selection task on the Mizar corpus while avoiding the hand-engineered features of existing state-of-the-art models. To our knowledge, this is the first time deep learning has been applied to theorem proving on a large scale.
C1 [Alemi, Alexander A.; Chollet, Francois; Een, Niklas; Irving, Geoffrey; Szegedy, Christian] Google Inc, Mountain View, CA 94043 USA.
   [Urban, Josef] Czech Tech Univ, Prague, Czech Republic.
RP Alemi, AA (reprint author), Google Inc, Mountain View, CA 94043 USA.
EM alemi@google.com; fchollet@google.com; een@google.com;
   geoffreyi@google.com; szegedy@google.com; josef.urban@gmail.com
FU ERC Consolidator grant [649043 AI4REASON]
FX Supported by ERC Consolidator grant nr. 649043 AI4REASON.
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Bancerek G, 2002, J AUTOM REASONING, V29, P189, DOI 10.1023/A:1021966832558
   Baudis Petr, 2016, ARXIV160306127
   Blanchette JC, 2016, J FORMALIZ REASON, V9, P101, DOI 10.6092/issn.1972-5787/4593
   Chollet F., 2015, KERAS
   Chung J., 2015, ARXIV150202367
   Dai A. M., 2015, ADV NEURAL INFORM PR, V28, P3061
   de Bruijn N., P S AUT DEM VERS FRA, P29
   Glorot X., 2010, JLMR P TRACK, P249, DOI DOI 10.1.1/207.2059
   Gonthier Georges, 2007, Computer Mathematics. 8th Asian Symposium, ASCM 2007. Revised and Invited Papers
   Gonthier G, 2013, LECT NOTES COMPUT SC, V7998, P163, DOI 10.1007/978-3-642-39634-2_14
   Grabowski A, 2010, J FORMALIZ REASON, V3, P153
   Hales T, 2017, FORUM MATH PI, V5, DOI 10.1017/fmp.2017.1
   Harrison J, 1996, LECT NOTES COMPUT SC, V1166, P265
   Harrison J, 2014, HDB HIST LOGIC COMPU, V9, P135
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Kaiser  Lukasz, 2015, ARXIV151108228
   Kaliszyk C., 2013, EPIC SER, V14, P87
   Kaliszyk C, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3084
   Kaliszyk C, 2015, J AUTOM REASONING, V55, P245, DOI 10.1007/s10817-015-9330-8
   Kaliszyk C, 2014, J AUTOM REASONING, V53, P173, DOI 10.1007/s10817-014-9303-3
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Klein G, 2010, COMMUN ACM, V53, P107, DOI 10.1145/1743546.1743574
   Kuehlwein D., 2013, EPIC SERIES, V21, P82
   Leroy X, 2009, COMMUN ACM, V52, P107, DOI 10.1145/1538788.1538814
   Mikolov T, 2010, NTFRSPEECH, V2, P3
   Mohamed O. A., 2008, THEOREM PROVING HIGH
   POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046
   Robinson  J., 2001, HDB AUTOMATED REASON
   Schulz S, 2002, AI COMMUN, V15, P111
   Sukhbaatar S., 2015, ADV NEURAL INFORM PR, P2431
   Urban Josef, 2013, Automated Reasoning and Mathematics. Essays in Memory of William W. McCune, P240, DOI 10.1007/978-3-642-36675-8_13
   Urban J, 2006, J AUTOM REASONING, V37, P21, DOI 10.1007/s10817-006-9032-3
   Vinyals O, 2015, ARXIV150605869
   Zaremba W., 2014, ARXIV14104615
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702027
DA 2019-06-15
ER

PT S
AU Ali, A
   Kolter, JZ
   Tibshirani, RJ
AF Ali, Alnur
   Kolter, J. Zico
   Tibshirani, Ryan J.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI The Multiple Quantile Graphical Model
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID ADDITIVE-MODELS; SELECTION; NETWORKS; INFERENCE
AB We introduce the Multiple Quantile Graphical Model (MQGM), which extends the neighborhood selection approach of Meinshausen and Buhlmann for learning sparse graphical models. The latter is defined by the basic subproblem of modeling the conditional mean of one variable as a sparse function of all others. Our approach models a set of conditional quantiles of one variable as a sparse function of all others, and hence offers a much richer, more expressive class of conditional distribution estimates. We establish that, under suitable regularity conditions, the MQGM identifies the exact conditional independencies with probability tending to one as the problem size grows, even outside of the usual homoskedastic Gaussian data model. We develop an efficient algorithm for fitting the MQGM using the alternating direction method of multipliers. We also describe a strategy for sampling from the joint distribution that underlies the MQGM estimate. Lastly, we present detailed experiments that demonstrate the flexibility and effectiveness of the MQGM in modeling hetereoskedastic non-Gaussian data.
C1 [Ali, Alnur] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.
   [Kolter, J. Zico] Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15213 USA.
   [Tibshirani, Ryan J.] Carnegie Mellon Univ, Dept Stat, Pittsburgh, PA 15213 USA.
RP Ali, A (reprint author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.
EM alnurali@cmu.edu; zkolter@cs.cmu.edu; ryantibs@cmu.edu
FU DOE [DE-FG02-97ER25308]; NSF Expeditions in Computation Award,
   CompSustNet [CCF-1522054]; NSF [DMS-1309174, DMS-1554123]
FX AA was supported by DOE Computational Science Graduate Fellowship
   DE-FG02-97ER25308. JZK was supported by an NSF Expeditions in
   Computation Award, CompSustNet, CCF-1522054. RJT was supported by NSF
   Grants DMS-1309174 and DMS-1554123.
CR Ali Alnur, 2016, TECHNICAL REPORT
   Banerjee O, 2008, J MACH LEARN RES, V9, P485
   BESAG J, 1974, J ROY STAT SOC B MET, V36, P192
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Centers for Disease Control and Prevention (CDC), 2015, INFL NAT REG LEV GRA
   Chen SZ, 2015, BIOMETRIKA, V102, P47, DOI 10.1093/biomet/asu051
   DEMPSTER AP, 1972, BIOMETRICS, V28, P157, DOI 10.2307/2528966
   Fan JQ, 2014, ANN STAT, V42, P324, DOI 10.1214/13-AOS1191
   Finegold M, 2011, ANN APPL STAT, V5, P1057, DOI 10.1214/10-AOAS410
   Friedman J., 2010, TECHNICAL REPORT
   Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045
   Heckerman D, 2001, J MACH LEARN RES, V1, P49, DOI 10.1162/153244301753344614
   Hofling H, 2009, J MACH LEARN RES, V10, P883
   Johnson NA, 2013, J COMPUT GRAPH STAT, V22, P246, DOI 10.1080/10618600.2012.681238
   Kato Kengo, 2011, TECHNICAL REPORT
   Khare K, 2015, J R STAT SOC B, V77, P803, DOI 10.1111/rssb.12088
   KOENKER R, 1994, BIOMETRIKA, V81, P673
   KOENKER R, 1978, ECONOMETRICA, V46, P33, DOI 10.2307/1913643
   Koenker R., 2005, QUANTILE REGRESSION
   Koenker R, 2011, BRAZ J PROBAB STAT, V25, P239, DOI 10.1214/10-BJPS131
   Lauritzen SL, 1996, GRAPHICAL MODELS
   Lee J., 2013, J MACH LEARN RES, P388
   Liu H, 2012, TECHNICAL REPORT
   Liu H, 2012, ANN STAT, V40, P2293, DOI 10.1214/12-AOS1037
   Liu H, 2009, J MACH LEARN RES, V10, P2295
   Meier L, 2009, ANN STAT, V37, P3779, DOI 10.1214/09-AOS692
   Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281
   Neville J, 2004, FOURTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P170, DOI 10.1109/ICDM.2004.10101
   O'Donoghue Brendan, 2013, TECHNICAL REPORT
   Oh S., 2014, P ADV NEUR INF PROC, V27, P667
   Parikh N., 2013, FDN TRENDS OPTIM, V1, P123, DOI DOI 10.1561/2400000003
   Peng J, 2009, J AM STAT ASSOC, V104, P735, DOI 10.1198/jasa.2009.0126
   Raskutti G, 2012, J MACH LEARN RES, V13, P389
   Rocha Guilherme, 2008, TECHNICAL REPORT
   Rothman AJ, 2008, ELECTRON J STAT, V2, P494, DOI 10.1214/08-EJS176
   Sohn K. -A., 2012, P INT C ART INT STAT, P1081
   Takeuchi I, 2006, J MACH LEARN RES, V7, P1231
   Varin C, 2005, BIOMETRIKA, V92, P519, DOI 10.1093/biomet/92.3.519
   Voorman A, 2014, BIOMETRIKA, V101, P85, DOI 10.1093/biomet/ast053
   Wang YJ, 2008, BIOMETRIKA, V95, P735, DOI 10.1093/biomet/asn029
   Wytock M., 2013, P 30 INT C MACH LEAR, P1265
   Yang E., 2012, ADV NEURAL INFORM PR, V25, P1358
   Yang EH, 2015, J MACH LEARN RES, V16, P3813
   Yuan M, 2007, BIOMETRIKA, V94, P19, DOI 10.1093/biomet/asm018
   Yuan XT, 2014, IEEE T INFORM THEORY, V60, P1673, DOI 10.1109/TIT.2013.2296784
NR 45
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700056
DA 2019-06-15
ER

PT S
AU Allen-Zhu, Z
   Li, YZ
AF Allen-Zhu, Zeyuan
   Li, Yuanzhi
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI LazySVD: Even Faster SVD Decomposition Yet Without Agonizing Pain
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We study k-SVD that is to obtain the first k singular vectors of a matrix A. Recently, a few breakthroughs have been discovered on k-SVD: Musco and Musco [19] proved the first gap-free convergence result using the block Krylov method, Shamir [21] discovered the first variance-reduction stochastic method, and Bhojanapalli et al. [7] provided the fastest O(nnz(A) + poly(1/epsilon))-time algorithm using alternating minimization.
   In this paper, we put forward a new and simple LazySVD framework to improve the above breakthroughs. This framework leads to a faster gap-free method outperforming [19], and the first accelerated and stochastic method outperforming [21]. In the O(nnz(A) + poly(1=epsilon)) running-time regime, LazySVD outperforms [7] in certain parameter regimes without even using alternating minimization.
C1 [Allen-Zhu, Zeyuan] Inst Adv Study, Olden Lane, Princeton, NJ 08540 USA.
   [Allen-Zhu, Zeyuan; Li, Yuanzhi] Princeton Univ, Princeton, NJ 08544 USA.
RP Allen-Zhu, Z (reprint author), Inst Adv Study, Olden Lane, Princeton, NJ 08540 USA.; Allen-Zhu, Z (reprint author), Princeton Univ, Princeton, NJ 08544 USA.
EM zeyuan@csail.mit.edu; yuanzhil@cs.princeton.edu
FU Microsoft Research Award [0518584]; NSF [CCF-1412958]
FX The full version of this paper can be found on
   https://arxiv.org/abs/1607.03463.This paper is partially supported by a
   Microsoft Research Award, no. 0518584, and an NSF grant, no.
   CCF-1412958.
CR Allen-Zhu Z., 2016, ICML
   Allen-Zhu Zeyuan, 2016, ARXIV E PRINTS
   Allen-Zhu Zeyuan, 2017, ARXIV E PRINTS
   Arora S, 2009, J ACM, V56, DOI 10.1145/1502793.1502794
   Bhojanapalli S., 2015, P 26 ANN ACM SIAM S, P902
   Clarkson K. L., 2013, P 45 ANN ACM S THEOR, P81, DOI DOI 10.1145/2488608.2488620
   Cohen M. B., 2015, P 47 ANN ACM S THEOR, P163, DOI DOI 10.1145/2746539.2746569
   Drineas Petros, 2011, ARXIV E PRINTS
   Fan Rong-En, LIBSVM DATA CLASSIFI
   Garber Dan, 2015, ARXIV E PRINTS
   Garber Dan, 2016, ICML
   Golub G. H., 2012, MATRIX COMPUTATIONS
   Jain Prateek, 2016, COLT
   Leskovec J., 2014, SNAP DATASETS STANFO
   Li Chris J., 2016, ARXIV E PRINTS
   Li RC, 2015, NUMER MATH, V131, P83, DOI 10.1007/s00211-014-0681-6
   Musco C., 2015, ADV NEURAL INFORM PR, V28, P1396
   Shamir O., 2015, P 32 INT C MACH LEAR, P144
   Shamir Ohad, 2016, ICML
   Tropp Joel A., 2015, ARXIV E PRINTS
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704032
DA 2019-06-15
ER

PT S
AU Allen-Zhu, Z
   Yuan, Y
   Sridharan, K
AF Allen-Zhu, Zeyuan
   Yuan, Yang
   Sridharan, Karthik
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Exploiting the Structure: Stochastic Gradient Methods Using Raw Clusters
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB The amount of data available in the world is growing faster than our ability to deal with it. However, if we take advantage of the internal structure, data may become much smaller for machine learning purposes. In this paper we focus on one of the fundamental machine learning tasks, empirical risk minimization (ERM), and provide faster algorithms with the help from the clustering structure of the data. We introduce a simple notion of raw clustering that can be efficiently computed from the data, and propose two algorithms based on clustering information. Our accelerated algorithm ClusterACDM is built on a novel Haar transformation applied to the dual space of the ERM problem, and our variance-reduction based algorithm ClusterSVRG introduces a new gradient estimator using clustering. Our algorithms outperform their classical counterparts ACDM and SVRG respectively.
C1 [Allen-Zhu, Zeyuan] Princeton Univ, IAS, Princeton, NJ 08544 USA.
   [Yuan, Yang; Sridharan, Karthik] Cornell Univ, Ithaca, NY 14853 USA.
RP Allen-Zhu, Z (reprint author), Princeton Univ, IAS, Princeton, NJ 08544 USA.
EM zeyuan@csail.mit.edu; yangyuan@cs.cornell.edu; sridharan@cs.cornell.edu
CR Allen-Zhu Z., 2016, ICML
   Andoni A., 2015, ADV NEURAL INFORM PR, P1225
   Andoni Alexandr, 2004, E2LSH
   Bottou Leon, STOCHASTIC GRADIENT
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Defazio A., 2014, NIPS
   Defazio Aaron J., 2014, ICML
   Fan Rong-En, LIBSVM DATA CLASSIFI
   Frostig Roy, 2015, ICML, V37, P1
   Ge TZ, 2014, IEEE T PATTERN ANAL, V36, P744, DOI 10.1109/TPAMI.2013.240
   Hofmann Thomas, 2015, ADV NEURAL INFORM PR, P2296
   Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   LEE YT, 2013, FOCS, P147, DOI DOI 10.1109/FOCS.2013.24
   Lin H., 2015, NIPS
   Lin Qihang, 2014, NIPS, P3059
   Mairal J, 2015, SIAM J OPTIMIZ, V25, P829, DOI 10.1137/140957639
   Nesterov Yurii, 2004, INTRO LECT CONVEX PR, VI
   Schmidt Mark, 2013, ARXIV13092388, P1
   Shalev-Shwartz S., 2014, P 31 INT C MACH LEAR, P64
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Shalev-Shwartz Shai, 2012, ARXIV12112717, P1
   Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791
   Yang TT, 2016, DEEP-SEA RES PT II, V129, P282, DOI 10.1016/j.dsr2.2014.01.014
   Zhang Tong, 2004, ICML
   Zhang Yuchen, 2015, ICML
   Zhao Peilin, 2015, P INT C MACH LEARN, P1
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703039
DA 2019-06-15
ER

PT S
AU Allen-Zhu, Z
   Hazan, E
AF Allen-Zhu, Zeyuan
   Hazan, Elad
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Optimal Black-Box Reductions Between Optimization Objectives
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID MINIMIZATION
AB The diverse world of machine learning applications has given rise to a plethora of algorithms and optimization methods, finely tuned to the specific regression or classification task at hand. We reduce the complexity of algorithm design for machine learning by reductions: we develop reductions that take a method developed for one setting and apply it to the entire spectrum of smoothness and strong-convexity in applications.
   Furthermore, unlike existing results, our new reductions are optimal and more practical. We show how these new reductions give rise to new and faster running times on training linear classifiers for various families of loss functions, and conclude with experiments showing their successes also in practice.
C1 [Allen-Zhu, Zeyuan] Inst Adv Study, Olden Lane, Princeton, NJ 08540 USA.
   [Allen-Zhu, Zeyuan; Hazan, Elad] Princeton Univ, Princeton, NJ 08544 USA.
RP Allen-Zhu, Z (reprint author), Inst Adv Study, Olden Lane, Princeton, NJ 08540 USA.; Allen-Zhu, Z (reprint author), Princeton Univ, Princeton, NJ 08544 USA.
EM zeyuan@csail.mit.edu; ehazan@cs.princeton.edu
FU NSF [1523815]; Microsoft Research Grant [0518584]
FX This paper is partially supported by an NSF Grant, no. 1523815, and a
   Microsoft Research Grant, no. 0518584.
CR Allen-Zhu Z., 2016, ICML
   Allen-Zhu Zeyuan, 2016, ARXIV E PRINTS
   Allen-Zhu Zeyuan, 2014, ARXIV E PRINTS
   Beck A, 2012, SIAM J OPTIMIZ, V22, P557, DOI 10.1137/100818327
   Bot RI, 2015, TOP, V23, P124, DOI 10.1007/s11750-014-0326-z
   Bubeck Sebastien, 2015, ARXIV E PRINTS
   Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1
   Defazio A., 2014, NIPS
   Fan Rong-En, LIBSVM DATA CLASSIFI
   Frostig Roy, 2015, ICML, V37, P1
   Hazan E, 2014, J MACH LEARN RES, V15, P2489
   Hazan Elad, 2015, FDN TRENDS MACHINE L, VXX, P1
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Lacoste-Julien Simon, 2012, ARXIV E PRINTS
   LEE YT, 2013, FOCS, P147, DOI DOI 10.1109/FOCS.2013.24
   Lessard Laurent, 2014, CORR
   Lin H., 2016, COMMUNICATION
   Lin H., 2015, NIPS
   Lin Qihang, 2014, NIPS, P3059
   Nemirovski A, 2004, SIAM J OPTIMIZ, V15, P229, DOI 10.1137/S1052623403425629
   Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5
   Nesterov Y, 1983, SOVIET MATH DOCL USS, V269, P543, DOI DOI 10.1137/S0895479802413856
   Nesterov Yurii, 2004, INTRO LECT CONVEX PR, VI
   Orabona Francesco, 2012, ARXIV12062372
   Rakhlin Alexander, 2012, ICML
   Schmidt Mark, 2013, ARXIV13092388, P1
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Shalev-Shwartz Shai, 2012, ARXIV12112717, P1
   Tran-Dinh Quoc, 2015, ARXIV150900106
   Woodworth Blake, 2016, NIPS
   Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791
   Xiao L, 2013, SIAM J OPTIMIZ, V23, P1062, DOI 10.1137/120869997
   Zhang Yuchen, 2015, ICML
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702111
DA 2019-06-15
ER

PT S
AU Alvarez, JM
   Salzmann, M
AF Alvarez, Jose M.
   Salzmann, Mathieu
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning the Number of Neurons in Deep Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID ALGORITHMS
AB Nowadays, the number of layers and of neurons in each layer of a deep network are typically set manually. While very deep and wide networks have proven effective in general, they come at a high memory and computation cost, thus making them impractical for constrained platforms. These networks, however, are known to have many redundant parameters, and could thus, in principle, be replaced by more compact architectures. In this paper, we introduce an approach to automatically determining the number of neurons in each layer of a deep network during learning. To this end, we propose to make use of a group sparsity regularizer on the parameters of the network, where each group is defined to act on a single neuron. Starting from an overcomplete network, we show that our approach can reduce the number of parameters by up to 80% while retaining or even improving the network accuracy.
C1 [Alvarez, Jose M.] Data61 CSIRO, Canberra, ACT 2601, Australia.
   [Salzmann, Mathieu] Ecole Polytech Fed Lausanne, CVLab, CH-1015 Lausanne, Switzerland.
RP Alvarez, JM (reprint author), Data61 CSIRO, Canberra, ACT 2601, Australia.
EM jose.alvarez@data61.csiro.au; mathieu.salzmann@epfl.ch
CR Alvarez J. M., 2016, CORR
   Ash T., 1989, Connection Science, V1, P365, DOI 10.1080/09540098908915647
   Bartlett Peter L., 1996, NIPS
   BELLO MG, 1992, IEEE T NEURAL NETWOR, V3, P864, DOI 10.1109/72.165589
   Cheng Yu, 2015, ICCV
   Collins M. D., 2014, CORR
   Collobert R, 2011, BIGLEARN NIPS WORKSH
   Denil Misha, 2013, CORR
   Denton E., 2014, NIPS
   Gong Y., 2014, CORR
   Goodfellow I, 2013, ICML
   Hassibi B., 1993, ICNN
   He K., 2015, CORR
   Hinton G. E., 2014, DISTILLING KNOWLEDGE
   Jaderberg M., 2014, ECCV
   Jaderberg M., 2014, BMVC
   Ji CY, 1990, NEURAL COMPUT, V2, P188, DOI 10.1162/neco.1990.2.2.188
   Krogh Anders, 1992, NIPS
   LeCun  Y., 1990, NIPS
   Liu B., 2015, CVPR
   Montufar Guido F., 2014, NIPS
   Mozer M., 1988, NIPS
   Murray K., 2015, CORR
   Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003
   REED R, 1993, IEEE T NEURAL NETWOR, V4, P740, DOI 10.1109/72.248452
   Romero A., 2015, ICLR
   Russakovsky Olga, 2015, IJCV
   Simon N., 2013, J COMPUTATIONAL GRAP
   Simonyan K., 2014, CORR
   Srivastava R. K., 2015, NIPS
   Theodoridis S., 2015, MACHINE LEARNING BAY, V8
   Wang J., 2014, J FIBER BIOENGINEERI, V7, P603
   Weigend A. S., 1991, NIPS
   Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x
   Zhou B., 2015, PLACES IMAGE DATABAS
   Zhou H., 2016, ECCV
NR 36
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703008
DA 2019-06-15
ER

PT S
AU Anagnostopoulos, A
   Lacki, J
   Lattanzi, S
   Leonardi, S
   Mahdian, M
AF Anagnostopoulos, Aris
   Lacki, Jakub
   Lattanzi, Silvio
   Leonardi, Stefano
   Mahdian, Mohammad
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Community Detection on Evolving Graphs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID STOCHASTIC BLOCKMODELS
AB Clustering is a fundamental step in many information-retrieval and data-mining applications. Detecting clusters in graphs is also a key tool for finding the community structure in social and behavioral networks. In many of these applications, the input graph evolves over time in a continual and decentralized manner, and, to maintain a good clustering, the clustering algorithm needs to repeatedly probe the graph. Furthermore, there are often limitations on the frequency of such probes, either imposed explicitly by the online platform (e.g., in the case of crawling proprietary social networks like twitter) or implicitly because of resource limitations (e.g., in the case of crawling the web).
   In this paper, we study a model of clustering on evolving graphs that captures this aspect of the problem. Our model is based on the classical stochastic block model, which has been used to assess rigorously the quality of various static clustering methods. In our model, the algorithm is supposed to reconstruct the planted clustering, given the ability to query for small pieces of local information about the graph, at a limited rate. We design and analyze clustering algorithms that work in this model, and show asymptotically tight upper and lower bounds on their accuracy. Finally, we perform simulations, which demonstrate that our main asymptotic results hold true also in practice.
C1 [Anagnostopoulos, Aris; Lacki, Jakub; Leonardi, Stefano] Sapienza Univ Rome, Rome, Italy.
   [Lattanzi, Silvio; Mahdian, Mohammad] Google, Mountain View, CA USA.
RP Anagnostopoulos, A (reprint author), Sapienza Univ Rome, Rome, Italy.
EM aris@dis.uniroma1.it; j.lacki@mimuw.edu.pl; silviol@google.com;
   leonardi@dis.uniroma1.it; mahdian@google.com
FU EU FET project MULTIPLEX [317532]; Google Focused Award on "Algorithms
   for Large-scale Data Analysis"
FX We would like to thank Marek Adamczyk for helping in some mathematical
   derivations. This work is partly supported by the EU FET project
   MULTIPLEX no. 317532 and the Google Focused Award on "Algorithms for
   Large-scale Data Analysis."
CR Abbe E, 2016, IEEE T INFORM THEORY, V62, P471, DOI 10.1109/TIT.2015.2490670
   Abbe E, 2015, ANN IEEE SYMP FOUND, P670, DOI 10.1109/FOCS.2015.47
   Anagnostopoulos A., 2012, 3 INN THEOR COMP SCI, P149
   Bahmani B., 2012, KDD, P24
   Bshouty Nader H., 2010, P 27 INT C MACH LEAR, P135
   Choi DS, 2012, BIOMETRIKA, V99, P273, DOI 10.1093/biomet/asr053
   Coja-Oghlan A, 2010, COMB PROBAB COMPUT, V19, P227, DOI 10.1017/S0963548309990514
   HAN Q., 2015, P 32 INT C MACH LEAR, P1511
   Hartmann Tanja, 2014, ABS14013516 CORR
   HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7
   Javanmard A, 2016, P NATL ACAD SCI USA, V113, pE2218, DOI 10.1073/pnas.1523097113
   Kumar R, 2006, P 12 ACM SIGKDD INT, P611, DOI DOI 10.1145/1150402.1150476
   Linden G, 2003, IEEE INTERNET COMPUT, V7, P76, DOI 10.1109/MIC.2003.1167344
   MASSOULIE L., 2014, P 46 ANN ACM S THEOR, P694, DOI DOI 10.1145/2591796.2591857
   Mossel E., 2015, P 47 ANN ACM S THEOR, P69
   Newman M., 2010, NETWORKS INTRO
   Rohe K, 2011, ANN STAT, V39, P1878, DOI 10.1214/11-AOS887
   Xu Kevin S., 2013, Social Computing, Behavioral-Cultural Modeling and Prediction. 6th International Conference, SBP 2013. Proceedings, P201, DOI 10.1007/978-3-642-37210-0_22
   Xu Kevin S., 2014, ABS14115404 CORR
   Zreik Rawya, 2016, COMPUTATION STAT, P1
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701030
DA 2019-06-15
ER

PT S
AU Anava, O
   Levy, KY
AF Anava, Oren
   Levy, Kfir Y.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI k*-Nearest Neighbors: From Global to Local
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID CLASSIFICATION
AB The weighted k-nearest neighbors algorithm is one of the most fundamental non parametric methods in pattern recognition and machine learning. The question of setting the optimal number of neighbors as well as the optimal weights has received much attention throughout the years, nevertheless this problem seems to have remained unsettled. In this paper we offer a simple approach to locally weighted regression/classification, where we make the bias-variance tradeoff explicit. Our formulation enables us to phrase a notion of optimal weights, and to efficiently find these weights as well as the optimal number of neighbors efficiently and adaptively, for each data point whose value we wish to estimate. The applicability of our approach is demonstrated on several datasets, showing superior performance over standard locally weighted methods.
C1 [Anava, Oren] Voleon Grp, Berkeley, CA 94704 USA.
   [Levy, Kfir Y.] Swiss Fed Inst Technol, Zurich, Switzerland.
RP Anava, O (reprint author), Voleon Grp, Berkeley, CA 94704 USA.
EM oren@voleon.com; yehuda.levy@inf.ethz.ch
CR ABRAMSON IS, 1982, ANN STAT, V10, P1217, DOI 10.1214/aos/1176345986
   Adeniyi D. A., 2016, Applied Computing and Informatics, V12, P90, DOI 10.1016/j.aci.2014.10.001
   Aljuhani Khulood Hamed, 2014, SCI RES ESSAYS, V9, P966, DOI DOI 10.5897/SRE2014.6121
   Biau Gerard, 2015, LECT NEAREST NEIGHBO, V1
   COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964
   Demir Serdar, 2010, HACETTEPE J MATH STA, V39
   DEVROYE LP, 1980, ANN STAT, V8, P231, DOI 10.1214/aos/1176344949
   Devroye Luc, 2013, PROBABILISTIC THEORY, V31
   Fan J., 1996, LOCAL POLYNOMIAL MOD, V66
   Fix E, 1951, TECHNICAL REPORT
   Ghosh AK, 2007, J COMPUT GRAPH STAT, V16, P482, DOI 10.1198/106186007x208380
   Gyorfi L, 2006, DISTRIBUTION FREE TH
   Imandoust S. B., 2013, INT J ENG RES APPL, V3, P605
   Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876
   Jabbar MA, 2013, PROC TECH, V10, P85, DOI 10.1016/j.protcy.2013.12.340
   Kulis B, 2013, FOUND TRENDS MACH LE, V5, P287, DOI 10.1561/2200000019
   Li B., 2004, ACM T ASIAN LANGUAGE, V3, P215
   Nadaraya E., 1964, THEOR PROBAB APPL, V9, P141, DOI DOI 10.1137/1109020
   Samworth RJ, 2012, ANN STAT, V40, P2733, DOI 10.1214/12-AOS1049
   Silverman B. W., 1986, DENSITY ESTIMATION S, V26
   STONE CJ, 1977, ANN STAT, V5, P595, DOI 10.1214/aos/1176343886
   Sun Shiliang, 2010 7 INT C FUZZ SY
   Trstenjak B, 2014, PROCEDIA ENGINEER, V69, P1356, DOI 10.1016/j.proeng.2014.03.129
   Watson G., 1964, SANKHYA A, V26, P359, DOI DOI 10.2307/25049340
   Wettschereck Dietrich, 1994, ADV NEURAL INFORMATI, P184
   Wu XD, 2008, KNOWL INF SYST, V14, P1, DOI 10.1007/s10115-007-0114-2
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703009
DA 2019-06-15
ER

PT S
AU Anava, O
   Karnin, Z
AF Anava, Oren
   Karnin, Zohar
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Multi-armed Bandits: Competing with Optimal Sequences
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID ALGORITHMS
AB We consider sequential decision making problem in the adversarial setting, where regret is measured with respect to the optimal sequence of actions and the feedback adheres the bandit setting. It is well-known that obtaining sublinear regret in this setting is impossible in general, which arises the question of when can we do better than linear regret? Previous works show that when the environment is guaranteed to vary slowly and furthermore we are given prior knowledge regarding its variation (i.e., a limit on the amount of changes suffered by the environment), then this task is feasible. The caveat however is that such prior knowledge is not likely to be available in practice, which causes the obtained regret bounds to be somewhat irrelevant.
   Our main result is a regret guarantee that scales with the variation parameter of the environment, without requiring any prior knowledge about it whatsoever. By that, we also resolve an open problem posted by Gur, Zeevi and Besbes [8]. An important key component in our result is a statistical test for identifying non-stationarity in a sequence of independent random variables. This test either identifies non-stationarity or upper-bounds the absolute deviation of the corresponding sequence of mean values in terms of its total variation. This test is interesting on its own right and has the potential to be found useful in additional settings.
C1 [Anava, Oren] Voleon Grp, Berkeley, CA 94704 USA.
   [Karnin, Zohar] Yahoo Res, New York, NY USA.
RP Anava, O (reprint author), Voleon Grp, Berkeley, CA 94704 USA.
EM oren@voleon.com; zkarnin@yahoo-inc.com
CR Auer P, 2003, SIAM J COMPUT, V32, P48
   Bertsimas D, 2000, OPER RES, V48, P80, DOI 10.1287/opre.48.1.80.12444
   Besbes O., 2014, ADV NEURAL INFORM PR, P199
   Bousquet O., 2003, Journal of Machine Learning Research, V3, P363, DOI 10.1162/153244303321897654
   Daniely A., 2015, P 32 INT C MACH LEAR, P1405
   Flaxman AD, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P385
   Gheshlaghi-Azar M., 2014, P 31 INT C MACH LEAR, V32, P1557
   Guha S, 2007, ANN IEEE SYMP FOUND, P483, DOI 10.1109/FOCS.2007.23
   Hazan E, 2011, J MACH LEARN RES, V12, P1287
   Hazan E, 2010, MACH LEARN, V80, P165, DOI 10.1007/s10994-010-5175-x
   Hazan Elad, 2009, P 26 ANN INT C MACH, V382, P393
   HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952
   Jadbabaie Ali, 2015, JMLR WORKSHOP C P, V38
   LITTLESTONE N, 1994, INFORM COMPUT, V108, P212, DOI 10.1006/inco.1994.1009
   Ortner R, 2014, THEOR COMPUT SCI, V558, P62, DOI 10.1016/j.tcs.2014.09.026
   Whittle P., 1988, J APPL PROB A, P287, DOI [10.2307/3214163, DOI 10.2307/3214163]
   Zinkevich Martin, 2003, P 20 INT C MACH LEAR, P928
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702088
DA 2019-06-15
ER

PT S
AU Andrychowicz, M
   Denil, M
   Colmenarejo, SG
   Hoffman, MW
   Pfau, D
   Schaul, T
   Shillingford, B
   de Freitas, N
AF Andrychowicz, Marcin
   Denil, Misha
   Colmenarejo, Sergio Gomez
   Hoffman, Matthew W.
   Pfau, David
   Schaul, Tom
   Shillingford, Brendan
   de Freitas, Nando
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning to learn by gradient descent by gradient descent
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID ADAPTATION; SEARCH; TERM
AB The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.
C1 [Andrychowicz, Marcin; Denil, Misha; Colmenarejo, Sergio Gomez; Hoffman, Matthew W.; Pfau, David; Schaul, Tom; Shillingford, Brendan; de Freitas, Nando] Google DeepMind, London, England.
   [Shillingford, Brendan; de Freitas, Nando] Univ Oxford, Oxford, England.
   [de Freitas, Nando] Canadian Inst Adv Res, Toronto, ON, Canada.
RP Andrychowicz, M (reprint author), Google DeepMind, London, England.
EM marcin.andrychowicz@gmail.com; mdenil@google.com; sergomez@google.com;
   mwhoffman@google.com; pfau@google.com; schaul@google.com;
   brendan.shillingford@cs.ox.ac.uk; nandodefreitas@google.com
CR Bach F, 2012, FOUND TRENDS MACH LE, V4, P1, DOI 10.1561/2200000015
   BENGIO S, 1995, NEURAL PROCESS LETT, V2, P26, DOI 10.1007/BF02279935
   Bengio Y., 1990, LEARNING SYNAPTIC LE
   Bobolas F., 2009, BRAIN NEURONS
   Cotter N. E., 1990, IJCNN International Joint Conference on Neural Networks (Cat. No.90CH2879-5), P553, DOI 10.1109/IJCNN.1990.137898
   Daniel C., 2016, ASS ADVANCEMENT ARTI
   DENG J, 2009, PROC CVPR IEEE, P248, DOI DOI 10.1109/CVPR.2009.5206848
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Feldkamp LA, 1998, P IEEE, V86, P2259, DOI 10.1109/5.726790
   Gatys L. A., 2015, 150806576 ARXIV
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hochreiter S, 2001, LECT NOTES COMPUT SC, V2130, P87
   Kingma D. P., 2015, INT C LEARN REPR
   Krizhevsky A., 2009, TECHNICAL REPORT
   Lake B. M., 2016, 160400289 ARXIV
   Maley T., 2011, NEURON
   Martens J., 2015, INT C MACH LEARN, P2408
   Nemhauser G, 1988, INTEGER COMBINATORIA
   Nesterov Y., 1983, SOV MATH DOKL, V27, P372
   RIEDMILLER M, 1993, 1993 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOLS 1-3, P586, DOI 10.1109/ICNN.1993.298623
   Runarsson TP, 2000, 2000 IEEE SYMPOSIUM ON COMBINATIONS OF EVOLUTIONARY COMPUTATION AND NEURAL NETWORKS, P59, DOI 10.1109/ECNN.2000.886220
   Santoro A., 2016, INT C MACH LEARN
   Schmidhuber J, 1997, MACH LEARN, V28, P105, DOI 10.1023/A:1007383707642
   SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P131, DOI 10.1162/neco.1992.4.1.131
   SCHMIDHUBER J, 1993, 1993 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOLS 1-3, P407, DOI 10.1109/ICNN.1993.298591
   SCHMIDHUBER J, 1987, THESIS
   Schraudolph NN, 1999, IEE CONF PUBL, P569, DOI 10.1049/cp:19991170
   SUTTON RS, 1992, AAAI-92 PROCEEDINGS : TENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, P171
   Thrun S., 1998, LEARNING LEARN
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P2
   Tseng P, 1998, SIAM J OPTIMIZ, V8, P506, DOI 10.1137/S1052623495294797
   Wolpert D. H., 1997, IEEE Transactions on Evolutionary Computation, V1, P67, DOI 10.1109/4235.585893
   Younger A. S., 2001, INT JOINT C NEUR NET
   Younger AS, 1999, IEEE T NEURAL NETWOR, V10, P272, DOI 10.1109/72.750553
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703097
DA 2019-06-15
ER

PT S
AU Nguyen, AT
   Xu, J
   Yang, Z
AF Anh Tuan Nguyen
   Xu, Jian
   Yang, Zhi
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI A Bio-inspired Redundant Sensing Architecture
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID ACUITY
AB Sensing is the process of deriving signals from the environment that allows artificial systems to interact with the physical world. The Shannon theorem specifies the maximum rate at which information can be acquired [1]. However, this upper bound is hard to achieve in many man-made systems. The biological visual systems, on the other hand, have highly efficient signal representation and processing mechanisms that allow precise sensing. In this work, we argue that redundancy is one of the critical characteristics for such superior performance. We show architectural advantages by utilizing redundant sensing, including correction of mismatch error and significant precision enhancement. For a proof-of-concept demonstration, we have designed a heuristic-based analog-to-digital converter - a zero-dimensional quantizer. Through Monte Carlo simulation with the error probabilistic distribution as a priori, the performance approaching the Shannon limit is feasible. In actual measurements without knowing the error distribution, we observe at least 2-bit extra precision. The results may also help explain biological processes including the dominance of binocular vision, the functional roles of the fixational eye movements, and the structural mechanisms allowing hyperacuity.
C1 [Anh Tuan Nguyen; Xu, Jian; Yang, Zhi] Univ Minnesota, Dept Biomed Engn, Minneapolis, MN 55455 USA.
RP Yang, Z (reprint author), Univ Minnesota, Dept Biomed Engn, Minneapolis, MN 55455 USA.
EM yang5029@umn.edu
CR Analog Devices, 2016, 24 BIT DELT SIGM ADC
   BECK J, 1979, VISION RES, V19, P313, DOI 10.1016/0042-6989(79)90176-7
   Biveroni Jonas, 2008, 2008 Information Theory and Applications Workshop Conference, P185, DOI 10.1109/ITA.2008.4601045
   CAGENELLO R, 1993, J OPT SOC AM A, V10, P1841, DOI 10.1364/JOSAA.10.001841
   Crick F, 1980, ORG CEREBRAL CORTEX, P505
   CURCIO CA, 1990, J COMP NEUROL, V292, P497, DOI 10.1002/cne.902920402
   CURCIO CA, 1990, J COMP NEUROL, V300, P5, DOI 10.1002/cne.903000103
   Frey M, 2007, IEEE T CIRCUITS-I, V54, P229, DOI 10.1109/TCSI.2006.887453
   Hennig M. H, 2004, ADV NEURAL INFORM PR
   Hicheur H, 2013, J VISION, V13, DOI 10.1167/13.13.18
   Land M.F., 1985, P53
   Martinez-Conde S, 2013, NAT REV NEUROSCI, V14, P83, DOI 10.1038/nrn3405
   Murmann B, 2008, IEEE CUST INTEGR CIR, P105, DOI 10.1109/CICC.2008.4672032
   Nguyen A, 2015, P WORKSH SDNFLEX NET, P1
   Read JCA, 2015, PROC SPIE, V9391, DOI 10.1117/12.2184988
   Reece J. B., 2010, CAMPBELL BIOL
   SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x
   Um JY, 2013, IEEE T CIRCUITS-I, V60, P2845, DOI 10.1109/TCSI.2013.2252475
   WESTHEIMER G, 1978, J OPT SOC AM, V68, P450, DOI 10.1364/JOSA.68.000450
   WESTHEIMER G, 1977, J OPT SOC AM, V67, P207, DOI 10.1364/JOSA.67.000207
   Xu RY, 2012, IEEE J SOLID-ST CIRC, V47, P2129, DOI 10.1109/JSSC.2012.2198350
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704088
DA 2019-06-15
ER

PT S
AU Apthorpe, NJ
   Riordan, AJ
   Aguilar, RE
   Homann, J
   Gu, Y
   Tank, DW
   Seung, HS
AF Apthorpe, Noah J.
   Riordan, Alexander J.
   Aguilar, Rob E.
   Homann, Jan
   Gu, Yi
   Tank, David W.
   Seung, H. Sebastian
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Automatic Neuron Detection in Calcium Imaging Data Using Convolutional
   Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID CELLULAR-RESOLUTION; CORTEX; AWAKE
AB Calcium imaging is an important technique for monitoring the activity of thousands of neurons simultaneously. As calcium imaging datasets grow in size, automated detection of individual neurons is becoming important. Here we apply a supervised learning approach to this problem and show that convolutional networks can achieve near-human accuracy and superhuman speed. Accuracy is superior to the popular PCA/ICA method based on precision and recall relative to ground truth annotation by a human expert. These results suggest that convolutional networks are an efficient and flexible tool for the analysis of large-scale calcium imaging data.
C1 [Apthorpe, Noah J.; Aguilar, Rob E.; Seung, H. Sebastian] Princeton Univ, Comp Sci Dept, Princeton, NJ 08544 USA.
   [Riordan, Alexander J.; Homann, Jan; Gu, Yi; Tank, David W.; Seung, H. Sebastian] Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA.
RP Apthorpe, NJ (reprint author), Princeton Univ, Comp Sci Dept, Princeton, NJ 08544 USA.
EM apthorpe@princeton.edu; ariordan@princeton.edu; dwtank@princeton.edu;
   sseung@princeton.edu
FU IARPA [D16PC00005]; Mathers Foundation; Simons Foundation SCGB; U.S.
   Army Research Office [W911NF-12-1-0594]; NIH [R01 MH083686, U01
   NS090541, U01 NS090562]
FX We thank Kisuk Lee, Jingpeng Wu, Nicholas Turner, and Jeffrey Gauthier
   for technical assistance. We also thank Sue Ann Koay, Niranjani Prasad,
   Cyril Zhang, and Hussein Nagree for discussions. This work was supported
   by IARPA D16PC00005 (HSS), the Mathers Foundation (HSS), NIH R01
   MH083686 (DWT), NIH U01 NS090541 (DWT, HSS), NIH U01 NS090562 (HSS),
   Simons Foundation SCGB (DWT), and U.S. Army Research Office
   W911NF-12-1-0594 (HSS).
CR Andilla F. D., 2014, ADV NEURAL INFORM PR, P64
   Chen TW, 2013, NATURE, V499, P295, DOI 10.1038/nature12354
   DENK W, 1990, SCIENCE, V248, P73, DOI 10.1126/science.2321027
   Dombeck DA, 2007, NEURON, V56, P43, DOI 10.1016/j.neuron.2007.08.003
   Greenberg DS, 2008, NAT NEUROSCI, V11, P749, DOI 10.1038/nn.2140
   Harvey CD, 2012, NATURE, V484, P62, DOI 10.1038/nature10918
   Huber D, 2012, NATURE, V484, P473, DOI 10.1038/nature11039
   Kaifosh P, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00080
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   LeCun Y, 2010, IEEE INT SYMP CIRC S, P253, DOI 10.1109/ISCAS.2010.5537907
   Mukamel EA, 2009, NEURON, V63, P747, DOI 10.1016/j.neuron.2009.08.009
   Pachitariu M, 2013, ADV NEURAL INFORM PR, P1745
   Pnevmatikakis EA, 2016, NEURON, V89, P285, DOI 10.1016/j.neuron.2015.11.037
   Rickgauer JP, 2014, NAT NEUROSCI, V17, P1816, DOI 10.1038/nn.3866
   Smith SL, 2010, NAT NEUROSCI, V13, P1144, DOI 10.1038/nn.2620
   Valmianski I, 2010, J NEUROPHYSIOL, V104, P1803, DOI 10.1152/jn.00484.2010
   Walker Theo, 2014, CELL MAGIC WAND TOOL
   Zlateski Aleksandar, 2015, ARXIV151006706
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700102
DA 2019-06-15
ER

PT S
AU Arjevani, Y
   Shamir, O
AF Arjevani, Yossi
   Shamir, Ohad
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Dimension-Free Iteration Complexity of Finite Sum Optimization Problems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Many canonical machine learning problems boil down to a convex optimization problem with a finite sum structure. However, whereas much progress has been made in developing faster algorithms for this setting, the inherent limitations of these problems are not satisfactorily addressed by existing lower bounds. Indeed, current bounds focus on first-order optimization algorithms, and only apply in the often unrealistic regime where the number of iterations is less than O (d=n) (where d is the dimension and n is the number of samples). In this work, we extend the framework of Arjevani et al. [3, 5] to provide new lower bounds, which are dimension-free, and go beyond the assumptions of current bounds, thereby covering standard finite sum optimization methods, e.g., SAG, SAGA, SVRG, SDCA without duality, as well as stochastic coordinate-descent methods, such as SDCA and accelerated proximal SDCA.
C1 [Arjevani, Yossi; Shamir, Ohad] Weizmann Inst Sci, IL-7610001 Rehovot, Israel.
RP Arjevani, Y (reprint author), Weizmann Inst Sci, IL-7610001 Rehovot, Israel.
EM yossi.arjevani@weizmann.ac.il; ohad.shamir@weizmann.ac.il
NR 0
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 8
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700051
DA 2019-06-15
ER

PT S
AU Arvanitidis, G
   Hansen, LK
   Hauberg, S
AF Arvanitidis, Georgios
   Hansen, Lars Kai
   Hauberg, Soren
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI A Locally Adaptive Normal Distribution
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID DIMENSIONALITY REDUCTION; GEOMETRIC FRAMEWORK
AB The multivariate normal density is a monotonic function of the distance to the mean, and its ellipsoidal shape is due to the underlying Euclidean metric. We suggest to replace this metric with a locally adaptive, smoothly changing (Riemannian) metric that favors regions of high local density. The resulting locally adaptive normal distribution (LAND) is a generalization of the normal distribution to the "manifold" setting, where data is assumed to lie near a potentially low-dimensional manifold embedded in RD. The LAND is parametric, depending only on a mean and a covariance, and is the maximum entropy distribution under the given metric. The underlying metric is, however, non-parametric. We develop a maximum likelihood algorithm to infer the distribution parameters that relies on a combination of gradient descent and Monte Carlo integration. We further extend the LAND to mixture models, and provide the corresponding EM algorithm. We demonstrate the efficiency of the LAND to fit non-trivial probability distributions over both synthetic data, and EEG measurements of human sleep.
C1 [Arvanitidis, Georgios; Hansen, Lars Kai; Hauberg, Soren] Tech Univ Denmark, DTU Compute, Sect Cognit Syst, Lyngby, Denmark.
RP Arvanitidis, G (reprint author), Tech Univ Denmark, DTU Compute, Sect Cognit Syst, Lyngby, Denmark.
EM gear@dtu.dk; lkai@dtu.dk; sohau@dtu.dk
RI Hauberg, Soren/L-2104-2016
OI Hauberg, Soren/0000-0001-7223-877X
FU Novo Nordisk Foundation Interdisciplinary Synergy Program 2014,
   'Biophysically adjusted state-informed cortex stimulation (BASICS)';
   Danish Council for Independent Research, Natural Sciences
FX LKH was funded in part by the Novo Nordisk Foundation Interdisciplinary
   Synergy Program 2014, 'Biophysically adjusted state-informed cortex
   stimulation (BASICS)'. SH was funded in part by the Danish Council for
   Independent Research, Natural Sciences.
CR Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Bishop C. M., 2006, INFORM SCI STAT
   Boyce R, 2016, SCIENCE, V352, P812, DOI 10.1126/science.aad5252
   Delorme A., 2004, J NEUROSCI METH, P21
   doCarmo M., 1992, MATHEMATICS
   Goldberger AL, 2000, CIRCULATION, V101, pE215, DOI 10.1161/01.CIR.101.23.e215
   Hauberg S., 2012, ADV NEURAL INFORM PR, P2033
   Hauberg S., 2016, IEEE T PATTERN ANAL
   Hennig P., 2014, P 17 INT C ART INT S, V33
   Imtiaz SA, 2015, IEEE ENG MED BIO, P6014, DOI 10.1109/EMBC.2015.7319762
   KARCHER H, 1977, COMMUN PUR APPL MATH, V30, P509, DOI 10.1002/cpa.3160300502
   Marxer R., 2008, F MEASURE EVALUATION, P1
   Pennec X, 2006, J MATH IMAGING VIS, V25, P127, DOI 10.1007/s10851-006-6228-4
   Purves D., 2008, NEUROSCIENCE
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Scholkopf B, 1999, ADVANCES IN KERNEL METHODS, P327
   Simo-Serra E., 2014, P BRIT MACH VIS C
   Straub J., 2015, INT C ART INT STAT A
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Tosi A., 2014, C UNC ART INT UAI JU
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Zhang M., 2013, ADV NEURAL INFORM PR, P1178
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704028
DA 2019-06-15
ER

PT S
AU Ashtiani, H
   Kushagra, S
   Ben-David, S
AF Ashtiani, Hassan
   Kushagra, Shrinu
   Ben-David, Shai
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Clustering with Same-Cluster Queries
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We propose a framework for Semi-Supervised Active Clustering framework (SSAC), where the learner is allowed to interact with a domain expert, asking whether two given instances belong to the same cluster or not. We study the query and computational complexity of clustering in this framework. We consider a setting where the expert conforms to a center-based clustering with a notion of margin. We show that there is a trade off between computational complexity and query complexity; We prove that for the case of k-means clustering (i.e., when the expert conforms to a solution of k-means), having access to relatively few such queries allows efficient solutions to otherwise NP hard problems.
   In particular, we provide a probabilistic polynomial-time (BPP) algorithm for clustering in this setting that asks O k(2) log k + k log n) same-cluster queries and runs with time complexity O kn log n) (where k is the number of clusters and n is the number of instances). The algorithm succeeds with high probability for data satisfying margin conditions under which, without queries, we show that the problem is NP hard. We also prove a lower bound on the number of queries needed to have a computationally efficient clustering algorithm in this setting.
C1 [Ashtiani, Hassan; Kushagra, Shrinu; Ben-David, Shai] Univ Waterloo, David R Cheriton Sch Comp Sci, Waterloo, ON, Canada.
RP Ashtiani, H (reprint author), Univ Waterloo, David R Cheriton Sch Comp Sci, Waterloo, ON, Canada.
EM mhzokaei@uwaterloo.ca; skushagr@uwaterloo.ca; shai@uwaterloo.ca
CR Ashtiani Hassan, 2015, UNCERTAINTY AI UAI
   Awasthi P, 2012, INFORM PROCESS LETT, V112, P49, DOI 10.1016/j.ipl.2011.10.006
   Balcan MF, 2012, LECT NOTES COMPUT SC, V7391, P63, DOI 10.1007/978-3-642-31594-7_6
   Balcan MF, 2008, LECT NOTES ARTIF INT, V5254, P316, DOI 10.1007/978-3-540-87987-9_27
   Basu S., 2004, P 10 ACM SIGKDD INT, P59, DOI DOI 10.1145/1014052.1014062
   Basu S., 2002, P 19 INT C MACH LEAR
   Ben-David S, 2014, THEOR COMPUT SCI, V558, P51, DOI 10.1016/j.tcs.2014.09.025
   Dasgupta S., 2008, HARDNESS K MEANS CLU
   Garey M. R., 2002, COMPUTERS INTRACTABI, V29
   Kulis B, 2009, MACH LEARN, V74, P1, DOI 10.1007/s10994-008-5084-4
   Mahajan M, 2009, LECT NOTES COMPUT SC, V5431, P274
   Shai Ben-David, 2015, CORR
   Vattani Andrea, 2009, HARDNESS K MEA UNPUB
NR 13
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703085
DA 2019-06-15
ER

PT S
AU Aybat, NS
   Hamedani, EY
AF Aybat, Necdet Serhat
   Hamedani, Erfan Yazdandoost
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI A primal-dual method for conic constrained distributed optimization
   problems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID CONSENSUS
AB We consider cooperative multi-agent consensus optimization problems over an undirected network of agents, where only those agents connected by an edge can directly communicate. The objective is to minimize the sum of agent specific composite convex functions over agent-specific private conic constraint sets; hence, the optimal consensus decision should lie in the intersection of these private sets. We provide convergence rates in sub-optimality; infeasibility and consensus violation; examine the effect of underlying network topology on the convergence rates of the proposed decentralized algorithms; and show how to extend these methods to handle time-varying communication networks.
C1 [Aybat, Necdet Serhat; Hamedani, Erfan Yazdandoost] Penn State Univ, Dept Ind Engn, University Pk, PA 16802 USA.
RP Aybat, NS (reprint author), Penn State Univ, Dept Ind Engn, University Pk, PA 16802 USA.
EM nsa10@psu.edu; evy5047@psu.edu
CR Bach F. R., 2004, P 21 INT C MACH LEAR, P6, DOI DOI 10.1145/1015330.1015424
   Chambolle A, 2016, MATH PROGRAM, V159, P253, DOI 10.1007/s10107-015-0957-3
   Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1
   Chang TH, 2014, IEEE T AUTOMAT CONTR, V59, P1524, DOI 10.1109/TAC.2014.2308612
   Chen AI, 2012, ANN ALLERTON CONF, P601, DOI 10.1109/Allerton.2012.6483273
   Chen YM, 2014, SIAM J OPTIMIZ, V24, P1779, DOI 10.1137/130919362
   Forero PA, 2010, J MACH LEARN RES, V11, P1663
   He BS, 2012, SIAM J IMAGING SCI, V5, P119, DOI 10.1137/100814494
   Ling Q, 2010, IEEE T SIGNAL PROCES, V58, P3816, DOI 10.1109/TSP.2010.2047721
   Mateos G, 2010, IEEE T SIGNAL PROCES, V58, P5262, DOI 10.1109/TSP.2010.2055862
   Mateos-Nunez D, 2015, IEEE DECIS CONTR P, P5462, DOI 10.1109/CDC.2015.7403075
   McDonald R, 2010, HUMAN LANGUAGE TECHN, V2010, P456
   Nedic A, 2009, J OPTIMIZ THEORY APP, V142, P205, DOI 10.1007/s10957-009-9522-7
   Nedic A., 2010, CONVEX OPTIMIZATION, P340
   Nedic A., 2014, ENCY SYSTEMS CONTROL, P1
   Nedic A, 2010, IEEE T AUTOMAT CONTR, V55, P922, DOI 10.1109/TAC.2010.2041686
   Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P48, DOI 10.1109/TAC.2008.2009515
   Robbins H., 1971, P S OH STAT U JUN 14, P233
   Rockafellar R. T., 2015, CONVEX ANAL
   Schizas ID, 2008, IEEE T SIGNAL PROCES, V56, P350, DOI 10.1109/TSP.2007.906734
   Srivastava K, 2010, IEEE DECIS CONTR P, P1945, DOI 10.1109/CDC.2010.5717947
   Yan F, 2013, IEEE T KNOWL DATA EN, V25, P2483, DOI 10.1109/TKDE.2012.191
   Yuan DM, 2011, IEEE T SYST MAN CY B, V41, P1715, DOI 10.1109/TSMCB.2011.2160394
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701099
DA 2019-06-15
ER

PT S
AU Aytar, Y
   Vondrick, C
   Torralba, A
AF Aytar, Yusuf
   Vondrick, Carl
   Torralba, Antonio
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI SoundNet: Learning Sound Representations from Unlabeled Video
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We learn rich natural sound representations by capitalizing on large amounts of unlabeled sound data collected in the wild. We leverage the natural synchronization between vision and sound to learn an acoustic representation using two-million unlabeled videos. Unlabeled video has the advantage that it can be economically acquired at massive scales, yet contains useful signals about natural sound. We propose a student-teacher training procedure which transfers discriminative visual knowledge from well established visual recognition models into the sound modality using unlabeled video as a bridge. Our sound representation yields significant performance improvements over the state-of-the-art results on standard benchmarks for acoustic scene/object classification. Visualizations suggest some high-level semantics automatically emerge in the sound network, even though it is trained without ground truth labels.
C1 [Aytar, Yusuf; Vondrick, Carl; Torralba, Antonio] MIT, Cambridge, MA 02139 USA.
RP Aytar, Y (reprint author), MIT, Cambridge, MA 02139 USA.
EM yusuf@csail.mit.edu; vondrick@mit.edu; torralba@mit.edu
FU NSF [1524817]; Google PhD fellowship
FX We thank MIT TIG, especially Garrett Wollman, for helping store 26 TB of
   video. We are grateful for the GPUs donated by NVidia. This work was
   supported by NSF grant #1524817 to AT and the Google PhD fellowship to
   CV.
CR Aytar Yusuf, 2015, CVIU
   Aytar Yusuf, 2011, ICCV
   Ba Jimmy, 2014, NIPS
   Barchiesi Daniele, 2015, SPM
   Bertin-Mahieux T., 2011, ISMIR
   Cakir E., 2015, IJCNN
   Castrejon Lluis, 2016, CVPR
   Chen Chao- Yeh, 2013, CVPR
   Gupta Saurabh, 2015, ARXIV150700448
   Hannun A., 2014, ARXIV14125567
   Hershey S, 2016, CNN ARCHITECTURES LA
   Hertel Lars, 2016, COMP TIME FREQUENCY
   Hinton G., 2008, JMLR
   Hinton Geoffrey, 2015, DISTILLING KNOWLEDGE
   Huang J. T., 2013, ICASSP
   Ioffe S., 2015, BATCH NORMALIZATION
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Krizhevsky A., 2012, NIPS
   Kuettel Daniel, 2012, CVPR
   LeCun Yann, 1998, IEEE
   Lee Honglak, 2009, NIPS
   Li David, 2013, AASP CHALLENGE
   Long  J., 2015, CVPR
   McLoughlin Ian, 2015, ASL
   Ngiam  Jiquan, 2011, ICML
   Nguyen Phuc Xuan, 2016, OPEN WORLD MICROVIDE
   Owens Andrew, 2015, ARXIV151208512
   Piczak Karol J, 2015, ACM MULTIMEDIA
   Piczak Karol J, 2015, MLSP
   Rakotomamonjy Alain, 2015, TASLP
   Roma Guido, 2013, WASPAA
   Russakovsky Olga, 2015, IJCV
   Salamon Justin, 2015, ICASSP
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Stowell Dan, 2015, TM
   Sutskever  I., 2014, NIPS
   Thomee B., 2016, COMMUNICATIONS ACM
   Van den Oord Aaron, 2013, NIPS
   Vondrick C., 2016, NIPS
   Vondrick  Carl, 2016, CVPR, P2
   Walker  Jacob, 2015, ICCV, P2
   Zhou  B., 2014, NIPS
NR 42
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701003
DA 2019-06-15
ER

PT S
AU Ba, J
   Hinton, G
   Mnih, V
   Leibo, JZ
   Ionescu, C
AF Ba, Jimmy
   Hinton, Geoffrey
   Mnih, Volodymyr
   Leibo, Joel Z.
   Ionescu, Catalin
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Using FastWeights to Attend to the Recent Past
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID NEURAL-NETWORKS
AB Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These "fast weights" can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proved very helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns.
C1 [Ba, Jimmy; Hinton, Geoffrey] Univ Toronto, Toronto, ON, Canada.
   [Hinton, Geoffrey] Google Brain, Mountain View, CA USA.
   [Mnih, Volodymyr; Leibo, Joel Z.; Ionescu, Catalin] Google DeepMind, London, England.
RP Ba, J (reprint author), Univ Toronto, Toronto, ON, Canada.
EM jimmy@psi.toronto.edu; geoffhinton@google.com; vmnih@google.com;
   jzl@google.com; cdi@google.com
CR Abbott LF, 2004, NATURE, V431, P796, DOI 10.1038/nature03010
   Anderson James A, 1981, PARALLEL MODELS ASS, P9
   Ba J., 2015, INT C LEARN REPR
   Ba J. L., 2016, ARXIV160706450
   Bahdanau D., 2015, INT C LEARN REPR
   Barak O, 2007, PLOS COMPUT BIOL, V3, P323, DOI 10.1371/journal.pcbi.0030035
   Bi GQ, 1998, J NEUROSCI, V18, P10464
   Danihelka I., 2016, ARXIV160203032
   GARDNER E, 1988, J PHYS A-MATH GEN, V21, P257, DOI 10.1088/0305-4470/21/1/030
   Graves A., 2014, ARXIV13080850
   Graves A., 2014, ARXIV14105401
   Grefenstette E., 2015, ADV NEURAL INFORM PR, V29, P1819
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Hinton GE, 1987, P 9 ANN C COGN SCI S, P177
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   KOHONEN T, 1972, IEEE T COMPUT, VC 21, P353, DOI 10.1109/TC.1972.5008975
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Mnih V., 2014, NEURAL INFORM PROCES
   Mnih  V., 2016, INT C MACH LEARN
   Schmidhuber J., 1993, ICANN93, P460
   Tsodyks M, 1998, NEURAL COMPUT, V10, P821, DOI 10.1162/089976698300017502
   Weston J., 2014, ARXIV14103916
   Willshaw David J, 1969, NATURE
   Xu K., 2015, INT C MACH LEARN
   Zucker RS, 2002, ANNU REV PHYSIOL, V64, P355, DOI 10.1146/annurev.physiol.64.092501.114547
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700021
DA 2019-06-15
ER

PT S
AU Bachem, O
   Lucic, M
   Hassani, SH
   Krause, A
AF Bachem, Olivier
   Lucic, Mario
   Hassani, S. Hamed
   Krause, Andreas
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Fast and Provably Good Seedings for k-Means
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID PTAS
AB Seeding - the task of finding initial cluster centers - is critical in obtaining high-quality clusterings for k-Means. However, k-means++ seeding, the state of the art algorithm, does not scale well to massive datasets as it is inherently sequential and requires k full passes through the data. It was recently shown that Markov chain Monte Carlo sampling can be used to efficiently approximate the seeding step of k-means++. However, this result requires assumptions on the data generating distribution. We propose a simple yet fast seeding algorithm that produces provably good clusterings even without assumptions on the data. Our analysis shows that the algorithm allows for a favourable trade-off between solution quality and computational cost, speeding up k-means++ seeding by up to several orders of magnitude. We validate our theoretical results in extensive experiments on a variety of real-world data sets.
C1 [Bachem, Olivier; Lucic, Mario; Hassani, S. Hamed; Krause, Andreas] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
RP Bachem, O (reprint author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
EM olivier.bachem@inf.ethz.ch; lucic@inf.ethz.ch; hamed@inf.ethz.ch;
   krausea@ethz.ch
FU ERC [StG 307036]; Google Ph.D. Fellowship; IBM Ph.D. Fellowship
FX This research was partially supported by ERC StG 307036, a Google Ph.D.
   Fellowship and an IBM Ph.D. Fellowship.
CR Acharyya Sreangsu, 2013, SIAM INT C DAT MIN S, P476
   ACKERMANN MR, 2010, SWAT, V6139, P212
   Aggarwal A, 2009, LECT NOTES COMPUT SC, V5687, P15, DOI 10.1007/978-3-642-03685-9_2
   Ailon N., 2009, NIPS, P10
   Arthur D, 2007, P 18 ANN ACM SIAM S, P1027, DOI DOI 10.1145/1283383.1283494
   Bachem Olivier, 2016, C ART INT AAAI FEBR
   Bahmani B, 2012, PROC VLDB ENDOW, V5, P622, DOI 10.14778/2180912.2180915
   Bottou L., 1994, ADV NEURAL INFORM PR, P585
   Brunsch T, 2011, LECT NOTES COMPUT SC, V6648, P344
   Cai HY, 2000, STOCH ANAL APPL, V18, P63, DOI 10.1080/07362990008809654
   Celebi ME, 2013, EXPERT SYST APPL, V40, P200, DOI 10.1016/j.eswa.2012.07.021
   HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.2307/2334940
   Jaiswal R, 2015, INFORM PROCESS LETT, V115, P100, DOI 10.1016/j.ipl.2014.07.009
   Jaiswal R, 2014, ALGORITHMICA, V70, P22, DOI 10.1007/s00453-013-9833-9
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   OSTROVSKY R, 2006, FOCS, P165
   Sculley D., 2010, P INT C WORLD WID WE, P1177, DOI [10.1145/1772690.1772862, DOI 10.1145/1772690.1772862]
   Zhao WZ, 2009, LECT NOTES COMPUT SC, V5931, P674, DOI 10.1007/978-3-642-10665-1_71
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704003
DA 2019-06-15
ER

PT S
AU Bachman, P
AF Bachman, Philip
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI An Architecture for Deep, Hierarchical Generative Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We present an architecture which lets us train deep, directed generative models with many layers of latent variables. We include deterministic paths between all latent variables and the generated output, and provide a richer set of connections between computations for inference and generation, which enables more effective communication of information throughout the model during training. To improve performance on natural images, we incorporate a lightweight autoregressive model in the reconstruction distribution. These techniques permit end-to-end training of models with 10+ layers of latent variables. Experiments show that our approach achieves state-of-the-art performance on standard image modelling benchmarks, can expose latent class structure in the absence of label information, and can provide convincing imputations of occluded regions in natural images.
C1 [Bachman, Philip] Maluuba Res, Montreal, PQ, Canada.
RP Bachman, P (reprint author), Maluuba Res, Montreal, PQ, Canada.
EM phil.bachman@maluuba.com
CR Bachman P., 2015, ADV NEURAL INFORM PR
   Burda Y., 2015, ARXIV150900519V1CSLG
   Denton E. L., 2015, ARXIV150605751CSCV
   Eslami S. M. A., 2016, ARXIV160308575CSCV
   Gregor K., 2016, ARXIV160408772V1STAT
   Gregor K., 2015, INT C MACH LEARN ICM
   He Kaiming, 2015, ARXIV151203385V1CSCV
   Honari S., 2016, COMPUTER VISION PATT
   Kaiser Lukasz, 2016, INT C LEARN REPR ICL
   Kingma D, 2014, INT C LEARN REPR ICL
   Kingma D. P., 2016, ARXIV160604934CSLG
   Krizhevsky A., 2009, THESIS
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   Liu Z, 2015, INT C COMP VIS ICCV
   Maaloe L., 2016, INT C MACH LEARN ICM
   Ranganath R., 2016, INT C MACH LEARN ICM
   Rasmus  Antti, 2015, ADV NEURAL INFORM PR
   Rezende D. J., 2016, INT C MACH LEARN ICM
   Rezende Danilo Jimenez, 2014, INT C MACH LEARN ICM
   Salakhutdinov R., 2008, INT C MACH LEARN ICM
   Sohl-Dickstein J., 2015, INT C MACH LEARN ICM
   Sohn K., 2015, ADV NEURAL INFORM PR
   Sonderby C. K., 2016, INT C MACH LEARN ICM
   Theis  Lucas, 2015, ADV NEURAL INFORM PR
   van den Oord A, 2016, ARXIV160605328CSCV
   van den Oord A, 2016, INT C MACH LEARN ICM
NR 26
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700105
DA 2019-06-15
ER

PT S
AU Bak, JH
   Choi, JY
   Akrami, A
   Witten, I
   Pillow, JW
AF Bak, Ji Hyun
   Choi, Jung Yoon
   Akrami, Athena
   Witten, Ilana
   Pillow, Jonathan W.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Adaptive optimal training of animal behavior
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Neuroscience experiments often require training animals to perform tasks designed to elicit various sensory, cognitive, and motor behaviors. Training typically involves a series of gradual adjustments of stimulus conditions and rewards in order to bring about learning. However, training protocols are usually hand-designed, relying on a combination of intuition, guesswork, and trial-and-error, and often require weeks or months to achieve a desired level of task performance. Here we combine ideas from reinforcement learning and adaptive optimal experimental design to formulate methods for adaptive optimal training of animal behavior. Our work addresses two intriguing problems at once: first, it seeks to infer the learning rules underlying an animal's behavioral changes during training; second, it seeks to exploit these rules to select stimuli that will maximize the rate of learning toward a desired objective. We develop and test these methods using data collected from rats during training on a two-interval sensory discrimination task. We show that we can accurately infer the parameters of a policy-gradient-based learning algorithm that describes how the animal's internal model of the task evolves over the course of training. We then formulate a theory for optimal training, which involves selecting sequences of stimuli that will drive the animal's internal policy toward a desired location in the parameter space. Simulations show that our method can in theory provide a substantial speedup over standard training methods. We feel these results will hold considerable theoretical and practical implications both for researchers in reinforcement learning and for experimentalists seeking to train animals.
C1 [Bak, Ji Hyun] Princeton Univ, Dept Phys, Princeton, NJ 08544 USA.
   [Choi, Jung Yoon; Witten, Ilana; Pillow, Jonathan W.] Princeton Univ, Dept Psychol, Princeton, NJ 08544 USA.
   [Choi, Jung Yoon; Akrami, Athena; Witten, Ilana; Pillow, Jonathan W.] Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA.
   [Bak, Ji Hyun] Korea Inst Adv Study, Sch Computat Sci, Seoul, South Korea.
   [Akrami, Athena] Howard Hughes Med Inst, Chevy Chase, MA USA.
RP Bak, JH (reprint author), Princeton Univ, Dept Phys, Princeton, NJ 08544 USA.
EM jhbak@kias.re.kr; jungchoi@princeton.edu; aakrami@princeton.edu;
   iwitten@princeton.edu; pillow@princeton.edu
FU Samsung Scholarship; NSF PoLS program; McKnight Foundation; Simons
   Collaboration on the Global Brain [SCGB AWD1004351]; NSF CAREER Award
   [IIS-1150186]
FX JHB was supported by the Samsung Scholarship and the NSF PoLS program.
   JWP was supported by grants from the McKnight Foundation, Simons
   Collaboration on the Global Brain (SCGB AWD1004351) and the NSF CAREER
   Award (IIS-1150186). We thank Nicholas Roy for the careful reading of
   the manuscript.
CR Abrahamyan A, 2016, P NATL ACAD SCI USA, V113, pE3548, DOI 10.1073/pnas.1518786113
   Ahmadian Y, 2011, NEURAL COMPUT, V23, P46, DOI 10.1162/NECO_a_00059
   Akrami A., 2016, SOC NEUR ABSTR
   Bishop C. M., 2006, PATTERN RECOGNITION
   Busse L, 2011, J NEUROSCI, V31, P11351, DOI 10.1523/JNEUROSCI.6689-10.2011
   Fassihi A, 2014, P NATL ACAD SCI USA, V111, P2331, DOI 10.1073/pnas.1315171111
   Frund I, 2014, J VISION, V14, DOI 10.1167/14.7.9
   Green D. M., 1966, SIGNAL DETECTION THE
   Hernandez A, 1997, J NEUROSCI, V17, P6391
   Li JA, 2011, J NEUROSCI, V31, P5504, DOI 10.1523/JNEUROSCI.6316-10.2011
   Paninski L, 2010, J COMPUT NEUROSCI, V29, P107, DOI 10.1007/s10827-009-0179-x
   Pillow JW, 2011, NEURAL COMPUT, V23, P1, DOI 10.1162/NECO_a_00058
   Sahani M, 2003, ADV NEURAL INFORM PR, V15, P317
   Smith AC, 2004, J NEUROSCI, V24, P447, DOI 10.1523/JNEUROSCI.2908-03.2004
   Sutton RS, 2000, ADV NEUR IN, V12, P1057
   Tyler CW, 2000, VISION RES, V40, P3121, DOI 10.1016/S0042-6989(00)00157-7
NR 16
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702091
DA 2019-06-15
ER

PT S
AU Balamurugan, P
   Bach, F
AF Balamurugan, P.
   Bach, Francis
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Stochastic Variance Reduction Methods for Saddle-Point Problems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID CONVERGENCE
AB We consider convex-concave saddle-point problems where the objective functions may be split in many components, and extend recent stochastic variance reduction methods (such as SVRG or SAGA) to provide the first large-scale linearly convergent algorithms for this class of problems which are common in machine learning. While the algorithmic extension is straightforward, it comes with challenges and opportunities: (a) the convex minimization analysis does not apply and we use the notion of monotone operators to prove convergence, showing in particular that the same algorithm applies to a larger class of problems, such as variational inequalities, (b) there are two notions of splits, in terms of functions, or in terms of partial derivatives, (c) the split does need to be done with convex-concave terms, (d) non-uniform sampling is key to an efficient algorithm, both in theory and practice, and (e) these incremental algorithms can be easily accelerated using a simple extension of the "catalyst" framework, leading to an algorithm which is always superior to accelerated batch algorithms.
C1 [Balamurugan, P.; Bach, Francis] INRIA, Ecole Normale Super, Paris, France.
RP Balamurugan, P (reprint author), INRIA, Ecole Normale Super, Paris, France.
EM balamurugan.palaniappan@inria.fr; francis.bach@ens.fr
CR Bach F., 2008, 08121869 ARXIV
   Bach F, 2012, FOUND TRENDS MACH LE, V4, P1, DOI 10.1561/2200000015
   Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   BenTal A, 2009, PRINC SER APPL MATH, P1
   Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1
   Chen GHG, 1997, SIAM J OPTIMIZ, V7, P421, DOI 10.1137/S1052623495290179
   Clarkson KL, 2012, J ACM, V59, DOI 10.1145/2371656.2371658
   Davis D., 2016, 160100698 ARXIV
   Defazio A., 2014, ADV NIPS
   Harikandeh R., 2015, ADV NIPS
   HARKER PT, 1990, MATH PROGRAM, V48, P161, DOI 10.1007/BF01582255
   Herbrich R., 1999, ADV NIPS
   Joachims T., 2005, P ICML
   Johnson R., 2013, ADV NIPS
   Le Roux N., 2012, ADV NIPS
   Lin H., 2015, ADV NIPS
   Nesterov Y., 2004, INTRO LECT CONVEX OP
   Raguet H, 2013, SIAM J IMAGING SCI, V6, P1199, DOI 10.1137/120872802
   Rockafellar R. T., 1970, NONLINEAR ANAL 1, V18, P397
   ROCKAFELLAR RT, 1976, SIAM J CONTROL, V14, P877, DOI 10.1137/0314056
   Rosasco L., 2014, 14037999 ARXIV
   Ryu EK, 2016, APPL COMPUT MATH-BAK, V15, P3
   Schmidt M., 2015, P AISTATS
   Woodruff D., 2014, 14114357 ARXIV
   Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791
   Xu L., 2004, ADV NIPS
   Zeng XR, 2014, DIGIT SIGNAL PROCESS, V31, P124, DOI 10.1016/j.dsp.2014.03.010
   Zhang Y., 2015, P ICML
   Zhu DL, 1996, SIAM J OPTIMIZ, V6, P714, DOI 10.1137/S1052623494250415
   Zhu ZX, 2015, LECT NOTES ARTIF INT, V9284, P645, DOI 10.1007/978-3-319-23528-8_40
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703107
DA 2019-06-15
ER

PT S
AU Balandat, M
   Krichene, W
   Tomlin, C
   Bayen, A
AF Balandat, Maximilian
   Krichene, Walid
   Tomlin, Claire
   Bayen, Alexandre
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Minimizing Regret on Reflexive Banach Spaces and Nash Equilibria in
   Continuous Zero-Sum Games
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We study a general adversarial online learning problem, in which we are given a decision set X in a reflexive Banach space X and a sequence of reward vectors in the dual space of X. At each iteration, we choose an action from X, based on the observed sequence of previous rewards. Our goal is to minimize regret. Using results from infinite dimensional convex analysis, we generalize the method of Dual Averaging to our setting and obtain upper bounds on the worst-case regret that generalize many previous results. Under the assumption of uniformly continuous rewards, we obtain explicit regret bounds in a setting where the decision set is the set of probability distributions on a compact metric space S. Importantly, we make no convexity assumptions on either S or the reward functions. We also prove a general lower bound on the worst-case regret for any online algorithm. We then apply these results to the problem of learning in repeated two-player zero-sum games on compact metric spaces. In doing so, we first prove that if both players play a Hannan-consistent strategy, then with probability 1 the empirical distributions of play weakly converge to the set of Nash equilibria of the game. We then show that, under mild assumptions, Dual Averaging on the (infinite-dimensional) space of probability distributions indeed achieves Hannan-consistency.
C1 [Balandat, Maximilian; Krichene, Walid; Tomlin, Claire; Bayen, Alexandre] Univ Calif Berkeley, Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
RP Balandat, M (reprint author), Univ Calif Berkeley, Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
EM balandat@eecs.berkeley.edu; walid@eecs.berkeley.edu;
   tomlin@eecs.berkeley.edu; bayen@berkeley.edu
CR Arora S, 2012, THEORY COMPUT, V8, P121, DOI DOI 10.4086/TOC.2012.V008A006
   Audibert JY, 2014, MATH OPER RES, V39, P31, DOI 10.1287/moor.2013.0598
   Bubeck S., 2014, ARXIV E PRINTS
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Cover T.M., 1991, MATH FINANC, V1, P1, DOI DOI 10.1111/J.1467-9965.1991.TB00002.X
   Csiszar I., 1967, STUD SCI MATH HUNG, V2, P299
   Glicksberg Irving L., 1950, RM478 RAND CORP
   Hannan James, 1957, ANN MATH STUDIES 39, VIII
   Hart S, 2001, J ECON THEORY, V98, P26, DOI 10.1006/jeth.2000.2746
   Hazan E, 2007, MACH LEARN, V69, P169, DOI 10.1007/s10994-007-5016-8
   Heinonen J, 2015, NEW MATH MONOGRAPHS
   Krichene Walid, 2015, P 32 INT C INT C MAC, P824
   Krichene Walid, 2015, ABS150407720 CORR
   Kwon Joon, 2014, ARXIV E PRINTS
   Lehrer E, 2003, INT J GAME THEORY, V31, P253, DOI 10.1007/s001820200115
   Monderer D, 1996, GAME ECON BEHAV, V14, P124, DOI 10.1006/game.1996.0044
   Nesterov Y, 2009, MATH PROGRAM, V120, P221, DOI 10.1007/s10107-007-0149-x
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Srebro  N., 2011, ADV NEURAL INFORM PR, P2645
   Sridharan Karthik, 2010, P 23 ANN C LEARN THE, P1
   Stromberg T, 2011, POSITIVITY, V15, P527, DOI 10.1007/s11117-010-0105-5
   Xiao L, 2010, J MACH LEARN RES, V11, P2543
NR 23
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701073
DA 2019-06-15
ER

PT S
AU Balcan, MF
   Sandholm, T
   Vitercik, E
AF Balcan, Maria-Florina
   Sandholm, Tuomas
   Vitercik, Ellen
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Sample Complexity of Automated Mechanism Design
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB The design of revenue-maximizing combinatorial auctions, i.e. multi-item auctions over bundles of goods, is one of the most fundamental problems in computational economics, unsolved even for two bidders and two items for sale. In the traditional economic models, it is assumed that the bidders' valuations are drawn from an underlying distribution and that the auction designer has perfect knowledge of this distribution. Despite this strong and oftentimes unrealistic assumption, it is remarkable that the revenue-maximizing combinatorial auction remains unknown. In recent years, automated mechanism design has emerged as one of the most practical and promising approaches to designing high-revenue combinatorial auctions. The most scalable automated mechanism design algorithms take as input samples from the bidders' valuation distribution and then search for a high-revenue auction in a rich auction class. In this work, we provide the first sample complexity analysis for the standard hierarchy of deterministic combinatorial auction classes used in automated mechanism design. In particular, we provide tight sample complexity bounds on the number of samples needed to guarantee that the empirical revenue of the designed mechanism on the samples is close to its expected revenue on the underlying, unknown distribution over bidder valuations, for each of the auction classes in the hierarchy. In addition to helping set automated mechanism design on firm foundations, our results also push the boundaries of learning theory. In particular, the hypothesis functions used in our contexts are defined through multi-stage combinatorial optimization procedures, rather than simple decision boundaries, as are common in machine learning.
C1 [Balcan, Maria-Florina; Sandholm, Tuomas; Vitercik, Ellen] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
RP Balcan, MF (reprint author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
EM ninamf@cs.cmu.edu; sandholm@cs.cmu.edu; vitercik@cs.cmu.edu
FU NSF [CCF-1535967, CCF-1451177, CCF-1422910, IIS-1618714, IIS-1617590,
   IIS-1320620, IIS-1546752]; ARO award [W911NF-16-1-0061]; Sloan Research
   Fellowship; Microsoft Research Faculty Fellowship; NSF Graduate Research
   Fellowship; Microsoft Research Women's Fellowship
FX This work was supported in part by NSF grants CCF-1535967, CCF-1451177,
   CCF-1422910, IIS-1618714, IIS-1617590, IIS-1320620, IIS-1546752, ARO
   award W911NF-16-1-0061, a Sloan Research Fellowship, a Microsoft
   Research Faculty Fellowship, an NSF Graduate Research Fellowship, and a
   Microsoft Research Women's Fellowship.
CR Balcan Maria- Florina, 2008, J COMPUT SYST SCI, V74, P78
   Cole Richard, 2014, P 46 ANN ACM S THEOR, P243
   Conitzer V., 2002, P 18 ANN C UNC ART I, P103
   Conitzer  V., 2004, P ACM C EL COMM ACM, P132
   Devanur Nikhil R, 2016, P ANN S THEOR COMP S
   Dughmi S, 2014, LECT NOTES COMPUT SC, V8877, P277, DOI 10.1007/978-3-319-13129-0_22
   Elkind E, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P736
   Feldman Michal, 2015, ANN ACM SIAM S DISCR
   Hsu Justin, 2016, P ANN S THEOR COMP S
   Huang  Zhiyi, 2015, P 16 ACM C EC COMP E, P45
   Jehiel P, 2007, J ECON THEORY, V134, P494, DOI 10.1016/j.jet.2006.02.001
   Likhodedov A, 2004, PROCEEDING OF THE NINETEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE SIXTEENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE, P232
   Likhodedov A., 2005, P NAT C ART INT AAAI
   Medina Andres Munoz, 2014, P 31 INT C MACH LEAR, P262
   Mohri M., 2012, FDN MACHINE LEARNING
   Morgenstern Jamie, 2016, C LEARN THEORY COLT
   Morgenstern Jamie H, 2015, ADV NEURAL INFORM PR, P136
   MYERSON RB, 1981, MATH OPER RES, V6, P58, DOI 10.1287/moor.6.1.58
   Roberts K., 1979, AGGREGATION REVELATI
   Roughgarden Tim, 2016, P ACM C EC COMP EC
   Sandholm T, 2003, LECT NOTES COMPUT SC, V2833, P19
   Sandholm T, 2015, OPER RES, V63, P1000, DOI 10.1287/opre.2015.1398
   Tang Pingzhong, 2012, INT C AUT AG MULT SY
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702098
DA 2019-06-15
ER

PT S
AU Balcan, MF
   Zhang, HY
AF Balcan, Maria-Florina
   Zhang, Hongyang
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Noise-Tolerant Life-Long Matrix Completion via Adaptive Sampling
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID SUBSPACE; PCA
AB We study the problem of recovering an incomplete m x n matrix of rank r with columns arriving online over time. This is known as the problem of life-long matrix completion, and is widely applied to recommendation system, computer vision, system identification, etc. The challenge is to design provable algorithms tolerant to a large amount of noises, with small sample complexity. In this work, we give algorithms achieving strong guarantee under two realistic noise models. In bounded deterministic noise, an adversary can add any bounded yet unstructured noise to each column. For this problem, we present an algorithm that returns a matrix of a small error, with sample complexity almost as small as the best prior results in the noiseless case. For sparse random noise, where the corrupted columns are sparse and drawn randomly, we give an algorithm that exactly recovers an mu(0)-incoherent matrix by probability at least 1 - delta with sample complexity as small as O (mu(0)rn log(r/delta)). This result advances the state-of-the-art work and matches the lower bound in a worst case. We also study the scenario where the hidden matrix lies on a mixture of subspaces and show that the sample complexity can be even smaller. Our proposed algorithms perform well experimentally in both synthetic and real-world datasets.
C1 [Balcan, Maria-Florina; Zhang, Hongyang] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.
RP Balcan, MF (reprint author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.
EM ninamf@cs.cmu.edu; hongyanz@cs.cmu.edu
FU Sloan Fellowship; Microsoft Research Fellowship;  [NSF-CCF 1535967]; 
   [NSF CCF-1422910];  [NSF CCF-1451177]
FX This work was supported in part by grants NSF-CCF 1535967, NSF
   CCF-1422910, NSF CCF-1451177, a Sloan Fellowship, and a Microsoft
   Research Fellowship.
CR Awasthi P., 2014, STOC, P449
   Balcan M.-F., 2015, ANN C LEARN THEOR
   Balcan M.-F. F., 2013, ADV NEURAL INFORM PR, P1295
   Balzano L, 2010, 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P704, DOI 10.1109/ALLERTON.2010.5706976
   Balzano L, 2010, IEEE INT SYMP INFO, P1638, DOI 10.1109/ISIT.2010.5513344
   Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061
   Candes EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722
   Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Carlson A., 2010, AAAI C ART INT
   Costeira JP, 1998, INT J COMPUT VISION, V29, P159, DOI 10.1023/A:1008000628999
   Dhanjal C., 2014, P INT C DAT MIN PHIL, P623
   Gittens A., 2011, ARXIV11105305
   Gopnik A., 2001, BABIES THINK SCI CHI
   Hastie T, 1998, STAT SCI, V13, P54
   Kennedy R, 2014, 2014 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (GLOBALSIP), P507, DOI 10.1109/GlobalSIP.2014.7032169
   Krishnamurthy A., 2014, ARXIV14073619
   Krishnamurthy A., 2013, ADV NEURAL INFORM PR, P836
   Lerman G, 2014, CONSTR APPROX, V40, P329, DOI 10.1007/s00365-014-9242-6
   Lois B, 2015, IEEE INT SYMP INFO, P1826, DOI 10.1109/ISIT.2015.7282771
   Recht B, 2011, J MACH LEARN RES, V12, P3413
   SCHARF LL, 1994, IEEE T SIGNAL PROCES, V42, P2146, DOI 10.1109/78.301849
   Warmuth MK, 2008, J MACH LEARN RES, V9, P2287
   Xu H, 2012, IEEE T INFORM THEORY, V58, P3047, DOI 10.1109/TIT.2011.2173156
   Zhang H., 2015, AAAI C ART INT, P3143
   Zhang HY, 2016, IEEE T INFORM THEORY, V62, P4748, DOI 10.1109/TIT.2016.2573311
   Zhang HY, 2015, NEURAL COMPUT, V27, P1915, DOI 10.1162/NECO_a_00762
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704104
DA 2019-06-15
ER

PT S
AU Balkanski, E
   Rubinstein, A
   Singer, Y
AF Balkanski, Eric
   Rubinstein, Aviad
   Singer, Yaron
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI The Power of Optimization from Samples
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We consider the problem of optimization from samples of monotone submodular functions with bounded curvature. In numerous applications, the function optimized is not known a priori, but instead learned from data. What are the guarantees we have when optimizing functions from sampled data?
   In this paper we show that for any monotone submodular function with curvature c there is a (1-c)/(1 + c-c(2)) approximation algorithm for maximization under cardinality constraints when polynomially-many samples are drawn from the uniform distribution over feasible sets. Moreover, we show that this algorithm is optimal. That is, for any c < 1, there exists a submodular function with curvature c for which no algorithm can achieve a better approximation. The curvature assumption is crucial as for general monotone submodular functions no algorithm can obtain a constant-factor approximation for maximization under a cardinality constraint when observing polynomially-many samples drawn from any distribution over feasible sets, even when the function is statistically learnable.
C1 [Balkanski, Eric; Singer, Yaron] Harvard Univ, Cambridge, MA 02138 USA.
   [Rubinstein, Aviad] Univ Calif Berkeley, Berkeley, CA 94720 USA.
RP Balkanski, E (reprint author), Harvard Univ, Cambridge, MA 02138 USA.
EM ericbalkanski@g.harvard.edu; aviad@eecs.berkeley.edu;
   yaron@seas.harvard.edu
CR Balcan M., 2012, COLT
   Balcan M., 2015, AAMAS
   Balcan M., 2011, STOC
   Balkanski E., 2015, ARXIV151206238
   Conforti M., 1984, DISCRETE APPL MATH
   Feige U., 2011, SIAM J COMPUTING
   Feige U., 1998, JACM
   Feldman V., 2014, COLT
   Feldman V., 2013, COLT
   Feldman V., 2015, CORR
   Feldman V., 2013, FOCS
   Golovin  D., 2010, IPSN
   Gomez Rodriguez M., 2010, SIGKDD
   Hang L., 2011, IEICE
   Iyer R., 2013, NIPS
   Jegelka S., 2011, ICML
   Jegelka S., 2011, CVPR
   Kempe D., 2003, SIGKDD
   Leskovec J., 2007, SIGKDD
   Lin H., 2011, NAACL HLT
   Lin H., 2011, INTERSPEECH
   Nemhauser G. L., 1978, MATH PROGRAMMING STU, V8
   Rosenfeld N., 2016, ARXIV160504719
   Sviridenko M., 2015, SODA
   Valiant  L., 1984, COMMUN ACM
   Vondrak J., 2010, RIMS
   Yue Y., 2008, ICML
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703083
DA 2019-06-15
ER

PT S
AU Balsubramani, A
   Freund, Y
AF Balsubramani, Akshay
   Freund, Yoav
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Optimal Binary Classifier Aggregation for General Losses
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We address the problem of aggregating an ensemble of predictors with known loss bounds in a semi-supervised binary classification setting, to minimize prediction loss incurred on the unlabeled data. We find the minimax optimal predictions for a very general class of loss functions including all convex and many non-convex losses, extending a recent analysis of the problem for misclassification error. The result is a family of semi-supervised ensemble aggregation algorithms which are as efficient as linear learning by convex optimization, but are minimax optimal without any relaxations. Their decision rules take a form familiar in decision theory - applying sigmoid functions to a notion of ensemble margin - without the assumptions typically made in margin-based learning.
C1 [Balsubramani, Akshay; Freund, Yoav] Univ Calif San Diego, La Jolla, CA 92093 USA.
RP Balsubramani, A (reprint author), Univ Calif San Diego, La Jolla, CA 92093 USA.
EM abalsubr@ucsd.edu; yfreund@ucsd.edu
FU NSF [IIS-1162581]
FX AB is grateful to Chris "Ceej" Tosh for feedback that made the
   manuscript clearer. This work was supported by the NSF (grant
   IIS-1162581).
NR 0
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 8
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973705011
DA 2019-06-15
ER

PT S
AU Bastani, O
   Ioannou, Y
   Lampropoulos, L
   Vytiniotis, D
   Nori, AV
   Criminisi, A
AF Bastani, Osbert
   Ioannou, Yani
   Lampropoulos, Leonidas
   Vytiniotis, Dimitrios
   Nori, Aditya, V
   Criminisi, Antonio
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Measuring Neural Net Robustness with Constraints
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Despite having high accuracy, neural nets have been shown to be susceptible to adversarial examples, where a small perturbation to an input can cause it to become mislabeled. We propose metrics for measuring the robustness of a neural net and devise a novel algorithm for approximating these metrics based on an encoding of robustness as a linear program. We show how our metrics can be used to evaluate the robustness of deep neural nets with experiments on the MNIST and CIFAR-10 datasets. Our algorithm generates more informative estimates of robustness metrics compared to estimates based on existing algorithms. Furthermore, we show how existing approaches to improving robustness "overfit" to adversarial examples generated using a specific algorithm. Finally, we show that our techniques can be used to additionally improve neural net robustness both according to the metrics that we propose, but also according to previously proposed metrics.
C1 [Bastani, Osbert] Stanford Univ, Stanford, CA 94305 USA.
   [Ioannou, Yani] Univ Cambridge, Cambridge, England.
   [Lampropoulos, Leonidas] Univ Penn, Philadelphia, PA 19104 USA.
   [Vytiniotis, Dimitrios; Nori, Aditya, V; Criminisi, Antonio] Microsoft Res, Redmond, WA USA.
RP Bastani, O (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM obastani@cs.stanford.edu; yai20@cam.ac.uk; llamp@seas.upenn.edu;
   dimitris@microsoft.com; adityan@microsoft.com; antcrim@microsoft.com
CR Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640
   Chalupka K., 2015, VISUAL CAUSAL FEATUR
   Dezfooli S. M. Moosavi, 2016, P 2016 IEEE C COMP V
   Fawzi A., 2015, ARXIV E PRINTS
   Feng Jiashi, 2016, ARXIV160202389
   Globerson A., 2006, P 23 INT C MACH LEAR, V148, P353
   Goodfellow I. J., 2015, EXPLAINING HARNESSIN
   Gu S., 2014, DEEP NEURAL NETWORK
   Huang Ruitong, 2015, CORR
   Jia Y., 2014, ARXIV14085093
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A, 2012, IMAGENET CLASSIFICAT
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y., 2001, INTELLIGENT SIGNAL P, P306
   Lin Min, 2013, CORR
   Miyato Takeru, 2015, STAT, V1050, P25
   Montufar G. F., 2014, ADV NEURAL INFORM PR, P2924
   Papernot Nicolas, 2016, ARXIV160202697
   Sabour S., 2015, ARXIV151105122
   Shaham  U., 2015, ARXIV151105432
   Szegedy C., 2014, INTRIGUING PROPERTIE
   Tabacof Pedro, 2015, CORR
   Xu H, 2012, MACH LEARN, V86, P391, DOI 10.1007/s10994-011-5268-1
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702086
DA 2019-06-15
ER

PT S
AU Battaglia, PW
   Pascanu, R
   Lai, M
   Rezende, D
   Kavukcuoglu, K
AF Battaglia, Peter W.
   Pascanu, Razvan
   Lai, Matthew
   Rezende, Danilo
   Kavukcuoglu, Koray
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Interaction Networks for Learning about Objects, Relations and Physics
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Reasoning about objects, relations, and physics is central to human intelligence, and a key goal of artificial intelligence. Here we introduce the interaction network, a model which can reason about how objects in complex systems interact, supporting dynamical predictions, as well as inferences about the abstract properties of the system. Our model takes graphs as input, performs object- and relation-centric reasoning in a way that is analogous to a simulation, and is implemented using deep neural networks. We evaluate its ability to reason about several challenging physical domains: n-body problems, rigid-body collision, and non-rigid dynamics. Our results show it can be trained to accurately simulate the physical trajectories of dozens of objects over thousands of time steps, estimate abstract quantities such as energy, and generalize automatically to systems with different numbers and configurations of objects and relations. Our interaction network implementation is the first general-purpose, learnable physics engine, and a powerful general framework for reasoning about object and relations in a wide variety of complex real-world domains.
C1 [Battaglia, Peter W.; Pascanu, Razvan; Lai, Matthew; Rezende, Danilo; Kavukcuoglu, Koray] Google DeepMind, London N1C 4AG, England.
RP Battaglia, PW (reprint author), Google DeepMind, London N1C 4AG, England.
EM peterbattaglia@google.com; razp@google.com; matthewlai@google.com;
   danilor@google.com; korayk@google.com
CR Andreas J., 2016, NAACL
   Baraff D, 2001, SIGGRAPH COURSE NOTE, V2, P2
   Battaglia PW, 2013, P NATL ACAD SCI USA, V110, P18327, DOI 10.1073/pnas.1306572110
   Craik KJW, 1943, NATURE EXPLANATION
   Fragkiadaki Katerina, 2016, ICLR
   GARDIN F, 1989, ARTIF INTELL, V38, P139, DOI 10.1016/0004-3702(89)90055-6
   Ghahramani Z, 2015, NATURE, V521, P452, DOI 10.1038/nature14541
   Grzeszczuk R., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P9
   Hayes P. J, 1978, NAIVE PHYS MANIFESTO
   Hegarty M, 2004, TRENDS COGN SCI, V8, P280, DOI 10.1016/j.tics.2004.04.001
   Jaderberg M, 2015, ADV NEURAL INFORM PR, P2008
   Johnson-Laird P. N., 1983, MENTAL MODELS COGNIT, V6
   Kingma D. P., 2015, ICLR
   Ladicky L, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818129
   Lake B. M., 2016, ARXIV160400289
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lerer  A., 2016, ARXIV160301312
   Li  W., 2016, ARXIV160400066
   Mottaghi R, 2016, ARXIV160305600
   Mottaghi R, 2015, ARXIV151104048
   Reed Scott, 2016, ICLR
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Socher R., 2011, ADV NEURAL INFORM PR, P801
   SPELKE ES, 1992, PSYCHOL REV, V99, P605, DOI 10.1037/0033-295X.99.4.605
   Sutskever  I., 2009, NIPS 21, P1593
   Tenenbaum JB, 2011, SCIENCE, V331, P1279, DOI 10.1126/science.1192788
   Winston P, 1975, PSYCHOL COMPUTER VIS, V73
   Wu J., 2015, ADV NEURAL INFORM PR, P127, DOI DOI 10.1007/978-3-319-26532-2_15
NR 29
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703054
DA 2019-06-15
ER

PT S
AU Bautista, MA
   Sanakoyeu, A
   Sutter, E
   Ommer, B
AF Bautista, Miguel A.
   Sanakoyeu, Artsiom
   Sutter, Ekaterina
   Ommer, Bjoern
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI CliqueCNN: Deep Unsupervised Exemplar Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID SIMILARITY
AB Exemplar learning is a powerful paradigm for discovering visual similarities in an unsupervised manner. In this context, however, the recent breakthrough in deep learning could not yet unfold its full potential. With only a single positive sample, a great imbalance between one positive and many negatives, and unreliable relationships between most samples, training of Convolutional Neural networks is impaired. Given weak estimates of local distance we propose a single optimization problem to extract batches of samples with mutually consistent relations. Conflicting relations are distributed over different batches and similar samples are grouped into compact cliques. Learning exemplar similarities is framed as a sequence of clique categorization tasks. The CNN then consolidates transitivity relations within and between cliques and learns a single representation for all samples without the need for labels. The proposed unsupervised approach has shown competitive performance on detailed posture analysis and object classification.
C1 [Bautista, Miguel A.; Sanakoyeu, Artsiom; Sutter, Ekaterina; Ommer, Bjoern] Heidelberg Univ, Heidelberg Collaboratory Image Proc, IWR, Heidelberg, Germany.
RP Bautista, MA (reprint author), Heidelberg Univ, Heidelberg Collaboratory Image Proc, IWR, Heidelberg, Germany.
EM miguel.bautista@iwr.uni-heidelberg.de;
   artsiom.sanakoyeu@iwr.uni-heidelberg.de;
   ekaterina.sutter@iwr.uni-heidelberg.de;
   bjoern.ommer@iwr.uni-heidelberg.de
FU Ministry for Science, Baden-Wurttemberg; Heidelberg Academy of Sciences,
   Heidelberg, Germany
FX This research has been funded in part by the Ministry for Science,
   Baden-Wurttemberg and the Heidelberg Academy of Sciences, Heidelberg,
   Germany. We are grateful to the NVIDIA corporation for donating a Titan
   X GPU.
CR Burkard R. E, 1998, HDB COMBINATORIAL OP
   DOERSCH C., 2012, ACM T GRAPHICS SIGGR, V31, P4
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Dosovitskiy  A., 2014, ADV NEURAL INFORM PR, P766
   Eigenstetter A., CVPR 14
   El-Naqa I, 2004, IEEE T MED IMAGING, V23, P1233, DOI 10.1109/TMI.2004.834601
   Felzenszwalb P. F., 2008, IEEE C COMP VIS PATT, V1, P1, DOI DOI 10.1109/CVPR.2008.4587597
   Ferrari V, 2009, PROC CVPR IEEE, P1, DOI 10.1109/CVPRW.2009.5206495
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   HARIHARAN B, 2012, ECCV, V7575, P459
   He X., 2014, CVPR
   Johnson MK, 2011, PROC CVPR IEEE
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Malisiewicz T, 2011, IEEE I CONF COMP VIS, P89, DOI 10.1109/ICCV.2011.6126229
   NIEBLES JC, 2010, EUR C COMP VIS, V6312, P392
   Pirsiavash H., 2014, CVPR
   Ramakrishna V., 2014, ECCV 14
   Rubio JC, 2015, PATTERN RECOGN, V48, P3871, DOI 10.1016/j.patcog.2015.06.013
   Shapovalova N, 2015, IEEE IMAGE PROC, P93, DOI 10.1109/ICIP.2015.7350766
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Wang X., 2015, ICCV
   Xia H, 2014, IEEE T PATTERN ANAL, V36, P536, DOI 10.1109/TPAMI.2013.149
   Yuille AL, 2003, NEURAL COMPUT, V15, P915, DOI 10.1162/08997660360581958
   Zagoruyko S., CVPR 14
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703023
DA 2019-06-15
ER

PT S
AU Beatson, A
   Wang, ZR
   Liu, H
AF Beatson, Alex
   Wang, Zhaoran
   Liu, Han
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Blind Attacks on Machine Learners
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB The importance of studying the robustness of learners to malicious data is well established. While much work has been done establishing both robust estimators and effective data injection attacks when the attacker is omniscient, the ability of an attacker to provably harm learning while having access to little information is largely unstudied. We study the potential of a "blind attacker" to provably limit a learner's performance by data injection attack without observing the learner's training set or any parameter of the distribution from which it is drawn. We provide examples of simple yet effective attacks in two settings: firstly, where an "informed learner" knows the strategy chosen by the attacker, and secondly, where a "blind learner" knows only the proportion of malicious data and some family to which the malicious distribution chosen by the attacker belongs. For each attack, we analyze minimax rates of convergence and establish lower bounds on the learner's minimax risk, exhibiting limits on a learner's ability to learn under data injection attack even when the attacker is "blind".
C1 [Beatson, Alex] Princeton Univ, Dept Comp Sci, Princeton, NJ 08544 USA.
   [Wang, Zhaoran; Liu, Han] Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA.
RP Beatson, A (reprint author), Princeton Univ, Dept Comp Sci, Princeton, NJ 08544 USA.
EM abeatson@princeton.edu; zhaoran@princeton.edu; hanliu@princeton.edu
CR Azizyan M., 2013, NEURAL INFORM PROCES
   Barreno M, 2010, MACH LEARN, V81, P121, DOI 10.1007/s10994-010-5188-5
   Biggio B., 2012, ARXIV12066389
   Bolton RJ, 2002, STAT SCI, V17, P235
   Bruckner M., 2011, ACM SIGKDD
   Chaoji V., 2006, SDM WORKSH LINK AN C
   Chen M., 2015, ARXIV151104144
   Chen Y., 2013, ARXIV13127006
   Duchi J., 2014, ARXIV13023203V4
   Duchi J., 2013, NEURAL INFORM PROCES
   Dwork C, 2006, LECT NOTES COMPUT SC, V4052, P1
   KEARNS M, 1993, SIAM J COMPUT, V22, P807, DOI 10.1137/0222052
   Laskov P., 2009, ACM WORKSH SEC ART I
   Laskov P, 2010, MACH LEARN, V81, P115, DOI 10.1007/s10994-010-5207-6
   Rubinstein B. I., 2008, UCBEECS200873
   Shi YA, 2009, IEEE DATA MINING, P483, DOI 10.1109/ICDM.2009.75
   Sommer R, 2010, P IEEE S SECUR PRIV, P305, DOI 10.1109/SP.2010.25
   Tsybakov A. B., 2008, INTRO NONPARAMETRIC
   Xiao H, 2012, FRONT ARTIF INTEL AP, V242, P870, DOI 10.3233/978-1-61499-098-7-870
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704007
DA 2019-06-15
ER

PT S
AU Bellemare, MG
   Srinivasan, S
   Ostrovski, G
   Schaul, T
   Saxton, D
   Munos, R
AF Bellemare, Marc G.
   Srinivasan, Sriram
   Ostrovski, Georg
   Schaul, Tom
   Saxton, David
   Munos, Remi
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Unifying Count-Based Exploration and Intrinsic Motivation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across states. Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. Drawing inspiration from the intrinsic motivation literature, we use density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary density model. This technique enables us to generalize count-based exploration algorithms to the non-tabular case. We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We transform these pseudo-counts into exploration bonuses and obtain significantly improved exploration in a number of hard games, including the infamously difficult MONTEZUMA'S REVENGE.
C1 [Bellemare, Marc G.; Srinivasan, Sriram; Ostrovski, Georg; Schaul, Tom; Saxton, David; Munos, Remi] Google DeepMind, London, England.
RP Bellemare, MG (reprint author), Google DeepMind, London, England.
EM bellemare@google.com; srsrinivasan@google.com; ostrovski@google.com;
   schaul@google.com; saxton@google.com; munos@google.com
CR Barto A. G, 2013, INTRINSICALLY MOTIVA, P17, DOI DOI 10.1007/978-3-642-32375-1_2
   Bellemare M., 2014, P 31 INT C MACH LEAR, P1458
   Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912
   Bertsekas D.P., 1996, NEURODYNAMIC PROGRAM
   Cover T. M., 1991, ELEMENTS INFORM THEO
   Houthooft R., 2016, VARIATIONAL INFORM M
   Hutter M., 2013, P C ONL LEARN THEOR
   Kolter Z. J., 2009, P 26 INT C MACH LEAR
   Leike J., 2016, P C UNC ART INT
   Lopes M., 2012, ADV NEURAL INFORM PR, V25
   Machado M. C., 2015, AAAI WORKSH LEARN GE
   Mnih V., 2016, P INT C MACH LEARN
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Mohamed S, 2015, ADV NEUR IN, V28
   Ollivier Y., 2015, ARXIV150304304
   Orseau L., 2013, P C ALG LEARN THEOR
   Oudeyer PY, 2007, IEEE T EVOLUT COMPUT, V11, P265, DOI 10.1109/TEVC.2006.890271
   Pazis J., 2016, P 30 AAAI C ART INT
   Schmidhuber J., 1991, AN AN P 1 INT C SIM
   Singh S., 2004, ADV NEURAL INFORM PR, V16
   Stadie BC, 2015, ARXIV150700814
   Strehl AL, 2008, J COMPUT SYST SCI, V74, P1309, DOI 10.1016/j.jcss.2007.08.009
   Van den Oord A., 2016, P 33 INT C MACH LEAR
   Van Hasselt H., 2016, P 30 AAAI C ART INT
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703019
DA 2019-06-15
ER

PT S
AU Belousov, B
   Neumann, G
   Rothkopf, CA
   Peters, J
AF Belousov, Boris
   Neumann, Gerhard
   Rothkopf, Constantin A.
   Peters, Jan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Catching heuristics are optimal control policies
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID OPTIMIZATION; MODEL
AB Two seemingly contradictory theories attempt to explain how humans move to intercept an airborne ball. One theory posits that humans predict the ball trajectory to optimally plan future actions; the other claims that, instead of performing such complicated computations, humans employ heuristics to reactively choose appropriate actions based on immediate visual feedback. In this paper, we show that interception strategies appearing to be heuristics can be understood as computational solutions to the optimal control problem faced by a ball-catching agent acting under uncertainty. Modeling catching as a continuous partially observable Markov decision process and employing stochastic optimal control theory, we discover that the four main heuristics described in the literature are optimal solutions if the catcher has sufficient time to continuously visually track the ball. Specifically, by varying model parameters such as noise, time to ground contact, and perceptual latency, we show that different strategies arise under different circumstances. The catcher's policy switches between generating reactive and predictive behavior based on the ratio of system to observation noise and the ratio between reaction time and task duration. Thus, we provide a rational account of human ball-catching behavior and a unifying explanation for seemingly contradictory theories of target interception on the basis of stochastic optimal control.
C1 [Belousov, Boris; Neumann, Gerhard; Peters, Jan] Tech Univ Darmstadt, Dept Comp Sci, Darmstadt, Germany.
   [Rothkopf, Constantin A.] Tech Univ Darmstadt, Ctr Cognit Sci, Darmstadt, Germany.
   [Rothkopf, Constantin A.] Tech Univ Darmstadt, Dept Psychol, Darmstadt, Germany.
RP Belousov, B (reprint author), Tech Univ Darmstadt, Dept Comp Sci, Darmstadt, Germany.
RI Peters, Jan/P-6027-2019
OI Peters, Jan/0000-0002-5266-8091
FU European Union [640554]
FX This project has received funding from the European Union's Horizon 2020
   research and innovation programme under grant agreement No 640554.
CR Anderson FC, 2001, J BIOMECH ENG-T ASME, V123, P381, DOI 10.1115/1.1392310
   Andersson J., 2012, RECENT ADV ALGORITHM, P297
   Betts JT, 1998, J GUID CONTROL DYNAM, V21, P193, DOI 10.2514/2.4231
   BRANCAZIO PJ, 1985, AM J PHYS, V53, P849, DOI 10.1119/1.14350
   Bry Adam, 2011, IEEE International Conference on Robotics and Automation, P723
   CHAPMAN S, 1968, AM J PHYS, V36, P868, DOI 10.1119/1.1974297
   Diehl M, 2006, LECT NOTES CONTR INF, V340, P65
   Fink PW, 2009, J VISION, V9, DOI 10.1167/9.13.14
   FLASH T, 1985, J NEUROSCI, V5, P1688
   Gigerenzer G., 2007, GUT FEELINGS INTELLI
   Gigerenzer G, 2009, TOP COGN SCI, V1, P107, DOI 10.1111/j.1756-8765.2008.01006.x
   Harris CM, 1998, NATURE, V394, P780, DOI 10.1038/29528
   Hayhoe M. M., 2004, J VISION, V4, P156, DOI DOI 10.1167/4.8.156
   MCBEATH MK, 1995, SCIENCE, V268, P569, DOI 10.1126/science.7725104
   McIntyre J, 2001, NAT NEUROSCI, V4, P693, DOI 10.1038/89477
   McLeod P, 2006, J EXP PSYCHOL HUMAN, V32, P139, DOI 10.1037/0096-1523.32.1.139
   Miall R. C., 1996, FORWARD MODELS PHYSL
   Patil S, 2015, SPRINGER TRAC ADV RO, V107, P515, DOI 10.1007/978-3-319-16595-0_30
   Platt R, 2010, ROBOTICS SCI SYSTEMS
   Simon HA, 1955, Q J ECON, V69, P99, DOI 10.2307/1884852
   Thrun S., 2005, PROBABILISTIC ROBOTI
   Todorov E, 2002, NAT NEUROSCI, V5, P1226, DOI 10.1038/nn963
   UNO Y, 1989, BIOL CYBERN, V61, P89
   van den Berg J, 2012, INT J ROBOT RES, V31, P1263, DOI 10.1177/0278364912456319
   Vitus MP, 2011, IEEE INT CONF ROBOT, P2152, DOI 10.1109/ICRA.2011.5980257
   Wachter A, 2006, MATH PROGRAM, V106, P25, DOI 10.1007/s10107-004-0559-y
   Zago M., 2009, VISUO MOTOR COORDINA
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704072
DA 2019-06-15
ER

PT S
AU Berahas, AS
   Nocedal, J
   Takac, M
AF Berahas, Albert S.
   Nocedal, Jorge
   Takac, Martin
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI A Multi-Batch L-BFGS Method for Machine Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB The question of how to parallelize the stochastic gradient descent (SGD) method has received much attention in the literature. In this paper, we focus instead on batch methods that use a sizeable fraction of the training set at each iteration to facilitate parallelism, and that employ second-order information. In order to improve the learning process, we follow a multi-batch approach in which the batch changes at each iteration. This can cause difficulties because L-BFGS employs gradient differences to update the Hessian approximations, and when these gradients are computed using different data points the process can be unstable. This paper shows how to perform stable quasi-Newton updating in the multi-batch setting, illustrates the behavior of the algorithm in a distributed computing platform, and studies its convergence properties for both the convex and nonconvex cases.
C1 [Berahas, Albert S.; Nocedal, Jorge] Northwestern Univ, Evanston, IL 60208 USA.
   [Takac, Martin] Lehigh Univ, Bethlehem, PA 18015 USA.
RP Berahas, AS (reprint author), Northwestern Univ, Evanston, IL 60208 USA.
EM albertberahas@u.northwestern.edu; j-nocedal@northwestern.edu;
   takac.mt@gmail.com
FU Office of Naval Research [N000141410313]; Department of Energy
   [DE-FG02-87ER25047]; National Science Foundation [CCF-1618717,
   DMS-1620022]
FX The first two authors were supported by the Office of Naval Research
   award N000141410313, the Department of Energy grant DE-FG02-87ER25047
   and the National Science Foundation grant DMS-1620022. Martin Takac was
   supported by National Science Foundation grant CCF-1618717.
CR Agarwal A, 2014, J MACH LEARN RES, V15, P1111
   Bertsekas D. P., 1989, PARALLEL DISTRIBUTED, V23
   Bollapragada R., 2016, ARXIV160908502
   Bottou  L., 2016, ARXIV160604838
   Bottou L, 2008, ADV NEURAL INFORM PR, P161
   Bottou U, 2004, ADV NEUR IN, V16, P217
   Byrd RH, 2016, SIAM J OPTIMIZ, V26, P1008, DOI 10.1137/140954362
   Byrd RH, 2011, SIAM J OPTIMIZ, V21, P977, DOI 10.1137/10079923X
   Chen W., 2014, ADV NEURAL INFORM PR, P1332
   Dai YH, 2003, SIAM J OPTIMIZ, V13, P693
   Dean J., 2012, ADV NEURAL INFORM PR, P1223
   Defazio A., 2014, ADV NEURAL INFORM PR, P1646
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Li DH, 2001, SIAM J OPTIMIZ, V11, P1054, DOI 10.1137/S1052623499354242
   Mania Horia, 2015, ARXIV150706970
   Mascarenhas WF, 2004, MATH PROGRAM, V99, P49, DOI 10.1007/s10107-003-0421-7
   Nedic A, 2001, APPL OPTIMIZAT, V54, P223
   Ngiam J, 2011, P 28 INT C MACH LEAR, P265
   Niu F., 2011, ADV NEURAL INFORM PR, V24, P693
   Nocedal J., 1999, NUMERICAL OPTIMIZATI
   Powell M.J.D., 1976, NONLINEAR PROGRAMMIN, VIX, P53
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Schmidt M., 2016, MATH PROGRAM, P1
   Schraudolph N. N., 2007, P 11 INT C ART INT S, P436
   Takac M., 2013, CORNELL U LIBR, P1022
   Zhang Y., 2015, P 32 INT C MACH LEAR, P362
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701002
DA 2019-06-15
ER

PT S
AU Beygelzimer, A
   Hsu, D
   Langford, J
   Zhang, CC
AF Beygelzimer, Alina
   Hsu, Daniel
   Langford, John
   Zhang, Chicheng
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Search Improves Label for Active Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID CONVERGENCE
AB We investigate active learning with access to two distinct oracles: LABEL (which is standard) and SEARCH (which is not). The SEARCH oracle models the situation where a human searches a database to seed or counterexample an existing solution. SEARCH is stronger than LABEL while being natural to implement in many situations. We show that an algorithm using both oracles can provide exponentially large problem-dependent improvements over LABEL alone.
C1 [Beygelzimer, Alina] Yahoo Res, New York, NY 10003 USA.
   [Hsu, Daniel] Columbia Univ, New York, NY USA.
   [Langford, John] Microsoft Res, New York, NY USA.
   [Zhang, Chicheng] Univ Calif San Diego, La Jolla, CA USA.
RP Beygelzimer, A (reprint author), Yahoo Res, New York, NY 10003 USA.
EM beygel@yahoo-inc.com; djhsu@cs.columbia.edu; jcl@microsoft.com;
   chz038@cs.ucsd.edu
CR Attenberg  J., 2010, P 16 ACM SIGKDD INT, P423
   Balcan M., 2012, COLT
   Balcan MF, 2010, MACH LEARN, V80, P111, DOI 10.1007/s10994-010-5174-y
   Balcan Maria-Florina, 2006, ICML
   Beygelzimer Alina, 2010, ADV NEURAL INFORM PR, V23
   Beygelzimer Alina, 2011, ICML WORKSH ONL TRAD
   COHN D, 1994, MACH LEARN, V15, P201, DOI 10.1023/A:1022673506211
   Dasgupta Sanjoy, 2005, ADV NEURAL INFORM PR, V18
   Dasgupta Sanjoy, 2007, ADV NEURAL INFORM PR, V20
   Devroye L., 1996, PROBABILISTIC THEORY
   Hanneke S, 2014, FOUND TRENDS MACH LE, V7, DOI 10.1561/2200000037
   Hanneke S, 2011, ANN STAT, V39, P333, DOI 10.1214/10-AOS843
   Hanneke Steve, 2007, ICML, P249
   Huang Tzu-Kuo, 2008, ADV NEURAL INFORM PR, V28
   Kaariainen M, 2006, LECT NOTES ARTIF INT, V4264, P63
   Simard Patrice Y., 2014, ABS14094814 CORR
   Vapnik V. N., 1982, ESTIMATION DEPENDENC
   VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701040
DA 2019-06-15
ER

PT S
AU Bhaskara, A
   Ghadiri, M
   Mirrokni, V
   Svensson, O
AF Bhaskara, Aditya
   Ghadiri, Mehrdad
   Mirrokni, Vahab
   Svensson, Ola
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Linear Relaxations for Finding Diverse Elements in Metric Spaces
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID APPROXIMATION ALGORITHMS; FEATURE-SELECTION; RECOGNITION; DATABASE
AB Choosing a diverse subset of a large collection of points in a metric space is a fundamental problem, with applications in feature selection, recommender systems, web search, data summarization, etc. Various notions of diversity have been proposed, tailored to different applications. The general algorithmic goal is to find a subset of points that maximize diversity, while obeying a cardinality (or more generally, matroid) constraint. The goal of this paper is to develop a novel linear programming (LP) framework that allows us to design approximation algorithms for such problems. We study an objective known as sum-min diversity, which is known to be effective in many applications, and give the first constant factor approximation algorithm. Our LP framework allows us to easily incorporate additional constraints, as well as secondary objectives. We also prove a hardness result for two natural diversity objectives, under the so-called planted clique assumption. Finally, we study the empirical performance of our algorithm on several standard datasets. We first study the approximation quality of the algorithm by comparing with the LP objective. Then, we compare the quality of the solutions produced by our method with other popular diversity maximization algorithms.
C1 [Bhaskara, Aditya] Univ Utah, Salt Lake City, UT 84112 USA.
   [Ghadiri, Mehrdad] Sharif Univ Technol, Tehran, Iran.
   [Mirrokni, Vahab] Google Res, New York, NY USA.
   [Svensson, Ola] Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
RP Bhaskara, A (reprint author), Univ Utah, Salt Lake City, UT 84112 USA.
EM bhaskara@cs.utah.edu; ghadiri@ce.sharif.edu; mirrokni@google.com;
   ola.svensson@epfl.ch
CR Abbassi Z., 2013, P 19 ACM SIGKDD INT, P32, DOI DOI 10.1145/2487575.2487636
   Bhattacharya S., 2011, P 20 INT C WORLD WID, P317
   Borodin A., 2012, P 31 S PRINC DAT SYS, P155, DOI DOI 10.1145/2213556.2213580
   Cai D, 2011, IEEE T PATTERN ANAL, V33, P1548, DOI 10.1109/TPAMI.2010.231
   Chandra B, 2001, J ALGORITHM, V38, P438, DOI 10.1006/jagm.2000.1145
   Chekuri C, 2010, ANN IEEE SYMP FOUND, P575, DOI 10.1109/FOCS.2010.60
   Duygulu P, 2002, LECT NOTES COMPUT SC, V2353, P97
   FELDMAN V., 2013, P 45 ANN ACM S THEOR, P655
   Gollapudi S., 2009, P 18 INT C WORLD WID, P381, DOI DOI 10.1145/1526709.1526761
   Hassin R, 1997, OPER RES LETT, V21, P133, DOI 10.1016/S0167-6377(97)00034-5
   HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440
   Karp R. M., 1976, ALGORITHMS COMPLEXIT, P1
   Lichman M., 2013, UCI MACHINE LEARNING
   Liu TQ, 2007, NUCLEIC ACIDS RES, V35, pD198, DOI 10.1093/nar/gkl999
   Meinl T, 2011, J CHEM INF MODEL, V51, P237, DOI 10.1021/ci100426r
   Nayar S., 1996, CUCS00696
   Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159
   PLOTKIN SA, 1991, PROCEEDINGS - 32ND ANNUAL SYMPOSIUM ON FOUNDATIONS OF COMPUTER SCIENCE, P495, DOI 10.1109/SFCS.1991.185411
   Qin L, 2012, PROC VLDB ENDOW, V5, P1124, DOI 10.14778/2350229.2350233
   Radlinski F., 2006, SIGIR
   Schrijver A., 2003, COMBINATORIAL OPTIMI
   Vasconcelos N, 2003, PROC CVPR IEEE, P762
   Vieira MR, 2011, PROC INT CONF DATA, P1163, DOI 10.1109/ICDE.2011.5767846
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704025
DA 2019-06-15
ER

PT S
AU Bhojanapalli, S
   Neyshabur, B
   Srebro, N
AF Bhojanapalli, Srinadh
   Neyshabur, Behnam
   Srebro, Nathan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Global Optimality of Local Search for Low Rank Matrix Recovery
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We show that there are no spurious local minima in the non-convex factorized parametrization of low-rank matrix recovery from incoherent linear measurements. With noisy measurements we show all local minima are very close to a global optimum. Together with a curvature bound at saddle points, this yields a polynomial time global convergence guarantee for stochastic gradient descent from random initialization.
C1 [Srebro, Nathan] Toyota Technol Inst, Chicago, IL 60637 USA.
RP Srebro, N (reprint author), Toyota Technol Inst, Chicago, IL 60637 USA.
EM srinadh@ttic.edu; bneyshabur@ttic.edu; nati@ttic.edu
FU NSF RI/AF grant [1302662]
FX Authors would like to thank Afonso Bandeira for discussions, Jason Lee
   and Tengyu Ma for sharing and discussing their work. This research was
   supported in part by an NSF RI/AF grant 1302662.
CR Bandeira AS, 2016, ARXIV160204426
   Bhojanapalli Srinadh, 2015, ARXIV150903917
   Burer S, 2005, MATH PROGRAM, V103, P427, DOI 10.1007/s10107-004-0564-1
   Burer S, 2003, MATH PROGRAM, V95, P329, DOI 10.1007/s10107-002-0352-8
   Candes EJ, 2008, CR MATH, V346, P589, DOI 10.1016/j.crma.2008.03.014
   Candes EJ, 2011, IEEE T INFORM THEORY, V57, P2342, DOI 10.1109/TIT.2011.2111771
   Cartis C, 2012, J COMPLEXITY, V28, P93, DOI 10.1016/j.jco.2011.06.001
   Chen Y., 2015, ARXIV150903025
   DE SA C, 2015, P 32 INT C MACH LEAR, P2332
   Flammia ST, 2012, NEW J PHYS, V14, DOI 10.1088/1367-2630/14/9/095022
   Ge R., 2016, ARXIV160507272
   Ge R., 2015, P 28 C LEARN THEOR, P797
   Gross D, 2010, PHYS REV LETT, V105, DOI 10.1103/PhysRevLett.105.150401
   Hardt M., 2014, ANN C LEARN THEOR, P703
   Jain P., 2013, P 45 ANN ACM S THEOR, P665, DOI DOI 10.1145/2488608.2488693
   Jain P., 2010, ADV NEURAL INFORM PR, V23, P937
   Journee M, 2010, SIAM J OPTIMIZ, V20, P2327, DOI 10.1137/080731359
   Keshavan R. H., 2012, THESIS
   Montanari A, 2016, ARXIV160304064
   Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8
   Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835
   Rennie J. D. M., 2005, P 22 INT C MACH LEAR, P713, DOI [10.1145/1102351.1102441, DOI 10.1145/1102351.1102441]
   Srebro N., 2003, ICML, V20, P720
   Sun J., 2016, ARXIV160206664
   Sun J., 2015, P 32 INT C MACH LEAR, P2351
   Tu S., 2015, ARXIV150703566
   Yu H., 2014, P 31 INT C MACH LEAR, P593
   Zhang D., 2015, ARXIV150607405
   Zhao Tuo, 2015, Adv Neural Inf Process Syst, V28, P559
   Zheng Q., 2015, ADV NEURAL INFORM PR, P109
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702018
DA 2019-06-15
ER

PT S
AU Bilen, H
   Vedaldi, A
AF Bilen, Hakan
   Vedaldi, Andrea
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Integrated Perception with Recurrent Multi-Task Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Modern discriminative predictors have been shown to match natural intelligences in specific perceptual tasks in image classification, object and part detection, boundary extraction, etc. However, a major advantage that natural intelligences still have is that they work well for all perceptual problems together, solving them efficiently and coherently in an integrated manner. In order to capture some of these advantages in machine perception, we ask two questions: whether deep neural networks can learn universal image representations, useful not only for a single task but for all of them, and how the solutions to the different tasks can be integrated in this framework. We answer by proposing a new architecture, which we call multinet, in which not only deep image features are shared between tasks, but where tasks can interact in a recurrent manner by encoding the results of their analysis in a common shared representation of the data. In this manner, we show that the performance of individual tasks in standard benchmarks can be improved first by sharing features between them and then, more significantly, by integrating their solutions in the common representation.
C1 [Bilen, Hakan; Vedaldi, Andrea] Univ Oxford, Visual Geometry Grp, Oxford, England.
RP Bilen, H (reprint author), Univ Oxford, Visual Geometry Grp, Oxford, England.
EM hbilen@robots.ox.ac.uk; vedaldi@robots.ox.ac.uk
FU ERC Starting Grant Integrated and Detailed Image Understanding
   [EP/L024683/1]
FX This work acknowledges the support of the ERC Starting Grant Integrated
   and Detailed Image Understanding (EP/L024683/1).
CR Baxter Jonathan, 2000, J ARTIFICAL INTELLIG, V12, P3
   Belagiannis V., 2016, ARXIV160502914
   Breiman L, 1997, J ROY STAT SOC B MET, V59, P3, DOI 10.1111/1467-9868.00054
   Carreira J, 2016, CVPR
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Chatfield K., 2014, BMVC
   Chen X., 2014, P IEEE C COMP VIS PA, P1971
   Dai J., 2016, CVPR
   Donahue J., 2013, CORR
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Girshick R., 2015, ICCV
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Mikolov T., 2012, THESIS
   Mitchell T. M., 1993, ADV NEURAL INFORM PR, P287
   Najibi M., 2016, CVPR
   Pinheiro P.H., 2013, ARXIV13062795
   Ranzato M., 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383157
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Russakovsky Olga, 2015, IJCV
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Thrun S., 1998, LEARNING LEARN
   van de Sande K.E.A., 2011, ICCV
   Vedaldi A., 2015, P ACM INT C MULT
   Vincent P., 2008, P 25 INT C MACH LEAR, V2008, P1096, DOI DOI 10.1145/1390156.1390294
   Zeiler Matthew D., 2013, CORR
   Zhang TZ, 2013, INT J COMPUT VISION, V101, P367, DOI 10.1007/s11263-012-0582-z
   Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703029
DA 2019-06-15
ER

PT S
AU Blondel, M
   Fujino, A
   Ueda, N
   Ishihata, M
AF Blondel, Mathieu
   Fujino, Akinori
   Ueda, Naonori
   Ishihata, Masakazu
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Higher-Order Factorization Machines
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Factorization machines (FMs) are a supervised learning approach that can use second-order feature combinations even when the data is very high-dimensional. Unfortunately, despite increasing interest in FMs, there exists to date no efficient training algorithm for higher-order FMs (HOFMs). In this paper, we present the first generic yet efficient algorithms for training arbitrary-order HOFMs. We also present new variants of HOFMs with shared parameters, which greatly reduce model size and prediction times while maintaining similar accuracy. We demonstrate the proposed approaches on four different link prediction tasks.
C1 [Blondel, Mathieu; Fujino, Akinori; Ueda, Naonori] NTT Commun Sci Labs, Kyoto, Japan.
   [Ishihata, Masakazu] Hokkaido Univ, Sapporo, Hokkaido, Japan.
RP Blondel, M (reprint author), NTT Commun Sci Labs, Kyoto, Japan.
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   BAYDIN AG, 2015, ARXIV150205767
   Blondel M., 2016, P INT C MACH LEARN I
   Blondel M., 2015, P EUR C MACH LEARN P
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Li M., 2016, P INT C WEB SEARCH D
   Livni R., 2014, ADV NEURAL INFORM PR, V27, P855
   Menon AK, 2011, LECT NOTES ARTIF INT, V6912, P437, DOI 10.1007/978-3-642-23783-6_28
   Natarajan N, 2014, BIOINFORMATICS, V30, P60, DOI 10.1093/bioinformatics/btu269
   Pan V. Y, 2001, STRUCTURED MATRICES
   Rakotomamonjy A, 2008, J MACH LEARN RES, V9, P2491
   Rendle Steffen, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P995, DOI 10.1109/ICDM.2010.127
   Rendle S., 2011, P 34 INT ACM SIGIR C, P635, DOI DOI 10.1145/2009916.2010002
   Rendle S., 2009, P 25 C UNC ART INT, P452, DOI DOI 10.1145/1772690.1772773
   Rendle S, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2168752.2168771
   Shawe-Taylor John, 2004, KERNEL METHODS PATTE
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Wahba G, 1990, SPLINE MODELS OBSERV, V59
   Yamanishi Y, 2005, BIOINFORMATICS, V21, pI468, DOI 10.1093/bioinformatics/bti1012
   Yang J., 2015, ARXIV150401697
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701001
DA 2019-06-15
ER

PT S
AU Bluche, T
AF Bluche, Theodore
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Joint Line Segmentation and Transcription for End-to-End Handwritten
   Paragraph Recognition
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Offline handwriting recognition systems require cropped text line images for both training and recognition. On the one hand, the annotation of position and transcript at line level is costly to obtain. On the other hand, automatic line segmentation algorithms are prone to errors, compromising the subsequent recognition. In this paper, we propose a modification of the popular and efficient Multi-Dimensional Long Short-Term Memory Recurrent Neural Networks (MDLSTM-RNNs) to enable end-to-end processing of handwritten paragraphs. More particularly, we replace the collapse layer transforming the two-dimensional representation into a sequence of predictions by a recurrent version which can select one line at a time. In the proposed model, a neural network performs a kind of implicit line segmentation by computing attention weights on the image representation. The experiments on paragraphs of Rimes and IAM databases yield results that are competitive with those of networks trained at line level, and constitute a significant step towards end-to-end transcription of full documents.
C1 [Bluche, Theodore] A2iA SAS, 39 Rue Bienfaisance, F-75008 Paris, France.
RP Bluche, T (reprint author), A2iA SAS, 39 Rue Bienfaisance, F-75008 Paris, France.
EM tb@a2ia.com
CR Augustin  E., 2006, P WORKSH FRONT HANDW
   Ba J., 2014, MULTIPLE OBJECT RECO, V1412, P7755
   Bahdanau D., 2014, ARXIV14090473
   BENGIO Y, 1995, NEURAL COMPUT, V7, P1289, DOI 10.1162/neco.1995.7.6.1289
   Bluche T, 2014, INT C FRONT HANDWR R
   Bluche T., 2016, ARXIV160403286
   Bluche T., 2015, THESIS
   Bosch V, 2012, INT CONF FRONT HAND, P201, DOI 10.1109/ICFHR.2012.274
   Brunessaux S, 2014, 2014 11TH IAPR INTERNATIONAL WORKSHOP ON DOCUMENT ANALYSIS SYSTEMS (DAS 2014), P349, DOI 10.1109/DAS.2014.58
   Chan W., 2015, ARXIV150801211
   Chorowski JK, 2015, ADV NEURAL INFORM PR, P577
   Delakis M, 2008, VISAPP 2008: PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 2, P290
   Doetsch Patrick, 2014, FAST ROBUST TRAINING
   FUKUSHIMA K, 1987, APPL OPTICS, V26, P4985, DOI 10.1364/AO.26.004985
   Gatos B, 2014, 2014 11TH IAPR INTERNATIONAL WORKSHOP ON DOCUMENT ANALYSIS SYSTEMS (DAS 2014), P237, DOI 10.1109/DAS.2014.23
   Graves A, 2013, ARXIV13080850
   Graves A., 2008, NIPS, P545
   Graves A., 2006, P 23 INT C MACH LEAR, P369, DOI DOI 10.1145/1143844.1143891
   Gregor K., 2015, ARXIV150204623
   Hebert D, 2011, PROC INT CONF DOC, P493, DOI 10.1109/ICDAR.2011.105
   Jaderberg M, 2015, ADV NEURAL INFORM PR, P2008
   Jung K, 2001, PATTERN RECOGN LETT, V22, P1503, DOI 10.1016/S0167-8655(01)00096-4
   Kaltenmeier A., 1993, Proceedings of the Second International Conference on Document Analysis and Recognition (Cat. No.93TH0578-5), P139, DOI 10.1109/ICDAR.1993.395764
   Karpathy A., 2015, ARXIV151107571
   Kozielski M, 2013, PROC INT CONF DOC, P935, DOI 10.1109/ICDAR.2013.190
   Lee  C.-Y., 2016, ARXIV160303101
   Likforman-Sulem L, 2007, INT J DOC ANAL RECOG, V9, P123, DOI [10.1007/s10032-006-0023-z, 10.1007/s10032-007-0023-z]
   Marti U.-V., 2002, International Journal on Document Analysis and Recognition, V5, P39, DOI 10.1007/s100320200071
   Messina R., 2014, 11 IAPR WORKSH DOC A, P212
   Moysset Bastien, 2015, INT C DOC AN REC ICD
   Moysset Bastien, 2015, INT WORKSH HIST DOC
   Sanchez J. A., 2014, INT C FRONT HANDWR R
   Sermanet P., 2013, COMPUT VIS PATTERN R, V1312, P6229, DOI DOI 10.1109/CVPR.2015.7299176.ARXIV:1312.6229
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P5
   Tong  A., 2014, 11 IAPR WORKSH DOC A
   Vinciarelli A, 2004, IEEE T PATTERN ANAL, V26, P709, DOI 10.1109/TPAMI.2004.14
   Pham V, 2014, INT CONF FRONT HAND, P285, DOI 10.1109/ICFHR.2014.55
   Xu K, 2015, ARXIV150203044
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702004
DA 2019-06-15
ER

PT S
AU Bogolubsky, L
   Gusev, G
   Raigorodskii, A
   Tikhonov, A
   Zhukovskii, M
   Dvurechensky, P
   Gasnikov, A
   Nesterov, Y
AF Bogolubsky, Lev
   Gusev, Gleb
   Raigorodskii, Andrei
   Tikhonov, Aleksey
   Zhukovskii, Maksim
   Dvurechensky, Pavel
   Gasnikov, Alexander
   Nesterov, Yurii
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning Supervised PageRank with Gradient-Based and Gradient-Free
   Optimization Methods
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID DERIVATIVES
AB In this paper, we consider a non-convex loss-minimization problem of learning Supervised PageRank models, which can account for features of nodes and edges. We propose gradient-based and random gradient-free methods to solve this problem. Our algorithms are based on the concept of an inexact oracle and unlike the state-of-the-art gradient-based method we manage to provide theoretically the convergence rate guarantees for both of them. Finally, we compare the performance of the proposed optimization methods with the state of the art applied to a ranking task.
C1 [Bogolubsky, Lev; Gusev, Gleb; Raigorodskii, Andrei; Tikhonov, Aleksey; Zhukovskii, Maksim] Yandex, Moscow, Russia.
   [Bogolubsky, Lev; Raigorodskii, Andrei] Moscow MV Lomonosov State Univ, Moscow, Russia.
   [Raigorodskii, Andrei] Buryat State Univ, Ulan Ude, Russia.
   [Dvurechensky, Pavel] Weierstrass Inst, Berlin, Germany.
   [Dvurechensky, Pavel; Gasnikov, Alexander] Inst Informat Transmiss Problems RAS, Moscow, Russia.
   [Gusev, Gleb; Raigorodskii, Andrei; Zhukovskii, Maksim; Gasnikov, Alexander] Moscow Inst Phys & Technol, Moscow, Russia.
   [Nesterov, Yurii] Ctr Operat Res & Econometr, Louvain La Neuve, Belgium.
   [Nesterov, Yurii] Higher Sch Econ, Moscow, Russia.
RP Bogolubsky, L (reprint author), Yandex, Moscow, Russia.; Bogolubsky, L (reprint author), Moscow MV Lomonosov State Univ, Moscow, Russia.
EM bogolubsky@yandex-team.ru; gleb57@yandex-team.ru;
   raigorodsky@yandex-team.ru; altsoph@yandex-team.ru;
   zhukmax@yandex-team.ru; pavel.dvurechensky@wias-berlin.de;
   gasnikov@yandex.ru; yurii.nesterov@uclouvain.be
RI Dvurechensky, Pavel E./P-7295-2015
OI Dvurechensky, Pavel E./0000-0003-1201-2343
FU Russian Science Foundation [14-50-00150]; RFBR
FX The research by P. Dvurechensky and A. Gasnikov presented in Section 4
   of this paper was conducted in IITP RAS and supported by the Russian
   Science Foundation grant (project 14-50-00150), the research presented
   in Section 5 was supported by RFBR.
CR Agarwal A., 2010, 23 ANN C LEARN THEOR
   ANDREW AL, 1979, J I MATH APPL, V24, P209
   ANDREW AL, 1978, J COMPUT PHYS, V26, P107, DOI 10.1016/0021-9991(78)90102-X
   Backstrom L., 2011, SUPERVISED RANDOM WA
   Dai Na, 2010, FLOWERS FOOD WEB AUT
   Devolder O, 2014, MATH PROGRAM, V146, P37, DOI 10.1007/s10107-013-0677-5
   Eiron N., 2004, RANKING WEB FRONTIER
   Gao B., 2011, SEMISUPERVISED RANKI
   Gasnikov A., 2015, COMP MATH MATH PHYS, V55, P1
   Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811
   Haveliwala T., 1999, EFFICIENT COMPUTATIO
   Haveliwala T. H., 2002, TOPIC SENSITIVE PAGE
   Jeh G., 2003, SCALING PERSONALIZED
   Kleinberg J. M., 1998, AUTHORITATIVE SOURCE
   Liu Y., 2008, BROWSERANK LETTING W
   MATYAS J, 1965, AUTOMAT REM CONTR+, V26, P244
   Nesterov Y., 2004, INTRO LECT CONVEX OP
   Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5
   Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001
   Nesterov Yu., 2015, RANDOM GRADIENT FREE, P1
   Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8
   Nesterov Y, 2015, APPL MATH COMPUT, V255, P58, DOI 10.1016/j.amc.2014.04.053
   Page L, 1999, PAGERANK CITATION RA
   Richardson M., 2002, INTELLIGENT SURFER P
   Zhukovskii M., 2013, URL REDIRECTION ACCO
   Zhukovskii M., 2014, SUPERVISED NESTED PA
   Zhukovskii M., 2013, FRESH BROWSERANK
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704089
DA 2019-06-15
ER

PT S
AU Bogunovic, I
   Scarlett, J
   Krause, A
   Cevher, V
AF Bogunovic, Ilija
   Scarlett, Jonathan
   Krause, Andreas
   Cevher, Volkan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Truncated Variance Reduction: A Unified Approach to Bayesian
   Optimization and Level-Set Estimation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We present a new algorithm, truncated variance reduction (TRUVAR), that treats Bayesian optimization (BO) and level-set estimation (LSE) with Gaussian processes in a unified fashion. The algorithm greedily shrinks a sum of truncated variances within a set of potential maximizers (BO) or unclassified points (LSE), which is updated based on confidence bounds. TRUVAR is effective in several important settings that are typically non-trivial to incorporate into myopic algorithms, including pointwise costs and heteroscedastic noise. We provide a general theoretical guarantee for TRUVAR covering these aspects, and use it to recover and strengthen existing results on BO and LSE. Moreover, we provide a new result for a setting where one can select from a number of noise levels having associated costs. We demonstrate the effectiveness of the algorithm on both synthetic and real-world data sets.
C1 [Bogunovic, Ilija; Scarlett, Jonathan; Cevher, Volkan] Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst LIONS, Lausanne, Switzerland.
   [Krause, Andreas] Swiss Fed Inst Technol, Learning & Adapt Syst Grp, Zurich, Switzerland.
RP Bogunovic, I (reprint author), Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst LIONS, Lausanne, Switzerland.
EM ilija.bogunovic@epfl.ch; jonathan.scarlett@epfl.ch; krausea@ethz.ch;
   volkan.cevher@epfl.ch
FU European Commission under Grant ERC Future Proof; SNF Sinergia project
   [CRSII2-147633]; SNF [200021-146750]; EPFL Fellows Horizon2020 grant
   [665667]
FX This work was supported in part by the European Commission under Grant
   ERC Future Proof, SNF Sinergia project CRSII2-147633, SNF 200021-146750,
   and EPFL Fellows Horizon2020 grant 665667.
CR Bryan B., 2008, INT C MACH LEARN ICM
   Bubeck S., 2012, FDN TREND MACH LEARN
   Contal Emile, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8188, P225, DOI 10.1007/978-3-642-40988-2_15
   Das A, 2008, ACM S THEORY COMPUT, P45
   Goldberg P. W., 1997, P ADV NEUR INF PROC, V10, P493
   Gotovos A., 2013, INT JOINT C ART INT
   Hennig P, 2012, J MACH LEARN RES, V13, P1809
   Hernandez-Lobato J M, 2014, ADV NEURAL INFORM PR, P918
   Hitz G, 2012, IEEE ROBOT AUTOM MAG, V19, P62, DOI 10.1109/MRA.2011.2181771
   Jamieson K, 2014, INF SCI SYST CISS 20, P1
   JONES DR, 1993, J OPTIMIZ THEORY APP, V79, P157, DOI 10.1007/BF00941892
   Kleinberg R., 2008, P ACM S THEOR COMP
   Krause A., 2005, TECHNICAL REPORT
   Krause A., 2012, TRACTABILITY PRACTIC, V3
   Madani O, 2004, LECT NOTES COMPUT SC, V3120, P643, DOI 10.1007/978-3-540-27819-1_46
   Metzen J. H., 2016, INT C MACH LEARN ICM
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218
   Snoek J, 2012, ADV NEUR INF PROC SY
   Srinivas N, 2012, IEEE T INFORM THEORY, V58, P3250, DOI 10.1109/TIT.2011.2182033
   Swersky K., 2013, ADV NEURAL INFORM PR, P2004
   Swersky Kevin, 2014, FREEZE THAW BAYESIAN
   Wang Z., BAYESIAN MULTISCALE
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700044
DA 2019-06-15
ER

PT S
AU Boscaini, D
   Masci, J
   Rodola, E
   Bronstein, M
AF Boscaini, Davide
   Masci, Jonathan
   Rodola, Emanuele
   Bronstein, Michael
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning shape correspondence with anisotropic convolutional neural
   networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Convolutional neural networks have achieved extraordinary results in many computer vision and pattern recognition applications; however, their adoption in the computer graphics and geometry processing communities is limited due to the non-Euclidean structure of their data. In this paper, we propose Anisotropic Convolutional Neural Network (ACNN), a generalization of classical CNNs to non-Euclidean domains, where classical convolutions are replaced by projections over a set of oriented anisotropic diffusion kernels. We use ACNNs to effectively learn intrinsic dense correspondences between deformable shapes, a fundamental problem in geometry processing, arising in a wide variety of applications. We tested ACNNs performance in challenging settings, achieving state-of-the-art results on recent correspondence benchmarks.
C1 [Boscaini, Davide; Masci, Jonathan; Rodola, Emanuele; Bronstein, Michael] USI Lugano, Lugano, Switzerland.
   [Bronstein, Michael] Tel Aviv Univ, Tel Aviv, Israel.
   [Bronstein, Michael] Intel, Haifa, Israel.
RP Boscaini, D (reprint author), USI Lugano, Lugano, Switzerland.
EM davide.boscaini@usi.ch; jonathan.masci@usi.ch; emanuele.rodola@usi.ch;
   michael.Bronstein@usi.ch
FU ERC [307047]; Google; Nvidia
FX The authors wish to thank Matteo Sala for the textured models. This
   research was supported by the ERC Starting Grant No. 307047 (COMET), a
   Google Faculty Research Award, and Nvidia equipment grant.
CR Andreux M., 2014, P NORDIA
   Bergstra J., 2010, P SCIPY JUN
   Bogo F., 2014, P CVPR
   Boscaini D, 2015, COMPUT GRAPH FORUM, V34, P13, DOI 10.1111/cgf.12693
   Boscaini D., 2016, COMPUTER GRAPHICS FO, V35
   Bruna J., 2014, P ICLR
   Cosmo L., 2016, P 3DOR
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Kaick O. V., 2010, COMPUT GRAPH FORUM, V30, P1
   Kim VG, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964974
   Kingma D. P., 2015, ICLR
   Kokkinos I., 2012, P CVPR
   Krizhevsky A., 2012, P NIPS
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Litman R., 2014, TPAMI, V36, P170
   Masci  J., 2015, P 3DRR
   Memoli F, 2011, FOUND COMPUT MATH, V11, P417, DOI 10.1007/s10208-011-9093-5
   Ovsjanikov M., 2012, TOG, V31, P1
   Rodola E., 2016, COMPUTER GRAPHICS FO
   Rodola E., 2014, P CVPR
   Salti S, 2014, COMPUT VIS IMAGE UND, V125, P251, DOI 10.1016/j.cviu.2014.04.011
   Shuman D. I, 2013, ARXIV13075708
   Solomon J, 2012, COMPUT GRAPH FORUM, V31, P1617, DOI 10.1111/j.1467-8659.2012.03167.x
   Su H., 2015, P ICCV
   Wei L., 2016, P CVPR
   Windheuser T., 2014, P BMVC
   Wu Z., 2015, P CVPR
NR 28
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700010
DA 2019-06-15
ER

PT S
AU Bouchacourt, D
   Kumar, MP
   Nowozin, S
AF Bouchacourt, Diane
   Kumar, M. Pawan
   Nowozin, Sebastian
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI DISCO Nets: DISsimilarity COefficient Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We present a new type of probabilistic model which we call DISsimilarity COefficient Networks (DISCO Nets). DISCO Nets allow us to efficiently sample from a posterior distribution parametrised by a neural network. During training, DISCO Nets are learned by minimising the dissimilarity coefficient between the true distribution and the estimated distribution. This allows us to tailor the training to the loss related to the task at hand. We empirically show that (i) by modeling uncertainty on the output value, DISCO Nets outperform equivalent non-probabilistic predictive networks and (ii) DISCO Nets accurately model the uncertainty of the output, outperforming existing probabilistic models based on deep neural networks.
C1 [Bouchacourt, Diane; Kumar, M. Pawan] Univ Oxford, Oxford, England.
   [Nowozin, Sebastian] Microsoft Res Cambridge, Cambridge, England.
RP Bouchacourt, D (reprint author), Univ Oxford, Oxford, England.
EM diane@robots.ox.ac.uk; pawan@robots.ox.ac.uk;
   sebastian.nowozin@microsoft.com
FU Microsoft Research PhD Scholarship Programme
FX This work is funded by the Microsoft Research PhD Scholarship Programme.
   We would like to thank Pankaj Pansari, Leonard Berrada and Ondra Miksik
   for their useful discussions and insights.
CR Denton E. L., 2015, NIPS
   Dziugaite G. K., 2015, UAI
   Fukumizu K., 2013, JMLR
   Gauthier J., 2014, CONDITIONAL GENERATI
   Gneiting T., 2007, J AM STAT ASS
   Gneiting Tilmann, 2008, TEST
   Goodfellow I., 2014, NIPS
   Gretton A., 2012, JMLR
   Gretton A, 2007, NIPS
   Kingma Diederik P, 2014, ICLR
   Kumar M. P., 2012, ICML
   Lacoste-Julien S., 2011, AISTATS
   Li Y., 2015, ICML
   Makhzani A., 2015, ICLR WORKSH
   Mirza M., 2014, NIPS DEEP LEARN WORK
   Oberweger M., 2015, COMP VIS WINT WORKSH
   Oberweger M., 2015, ICCV
   Pinson Pierre, 2013, DISCRIMINATION ABILI
   Polyak B. T., 1964, SOME METHODS SPEEDIN
   Premachandran V., 2014, CVPR
   Radford A., 2015, ICLR
   RAO CR, 1982, THEOR POPUL BIOL, V21, P24, DOI 10.1016/0040-5809(82)90004-1
   Reed S., 2016, ICML
   Springenberg J. T., 2016, ICLR
   Taylor J., 2012, CVPR
   Tompson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629500
   Yan X., 2016, ATTRIBUTE2IMAGE COND
   Zawadzki E., 2015, AAAI C ART INT
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700107
DA 2019-06-15
ER

PT S
AU Boumal, N
   Voroninski, V
   Bandeira, AS
AF Boumal, Nicolas
   Voroninski, Vladislav
   Bandeira, Afonso S.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI The non-convex Burer-Monteiro approach works on smooth semidefinite
   programs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID RANK; OPTIMIZATION; RECOVERY; CUT
AB Semidefinite programs (SDPs) can be solved in polynomial time by interior point methods, but scalability can be an issue. To address this shortcoming, over a decade ago, Burer and Monteiro proposed to solve SDPs with few equality constraints via rank-restricted, non-convex surrogates. Remarkably, for some applications, local optimization methods seem to converge to global optima of these non-convex surrogates reliably. Although some theory supports this empirical success, a complete explanation of it remains an open question. In this paper, we consider a class of SDPs which includes applications such as max-cut, community detection in the stochastic block model, robust PCA, phase retrieval and synchronization of rotations. We show that the low-rank Burer-Monteiro formulation of SDPs in that class almost never has any spurious local optima.
C1 [Boumal, Nicolas] Princeton Univ, Dept Math, Princeton, NJ 08544 USA.
   [Voroninski, Vladislav] MIT, Dept Math, Cambridge, MA 02139 USA.
   [Bandeira, Afonso S.] NYU, Dept Math, New York, NY 10003 USA.
   [Bandeira, Afonso S.] NYU, Courant Inst Math Sci, Ctr Data Sci, New York, NY 10003 USA.
RP Boumal, N (reprint author), Princeton Univ, Dept Math, Princeton, NJ 08544 USA.
EM nboumal@math.princeton.edu; vvlad@math.mit.edu; bandeira@cims.nyu.edu
FU Office of Naval Research; NSF [DMS-1317308]
FX VV was partially supported by the Office of Naval Research. ASB was
   supported by NSF Grant DMS-1317308. Part of this work was done while ASB
   was with the Department of Mathematics at the Massachusetts Institute of
   Technology. We thank Wotao Yin and Michel Goemans for helpful
   discussions.
CR Abbe E, 2016, IEEE T INFORM THEORY, V62, P471, DOI 10.1109/TIT.2015.2490670
   Absil PA, 2007, FOUND COMPUT MATH, V7, P303, DOI 10.1007/s10208-005-0179-9
   Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1
   Alon N, 2006, SIAM J COMPUT, V35, P787, DOI 10.1137/S0097539704441629
   Andreani R, 2010, J OPTIMIZ THEORY APP, V146, P255, DOI 10.1007/s10957-010-9671-8
   BANDEIRA A. S., 2016, P 29 C LEARN THEOR C
   Bandeira AS, 2017, MATH PROGRAM, V163, P145, DOI 10.1007/s10107-016-1059-6
   Bandeira AS, 2016, MATH PROGRAM, V160, P433, DOI 10.1007/s10107-016-0993-7
   BARVINOK AI, 1995, DISCRETE COMPUT GEOM, V13, P189, DOI 10.1007/BF02574037
   BHOJANAPALLI S, 2016, ADV NEURAL INFORM PR, P3873
   Bhojanapalli S., 2016, C LEARN THEOR COLT
   Boumal N., 2015, ARXIV150600575
   Boumal  N., 2016, ARXIV160508101
   Boumal N, 2014, J MACH LEARN RES, V15, P1455
   Burer S, 2005, MATH PROGRAM, V103, P427, DOI 10.1007/s10107-004-0564-1
   Burer S, 2003, MATH PROGRAM, V95, P329, DOI 10.1007/s10107-002-0352-8
   CVX, 2012, CVX MATLAB SOFTWARE
   GE R, 2016, ADV NEURAL INFORM PR, P2973
   Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684
   Helmberg C, 1996, SIAM J OPTIMIZ, V6, P342, DOI 10.1137/0806020
   Javanmard A, 2016, P NATL ACAD SCI USA, V113, pE2218, DOI 10.1073/pnas.1523097113
   Journee M, 2010, SIAM J OPTIMIZ, V20, P2327, DOI 10.1137/080731359
   Laurent M, 1996, SIAM J MATRIX ANAL A, V17, P530, DOI 10.1137/0617031
   LEE J. M., 2013, GRADUATE TEXTS MATH, V218
   McCoy M, 2011, ELECTRON J STAT, V5, P1123, DOI 10.1214/11-EJS636
   Nesterov Yurii, 2004, APPL OPTIMIZATION, V87, pxviii
   Pataki G, 1998, MATH OPER RES, V23, P339, DOI 10.1287/moor.23.2.339
   Rockafellar R T, 1970, CONVEX ANAL
   Ruszczynski A, 2006, NONLINEAR OPTIMIZATI
   Shah S., 2016, ARXIV160509527
   Singer A, 2011, APPL COMPUT HARMON A, V30, P20, DOI 10.1016/j.acha.2010.02.001
   Toh KC, 1999, OPTIM METHOD SOFTW, V11-2, P545, DOI 10.1080/10556789908805762
   VAVASIS S., 1991, NONLINEAR OPTIMIZATI
   Waldspurger I, 2015, MATH PROGRAM, V149, P47, DOI 10.1007/s10107-013-0738-9
   Wang LH, 2013, INF INFERENCE, V2, P145, DOI 10.1093/imaiai/iat005
   Wen ZW, 2013, MATH PROGRAM, V142, P397, DOI 10.1007/s10107-012-0584-1
   Yang WH, 2014, PAC J OPTIM, V10, P415
NR 37
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704042
DA 2019-06-15
ER

PT S
AU Bousmalis, K
   Trigeorgis, G
   Silberman, N
   Krishnan, D
   Erhan, D
AF Bousmalis, Konstantinos
   Trigeorgis, George
   Silberman, Nathan
   Krishnan, Dilip
   Erhan, Dumitru
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Domain Separation Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID KERNEL
AB The cost of large scale data collection and annotation often makes the application of machine learning algorithms to new tasks or datasets prohibitively expensive. One approach circumventing this cost is training models on synthetic data where annotations are provided automatically. Despite their appeal, such models often fail to generalize from synthetic to real images, necessitating domain adaptation algorithms to manipulate these models before they can be successfully applied. Existing approaches focus either on mapping representations from one domain to the other, or on learning to extract features that are invariant to the domain from which they were extracted. However, by focusing only on creating a mapping or shared representation between the two domains, they ignore the individual characteristics of each domain. We hypothesize that explicitly modeling what is unique to each domain can improve a model's ability to extract domain-invariant features. Inspired by work on private-shared component analysis, we explicitly learn to extract image representations that are partitioned into two subspaces: one component which is private to each domain and one which is shared across domains. Our model is trained to not only perform the task we care about in the source domain, but also to use the partitioned representation to reconstruct the images from both domains. Our novel architecture results in a model that outperforms the state-of-the-art on a range of unsupervised domain adaptation scenarios and additionally produces visualizations of the private and shared representations enabling interpretation of the domain adaptation process.
C1 [Bousmalis, Konstantinos; Trigeorgis, George; Erhan, Dumitru] Google Brain, Mountain View, CA 94040 USA.
   [Trigeorgis, George] Imperial Coll London, London, England.
   [Silberman, Nathan] Google Res, New York, NY USA.
   [Krishnan, Dilip] Google Res, Cambridge, MA USA.
RP Bousmalis, K (reprint author), Google Brain, Mountain View, CA 94040 USA.
EM konstantinos@google.com; g.trigeorgis@imperial.ac.uk;
   nsilberman@google.com; dilipkay@google.com; dumitru@google.com
CR Abadi M., 2016, ARXIV160304467
   Ajakan H., 2014, PREPRINT
   Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Caseiro R., 2015, CVPR
   Darrell T., 2014, ARXIV14123474
   Eigen D., 2014, ADV NEURAL INFORM PR, P2366
   Ganin Y., 2015, ICML, P513
   Ganin Y, 2016, J MACH LEARN RES, V17
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Gopalan R., 2011, ICCV
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Hinterstoisser S., 2012, ACCV
   Huynh DQ, 2009, J MATH IMAGING VIS, V35, P155, DOI 10.1007/s10851-009-0161-2
   Jia Y., 2010, ADV NEURAL INFORM PR, V23, P982
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Long M., 2015, ICML
   Mansour Y., 2009, NIPS
   Moiseev B, 2013, LECT NOTES COMPUT SC, V8192, P576, DOI 10.1007/978-3-319-02895-8_52
   Netzer Y., 2011, NIPS WORKSH
   Olson Edwin, 2011, 2011 IEEE International Conference on Robotics and Automation, P3400
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saenko K., 2010, ECCV
   Salzmann M., 2010, J MACHINE LEARNING R, P701
   Stallkamp J., 2012, NEURAL NETWORKS
   Sun B., 2016, AAAI
   Sutskever I., 2013, P 30 INT C MACH LEAR, V28, P1139
   Tsung-Yi Lin, 2014, Computer Vision - ECCV 2014. 13th European Conference. Proceedings: LNCS 8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463
   Virtanen S, 2011, P 28 INT C MACH LEAR, P457
   Wohlhart P, 2015, PROC CVPR IEEE, P3109, DOI 10.1109/CVPR.2015.7298930
NR 31
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702001
DA 2019-06-15
ER

PT S
AU Bresson, X
   Laurent, T
   Szlam, A
   von Brecht, JH
AF Bresson, Xavier
   Laurent, Thomas
   Szlam, Arthur
   von Brecht, James H.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI The Product Cut
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We introduce a theoretical and algorithmic framework for multi-way graph partitioning that relies on a multiplicative cut-based objective. We refer to this objective as the Product Cut. We provide a detailed investigation of the mathematical properties of this objective and an effective algorithm for its optimization. The proposed model has strong mathematical underpinnings, and the corresponding algorithm achieves state-of-the-art performance on benchmark data sets.
C1 [Bresson, Xavier] Nanyang Technol Univ, Singapore, Singapore.
   [Laurent, Thomas] Loyola Marymount Univ, Los Angeles, CA 90045 USA.
   [Szlam, Arthur] Facebook AI Res, New York, NY USA.
   [von Brecht, James H.] Calif State Univ Long Beach, Long Beach, CA 90840 USA.
RP Bresson, X (reprint author), Nanyang Technol Univ, Singapore, Singapore.
EM xavier.bresson@ntu.edu.sg; tlaurent@lmu.edu; aszlam@fb.com;
   james.vonbrecht@csulb.edu
FU NSF [DMS-1414396]
FX TL was supported by NSF DMS-1414396.
CR Andersen R, 2006, ANN IEEE SYMP FOUND, P475
   Arora R., 2011, ICML, V28, P761
   Bresson Xavier, 2013, ADV NEURAL INFORM PR
   Dhillon IS, 2007, IEEE T PATTERN ANAL, V29, P1944, DOI 10.1109/TP'AMI.2007.1115
   Karypis G, 1998, SIAM J SCI COMPUT, V20, P359, DOI 10.1137/S1064827595287997
   Krishnan D, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461992
   Livne OE, 2012, SIAM J SCI COMPUT, V34, pB499, DOI 10.1137/110843563
   LOVASZ L, 1993, RANDOM STRUCT ALGOR, V4, P359, DOI 10.1002/rsa.3240040402
   Rangapuram Syama Sundar, 2014, ADV NEURAL INFORM PR, V27, P3131
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Spielman D.A., 2004, P 36 ANN ACM S THEOR, P81, DOI DOI 10.1145/1007352.1007372
   Spielman DA, 2013, SIAM J COMPUT, V42, P1, DOI 10.1137/080744888
   Stella X. Yu, 2003, INT C COMP VIS
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Yang Z., 2012, P ADV NEUR INF PROC, V25, P1088
NR 15
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701083
DA 2019-06-15
ER

PT S
AU Bullins, B
   Hazan, E
   Koren, T
AF Bullins, Brian
   Hazan, Elad
   Koren, Tomer
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI The Limits of Learning with Missing Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We study linear regression and classification in a setting where the learning algorithm is allowed to access only a limited number of attributes per example, known as the limited attribute observation model. In this well-studied model, we provide the first lower bounds giving a limit on the precision attainable by any algorithm for several variants of regression, notably linear regression with the absolute loss and the squared loss, as well as for classification with the hinge loss. We complement these lower bounds with a general purpose algorithm that gives an upper bound on the achievable precision limit in the setting of learning with missing data.
C1 [Bullins, Brian; Hazan, Elad] Princeton Univ, Princeton, NJ 08544 USA.
   [Koren, Tomer] Google Brain, Mountain View, CA USA.
RP Bullins, B (reprint author), Princeton Univ, Princeton, NJ 08544 USA.
EM bbullins@cs.princeton.edu; ehazan@cs.princeton.edu; tkoren@google.com
CR Ben-David S, 1998, J COMPUT SYST SCI, V56, P277, DOI 10.1006/jcss.1998.1569
   Cesa-Bianchi N., 2010, P 27 INT C MACH LEAR
   Cesa-Bianchi N, 2011, IEEE T INFORM THEORY, V57, P7907, DOI 10.1109/TIT.2011.2164053
   Dekel O, 2010, MACH LEARN, V81, P149, DOI 10.1007/s10994-009-5124-8
   HAUSSLER D, 1992, INFORM COMPUT, V100, P78, DOI 10.1016/0890-5401(92)90010-D
   Hazan E., 2015, P 32 INT C MACH LEAR
   Hazan  Elad, 2012, P 29 INT C MACH LEAR
   Kukliansky D., 2015, P 32 INT C MACH LEAR
   Little R. A., 2002, STAT ANAL MISSING DA
   Loh P.-L., 2011, ADV NEURAL INFORM PR
   Rostamizadeh A., 2011, 27 C UNC ART INT
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Zinkevich M., 2003, P 20 INT C MACH LEAR
NR 13
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701028
DA 2019-06-15
ER

PT S
AU Bunel, R
   Desmaison, A
   Kohli, P
   Torr, PHS
   Kumar, MP
AF Bunel, Rudy
   Desmaison, Alban
   Kohli, Pushmeet
   Torr, Philip H. S.
   Kumar, M. Pawan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Adaptive Neural Compilation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB This paper proposes an adaptive neural-compilation framework to address the problem of learning efficient programs. Traditional code optimisation strategies used in compilers are based on applying pre-specified set of transformations that make the code faster to execute without changing its semantics. In contrast, our work involves adapting programs to make them more efficient while considering correctness only on a target input distribution. Our approach is inspired by the recent works on differentiable representations of programs. We show that it is possible to compile programs written in a low-level language to a differentiable representation. We also show how programs in this representation can be optimised to make them efficient on a target input distribution. Experimental results demonstrate that our approach enables learning specifically-tuned algorithms for given data distributions with a high success rate.
C1 [Bunel, Rudy; Desmaison, Alban; Torr, Philip H. S.; Kumar, M. Pawan] Univ Oxford, Oxford, England.
   [Kohli, Pushmeet] Microsoft Res, Bengaluru, India.
RP Bunel, R (reprint author), Univ Oxford, Oxford, England.
EM rudy@robots.ox.ac.uk; alban@robots.ox.ac.uk; pkohli@microsoft.com;
   philip.torr@eng.ox.ac.uk; pawan@robots.ox.ac.uk
FU EPSRC; Leverhulme Trust; Clarendon Fund; ERC [ERC-2012-AdG
   321162-HELIOS]; EPSRC/MURI grant [EP/N019474/1]; EPSRC grant
   [EP/M013774/1]; EPSRC Programme Grant [Seebibyte EP/M013774/1];
   Microsoft Research PhD Scolarship Program
FX We would like to thank Siddharth Narayanaswamy and Diane Bouchacourt for
   helpful discussions and proofreading the paper. This work was supported
   by the EPSRC, Leverhulme Trust, Clarendon Fund and the ERC grant
   ERC-2012-AdG 321162-HELIOS, EPSRC/MURI grant ref EP/N019474/1, EPSRC
   grant EP/M013774/1, EPSRC Programme Grant Seebibyte EP/M013774/1 and
   Microsoft Research PhD Scolarship Program.
CR Andrychowicz Marcin, 2016, CORR
   Graves A., 2014, CORR
   Grefenstette  Edward, 2015, NIPS
   Gruau Frederic, 1995, THEORETICAL COMPUTER
   Joulin  Armand, 2015, NIPS
   Kaiser Lukasz, 2016, ICLR
   Kingma D. P., 2015, ICLR
   Kurach  Karol, 2016, ICLR
   Massalin H., 1987, Proceedings of the Second International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS II) (Cat. No.87CH2440-6), P122
   Neelakantan Arvind, 2016, ICLR
   Neto Joao Pedro, 2003, J BRAZILIAN COMPUTER
   Reed Scott, 2016, ICLR
   Schkufza Eric, 2013, ACM SIGARCH COMPUTER
   Sharma Rahul, 2015, OOPSLA
   Siegelmann Hava, 1994, AAAI
   Williams Ronald J, 1992, MACHINE LEARNING
   Zaremba Wojciech, 2015, ARXIV150500521
   Zaremba Wojciech, 2015, CORR
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703047
DA 2019-06-15
ER

PT S
AU Cai, MB
   Schuck, NW
   Pillow, JW
   Niv, Y
AF Cai, Ming Bo
   Schuck, Nicolas W.
   Pillow, Jonathan W.
   Niv, Yael
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI A Bayesian method for reducing bias in neural representational
   similarity analysis
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID OBJECT REPRESENTATIONS; STATISTICS; CORTEX
AB In neuroscience, the similarity matrix of neural activity patterns in response to different sensory stimuli or under different cognitive states reflects the structure of neural representational space. Existing methods derive point estimations of neural activity patterns from noisy neural imaging data, and the similarity is calculated from these point estimations. We show that this approach translates structured noise from estimated patterns into spurious bias structure in the resulting similarity matrix, which is especially severe when signal-to-noise ratio is low and experimental conditions cannot be fully randomized in a cognitive task. We propose an alternative Bayesian framework for computing representational similarity in which we treat the covariance structure of neural activity patterns as a hyperparameter in a generative model of the neural data, and directly estimate this covariance structure from imaging data while marginalizing over the unknown activity patterns. Converting the estimated covariance structure into a correlation matrix offers a much less biased estimate of neural representational similarity. Our method can also simultaneously estimate a signal-to-noise map that informs where the learned representational structure is supported more strongly, and the learned covariance matrix can be used as a structured prior to constrain Bayesian estimation of neural activity patterns. Our code is freely available in Brain Imaging Analysis Kit (Brainiak) (https://github.com/IntelPNI/brainiak).
C1 [Cai, Ming Bo; Schuck, Nicolas W.; Pillow, Jonathan W.; Niv, Yael] Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA.
RP Cai, MB (reprint author), Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA.
EM mcai@princeton.edu; nschuck@princeton.edu; pillow@princeton.edu;
   yael@princeton.edu
FU John Templeton Foundation; Intel Corporation; McKnight Foundation; NSF
   CAREER Award [IIS-1150186]; Simons Collaboration on the Global Brain
   [SCGB AWD1004351]
FX This publication was made possible through the support of grants from
   the John Templeton Foundation and the Intel Corporation. The opinions
   expressed in this publication are those of the authors and do not
   necessarily reflect the views of the John Templeton Foundation. JWP was
   supported by grants from the McKnight Foundation, Simons Collaboration
   on the Global Brain (SCGB AWD1004351) and the NSF CAREER Award
   (IIS-1150186). We thank Andrew C. Connolly etc. for sharing of the data
   used in 4.2. Data used in the supplementary material were obtained from
   the MGH-USC Human Connectome Project (HCP) database.
CR Alink A., 2015, BIORXIV
   Connolly AC, 2012, J NEUROSCI, V32, P2608, DOI 10.1523/JNEUROSCI.5547-11.2012
   Cox RW, 1996, COMPUT BIOMED RES, V29, P162, DOI 10.1006/cbmr.1996.0014
   Davis T, 2013, ANN NY ACAD SCI, V1296, P108, DOI 10.1111/nyas.12156
   deCharms RC, 2000, ANNU REV NEUROSCI, V23, P613, DOI 10.1146/annurev.neuro.23.1.613
   Diedrichsen J, 2011, NEUROIMAGE, V55, P1665, DOI 10.1016/j.neuroimage.2011.01.044
   Haxby JV, 2001, SCIENCE, V293, P2425, DOI 10.1126/science.1063736
   Henriksson L, 2015, NEUROIMAGE, V114, P275, DOI 10.1016/j.neuroimage.2015.04.026
   Jazzard P., 2003, FUNCTIONAL MAGNETIC
   Kravitz DJ, 2011, J NEUROSCI, V31, P7322, DOI 10.1523/JNEUROSCI.4588-10.2011
   Kriegeskorte N, 2008, FRONT SYST NEUROSCI, V2, DOI 10.3389/neuro.06.004.2008
   Kriegeskorte N, 2008, NEURON, V60, P1126, DOI 10.1016/j.neuron.2008.10.043
   KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P1, DOI 10.1007/BF02289565
   Nishimoto S, 2011, CURR BIOL, V21, P1641, DOI 10.1016/j.cub.2011.08.031
   Norman KA, 2006, TRENDS COGN SCI, V10, P424, DOI 10.1016/j.tics.2006.07.005
   Peelen MV, 2012, J NEUROSCI, V32, P15728, DOI 10.1523/JNEUROSCI.1953-12.2012
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Ritchey M., 2012, CEREB CORTEX
   Schuck NW, 2016, NEURON, V91, P1402, DOI 10.1016/j.neuron.2016.08.019
   Simoncelli EP, 2001, ANNU REV NEUROSCI, V24, P1193, DOI 10.1146/annurev.neuro.24.1.1193
   Walther A., 2015, NEUROIMAGE
   Woolrich MW, 2001, NEUROIMAGE, V14, P1370, DOI 10.1006/nimg.2001.0931
   Xue G, 2010, SCIENCE, V330, P97, DOI 10.1126/science.1193125
   Zarahn E, 1997, NEUROIMAGE, V5, P179, DOI 10.1006/nimg.1997.0263
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700095
DA 2019-06-15
ER

PT S
AU Canini, K
   Cotter, A
   Gupta, MR
   Fard, MM
   Pfeifer, J
AF Canini, K.
   Cotter, A.
   Gupta, M. R.
   Fard, M. Milani
   Pfeifer, J.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Fast and Flexible Monotonic Functions with Ensembles of Lattices
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID REGRESSION
AB For many machine learning problems, there are some inputs that are known to be positively (or negatively) related to the output, and in such cases training the model to respect that monotonic relationship can provide regularization, and makes the model more interpretable. However, flexible monotonic functions are computationally challenging to learn beyond a few features. We break through this barrier by learning ensembles of monotonic calibrated interpolated look-up tables (lattices). A key contribution is an automated algorithm for selecting feature subsets for the ensemble base models. We demonstrate that compared to random forests, these ensembles produce similar or better accuracy, while providing guaranteed monotonicity consistent with prior knowledge, smaller model size and faster evaluation.
C1 [Canini, K.; Cotter, A.; Gupta, M. R.; Fard, M. Milani; Pfeifer, J.] Google Inc, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA.
RP Canini, K (reprint author), Google Inc, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA.
EM canini@google.com; acotter@google.com; mayagupta@google.com;
   mmilanifard@google.com; janpf@google.com
CR Abu-Mostafa Y., 1993, ADV NEURAL INFORM PR, V5, P73
   Amaratunga D, 2008, BIOINFORMATICS, V24, P2010, DOI 10.1093/bioinformatics/btn356
   Blake C., 1998, UCI REPOSITORY MACHI
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Cotter A., 2016, 29 ANN C LEARN THEOR, P729
   Daniels H, 2010, IEEE T NEURAL NETWOR, V21, P906, DOI 10.1109/TNN.2010.2044803
   Dugas C., 2009, J MACHINE LEARNING R
   Fernandez-Delgado Manuel, 2014, J MACHINE LEARNING R
   Garcia E. K., 2009, ADV NEURAL INFORM PR
   Garcia E, 2012, IEEE T IMAGE PROCESS, V21, P4128, DOI 10.1109/TIP.2012.2200902
   Gupta M. R., 2016, J MACHINE LEARNING R, V17, P1
   Ho TK, 1998, IEEE T PATTERN ANAL, V20, P832, DOI 10.1109/34.709601
   Howard A., 2007, ADV NEURAL INFORM PR
   Kotlowski W., 2009, P 26 ANN INT C MACH, P537
   Qu YJ, 2011, IEEE T NEURAL NETWOR, V22, P2447, DOI 10.1109/TNN.2011.2167348
   Sharma Gaurav, 2002, DIGITAL COLOR IMAGIN
   Spouge J, 2003, J OPTIMIZ THEORY APP, V117, P585, DOI 10.1023/A:1023901806339
   Weichselbaumer D, 2005, J ECON SURV, V19, P479, DOI 10.1111/j.0950-0804.2005.00256.x
   Xu H., 2015, P INTERSPEECH
   Ye Y., 2013, PATTERN RECOGNITION
   Zhang L, 2014, INT RELIAB PHY SYM
NR 21
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703013
DA 2019-06-15
ER

PT S
AU Carreira-Perpinan, MA
   Raziperchikolaei, R
AF Carreira-Perpinan, Miguel A.
   Raziperchikolaei, Ramin
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI An Ensemble Diversity Approach to Supervised Binary Hashing
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID DIMENSIONALITY REDUCTION
AB Binary hashing is a well-known approach for fast approximate nearest-neighbor search in information retrieval. Much work has focused on affinity-based objective functions involving the hash functions or binary codes. These objective functions encode neighborhood information between data points and are often inspired by manifold learning algorithms. They ensure that the hash functions differ from each other through constraints or penalty terms that encourage codes to be orthogonal or dissimilar across bits, but this couples the binary variables and complicates the already difficult optimization. We propose a much simpler approach: we train each hash function (or bit) independently from each other, but introduce diversity among them using techniques from classifier ensembles. Surprisingly, we find that not only is this faster and trivially parallelizable, but it also improves over the more complex, coupled objective function, and achieves state-of-the-art precision and recall in experiments with image retrieval.
C1 [Carreira-Perpinan, Miguel A.; Raziperchikolaei, Ramin] Univ Calif, EECS, Merced, CA 95343 USA.
RP Carreira-Perpinan, MA (reprint author), Univ Calif, EECS, Merced, CA 95343 USA.
EM mcarreira-perpinan@ucmerced.edu; rraziperchikolaei@ucmerced.edu
FU NSF [IIS-1423515]
FX Work supported by NSF award IIS-1423515.
CR Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Boros E., 2002, DISCRETE APPL MATH
   Boykov Y., 2001, PAMI
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Carreira-Perpinan M. A, 2010, ICML
   Carreira-PerpiNanan M. A., 2015, CVPR
   Cormen T H., 2009, INTRO ALGORITHMS
   Dietterich T. G., 1995, J ARTIFICIAL INTELLI, V2, P253
   Dietterich T. G., 2000, ENSEMBLE METHODS MAC
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Garey M. R, 1979, COMPUTERS INTRACTABI
   Ge T., 2014, ECCV
   GEMAN S, 1992, NEURAL COMPUT, V4, P1, DOI 10.1162/neco.1992.4.1.1
   Gong Y., 2013, PAMI
   Grauman K., 2013, MACHINE LEARNING COM, V411, P49
   Kolmogorov V., 2003, PAMI
   Krizhevsky A., 2009, THESIS
   Krogh A., 1995, NIPS
   Kulis B., 2009, NIPS
   Kuncheva L, 2014, COMBINING PATTERN CL
   Leng C., 2014, ECML
   Lin B., 2014, ICML
   Lin G., 2014, CVPR
   Lin G., 2013, ICCV
   Liu W., 2011, ICML
   Liu W., 2012, CVPR
   Loosli G., 2007, LARGE SCALE KERNEL M, p[301, 6]
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Raziperchikolaei R., 2016, NIPS
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Shakhnarovich G, 2006, NEURAL INFORM PROCES
   Wang J., 2012, PAMI
   Weiss Y., 2009, NIPS
   Zhang D., 2010, SIGIR
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702036
DA 2019-06-15
ER

PT S
AU Carrillo, F
   Mota, N
   Copelli, M
   Ribeiro, S
   Sigman, M
   Cecchi, G
   Slezak, DF
AF Carrillo, Facundo
   Mota, Natalia
   Copelli, Mauro
   Ribeiro, Sidarta
   Sigman, Mariano
   Cecchi, Guillermo
   Slezak, Diego Fernandez
BE Rish, I
   Langs, G
   Wehbe, L
   Cecchi, G
   Chang, KMK
   Murphy, B
TI Automated Speech Analysis for Psychosis Evaluation
SO MACHINE LEARNING AND INTERPRETATION IN NEUROIMAGING, MLINI 2014
SE Lecture Notes in Artificial Intelligence
LA English
DT Proceedings Paper
CT 4th International Workshop on Machine Learning and Interpretation in
   Neuroimaging (MLINI) Held at Conference on Neural Information Processing
   Systems (NIPS)
CY DEC 13, 2014
CL Montreal, CANADA
AB Psychosis is a mental syndrome associated to loss of contact with reality which may arise in patients with different diseases, such as schizophrenia or bipolar disorder. Symptoms include hallucinations, confused and disturbed thoughts or lack of self-awareness. Recent studies have found that psychotic patients can be objectively screened using graph-theoretical algorithms for speech analysis. This analysis often relies in manually executed tasks such as syntagma generation, text splitting or manual feature selection for classification. To solve this fundamental limitation, we use three fully-automated text analysis tools graph generation methods. In addition, since aspects of psychosis may be manifested in semantic aspects of speech, we also developed a semantic features index based on speech coherence. We show that using this combined approach, classifications obtained from automatic techniques are higher than 85% in a database of 20 schizophrenic patients, with similar results to previous works. In summary, here we develop and validate a new tool for automated speech processing which includes semantic and structural aspects. The tool performs similar to manual screening procedures providing a new method to complement standard psychometric scales and fostering automated psychiatric diagnosis.
C1 [Carrillo, Facundo; Slezak, Diego Fernandez] Univ Buenos Aires, Lab Inteligencia Artificial Aplicada, Fac Ciencias Exactas & Nat, Dept Comp, Buenos Aires, DF, Argentina.
   [Mota, Natalia; Ribeiro, Sidarta] Univ Fed Rio Grande do Norte, Inst Cerebro, Natal, RN, Brazil.
   [Copelli, Mauro] Univ Fed Pernambuco, Recife, PE, Brazil.
   [Sigman, Mariano] Univ Torcuato Di Tella, Buenos Aires, DF, Argentina.
   [Cecchi, Guillermo] IBM Corp, TJ Watson Res Ctr, Yorktown Hts, NY USA.
RP Slezak, DF (reprint author), Univ Buenos Aires, Lab Inteligencia Artificial Aplicada, Fac Ciencias Exactas & Nat, Dept Comp, Buenos Aires, DF, Argentina.
EM dfslezak@dc.uba.ar
RI Copelli, Mauro/C-6336-2009
OI Copelli, Mauro/0000-0001-7441-2858; Mota, Natalia/0000-0003-2802-2001;
   Ribeiro, Sidarta/0000-0001-9325-9545
CR Bedi G, 2014, NEUROPSYCHOPHARMACOL, V39, P2340, DOI 10.1038/npp.2014.80
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Chapman LJ, 1973, DISORDERED THOUGHT S
   Church K. W., 1990, Computational Linguistics, V16, P22
   DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9
   Diuk CG, 2012, FRONT INTEGR NEUROSC, V6, DOI 10.3389/fnint.2012.00080
   Elvevag B, 2007, SCHIZOPHR RES, V93, P304, DOI 10.1016/j.schres.2007.03.001
   First MB, 2012, STRUCTURED CLIN INTE
   FOLTIN FW, 1988, PHARMACOL BIOCHEM BE, V30, P539, DOI 10.1016/0091-3057(88)90494-7
   Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223
   Graesser AC, 2004, BEHAV RES METH INS C, V36, P193, DOI 10.3758/BF03195564
   Hall M., 2009, SIGKDD EXPLORATIONS, V11, P10, DOI [DOI 10.1145/1656274.1656278, 10.1145/1656274.1656278]
   HIGGINS ST, 1988, PSYCHOPHARMACOLOGY, V95, P189
   Landauer TK, 1997, PSYCHOL REV, V104, P211, DOI 10.1037//0033-295X.104.2.211
   Loper E., 2002, ETMTNLP 02 P ACL 02, V1, P63, DOI DOI 10.3115/1118108.1118117
   Lopez-Rosenfeld M, 2013, COMPUT EDUC, V68, P307, DOI 10.1016/j.compedu.2013.05.018
   McNamara DS, 2002, COH METRIX AUT UNPUB
   Michel JB, 2011, SCIENCE, V331, P176, DOI 10.1126/science.1199644
   Mikolov T., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951
   Montague PR, 2012, TRENDS COGN SCI, V16, P72, DOI 10.1016/j.tics.2011.11.018
   Mota NB, 2014, SCI REP-UK, V4, DOI 10.1038/srep03691
   Mota NB, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0034928
   Goldin AP, 2014, P NATL ACAD SCI USA, V111, P6443, DOI 10.1073/pnas.1320217111
   Sobocki P, 2006, J MENT HEALTH POLICY, V9, P87
   Toutanova K., 2003, P 2003 C N AM CHAPT, V1, P173, DOI DOI 10.3115/1073445.1073478
   Turing A. M., 1950, MIND, V59, P433, DOI DOI 10.1093/MIND/LIX.236.433
   von Ahn L, 2006, COMPUTER, V39, P92, DOI 10.1109/MC.2006.196
NR 27
TC 0
Z9 0
U1 1
U2 8
PU SPRINGER INTERNATIONAL PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
EI 1611-3349
BN 978-3-319-45173-2; 978-3-319-45174-9
J9 LECT NOTES ARTIF INT
PY 2016
VL 9444
BP 31
EP 39
DI 10.1007/978-3-319-45174-9_4
PG 9
WC Computer Science, Artificial Intelligence; Neuroimaging
SC Computer Science; Neurosciences & Neurology
GA BG4FZ
UT WOS:000388723400004
DA 2019-06-15
ER

PT S
AU Chakrabarti, A
   Shao, JY
   Shakhnarovich, G
AF Chakrabarti, Ayan
   Shao, Jingyu
   Shakhnarovich, Gregory
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Depth from a Single Image by Harmonizing Overcomplete Local Network
   Predictions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB A single color image can contain many cues informative towards different aspects of local geometric structure. We approach the problem of monocular depth estimation by using a neural network to produce a mid-level representation that summarizes these cues. This network is trained to characterize local scene geometry by predicting, at every image location, depth derivatives of different orders, orientations and scales. However, instead of a single estimate for each derivative, the network outputs probability distributions that allow it to express confidence about some coefficients, and ambiguity about others. Scene depth is then estimated by harmonizing this overcomplete set of network predictions, using a globalization procedure that finds a single consistent depth map that best matches all the local derivative distributions. We demonstrate the efficacy of this approach through evaluation on the NYU v2 depth data set.
C1 [Chakrabarti, Ayan; Shao, Jingyu; Shakhnarovich, Gregory] TTI Chicago, Chicago, IL 60637 USA.
   [Shao, Jingyu] Univ Calif Los Angeles, Dept Stat, Los Angeles, CA USA.
RP Chakrabarti, A (reprint author), TTI Chicago, Chicago, IL 60637 USA.
EM ayanc@ttic.edu; shaojy15@ucla.edu; gregory@ttic.edu
FU National Science Foundation [IIS-1618021]
FX AC acknowledges support for this work from the National Science
   Foundation under award no. IIS-1618021, and from a gift by Adobe
   Systems. AC and GS thank NVIDIA Corporation for donations of Titan X
   GPUs used in this research.
CR Baig  M., 2016, P WACV
   Chakrabarti  A., 2015, P CVPR
   Chatfield  K., 2014, P BMVC
   Clowes M. B., 1971, ARTIFICIAL INTELLIGE
   Eigen  D., 2015, P ICCV
   Eigen David, 2014, NIPS
   Horn B. K., 1986, SHAPE SHADING
   Karsch  K., 2012, P ECCV
   Ladicky  L., 2014, P CVPR
   Liu  F., 2015, P CVPR
   Long J., 2015, P CVPR
   Saxena A., 2005, NIPS
   Silberman  N., 2012, P ECCV
   Sugihara K, 1986, MACHINE INTERPRETATI
   Wang P., 2015, P CVPR
   Wang  X., 2015, P CVPR
   Zoran  D., 2015, P ICCV
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704035
DA 2019-06-15
ER

PT S
AU Chakrabarti, A
AF Chakrabarti, Ayan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning Sensor Multiplexing Design through Back-propagation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Recent progress on many imaging and vision tasks has been driven by the use of deep feed-forward neural networks, which are trained by propagating gradients of a loss defined on the final output, back through the network up to the first layer that operates directly on the image. We propose back-propagating one step further to learn camera sensor designs jointly with networks that carry out inference on the images they capture. In this paper, we specifically consider the design and inference problems in a typical color camera-where the sensor is able to measure only one color channel at each pixel location, and computational inference is required to reconstruct a full color image. We learn the camera sensor's color multiplexing pattern by encoding it as layer whose learnable weights determine which color channel, from among a fixed set, will be measured at each location. These weights are jointly trained with those of a reconstruction network that operates on the corresponding sensor measurements to produce a full color image. Our network achieves significant improvements in accuracy over the traditional Bayer pattern used in most color cameras. It automatically learns to employ a sparse color measurement approach similar to that of a recent design, and moreover, improves upon that design by learning an optimal layout for these measurements.
C1 [Chakrabarti, Ayan] Toyota Technol Inst Chicago, 6045 S Kenwood Ave, Chicago, IL 60637 USA.
RP Chakrabarti, A (reprint author), Toyota Technol Inst Chicago, 6045 S Kenwood Ave, Chicago, IL 60637 USA.
EM ayanc@ttic.edu
CR Baraniuk R., 2007, IEEE SIGNAL PROCESSI
   Bayer B. E., 1976, U.S. Patent, Patent No. [3 971 065, 3971065, US 3971065]
   Burger H. C., 2012, P CVPR
   Chakrabarti A., 2014, P ICCP
   Elad M., 2007, IEEE T SIG P
   Gehler P. V., 2008, P CVPR
   Han S., 2015, ARXIV151000149
   Holloway J., 2012, P ICCP
   Kaltzer T., 2016, P ICCP
   Kapah O., 2000, ELECT IMAGING
   Khashabi D., 2014, IEEE T IM PROC
   Krizhevsky A., 2012, NIPS
   LeCun Y., 1998, NEURAL NETWORKS TRIC
   Levin A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239521
   Li X., 2008, P SPIE
   Long J., 2015, P CVPR
   Mairal J., 2009, P ICCV
   Raskar R., 2006, ACM T GRAPHICS TOG
   Schuler C. J., 2013, P CVPR
   Sermanet P., 2013, ARXIV13126229
   Shi L., 2010, REPROCESSED VERSION
   Sun J., 2013, IEEE T IM PROC
   Veeraraghavan A., 2007, DAPPLED PHOTOGRAPHY
   Wang  X., 2015, P CVPR
   Waters A. E., 2011, NIPS
   Xu  L., 2014, NIPS
   Zhang L., 2005, IEEE T IMAG PROC
   Zoran D., 2011, P ICCV
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701108
DA 2019-06-15
ER

PT S
AU Chalk, M
   Marre, O
   Tkacik, G
AF Chalk, Matthew
   Marre, Olivier
   Tkacik, Gasper
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Relevant sparse codes with variational information bottleneck
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB In many applications, it is desirable to extract only the relevant aspects of data. A principled way to do this is the information bottleneck (IB) method, where one seeks a code that maximizes information about a 'relevance' variable, Y, while constraining the information encoded about the original data, X. Unfortunately however, the IB method is computationally demanding when data are high-dimensional and/or non-gaussian. Here we propose an approximate variational scheme for maximizing a lower bound on the IB objective, analogous to variational EM. Using this method, we derive an IB algorithm to recover features that are both relevant and sparse. Finally, we demonstrate how kernelized versions of the algorithm can be used to address a broad range of problems with non-linear relation between X and Y
C1 [Chalk, Matthew; Tkacik, Gasper] IST Austria, Campus 1, A-3400 Klosterneuburg, Austria.
   [Marre, Olivier] Inst Vis, 17 Rue Moreau, F-75012 Paris, France.
RP Chalk, M (reprint author), IST Austria, Campus 1, A-3400 Klosterneuburg, Austria.
RI Marre, Olivier/F-2751-2017
OI Marre, Olivier/0000-0002-0090-6190
CR ANDREWS DF, 1974, J ROY STAT SOC B MET, V36, P99
   Barber D, 2004, ADV NEUR IN, V16, P201
   Bialek W, 2001, NEURAL COMPUT, V13, P2409, DOI 10.1162/089976601753195969
   Bialek W, 2006, 2006 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, VOLS 1-6, PROCEEDINGS, P659, DOI 10.1109/ISIT.2006.261867
   Chechik G, 2005, J MACH LEARN RES, V6, P165
   Chechik G., 2002, ADV NEURAL INFORM PR, V15
   Doi E, 2005, ADV NEURAL INFORM PR, V17, P377
   Doi E, 2012, J NEUROSCI, V32, P16256, DOI 10.1523/JNEUROSCI.4036-12.2012
   Eichhorn J, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000336
   Elidan G, 2002, 19 C UNC ART INT, P200
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Hofmann T., 2003, 3 IEEE INT C DAT MIN
   Mika S., 1999, NIPS, P526
   Palmer SE, 2015, P NATL ACAD SCI USA, V112, P6908, DOI 10.1073/pnas.1506855112
   Scheffler C., 2008, DERIVATION EM UPDATE
   Simoncelli EP, 2001, ANNU REV NEUROSCI, V24, P1193, DOI 10.1146/annurev.neuro.24.1.1193
   Slonim N., 2002, ADV NEURAL INFORM PR, P335
   Slonim N., 2003, THESIS
   Tishby N., 1999, P 37 ANN ALL C COMM, P368
   Tkacik G, 2010, P NATL ACAD SCI USA, V107, P14419, DOI 10.1073/pnas.1004906107
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700065
DA 2019-06-15
ER

PT S
AU Chang, KW
   He, H
   Daume, H
   Langford, J
   Ross, S
AF Chang, Kai-Wei
   He, He
   Daume, Hal, III
   Langford, John
   Ross, Stephane
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI A Credit Assignment Compiler for Joint Prediction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Many machine learning applications involve jointly predicting multiple mutually dependent output variables. Learning to search is a family of methods where the complex decision problem is cast into a sequence of decisions via a search space. Although these methods have shown promise both in theory and in practice, implementing them has been burdensomely awkward. In this paper, we show the search space can be defined by an arbitrary imperative program, turning learning to search into a credit assignment compiler. Altogether with the algorithmic improvements for the compiler, we radically reduce the complexity of programming and the running time. We demonstrate the feasibility of our approach on multiple joint prediction tasks. In all cases, we obtain accuracies as high as alternative approaches, at drastically reduced execution and programming time.
C1 [Chang, Kai-Wei] Univ Virginia, Charlottesville, VA 22903 USA.
   [He, He; Daume, Hal, III] Univ Maryland, College Pk, MD 20742 USA.
   [Langford, John] Microsoft Res, Redmond, WA USA.
   [Ross, Stephane] Google, Mountain View, CA USA.
RP Chang, KW (reprint author), Univ Virginia, Charlottesville, VA 22903 USA.
EM kw@kwchang.net; hhe@cs.umd.edu; me@ha13.name; jc1@microsoft.com;
   stephaneross@google.com
FU NSF [IIS-1320538]
FX Part of this work was carried out while Kai-Wei, Hal and Stephane were
   visiting Microsoft Research. Hal and He are also supported by NSF grant
   IIS-1320538. Any opinions, findings, conclusions, or recommendations
   expressed here are those of the authors and do not necessarily reflect
   the view of the sponsor. The authors thank anonymous reviewers for their
   comments.
CR Agarwal A., 2011, ARXIV11104198
   Andor D, 2016, GLOBALLY NORMALIZED
   Beygelzimer A., 2005, P 22 INT C MACH LEAR, P49
   Bottou L., 2011, CRFSGD PROJECT
   Chang K.-W., 2015, ILLINOISSL JAVA LIB
   Chang K.-W., 2013, ECML
   Chang Kai-Wei, 2015, ICML
   Chen D, 2014, EMNLP, P740, DOI DOI 10.3115/V1/D14-1082
   Collins M., 2002, EMNLP
   Collins Michael, 2004, ACL
   Daume Hal, 2005, ICML
   Doppa J. R., 2014, JAIR, V50
   Doppa J. R., 2012, ICML
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Dyer C., 2015, ACL
   Eisner J., 2005, EMNLP
   Germann U, 2004, ARTIF INTELL, V154, P127, DOI 10.1016/j.artint.2003.06.001
   Goldberg Y., 2013, T ACL, V1
   Goodman N. D., 2008, UAI
   Gordon Andrew D., 2014, INT C SOFTW ENG ICSE
   Hal Daume III, 2009, MACHINE LEARNING J
   Huang L., 2012, NAACL
   Joachims T., 2009, MACHINE LEARNING J
   Karampatziakis N., 2011, UAI
   Kimmig A., 2012, NIPS WORKSH PROB PRO
   Kordjamshidi P., 2015, IJCAI
   Kudo T., 2005, CRF PROJECT
   Lafferty J. D., 2001, P 18 INT C MACH LEAR, V3, P282, DOI DOI 10.1038/NPROT.2006.61
   Langford John, 2007, VOWPAL WABBIT
   McCallum A., 2009, NIPS
   McCallum A., 2000, ICML
   Milch B., 2007, STAT RELATIONAL LEAR
   Minka T., 2010, INFER NET 2 4 2010
   Ng A. Y., 2000, P 16 C UNC ART INT, P406
   Nivre J., 2003, P 8 INT WORKSH PARS, P149
   Pfeffer  Avi, 2001, IJCAI
   Ratinov L., 2009, CONLL
   Ratliff N., 2007, NIPS
   Richardson  M., 2006, MACHINE LEARNING, V62
   Ross S., 2011, AI STATS
   Ross S., 2013, UAI
   Ross Stephane, 2014, ARXIV14065979
   Roth D, 2007, INTRO STAT RELATIONA
   Soon WM, 2001, COMPUT LINGUIST, V27, P521, DOI 10.1162/089120101753342653
   Syed U., 2011, NIPS
   Taskar B, 2003, NIPS
   Tsochantaridis I, 2004, ICML
   Xu YH, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2041
   Xu Yuehua, 2007, INT C MACH LEARN COR, P1047
NR 49
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702003
DA 2019-06-15
ER

PT S
AU Chaudhuri, S
   Tewari, A
AF Chaudhuri, Sougata
   Tewari, Ambuj
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Phased Exploration with Greedy Exploitation in Stochastic Combinatorial
   Partial Monitoring Games
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Partial monitoring games are repeated games where the learner receives feedback that might be different from adversary's move or even the reward gained by the learner. Recently, a general model of combinatorial partial monitoring (CPM) games was proposed [1], where the learner's action space can be exponentially large and adversary samples its moves from a bounded, continuous space, according to a fixed distribution. The paper gave a confidence bound based algorithm (GCB) that achieves O (T-2./3 log T) distribution independent and O (log T) distribution dependent regret bounds. The implementation of their algorithm depends on two separate offline oracles and the distribution dependent regret additionally requires existence of a unique optimal action for the learner. Adopting their CPM model, our first contribution is a Phased Exploration with Greedy Exploitation (PEGE) algorithmic framework for the problem. Different algorithms within the framework achieve O (T-2./3 root log T) distribution independent and O (log(2) T) distribution dependent regret respectively. Crucially, our framework needs only the simpler "argmax" oracle from GCB and the distribution dependent regret does not require existence of a unique optimal action. Our second contribution is another algorithm, PEGE2, which combines gap estimation with a PEGE algorithm, to achieve an O (log T) regret bound, matching the GCB guarantee but removing the dependence on size of the learner's action space. However, like GCB, PEGE2 requires access to both offline oracles and the existence of a unique optimal action. Finally, we discuss how our algorithm can be efficiently applied to a CPM problem of practical interest: namely, online ranking with feedback at the top.
C1 [Chaudhuri, Sougata; Tewari, Ambuj] Univ Michigan Ann Arbor, Dept Stat, Ann Arbor, MI 48109 USA.
   [Tewari, Ambuj] Univ Michigan Ann Arbor, Dept EECS, Ann Arbor, MI USA.
RP Chaudhuri, S (reprint author), Univ Michigan Ann Arbor, Dept Stat, Ann Arbor, MI 48109 USA.
EM sougata@umich.edu; tewaria@umich.edu
FU NSF [IIS 1452099, CCF 1422157]
FX We acknowledge the support of NSF via grants IIS 1452099 and CCF
   1422157.
CR AGRAWAL R, 1989, SYST CONTROL LETT, V13, P405, DOI 10.1016/0167-6911(89)90107-2
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Bartok G, 2014, MATH OPER RES, V39, P967, DOI 10.1287/moor.2014.0663
   Cesa-Bianchi N, 2006, MATH OPER RES, V31, P562, DOI 10.1287/moor.1060.0206
   Chaudhuri Sougata, 2015, P 18 INT C ART INT S, P129
   Chen  W., 2013, P 30 INT C MACH LEAR, P151
   Hayes T. P., 2005, COMBINATORICS PROBAB
   Komiyama Junpei, 2015, ADV NEURAL INFORM PR, P1783
   Kveton B, 2015, P 18 INT C ART INT S, P535
   Lin  Tian, 2014, P 31 INT C MACH LEAR
   Piccolboni A, 2001, LECT NOTES ARTIF INT, V2111, P208
   Robbins H., 1985, H ROBBINS SELECTED P, P169
   Rusmevichientong P, 2010, MATH OPER RES, V35, P395, DOI 10.1287/moor.1100.0446
NR 13
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701055
DA 2019-06-15
ER

PT S
AU Chazal, F
   Giulini, I
   Michel, B
AF Chazal, Frederic
   Giulini, Ilaria
   Michel, Bertrand
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Data driven estimation of Laplace-Beltrami operator
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID THEORETICAL FOUNDATION; PENALTIES
AB Approximations of Laplace-Beltrami operators on manifolds through graph Laplacians have become popular tools in data analysis and machine learning. These discretized operators usually depend on bandwidth parameters whose tuning remains a theoretical and practical problem. In this paper, we address this problem for the unnormalized graph Laplacian by establishing an oracle inequality that opens the door to a well-founded data-driven procedure for the bandwidth selection. Our approach relies on recent results by Lacour and Massart [LM15] on the so-called Lepski's method.
C1 [Chazal, Frederic; Giulini, Ilaria] Inria Saclay, Palaiseau, France.
   [Michel, Bertrand] Ecole Cent Nantes, Lab Math Jean Leray, UMR 6629, CNRS, Nantes, France.
RP Chazal, F (reprint author), Inria Saclay, Palaiseau, France.
EM frederic.chazal@inria.fr; ilaria.giulini@me.com;
   bertrand.michel@ec-nantes.fr
FU ANR [ANR-13-BS01-0008]; ERC Gudhi [339025]
FX The authors are grateful to Pascal Massart for helpful discussions on
   Lepski's method. This work was supported by the ANR project TopData
   ANR-13-BS01-0008 and ERC Gudhi No. 339025
CR Arlot S, 2010, STAT SURV, V4, P40, DOI 10.1214/09-SS054
   Arlot S, 2009, J MACH LEARN RES, V10, P245
   Baudry JP, 2012, STAT COMPUT, V22, P455, DOI 10.1007/s11222-011-9236-1
   Belkin M, 2005, LECT NOTES COMPUT SC, V3559, P486, DOI 10.1007/11503415_33
   Belkin M, 2004, MACH LEARN, V56, P209, DOI 10.1023/B:MACH.0000033120.25363.1e
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Belkin M., 2007, NIPS PROCESSING, P129
   Belkin M, 2008, J COMPUT SYST SCI, V74, P1289, DOI 10.1016/j.jcss.2007.08.006
   Birge L, 2007, PROBAB THEORY REL, V138, P33, DOI 10.1007/s00440-006-0011-8
   Gine E., 2006, IMS LECT NOTES MONOG, V51, P238, DOI DOI 10.1214/074921706000000888
   Goldenshluger A, 2009, PROBAB THEORY REL, V143, P41, DOI 10.1007/s00440-007-0119-5
   Goldenshluger A, 2008, BERNOULLI, V14, P1150, DOI 10.3150/08-BEJ144
   Grigoryan Alexander, 2009, HEAT KERNEL ANAL MAN, V47
   Hein M., 2007, J MACHINE LEARNING R, V8
   Lacour Claire, 2015, ARXIV150300946
   Lacour Claire, 2016, ARXIV160705091
   Lepski OV, 1997, ANN STAT, V25, P929
   Lepskii O. V., 1992, ADV SOVIET MATH, V12
   LEPSKII OV, 1991, THEOR PROBAB APPL+, V36, P682, DOI 10.1137/1136085
   Lepskii OV, 1993, THEORY PROBABILITY I, V37, P433
   Nadler B, 2006, APPL COMPUT HARMON A, V21, P113, DOI 10.1016/j.acha.2005.07.004
   Rieser Antonio, 2015, ARXIV150602633
   Rosenberg Steven, 1997, LAPLACIAN RIEMANNIAN
   Ting Daniel, 2011, ARXIV11015435
   von Luxburg U, 2008, ANN STAT, V36, P555, DOI 10.1214/009053607000000640
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701067
DA 2019-06-15
ER

PT S
AU Chen, B
AF Chen, Bryant
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Identification and Overidentification of Linear Structural Equation
   Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB In this paper, we address the problems of identifying linear structural equation models and discovering the constraints they imply. We first extend the half-trek criterion to cover a broader class of models and apply our extension to finding testable constraints implied by the model. We then show that any semi-Markovian linear model can be recursively decomposed into simpler sub-models, resulting in improved identification and constraint discovery power. Finally, we show that, unlike the existing methods developed for linear models, the resulting method subsumes the identification and constraint discovery algorithms for non-parametric models.
C1 [Chen, Bryant] Univ Calif Los Angeles, Comp Sci Dept, Los Angeles, CA 90095 USA.
RP Chen, B (reprint author), Univ Calif Los Angeles, Comp Sci Dept, Los Angeles, CA 90095 USA.
FU NSF [IIS-1302448, IIS-1527490]; ONR [N00014-13-1-0153]
FX I would like to thank Jin Tian and Judea Pearl for helpful comments and
   discussions. This research was supported in parts by grants from NSF
   #IIS-1302448 and #IIS-1527490 and ONR #N00014-13-1-0153 and
   #N00014-13-1-0153.
CR BALKE A, 1994, PROCEEDINGS OF THE TWELFTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 AND 2, P230
   BRITO C, 2004, THESIS
   Brito Carlos, 2002, UNCERTAINTY ARTIFICI, P85
   Chen B, 2014, R432 U CAL DEP COMP
   CHEN B, 2014, P 28 AAAI C ART INT
   DRTON M, 2016, SCANDINAVIAN J STAT, DOI [10.1111/sjos.12227, DOI 10.1111/SJ0S.12227]
   Foygel R, 2012, ANN STAT, V40, P1682, DOI 10.1214/12-AOS1012
   Huang Y, 2006, P 22 C UNC ART INT, P217
   Kang CS, 2009, J MACH LEARN RES, V10, P41
   PEARL J., 2004, P 20 C UNC ART INT, P446
   Pearl J, 2009, CAUSALITY MODELS REA
   SHPITSER I., 2008, P 23 NAT C ART INT, P1081
   Shpitser I, 2006, P 22 C UNC ART INT, P437
   Tian J, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P567
   TIAN J, 2009, P 21 INT JOINT C ART
   TIAN J, 2005, P NAT C ART INT, V20
   TIAN J., 2007, P 23 C ANN C UNC ART
   Tian J., 2002, THESIS
   Tian Jin, 2002, P 18 C UNC ART INT, P519
   Verma T., 1991, UNCERTAINTY ARTIFICI, V6, P255
   Wright S, 1920, J AGRIC RES, V20, P0557
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701080
DA 2019-06-15
ER

PT S
AU Chen, CY
   Ding, N
   Li, CY
   Zhang, YZ
   Carin, L
AF Chen, Changyou
   Ding, Nan
   Li, Chunyuan
   Zhang, Yizhe
   Carin, Lawrence
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Stochastic Gradient MCMC with Stale Gradients
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Stochastic gradient MCMC (SG-MCMC) has played an important role in large-scale Bayesian learning, with well-developed theoretical convergence properties. In such applications of SG-MCMC, it is becoming increasingly popular to employ distributed systems, where stochastic gradients are computed based on some outdated parameters, yielding what are termed stale gradients. While stale gradients could be directly used in SG-MCMC, their impact on convergence properties has not been well studied. In this paper we develop theory to show that while the bias and MSE of an SG-MCMC algorithm depend on the staleness of stochastic gradients, its estimation variance (relative to the expected estimate, based on a prescribed number of samples) is independent of it. In a simple Bayesian distributed system with SG-MCMC, where stale gradients are computed asynchronously by a set of workers, our theory indicates a linear speedup on the decrease of estimation variance w.r.t. the number of workers. Experiments on synthetic data and deep neural networks validate our theory, demonstrating the effectiveness and scalability of SG-MCMC with stale gradients.
C1 [Chen, Changyou; Li, Chunyuan; Zhang, Yizhe; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.
   [Ding, Nan] Google Inc, Venice, CA USA.
RP Chen, CY (reprint author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.
EM cc448@duke.edu; dingnan@google.com; cl319@duke.edu; yz196@duke.edu;
   lcarin@duke.edu
FU ARO; DARPA; DOE; NGA; ONR; NSF
FX Supported in part by ARO, DARPA, DOE, NGA, ONR and NSF.
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Agarwal A., 2011, NIPS
   Ahmed A., 2012, WSDM
   Ahn S., 2015, KDD
   Ahn S., 2014, ICML
   Bardenet R., 2015, ARXIV150502827
   Blundell C., 2015, ICML
   Bottou L., 1998, ONLINE ALGORITHMS ST
   Bottou L., 2012, TECHNICAL REPORT
   Chaturapruek S., 2015, NIPS
   Chen C., 2015, NIPS
   Chen T., 2014, ICML
   Chen Tianqi, 2015, ARXIV151201274
   Dean J., 2012, NIPS
   Ding N., 2014, NIPS
   Feyzmahdavian H. R., 2015, ARXIV150504824
   Ho Q., 2013, NIPS
   Jia T., 2014, ARXIV14085093
   Krizhevshy A., 2012, NIPS
   Li C., 2016, AAAI
   Li M., 2014, NIPS
   Lian X., 2015, NIPS
   Ma Y. A., 2015, NIPS
   Mattingly JC, 2010, SIAM J NUMER ANAL, V48, P552, DOI 10.1137/090770527
   Neiswanger W., 2014, UAI
   Niu F., 2011, NIPS
   Patterson S., 2013, NIPS
   Rabinovich M., 2015, NIPS
   Scott S. L., 2013, BAYES, V250
   Simsekli U., 2015, TECHNICAL REPORT
   Teh Y. W., 2015, ARXIV151209327V1
   Vollmer S. J., 2015, ARXIV150100438
   Wang X., 2015, NIPS
   Welling M., 2011, ICML
   Zhang S., 2015, NIPS
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702106
DA 2019-06-15
ER

PT S
AU Chen, EYJ
   Shen, Y
   Choi, A
   Darwiche, A
AF Chen, Eunice Yuh-Jie
   Shen, Yujia
   Choi, Arthur
   Darwiche, Adnan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning Bayesian networks with ancestral constraints
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We consider the problem of learning Bayesian networks optimally, when subject to background knowledge in the form of ancestral constraints. Our approach is based on a recently proposed framework for optimal structure learning based on non-decomposable scores, which is general enough to accommodate ancestral constraints. The proposed framework exploits oracles for learning structures using decomposable scores, which cannot accommodate ancestral constraints since they are non-decomposable. We show how to empower these oracles by passing them decomposable constraints that they can handle, which are inferred from ancestral constraints that they cannot handle. Empirically, we demonstrate that our approach can be orders-of-magnitude more efficient than alternative frameworks, such as those based on integer linear programming.
C1 [Chen, Eunice Yuh-Jie; Shen, Yujia; Choi, Arthur; Darwiche, Adnan] Univ Calif Los Angeles, Comp Sci Dept, Los Angeles, CA 90095 USA.
RP Chen, EYJ (reprint author), Univ Calif Los Angeles, Comp Sci Dept, Los Angeles, CA 90095 USA.
EM eyjchen@cs.ucla.edu; yujias@cs.ucla.edu; aychoi@cs.ucla.edu;
   darwiche@cs.ucla.edu
FU NSF [IIS-1514253]; ONR [N00014-15-1-2339]
FX This work was partially supported by NSF grant #IIS-1514253 and ONR
   grant #N00014-15-1-2339.
CR Bartlett M., 2015, ARTIFICIAL INTELLIGE
   Borboudakis G., 2013, P 29 C UNC ART INT
   Borboudakis G., 2012, P 29 INT C MACH LEAR
   Chen E. Y.-J., 2015, P 4 IJCAI WORKSH GRA
   Chen E. Y.-J., 2016, P 19 INT C ART INT S
   Chen Y, 2014, PROCEEDINGS OF THE 2014 11TH INTERNATIONAL CONFERENCE ON ELECTRONICS, COMPUTER AND COMPUTATION (ICECCO'14)
   COOPER GF, 1992, MACH LEARN, V9, P309, DOI 10.1007/BF00994110
   Cussens J., 2008, P 24 C UNC ART INT, P105
   Cussens J, 2013, GENET EPIDEMIOL, V37, P69, DOI 10.1002/gepi.21686
   Darwiche A, 2009, MODELING AND REASONING WITH BAYESIAN NETWORKS, P1, DOI 10.1017/CBO9780511811357
   Jaakkola T., 2010, P 13 INT C ART INT S, P358
   Koivisto M, 2004, J MACH LEARN RES, V5, P549
   Koller D., 2009, PROBABILISTIC GRAPHI
   Li CM, 2009, FRONT ARTIF INTEL AP, V185, P613, DOI 10.3233/978-1-58603-929-5-613
   Malone B., 2014, P 28 C ART INT
   Murphy KP, 2012, MACHINE LEARNING PRO
   Parviainen P, 2013, J MACH LEARN RES, V14, P1387
   Silander T., 2006, P 22 ANN C UNC ART I, P445
   Singh A. P., 2005, TECHNICAL REPORT
   Tian J., 2010, P 26 C ANN C UNC ART, P589
   van Beek P, 2015, LECT NOTES COMPUT SC, V9255, P429, DOI 10.1007/978-3-319-23219-5_31
   Yuan C, 2011, P 22 INT JOINT C ART, V22, P2186
   Yuan CH, 2013, J ARTIF INTELL RES, V48, P23, DOI 10.1613/jair.4039
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703022
DA 2019-06-15
ER

PT S
AU Chen, H
   Xia, HF
   Cai, WD
   Huang, H
AF Chen, Hong
   Xia, Haifeng
   Cai, Weidong
   Huang, Heng
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Error Analysis of Generalized Nystrom Kernel Regression
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID APPROXIMATION; MATRIX
AB Nystrom method has been successfully used to improve the computational efficiency of kernel ridge regression (KRR). Recently, theoretical analysis of Nystrom KRR, including generalization bound and convergence rate, has been established based on reproducing kernel Hilbert space (RKHS) associated with the symmetric positive semi-definite kernel. However, in real world applications, RKHS is not always optimal and kernel function is not necessary to be symmetric or positive semi-definite. In this paper, we consider the generalized Nystrom kernel regression (GNKR) with l(2) coefficient regularization, where the kernel just requires the continuity and boundedness. Error analysis is provided to characterize its generalization performance and the column norm sampling strategy is introduced to construct the refined hypothesis space. In particular, the fast learning rate with polynomial decay is reached for the GNKR. Experimental analysis demonstrates the satisfactory performance of GNKR with the column norm sampling.
C1 [Chen, Hong; Huang, Heng] Univ Texas Arlington, Comp Sci & Engn, Arlington, TX 76019 USA.
   [Xia, Haifeng] Huazhong Agr Univ, Math & Stat, Wuhan 430070, Peoples R China.
   [Cai, Weidong] Univ Sydney, Sch Informat Technol, Sydney, NSW 2006, Australia.
RP Chen, H (reprint author), Univ Texas Arlington, Comp Sci & Engn, Arlington, TX 76019 USA.
EM chenh@mail.hzau.edu.cn; haifeng.xia0910@gmail.com;
   tom.cai@sydney.edu.au; heng@uta.edu
FU U.S. NSF-IIS [1302675]; NSF-IIS [1633753, 1344152, 1619308]; NSF-DBI
   [1356628]; NIH [AG049371]; National Natural Science Foundation of China
   (NSFC) [11671161]
FX This work was partially supported by U.S. NSF-IIS 1302675, NSF-IIS
   1344152, NSF-DBI 1356628, NSF-IIS 1619308, NSF-IIS 1633753, NIH
   AG049371, and by National Natural Science Foundation of China (NSFC)
   11671161. We thank the anonymous NIPS reviewers for insightful comments.
CR Alaoui Ahmed, 2015, ADV NEURAL INFORM PR, V28, P775
   Bach F., 2013, COLT
   Drineas P, 2005, J MACH LEARN RES, V6, P2153
   Drineas P, 2006, SIAM J COMPUT, V36, P158, DOI 10.1137/S0097539704442696
   Drineas P, 2012, J MACH LEARN RES, V13, P3475
   Eberts M., 2011, ADV NEURAL INFORM PR, V24, P1539
   Feng Y., 2016, NEURAL COMPUT, V28, P1
   Gittens A., 2013, P 30 INT C MACH LEAR, V28, P567
   Hsieh CJ, 2014, ADV NEURAL INFORM PR, P3689
   Jin R, 2013, IEEE T INFORM THEORY, V59, P6939, DOI 10.1109/TIT.2013.2271378
   Kumar S, 2012, J MACH LEARN RES, V13, P981
   Lim W., 2015, ICML, P1367
   Liu CJ, 2004, IEEE T PATTERN ANAL, V26, P572, DOI 10.1109/TPAMI.2004.1273927
   Pekalska E, 2009, IEEE T PATTERN ANAL, V31, P1017, DOI 10.1109/TPAMI.2008.290
   Rahimi A., 2007, ADV NEURAL INFORM PR, P1177
   Rudi A., 2015, NIPS, P1657
   Scholkopf Bernhard, 2001, LEARNING KERNELS SUP
   Shi L, 2013, APPL COMPUT HARMON A, V34, P252, DOI 10.1016/j.acha.2012.05.001
   Shi L, 2011, APPL COMPUT HARMON A, V31, P286, DOI 10.1016/j.acha.2011.01.001
   Sun HW, 2015, IEEE T NEUR NET LEAR, V26, P2576, DOI 10.1109/TNNLS.2014.2375209
   Sun HW, 2011, APPL COMPUT HARMON A, V30, P96, DOI 10.1016/j.acha.2010.04.001
   Wang Y., 2016, MINIMAX SUBSAMPLING
   Williams CKI, 2001, ADV NEUR IN, V13, P682
   Yang T., 2012, NIPS, P485
   Yang Yun, 2015, ARXIV150106195
   Ying Y., 2009, P ADV NEUR INF PROC, V22, P2205
   Zhang K., 2008, P 25 INT C MACH LEAR, P1232, DOI DOI 10.1145/1390156.1390311
   Zhu R., 2015, ARXIV150905111
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973705016
DA 2019-06-15
ER

PT S
AU Chen, JX
   Yang, L
   Zhang, YZ
   Alber, M
   Chen, DZ
AF Chen, Jianxu
   Yang, Lin
   Zhang, Yizhe
   Alber, Mark
   Chen, Danny Z.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Combining Fully Convolutional and Recurrent Neural Networks for 3D
   Biomedical Image Segmentation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Segmentation of 3D images is a fundamental problem in biomedical image analysis. Deep learning (DL) approaches have achieved state-of-the-art segmentation performance. To exploit the 3D contexts using neural networks, known DL segmentation methods, including 3D convolution, 2D convolution on planes orthogonal to 2D image slices, and LSTM in multiple directions, all suffer incompatibility with the highly anisotropic dimensions in common 3D biomedical images. In this paper, we propose a new DL framework for 3D image segmentation, based on a combination of a fully convolutional network (FCN) and a recurrent neural network (RNN), which are responsible for exploiting the intra-slice and inter-slice contexts, respectively. To our best knowledge, this is the first DL framework for 3D image segmentation that explicitly leverages 3D image anisotropism. Evaluating using a dataset from the ISBI Neuronal Structure Segmentation Challenge and in-house image stacks for 3D fungus segmentation, our approach achieves promising results comparing to the known DL-based 3D segmentation approaches.
C1 [Chen, Jianxu; Yang, Lin; Zhang, Yizhe; Alber, Mark; Chen, Danny Z.] Univ Notre Dame, Notre Dame, IN 46556 USA.
RP Chen, JX (reprint author), Univ Notre Dame, Notre Dame, IN 46556 USA.
EM jchen16@nd.edu; lyang5@nd.edu; yzhang29@nd.edu; malber@nd.edu;
   dchen@nd.edu
FU NSF [CCF-1217906, CCF-1617735]; NIH [R01-GM095959, U01-HL116330]
FX This research was support in part by NSF Grants CCF-1217906 and
   CCF-1617735 and NIH Grants R01-GM095959 and U01-HL116330. Also, we would
   like to thank Dr. Viorica Patraucean at University of Cambridge (UK) for
   discussion of BDC-LSTM, and Prof. David P. Hughes and Dr. Maridel
   Fredericksen at Pennsylvania State University (US) for providing the 3D
   fungus datasets.
CR Cardona A, 2010, PLOS BIOL, V8, DOI 10.1371/journal.pbio.1000502
   Chen H., 2016, ARXIV160402677
   Chen H, 2016, PROCEEDINGS OF THE ASME 35TH INTERNATIONAL CONFERENCE ON OCEAN, OFFSHORE AND ARCTIC ENGINEERING , 2016, VOL 2
   Ciresan D., 2012, ADV NEURAL INFORM PR, P2843
   Collobert R, 2011, NIPS WORKSH
   Dauphin Y N., 2015, ARXIV150204390
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Lai M, 2015, ARXIV150502000
   Lee K., 2015, NIPS, P3559
   Leonard  N., 2015, ARXIV151107889
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Patraucean V, 2015, ARXIV151106309
   Prasoon A, 2013, LECT NOTES COMPUT SC, V8150, P246, DOI 10.1007/978-3-642-40763-5_31
   Ronneberger O., 2015, INT C MED IM COMP CO, P234, DOI [DOI 10.1007/978-3-319-24574-4_28, 10.1007/978-3-319-24574-4_28]
   Shi X, 2015, ARXIV150604214
   Stollenga M. F., 2015, ADV NEURAL INFORM PR, P2980
   Sun C., 2015, ARXIV151103776
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703084
DA 2019-06-15
ER

PT S
AU Chen, JC
   Sun, H
   Woodruff, DP
   Zhang, Q
AF Chen, Jiecao
   Sun, He
   Woodruff, David P.
   Zhang, Qin
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Communication-Optimal Distributed Clustering
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Clustering large datasets is a fundamental problem with a number of applications in machine learning. Data is often collected on different sites and clustering needs to be performed in a distributed manner with low communication. We would like the quality of the clustering in the distributed setting to match that in the centralized setting for which all the data resides on a single site. In this work, we study both graph and geometric clustering problems in two distributed models: (1) a point-to-point model, and (2) a model with a broadcast channel. We give protocols in both models which we show are nearly optimal by proving almost matching communication lower bounds. Our work highlights the surprising power of a broadcast channel for clustering problems; roughly speaking, to spectrally cluster n points or n vertices in a graph distributed across s servers, for a worst-case partitioning the communication complexity in a point-to-point model is n . s, while in the broadcast model it is n + s. A similar phenomenon holds for the geometric setting as well. We implement our algorithms and demonstrate this phenomenon on real life datasets, showing that our algorithms are also very efficient in practice.
C1 [Chen, Jiecao; Zhang, Qin] Indiana Univ, Bloomington, IN 47401 USA.
   [Sun, He] Univ Bristol, Bristol BS8 1UB, Avon, England.
   [Woodruff, David P.] IBM Res Almaden, San Jose, CA 95120 USA.
RP Chen, JC (reprint author), Indiana Univ, Bloomington, IN 47401 USA.
EM jiecchen@indiana.edu; h.sun@bristol.ac.uk; dpwoodru@us.ibm.com;
   qzhangcs@indiana.edu
FU NSF [CCF-1525024, IIS-1633215]; XDATA program of the Defense Advanced
   Research Projects Agency (DARPA), Air Force Research Laboratory
   [FA8750-12-C-0323]
FX Jiecao Chen and Qin Zhang are supported in part by NSF CCF-1525024 and
   IIS-1633215. D.W. thanks support from the XDATA program of the Defense
   Advanced Research Projects Agency (DARPA), Air Force Research Laboratory
   contract FA8750-12-C-0323.
CR Andoni A, 2016, ITCS'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INNOVATIONS IN THEORETICAL COMPUTER SCIENCE, P311, DOI 10.1145/2840728.2840753
   Arthur D, 2007, P 18 ANN ACM SIAM S, P1027, DOI DOI 10.1145/1283383.1283494
   Balcan Maria-Florina, 2014, CORR
   Balcan Maria-Florina, 2013, NIPS, P1995
   Braverman M, 2013, ANN IEEE SYMP FOUND, P668, DOI 10.1109/FOCS.2013.77
   Charikar M, 2001, SIAM PROC S, P642
   Cohen M. B., 2015, P 47 ANN ACM S THEOR, P163, DOI DOI 10.1145/2746539.2746569
   Cormode G., 2007, ICDE, P1036
   Feldman D, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1434
   Guha Sudipto, 2017, DISTRIBUTED PA UNPUB
   Korupolu M., 1998, SODA 98, P1
   Lee James R., 2012, STOC, P1117
   LEE YT, 2015, FDN COMP SCI FOCS 20, P250, DOI DOI 10.1109/FOCS.2015.24
   Lotker Z., 2003, P 15 ACM S PAR ALG A, P94
   Miller Gary L., 2012, CORR
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Peng Richard, 2015, P 28 C LEARN THEOR C, P1423
   Phillips JM, 2016, SIAM J COMPUT, V45, P174, DOI 10.1137/15M1007525
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Woodruff David P., 2012, P 44 ANN ACM S THEOR, P941
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704086
DA 2019-06-15
ER

PT S
AU Chen, L
   Karbasi, A
   Crawford, FW
AF Chen, Lin
   Karbasi, Amin
   Crawford, Forrest W.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Estimating the Size of a Large Network and its Communities from a Random
   Sample
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID SCALE-UP; RISK
AB Most real-world networks are too large to be measured or studied directly and there is substantial interest in estimating global network properties from smaller sub-samples. One of the most important global properties is the number of vertices/nodes in the network. Estimating the number of vertices in a large network is a major challenge in computer science, epidemiology, demography, and intelligence analysis. In this paper we consider a population random graph G = (V, E) from the stochastic block model (SBM) with K communities/blocks. A sample is obtained by randomly choosing a subset W subset of V and letting G (W) be the induced subgraph in G of the vertices in W. In addition to G (W), we observe the total degree of each sampled vertex and its block membership. Given this partial information, we propose an efficient PopULation Size Estimation algorithm, called PULSE, that accurately estimates the size of the whole population as well as the size of each community. To support our theoretical analysis, we perform an exhaustive set of experiments to study the effects of sample size, K, and SBM model parameters on the accuracy of the estimates. The experimental results also demonstrate that PULSE significantly outperforms a widely-used method called the network scale-up estimator in a wide variety of scenarios.
C1 [Chen, Lin; Karbasi, Amin] Yale Univ, Dept Elect Engn, New Haven, CT 06520 USA.
   [Chen, Lin; Karbasi, Amin; Crawford, Forrest W.] Yale Univ, Yale Inst Network Sci, New Haven, CT 06520 USA.
   [Crawford, Forrest W.] Yale Univ, Dept Biostat, New Haven, CT USA.
RP Chen, L (reprint author), Yale Univ, Dept Elect Engn, New Haven, CT 06520 USA.; Chen, L (reprint author), Yale Univ, Yale Inst Network Sci, New Haven, CT 06520 USA.
EM lin.chen@yale.edu; amin.karbasi@yale.edu; forrest.crawford@yale.edu
FU Google; DARPA [D16AP00046]; NIH from NICHD [DP2HD091799]; NIH from NCATS
   [KL2 TR000140]; NIH from NIMH [P30 MH062294]; Yale Center for Clinical
   Investigation; Yale Center for Interdisciplinary Research on AIDS
FX This research was supported by Google Faculty Research Award, DARPA
   Young Faculty Award (D16AP00046), NIH grants from NICHD DP2HD091799,
   NCATS KL2 TR000140, and NIMH P30 MH062294, the Yale Center for Clinical
   Investigation, and the Yale Center for Interdisciplinary Research on
   AIDS. LC thanks Zheng Wei for his consistent support.
CR Aldous D. J., 1985, EXCHANGEABILITY RELA
   Bernard H., 1988, ESTIMATING NUMBER PE
   Bernard H.R., 2001, CONNECTIONS, V24, P18
   Bernstein M., 2013, P SIGCHI C HUM FACT, P21, DOI [10.1145/2470654.2470658, DOI 10.1145/2470654.2470658]
   Chen L., 2016, ARXIV161008473
   Chen L., 2016, AAAI, P1174
   Crawford FW, 2016, SOCIOL METHODOL, V46, P187, DOI 10.1177/0081175016641713
   Erdos P, 1959, PUBL MATH-DEBRECEN, V6, P290, DOI DOI 10.1234/12345678
   Ezoe S, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0031184
   Feehan DM, 2016, SOCIOL METHODOL, V46, P153, DOI 10.1177/0081175016665425
   Girvan M, 2002, P NATL ACAD SCI USA, V99, P7821, DOI 10.1073/pnas.122653799
   Guo WW, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0070718
   Kadushin C., 2006, J DRUG ISSUES
   Katzir L, 2011, WWW, P597, DOI DOI 10.1145/1963405.1963489
   Killworth PD, 1998, EVALUATION REV, V22, P289, DOI 10.1177/0193841X9802200205
   Maiya AS, 2011, P 17 ACM SIGKDD INT, P105, DOI DOI 10.1145/2020408.2020431
   Massoulie L., 2006, P 25 ANN ACM S PRINC, P123
   Murray B. H., 2000, SIZING THE INTERNET, P3
   Newman MEJ, 2006, P NATL ACAD SCI USA, V103, P8577, DOI 10.1073/pnas.0601602103
   Papagelis M, 2013, IEEE T KNOWL DATA EN, V25, P662, DOI 10.1109/TKDE.2011.254
   Ribeiro B., 2010, IMC 10, P390, DOI DOI 10.1145/1879141.1879192
   Salganik MJ, 2011, AM J EPIDEMIOL, V174, P1190, DOI 10.1093/aje/kwr246
   Shokoohi M, 2012, INT J PREVENTIVE MED, V3, P471
   Xing S, 2003, IEEE J SEL AREA COMM, V21, P922, DOI 10.1109/JSAC.2003.814510
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701086
DA 2019-06-15
ER

PT S
AU Chen, S
   Banerjee, A
AF Chen, Sheng
   Banerjee, Arindam
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Structured Matrix Recovery via the Generalized Dantzig Selector
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID INEQUALITIES
AB In recent years, structured matrix recovery problems have gained considerable attention for its real world applications, such as recommender systems and computer vision. Much of the existing work has focused on matrices with low-rank structure, and limited progress has been made on matrices with other types of structure. In this paper we present non-asymptotic analysis for estimation of generally structured matrices via the generalized Dantzig selector based on sub-Gaussian measurements. We show that the estimation error can always be succinctly expressed in terms of a few geometric measures such as Gaussian widths of suitable sets associated with the structure of the underlying true matrix. Further, we derive general bounds on these geometric measures for structures characterized by unitarily invariant norms, a large family covering most matrix norms of practical interest. Examples are provided to illustrate the utility of our theoretical development.
C1 [Chen, Sheng; Banerjee, Arindam] Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.
RP Chen, S (reprint author), Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.
EM shengc@cs.umn.edu; banerjee@cs.umn.edu
FU NSF [IIS-1563950, IIS-1447566, IIS-1447574, IIS-1422557, CCF-1451986,
   CNS-1314560, IIS-0953274, IIS-1029711]; NASA [NNX12AQ39A]
FX The research was supported by NSF grants IIS-1563950, IIS-1447566,
   IIS-1447574, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274,
   IIS-1029711, NASA grant NNX12AQ39A, and gifts from Adobe, IBM, and
   Yahoo.
CR Amelunxen D, 2014, INF INFERENCE, V3, P224, DOI 10.1093/imaiai/iau005
   Argyriou A., 2012, NIPS
   Banerjee A., 2014, NIPS
   Bhatia R., 1997, MATRIX ANAL
   Bourgain J, 2015, GEOM FUNCT ANAL, V25, P1009, DOI 10.1007/s00039-015-0332-9
   Cai T. T., 2014, ARXIV14044408
   Cai TT, 2015, ANN STAT, V43, P102, DOI 10.1214/14-AOS1267
   Candes E, 2012, COMMUN ACM, V55, P111, DOI 10.1145/2184319.2184343
   Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Candes EJ, 2011, IEEE T INFORM THEORY, V57, P2342, DOI 10.1109/TIT.2011.2111771
   Chandrasekaran V, 2012, FOUND COMPUT MATH, V12, P805, DOI 10.1007/s10208-012-9135-7
   Chatterjee S., 2014, ADV NEURAL INFORM PR
   Chen S., 2015, NIPS, P2908
   Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S003614450037906X
   Dirksen S., 2014, ARXIV14023973
   Dirksen S., 2015, ELECTRON J PROBAB, P20
   Figueiredo M. A. T., 2016, AISTATS
   GORDON Y, 1988, LECT NOTES MATH, V1317, P84
   GORDON Y, 1985, ISR J MATH, V50, P265, DOI 10.1007/BF02759761
   Gunasekar S., 2015, P ADV NEUR INF PROC, V28, P1180
   Gunasekar S., 2014, INT C MACH LEARN ICM
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   LEWIS A. S., 1995, J CONVEX ANAL, V2, P173
   McDonald A. M., 2014, NIPS
   Mendelson S, 2007, GEOM FUNCT ANAL, V17, P1248, DOI 10.1007/s00039-007-0618-7
   Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400
   Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835
   SIVAKUMAR V., 2015, ADV NEURAL INFORM PR, P2206
   Talagrand  M., 2014, UPPER LOWER BOUNDS S
   Talagrand M., 1992, GEOM FUNCT ANAL, V2, P118
   VERSHYNIN R., 2012, COMPRESSED SENSING T, P210, DOI DOI 10.1017/CBO9780511794308.006
   Zhang X., 2013, NIPS
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
   Zuk O., 2015, INT C MACH LEARN ICM
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703030
DA 2019-06-15
ER

PT S
AU Chen, WF
   Fu, Z
   Yang, DW
   Deng, J
AF Chen, Weifeng
   Fu, Zhao
   Yang, Dawei
   Deng, Jia
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Single-Image Depth Perception in the Wild
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB This paper studies single-image depth perception in the wild, i.e., recovering depth from a single image taken in unconstrained settings. We introduce a new dataset "Depth in the Wild" consisting of images in the wild annotated with relative depth between pairs of random points. We also propose a new algorithm that learns to estimate metric depth using annotations of relative depth. Compared to the state of the art, our algorithm is simpler and performs better. Experiments show that our algorithm, combined with existing RGB-D data and our new relative depth annotations, significantly improves single-image depth perception in the wild.
   [GRAPHICS]
   .
C1 [Chen, Weifeng; Fu, Zhao; Yang, Dawei; Deng, Jia] Univ Michigan, Ann Arbor, MI 48109 USA.
RP Chen, WF (reprint author), Univ Michigan, Ann Arbor, MI 48109 USA.
EM wfchen@umich.edu; zhaofu@umich.edu; ydawei@umich.edu; jiadeng@umich.edu
FU National Science Foundation [1617767]
FX This work is partially supported by the National Science Foundation
   under Grant No. 1617767.
CR Baig M. H., 2015, ARXIV150104537, P5
   Baig M. H., 2014, WACV
   Barron Jonathan T, 2015, TPAMI
   Bell S., 2014, TOG
   Cao Z., 2007, ICML
   Chiu W. W.-C., 2011, BMVC
   Choi S., 2016, ARXIV160202481
   Eigen D., 2015, ICCV
   Eigen David, 2014, NIPS
   Geiger A, 2013, INT J ROBOT RES
   Hane C., 2015, CVPR
   Hoiem D., 2005, TOG
   Janoch A., 2013, CONSUMER DEPTH CAMER
   Joachims Thorsten, 2002, P 8 ACM SIGKDD INT C
   Karsch K., 2014, TPAMI
   Ladicky L., 2014, CVPR
   Li B., 2015, CVPR
   Liu B., 2010, CVPR
   Liu  F., 2015, CVPR
   Long  J., 2015, CVPR
   Narihira T., 2015, CVPR
   Newell A., 2016, ARXIV160306937
   Parikh D., 2011, ICCV
   Saxena A., 2005, NIPS
   Saxena A., 2008, IJCV
   Saxena A., 2009, TPAMI
   Sermanet P., 2013, COMPUT VIS PATTERN R, V1312, P6229, DOI DOI 10.1109/CVPR.2015.7299176.ARXIV:1312.6229
   Shelhamer E., 2015, ICCV WORKSH
   Shi J., 2015, TOG
   Silberman  N., 2012, ECCV
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Song Shuran, 2015, CVPR
   Szegedy C., 2015, CVPR
   Todd JT, 2003, PERCEPT PSYCHOPHYS, V65, P31, DOI 10.3758/BF03194781
   Wang  P., 2015, CVPR
   Xie S. M., 2015, CORR
   Xiong Y., 2015, TPAMI
   Zhang Z., 2015, ICCV
   Zhou T., 2015, ICCV
   Zhuo W., 2015, CVPR
   Zoran D., 2015, ICCV
NR 41
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704014
DA 2019-06-15
ER

PT S
AU Chen, X
   Cheng, Y
   Tang, B
AF Chen, Xi
   Cheng, Yu
   Tang, Bo
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI On the Recursive Teaching Dimension of VC Classes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB The recursive teaching dimension (RTD) of a concept class C subset of {0, 1}(n), introduced by Zilles et al. [ZLHZ11], is a complexity parameter measured by the worst-case number of labeled examples needed to learn any target concept of C in the recursive teaching model. In this paper, we study the quantitative relation between RTD and the well-known learning complexity measure VC dimension (VCD), and improve the best known upper and (worst-case) lower bounds on the recursive teaching dimension with respect to the VC dimension.
   Given a concept class C subset of {0,1}(n) with VCD(C) = d, we first show that RTD(C) is at most d . 2(d+1). This is the first upper bound for RTD(C) that depends only on VCD(C), independent of the size of the concept class vertical bar C vertical bar and its domain size n. Before our work, the best known upper bound for RTD(C) is 0(d2(d) log log vertical bar C vertical bar), obtained by Moran et al. [MSWY15]. We remove the log log vertical bar C vertical bar factor. We also improve the lower bound on the worst-case ratio of RTD(C) to VCD(C). We present a family of classes {C-k}(k >= 1) with VCD(C-k) = 3k and RTD (C-k) = 5k, which implies that the ratio of RTD(C) to VCD(C) in the worst case can be as large as 5/3. Before our work, the largest ratio known was 3/2 as obtained by Kuhlmann [Kuh99]. Since then, no finite concept class C has been known to satisfy RTD(C) > (3/2) . VCD(C).
C1 [Chen, Xi] Columbia Univ, Dept Comp Sci, New York, NY 10027 USA.
   [Cheng, Yu] Univ Southern Calif, Dept Comp Sci, Los Angeles, CA 90089 USA.
   [Tang, Bo] Univ Oxford, Dept Comp Sci, Oxford, England.
RP Chen, X (reprint author), Columbia Univ, Dept Comp Sci, New York, NY 10027 USA.
EM xichen@cs.columbia.edu; yu.cheng.1@usc.edu; tangbonk1@gmail.com
FU NSF [CCF-1149257, CCF-1423100]; Shang-Hua Teng's Simons Investigator
   Award; ERC [321171]
FX We thank the anonymous reviewers for their helpful comments and
   suggestions. We also thank Joseph Bebel for pointing us to the SAT
   solvers. This work was done in part while the authors were visiting the
   Simons Institute for the Theory of Computing. Xi Chen is supported by
   NSF grants CCF-1149257 and CCF-1423100. Yu Cheng is supported in part by
   Shang-Hua Teng's Simons Investigator Award. Bo Tang is supported by ERC
   grant 321171.
CR Audemard G., 2014, GLUCOSE 4 0
   Biere A., 2015, LINGELING PLINGELING
   BLUMER A, 1989, J ACM, V36, P929, DOI 10.1145/76359.76371
   Darnstadt M, 2016, THEOR COMPUT SCI, V620, P73, DOI 10.1016/j.tcs.2015.10.038
   Doliwa T, 2014, J MACH LEARN RES, V15, P3107
   Doliwa T, 2010, LECT NOTES ARTIF INT, V6331, P209, DOI 10.1007/978-3-642-16108-7_19
   Een N, 2004, LECT NOTES COMPUT SC, V2919, P502
   GOLDMAN SA, 1995, J COMPUT SYST SCI, V50, P20, DOI 10.1006/jcss.1995.1003
   Kuhlmann C, 1999, LECT NOTES ARTIF INT, V1572, P168
   Littlestone  N., 1986, TECHNICAL REPORT
   Moran S, 2015, ANN IEEE SYMP FOUND, P40, DOI 10.1109/FOCS.2015.12
   Samei R, 2014, THEOR COMPUT SCI, V558, P35, DOI 10.1016/j.tcs.2014.09.024
   Shinohara A., 1990, Algorithmic Learning Theory, P247
   Simon H.-U., 2015, P 28 ANN C LEARN THE, P1770
   VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025
   Warmuth MK, 2003, LECT NOTES ARTIF INT, V2777, P743, DOI 10.1007/978-3-540-45167-9_60
   Wigderson A, 2012, ANN IEEE SYMP FOUND, P390, DOI 10.1109/FOCS.2012.14
   Zilles S, 2011, J MACH LEARN RES, V12, P349
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 8
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703048
DA 2019-06-15
ER

PT S
AU Chen, X
   Duan, Y
   Houthooft, R
   Schulman, J
   Sutskever, I
   Abbeel, P
AF Chen, Xi
   Duan, Yan
   Houthooft, Rein
   Schulman, John
   Sutskever, Ilya
   Abbeel, Pieter
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI InfoGAN: Interpretable Representation Learning by Information Maximizing
   Generative Adversarial Nets
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID ALGORITHM
AB This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound of the mutual information objective that can be optimized efficiently. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing supervised methods. For an up-to-date version of this paper, please see https://arxiv.org/abs/1606.03657.
C1 [Chen, Xi; Duan, Yan; Houthooft, Rein; Schulman, John; Abbeel, Pieter] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
   [Chen, Xi; Duan, Yan; Houthooft, Rein; Schulman, John; Sutskever, Ilya; Abbeel, Pieter] OpenAI, San Francisco, CA 94110 USA.
RP Chen, X (reprint author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.; Chen, X (reprint author), OpenAI, San Francisco, CA 94110 USA.
FU ONR through a PECASE award; Berkeley AI Research lab Fellowship; Huawei
   Fellowship; Ph.D. Fellowship of the Research Foundation - Flanders (FWO)
FX We thank the anonymous reviewers. This research was funded in part by
   ONR through a PECASE award. Xi Chen was also supported by a Berkeley AI
   Research lab Fellowship. Yan Duan was also supported by a Berkeley AI
   Research lab Fellowship and a Huawei Fellowship. Rein Houthooft was
   supported by a Ph.D. Fellowship of the Research Foundation - Flanders
   (FWO).
CR Aubry M, 2014, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2014.487
   Barber D., 2005, ADV NEURAL INFORM PR, P17
   Barber David, 2003, NIPS
   Bengio Y., 2013, PATTERN ANAL MACHINE, V35
   Bengio Y., 2009, FDN TRENDS MACHINE L
   Bridle J. S., 1992, NIPS
   Cheung B., 2014, ARXIV14126583
   Comes R., 2010, ADV NEURAL INFORM PR, P775
   Desjardins G., 2012, ARXIV12105474
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Kingma D. P., 2014, ADV NEURAL INFORM PR, V4, P3581
   Kingma D.P., 2013, ARXIV13126114
   Kulkarni T. D., 2015, ADV NEURAL INFORM PR, P2530
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   Liu  Z., 2015, ICCV
   Maaloe L., 2016, ICML
   Makhzani A., 2015, ARXIV151105644
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Radford A., 2015, ARXIV151106434
   Rasmus A, 2015, ADV NEURAL INFORM PR, P3532
   Reed Scott, 2014, P 31 INT C MACH LEAR, V32, P1431
   Springenberg Jost Tobias, 2015, ARXIV151106390
   Susskind J., 2010, TECH REP
   Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349
   Vincent P., 2008, P 25 INT C MACH LEAR, V2008, P1096, DOI DOI 10.1145/1390156.1390294
   Whitney W. F., 2016, ARXIV160206822
   Yang J., 2015, ADV NEURAL INFORM PR, P1099
   Zhu Z., 2014, ADV NEURAL INFORM PR, P217
NR 31
TC 0
Z9 0
U1 4
U2 4
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703035
DA 2019-06-15
ER

PT S
AU Chen, XY
   Liu, C
   Shin, R
   Song, D
   Chen, MC
AF Chen, Xinyun
   Liu, Chang
   Shin, Richard
   Song, Dawn
   Chen, Mingcheng
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Latent Attention For If-Then Program Synthesis
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Automatic translation from natural language descriptions into programs is a long-standing challenging problem. In this work, we consider a simple yet important sub-problem: translation from textual descriptions to If-Then programs. We devise a novel neural network architecture for this task which we train end-toend. Specifically, we introduce Latent Attention, which computes multiplicative weights for the words in the description in a two-stage process with the goal of better leveraging the natural language structures that indicate the relevant parts for predicting program elements. Our architecture reduces the error rate by 28.57% compared to prior art [3]. We also propose a one-shot learning scenario of If-Then program synthesis and simulate it with our existing dataset. We demonstrate a variation on the training procedure for this scenario that outperforms the original procedure, significantly closing the gap to the model trained with all data.
C1 [Chen, Xinyun] Shanghai Jiao Tong Univ, Shanghai, Peoples R China.
   [Liu, Chang; Shin, Richard; Song, Dawn] Univ Calif Berkeley, Berkeley, CA USA.
   [Chen, Mingcheng] UIUC, Champaign, IL USA.
RP Chen, XY (reprint author), Shanghai Jiao Tong Univ, Shanghai, Peoples R China.
FU National Science Foundation [TWC-1409915]; DARPA grant
   [FA8750-15-2-0104]
FX We thank the anonymous reviewers for their valuable comments. This
   material is based upon work partially supported by the National Science
   Foundation under Grant No. TWC-1409915, and a DARPA grant
   FA8750-15-2-0104. Any opinions, findings, and conclusions or
   recommendations expressed in this material are those of the author(s)
   and do not necessarily reflect the views of the National Science
   Foundation and DARPA.
CR Artzi Y., 2015, EMNLP
   Bahdanau D., 2014, ARXIV14090473
   Beltagy I., 2016, ACL
   Berant J., 2013, EMNLP
   Branavan S. R., 2009, ACL
   Chung J., 2014, CORR
   Dong L., 2016, ACL
   Gulwani S., 2014, SIGMOD
   Jones B. K., 2012, ACL
   Kate R. J., 2005, AAAI
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kushman N., 2013, NAACL
   Le V., 2013, MOBISYS
   Lei T., 2013, ACL
   Ling W., 2016, CORR
   Quirk C., 2015, ACL
   Sukhbaatar S., 2015, NIPS
   Vinyals O., 2015, NIPS
   Wong Y. W., 2006, NAACL
   Xu K, 2015, ARXIV150203044
   Zaremba W, 2014, ARXIV14092329
   Zelle J. M, 1996, AAAI
NR 22
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702031
DA 2019-06-15
ER

PT S
AU Cheng, CA
   Boots, B
AF Cheng, Ching-An
   Boots, Byron
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Incremental Variational Sparse Gaussian Process Regression
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Recent work on scaling up Gaussian process regression (GPR) to large datasets has primarily focused on sparse GPR, which leverages a small set of basis functions to approximate the full Gaussian process during inference. However, the majority of these approaches are batch methods that operate on the entire training dataset at once, precluding the use of datasets that are streaming or too large to fit into memory. Although previous work has considered incrementally solving variational sparse GPR, most algorithms fail to update the basis functions and therefore perform suboptimally. We propose a novel incremental learning algorithm for variational sparse GPR based on stochastic mirror ascent of probability densities in reproducing kernel Hilbert space. This new formulation allows our algorithm to update basis functions online in accordance with the manifold structure of probability densities for fast convergence. We conduct several experiments and show that our proposed approach achieves better empirical performance in terms of prediction error than the recent state-of-the-art incremental solutions to variational sparse GPR.
C1 [Cheng, Ching-An; Boots, Byron] Georgia Inst Technol, Inst Robot & Intelligent Machines, Atlanta, GA 30332 USA.
RP Cheng, CA (reprint author), Georgia Inst Technol, Inst Robot & Intelligent Machines, Atlanta, GA 30332 USA.
EM cacheng@gatech.edu; bboots@cc.gatech.edu
CR Abdel-Gawad Ahmed H, 2012, ARXIV12033507
   Alexander G, 2016, P 19 INT C ART INT S
   Alvarez A.M., 2009, ADV NEURAL INFORM PR, V21, P57
   Alvarez M. A., 2010, INT C ART INT STAT, V9, P25
   Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746
   Banerjee A, 2005, J MACH LEARN RES, V6, P1705
   Csato L, 2002, NEURAL COMPUT, V14, P641, DOI 10.1162/089976602317250933
   Dai Bo, 2015, ARXIV150603101
   Figueiras-vidal Anibal, 2009, ADV NEURAL INFORM PR, P1087
   Hensman J, 2013, ARXIV13096835
   Hoang T. N., 2015, P 32 INT C MACH LEAR, P569
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Holmes I, 2015, INFIN DIMENS ANAL QU, V18, DOI 10.1142/S0219025715500198
   Khan Mohammad E, 2015, ADV NEURAL INFORM PR, P3384
   Meier F., 2014, ADV NEURAL INFORM PR, P972
   Micchelli CA, 2006, J MACH LEARN RES, V7, P2651
   Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277
   Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939
   Raskutti G, 2015, IEEE T INFORM THEORY, V61, P1451, DOI 10.1109/TIT.2015.2388583
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Seeger M., 2003, ARTIFICIAL INTELLIGE, V9
   Sheth Rishit, 2015, P 32 INT C MACH LEAR, P1302
   Snelson E., 2005, ADV NEURAL INFORM PR, V18, P1257
   Snelson E., 2007, INT C ART INT STAT, P524
   Theis Lucas, 2015, ARXIV150507649
   Titsias M, 2009, ARTIF INTELL, P567
   Walder C., 2008, P 25 INT C MACH LEAR, P1112
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703109
DA 2019-06-15
ER

PT S
AU Cheng, DH
   Peng, R
   Perros, I
   Liu, Y
AF Cheng, Dehua
   Peng, Richard
   Perros, Ioakeim
   Liu, Yan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI SPALS: Fast Alternating Least Squares via Implicit Leverage Scores
   Sampling
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Tensor CANDECOMP/PARAFAC (CP) decomposition is a powerful but computationally challenging tool in modern data analytics. In this paper, we show ways of sampling intermediate steps of alternating minimization algorithms for computing low rank tensor CP decompositions, leading to the sparse alternating least squares (SPALS) method. Specifically, we sample the Khatri-Rao product, which arises as an intermediate object during the iterations of alternating least squares. This product captures the interactions between different tensor modes, and form the main computational bottleneck for solving many tensor related tasks. By exploiting the spectral structures of the matrix Khatri-Rao product, we provide efficient access to its statistical leverage scores. When applied to the tensor CP decomposition, our method leads to the first algorithm that runs in sublinear time per-iteration and approximates the output of deterministic alternating least squares algorithms. Empirical evaluations of this approach show significant speedups over existing randomized and deterministic routines for performing CP decomposition. On a tensor of the size 2.4m x 6.6m x 92k with over 2 billion nonzeros formed by Amazon product reviews, our routine converges in two minutes to the same error as deterministic ALS.
C1 [Cheng, Dehua; Liu, Yan] Univ Southern Calif, Los Angeles, CA 90089 USA.
   [Peng, Richard; Perros, Ioakeim] Georgia Inst Technol, Atlanta, GA 30332 USA.
RP Cheng, DH (reprint author), Univ Southern Calif, Los Angeles, CA 90089 USA.
EM dehua.cheng@usc.edu; rpeng@cc.gatech.edu; perros@gatech.edu;
   yanliu.cs@usc.edu
FU U.S. Army Research Office [W911NF-15-1-0491]; NSF [IIS-1254206,
   IIS-1134990]
FX This work is supported in part by the U.S. Army Research Office under
   grant number W911NF-15-1-0491, NSF Research Grant IIS-1254206 and
   IIS-1134990. The views and conclusions are those of the authors and
   should not be interpreted as representing the official policies of the
   funding agency, or the U.S. Government.
CR Barak B., 2015, STOC
   Bhojanapalli S., 2015, ARXIV E PRINTS
   Carroll J. D., 1970, PSYCHOMETRIKA
   Cheng D., 2015, ARXIV150203496
   Clarkson K. L., 2015, FOCS
   Clarkson Kenneth L, 2013, STOC
   Clarkson Kenneth L., 2015, SODA
   Cohen M. B., 2015, ITCS
   Cohen Michael B, 2015, STOC
   Dasgupta A., 2009, SIAM J COMPUTING
   De Lathauwer L., 1998, I MATH ITS APPL C SE
   De Lathauwer Lieven, 2000, SIAM J MATRIX ANAL A
   De Silva V., 2008, SIAM J MATRIX ANAL A
   Drineas P., 2011, NUMERISCHE MATH
   Ge  R., 2015, COLT
   Harshman RA, 1970, FDN PARAFAC PROCEDUR
   Hillar C. J., 2013, J ACM JACM
   Jeon I., 2015, ICDE
   Kang U., 2012, KDD
   Kolda T. G., 2009, SIAM REV
   Li M., 2013, FOCS
   Mahoney M. W., 2011, FDN TRENDS MACHINE L
   McAuley Julian, 2013, RECSYS
   Meng X., 2013, STOC
   Needell D., 2014, NIPS
   Nelson J., 2013, FOCS
   Nguyen N. H., 2010, CORR
   Novikov A., 2015, NIPS
   Papalexakis E. E., 2012, MACHINE LEARNING KNO
   Pham N., 2013, KDD
   Phan A.-H., 2013, SIGNAL PROCESSING IE
   Smith  S., 2015, 29 IEEE INT PAR DIST
   Strohmer T., 2009, JFAA
   Sun J., 2009, SDM
   Tsourakakis C. E., 2010, SDM
   Wang  Y., 2015, NIPS
   Woodruff D. P., 2014, FDN TRENDS THEORETIC
   Yang J., 2016, SODA
   Yu R., 2015, JMLR P, P238
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703072
DA 2019-06-15
ER

PT S
AU Cheng, TY
   Lin, KH
   Gong, XY
   Liu, KJ
   Wu, SH
AF Cheng, Ting-Yu
   Lin, Kuan-Hua
   Gong, Xinyang
   Liu, Kang-Jun
   Wu, Shan-Hung
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning User Perceived Clusters with Feature-Level Supervision
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Semi-supervised clustering algorithms have been proposed to identify data clusters that align with user perceived ones via the aid of side information such as seeds or pairwise constrains. However, traditional side information is mostly at the instance level and subject to the sampling bias, where non-randomly sampled instances in the supervision can mislead the algorithms to wrong clusters. In this paper, we propose learning from the feature-level supervision. We show that this kind of supervision can be easily obtained in the form of perception vectors in many applications. Then we present novel algorithms, called Perception Embedded (PE) clustering, that exploit the perception vectors as well as traditional side information to find clusters perceived by the user. Extensive experiments are conducted on real datasets and the results demonstrate the effectiveness of PE empirically.
EM tycheng@datalab.cs.nthu.edu.tw; khlin@datalab.cs.nthu.edu.tw;
   xygong@datalab.cs.nthu.edu.tw; kjliu@datalab.cs.nthu.edu.tw;
   shwu@cs.nthu.edu.tw
CR Banerjee A., 2005, P 11 ACM SIGKDD INT, P532, DOI DOI 10.1145/1081870.1081932
   Bar-Hillel A., 2003, P INT C MACH LEARN, P11
   Basu S., 2004, P 10 ACM SIGKDD INT, P59, DOI DOI 10.1145/1014052.1014062
   Bhatia SK, 1998, IEEE T SYST MAN CY B, V28, P427, DOI 10.1109/3477.678640
   Bilenko M, 2003, P 9 ACM SIGKDD INT C, P39, DOI DOI 10.1145/956750.956759
   Chua T.-S., 2009, P ACM INT C IM VID R, P48, DOI [10.1145/1646396.1646452, DOI 10.1145/1646396.1646452]
   Cleuziou G, 2008, INT C PATT RECOG, P563
   Davidson I, 2012, 18 ACM SIGKDD INT C, P1312
   Demiriz Ayhan, 1999, ARTIF NEURAL NETW EN, P809
   Ding C., 2006, P 12 ACM SIGKDD INT, P126, DOI DOI 10.1145/1150402.1150420
   Finley T., 2005, INT C MACH LEARN, P217, DOI DOI 10.1145/1102351.1102379
   Frigui H, 1999, IEEE T PATTERN ANAL, V21, P450, DOI 10.1109/34.765656
   He XN, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P771, DOI 10.1145/2566486.2567975
   Jain AK, 2010, PATTERN RECOGN LETT, V31, P651, DOI 10.1016/j.patrec.2009.09.011
   Klein D., 2002, P 19 INT C MACH LEAR, P307
   Li Q, 2003, IEEE/WIC INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE, PROCEEDINGS, P33
   Li Z., 2008, ICML, P576
   Li ZG, 2009, IEEE I CONF COMP VIS, P421, DOI 10.1109/ICCV.2009.5459157
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Long M., 2012, P 12 SIAM SDM, V2012, P540
   Lu  Z., 2004, ADV NEURAL INFORM PR, P849
   Nie FP, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1181
   Poon L. K. M., 2010, ICML 10, P887
   Sadikov Eldar, 2010, P 19 INT C WORLD WID, P841, DOI DOI 10.1145/1772690.1772776
   Schedl M, 2013, P 4 ACM MULT SYST C, P78, DOI [10.1145/2483977.2483985, DOI 10.1145/2483977.2483985]
   Schultz  M., 2004, P NIPS
   Sugato Basu R. J. M., 2002, P ICML, P27
   Wagstaff K., 2001, P 18 INT C MACH LEAR, V18, P577, DOI 10.1109/TPAMI.2002.1017616
   Wagstaff  L., 2000, P 17 INT C MACH LEAR, P1103
   Xiaoyong Liu, 2004, Proceedings of Sheffield SIGIR 2004. The Twenty-Seventh Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P186
   Xing E.P., 2002, ADV NEURAL INFORM PR, P505
   Yi S., 2013, P 30 INT C MACH LEAR, P1400
   Yue YS, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P75, DOI 10.1145/2566486.2567991
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702007
DA 2019-06-15
ER

PT S
AU Choi, E
   Bahadori, MT
   Kulas, JA
   Schuetz, A
   Stewart, WF
   Sun, JM
AF Choi, Edward
   Bahadori, Mohammad Taha
   Kulas, Joshua A.
   Schuetz, Andy
   Stewart, Walter F.
   Sun, Jimeng
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI RETAIN: An Interpretable Predictive Model for Healthcare using Reverse
   Time Attention Mechanism
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID INFORMATION-TECHNOLOGY; COSTS
AB Accuracy and interpretability are two dominant features of successful predictive models. Typically, a choice must be made in favor of complex black box models such as recurrent neural networks (RNN) for accuracy versus less accurate but more interpretable traditional models such as logistic regression. This tradeoff poses challenges in medicine where both accuracy and interpretability are important. We addressed this challenge by developing the REverse Time AttentIoN model (RETAIN) for application to Electronic Health Records (EHR) data. RETAIN achieves high accuracy while remaining clinically interpretable and is based on a two-level neural attention model that detects influential past visits and significant clinical variables within those visits (e.g. key diagnoses). RETAIN mimics physician practice by attending the EHR data in a reverse time order so that recent clinical visits are likely to receive higher attention. RETAIN was tested on a large health system EHR dataset with 14 million visits completed by 263K patients over an 8 year period and demonstrated predictive accuracy and computational scalability comparable to state-of-the-art methods such as RNN, and ease of interpretability comparable to traditional models.
C1 [Choi, Edward; Bahadori, Mohammad Taha; Kulas, Joshua A.; Sun, Jimeng] Georgia Inst Technol, Atlanta, GA 30332 USA.
   [Schuetz, Andy; Stewart, Walter F.] Sutter Hlth, Sacramento, CA USA.
RP Choi, E (reprint author), Georgia Inst Technol, Atlanta, GA 30332 USA.
EM mp2893@gatech.edu; bahadori@gatech.edu; jkulas3@gatech.edu;
   schueta1@sutterhealth.org; stewarwf@sutterhealth.org; jsun@cc.gatech.edu
CR Ba  Jimmy, 2015, ICLR
   Bandanau D., 2015, ICLR
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Bergstra J, 2010, P SCIPY
   Black AD, 2011, PLOS MED, V8, DOI 10.1371/journal.pmed.1000387
   Caruana R., 2015, KDD
   Chaudhry B, 2006, ANN INTERN MED, V144, P742, DOI 10.7326/0003-4819-144-10-200605160-00125
   Che Z., 2015, ARXIV151203542
   Cho  K., 2014, EMNLP
   Choi  E., 2016, KDD
   Choi E, 2015, ARXIV151105942
   Chorowski JK, 2015, ADV NEURAL INFORM PR, P577
   Erhan D., 2009, VISUALIZING HIGHER L
   Esteban C., 2016, ARXIV160202685
   Fleisher AS, 2007, NEUROLOGY, V68, P1588, DOI 10.1212/01.wnl.0000258542.58725.4c
   Gallego B, 2015, J COMP EFFECT RES, V4, P191, DOI 10.2217/cer.15.12
   Ghosh J., 1992, Proceedings of the SPIE - The International Society for Optical Engineering, P449
   Goldzweig CL, 2009, HEALTH AFFAIR, V28, pW282, DOI 10.1377/hlthaff.28.2.w282
   Gregor K., 2015, ARXIV150204623
   Hermann K. M., 2015, ADV NEURAL INFORM PR, P1684
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Jha A. K., 2009, N ENGL J MED
   Karpathy A., 2015, ARXIV150602078
   Kho AN, 2012, J AM MED INFORM ASSN, V19, P212, DOI 10.1136/amiajnl-2011-000439
   Le Q. V., 2013, ICASSP
   Le Q. V., 2015, ARXIV150400941
   Lipton Zachary C, 2016, ICLR
   Martins A. F. T., 2016, ICML
   Mnih V., 2014, NIPS
   Rush A. M., 2015, EMNLP
   Saria S., 2010, NIPS PREDICTIVE MODE
   Schulam Peter, 2015, AMIA Annu Symp Proc, V2015, P143
   Sun J, 2012, SIGKDD EXPLOR, V14, P16, DOI DOI 10.1145/2408736.2408740
   W. K. C. D. Information, MED SPAN EL DRUG FIL
   Xu K, 2015, ICML
   Zeiler M.D., 2012, ARXIV12125701
NR 36
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702068
DA 2019-06-15
ER

PT S
AU Chowdhury, S
   Memoli, F
   Smith, Z
AF Chowdhury, Samir
   Memoli, Facundo
   Smith, Zane
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Improved Error Bounds for Tree Representations of Metric Spaces
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Estimating optimal phylogenetic trees or hierarchical clustering trees from metric data is an important problem in evolutionary biology and data analysis. Intuitively, the goodness-of-fit of a metric space to a tree depends on its inherent treeness, as well as other metric properties such as intrinsic dimension. Existing algorithms for embedding metric spaces into tree metrics provide distortion bounds depending on cardinality. Because cardinality is a simple property of any set, we argue that such bounds do not fully capture the rich structure endowed by the metric. We consider an embedding of a metric space into a tree proposed by Gromov. By proving a stability result, we obtain an improved additive distortion bound depending only on the hyperbolicity and doubling dimension of the metric. We observe that Gromov's method is dual to the well-known single linkage hierarchical clustering (SLHC) method. By means of this duality, we are able to transport our results to the setting of SLHC, where such additive distortion bounds were previously unknown.
C1 [Chowdhury, Samir; Memoli, Facundo] Ohio State Univ, Dept Math, 231 W 18th Ave, Columbus, OH 43210 USA.
   [Memoli, Facundo; Smith, Zane] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
RP Chowdhury, S (reprint author), Ohio State Univ, Dept Math, 231 W 18th Ave, Columbus, OH 43210 USA.
EM chowdhury.57@osu.edu; memoli@math.osu.edu; smith.9911@osu.edu
CR Abraham Ittai, 2007, P 26 ANN ACM S PRINC
   Abu-Ata Muad, 2014, ARXIV14023364
   Agarwala R, 1999, SIAM J COMPUT, V28, P1073, DOI 10.1137/S0097539795296334
   Bartal Yair, 1996, FDN COMPUTER SCI
   Barthelemy J. -P., 1991, TREES PROXIMITY REPR
   Carlsson Gunnar, 2010, J MACHINE LEARNING R
   Chakerian John, 2012, J COMPUTATIONAL GRAP
   Chepoi V, 2000, J MATH PSYCHOL, V44, P600, DOI 10.1006/jmps.1999.1270
   Deza MM, 2009, ENCY DISTANCES
   Fakcharoenphol J., 2003, P 35 ANN ACM S THEOR, P448, DOI DOI 10.1145/780542.780608
   FARACH M, 1995, ALGORITHMICA, V13, P155, DOI 10.1007/BF01188585
   Gromov Mikhael, 1987, HYPERBOLIC GROUPS
   Indyk P., 2004, HDB DISCRETE COMPUTA, V273, P177
   Kleindessner M., 2014, COLT, P40
   Krauthgamer R., 2004, P 15 ANN ACM SIAM S, P798
   KRIVANEK M, 1988, INFORM PROCESS LETT, V27, P265, DOI 10.1016/0020-0190(88)90090-7
   Li YL, 2006, LECT NOTES COMPUT SC, V3971, P889
   Mardia KV, 1980, MULTIVARIATE ANAL
   Semple C., 2003, PHYLOGENETICS, V24
   Shieh AD, 2011, P NATL ACAD SCI USA, V108, P16916, DOI 10.1073/pnas.1018393108
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703067
DA 2019-06-15
ER

PT S
AU Chu, X
   Ouyang, WL
   Li, HS
   Wang, XG
AF Chu, Xiao
   Ouyang, Wanli
   Li, Hongsheng
   Wang, Xiaogang
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI CRF-CNN: Modeling Structured Information in Human Pose Estimation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Deep convolutional neural networks (CNN) have achieved great success. On the other hand, modeling structural information has been proved critical in many vision problems. It is of great interest to integrate them effectively. In a classical neural network, there is no message passing between neurons in the same layer. In this paper, we propose a CRF-CNN framework which can simultaneously model structural information in both output and hidden feature layers in a probabilistic way, and it is applied to human pose estimation. A message passing scheme is proposed, so that in various layers each body joint receives messages from all the others in an efficient way. Such message passing can be implemented with convolution between features maps in the same layer, and it is also integrated with feed-forward propagation in neural networks. Finally, a neural network implementation of endto-end learning CRF-CNN is provided. Its effectiveness is demonstrated through experiments on two benchmark datasets.
C1 [Chu, Xiao; Ouyang, Wanli; Li, Hongsheng; Wang, Xiaogang] Chinese Univ Hong Kong, Hong Kong, Peoples R China.
RP Chu, X (reprint author), Chinese Univ Hong Kong, Hong Kong, Peoples R China.
EM xchu@ee.cuhk.edu.hk; wlouyang@ee.cuhk.edu.hk; hsli@ee.cuhk.edu.hk;
   xgwang@ee.cuhk.edu.hk
FU SenseTime Group Limited; Research Grants Council of Hong Kong
   [CUHK14206114, CUHK14205615, CUHK14207814, CUHK14203015, CUHK417011];
   National Natural Science Foundation of China [61371192, 61301269]
FX This work is supported by SenseTime Group Limited, Research Grants
   Council of Hong Kong (Project Number CUHK14206114, CUHK14205615,
   CUHK14207814, CUHK14203015, and CUHK417011) and National Natural Science
   Foundation of China (Number 61371192 and 61301269). W. Ouyang and X.
   Wang are the corresponding authors.
CR Chen X., 2014, NIPS
   Chu X, 2015, ICCV
   Chu X, 2016, CVPR
   Dalal  N., 2005, CVPR
   Deng J., 2014, ECCV
   Eslami SM, 2016, ARXIV160308575
   Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49
   Gal Y, 2015, ARXIV150602142
   He K., 2015, ARXIV150201852
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Johnson S., 2010, BMVC
   Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572
   Li H., 2016, CVPR
   Li  W., 2014, CVPR
   Lin G., 2015, NIPS
   Ouyang W, 2014, CVPR
   Pishchulin L., 2015, DEEPCUT JOINT SUBSET
   Sapp B., 2013, CVPR
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Szegedy C., 2015, CVPR
   Tompson J., 2015, CVPR
   Tompson J. J., 2014, NIPS
   Toshev A., 2014, CVPR
   Wan L., 2014, ARXIV14115309
   Wang Y., 2011, CVPR
   Yang W., 2016, CVPR
   Yang Y, 2013, IEEE T PATTERN ANAL, V35, P2878, DOI 10.1109/TPAMI.2012.261
   Zheng S., 2015, ICCV
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702025
DA 2019-06-15
ER

PT S
AU Ciliberto, C
   Rudi, A
   Rosasco, L
AF Ciliberto, Carlo
   Rudi, Alessandro
   Rosasco, Lorenzo
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI A Consistent Regularization Approach for Structured Prediction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We propose and analyze a regularization approach for structured prediction problems. We characterize a large class of loss functions that allows to naturally embed structured outputs in a linear space. We exploit this fact to design learning algorithms using a surrogate loss approach and regularization techniques. We prove universal consistency and finite sample bounds characterizing the generalization properties of the proposed method. Experimental results are provided to demonstrate the practical usefulness of the proposed approach.
C1 [Ciliberto, Carlo; Rudi, Alessandro; Rosasco, Lorenzo] Ist Italiano Tecnol, Lab Computat & Stat Learning, Genoa, Italy.
   [Ciliberto, Carlo; Rudi, Alessandro; Rosasco, Lorenzo] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
   [Rudi, Alessandro; Rosasco, Lorenzo] Univ Genoa, Genoa, Italy.
RP Ciliberto, C (reprint author), Ist Italiano Tecnol, Lab Computat & Stat Learning, Genoa, Italy.; Ciliberto, C (reprint author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
EM cciliber@mit.edu; ale_rudi@mit.edu; lrosasco@mit.edu
CR Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036
   Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907
   Berlinet A., 2011, REPRODUCING KERNEL H
   Brouard C, 2016, J MACH LEARN RES, V17
   Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]
   Collins M, 2002, PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P1
   Cortes C., 2005, P 22 INT C MACH LEAR
   CUTURI M., 2013, ADV NEURAL INFORM PR, V26, P2292
   Dekel Ofer, 2004, ADV NEURAL INFORM PR
   Duchi John C, 2010, P 27 INT C MACH LEAR, P327
   EADES P, 1993, INFORM PROCESS LETT, V47, P319, DOI 10.1016/0020-0190(93)90079-O
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Gao Wei, 2013, ARTIFICIAL INTELLIGE
   Geurts P., 2006, ICML
   Giguere S., 2013, ICML
   HARDLE W, 1984, J MULTIVARIATE ANAL, V14, P169, DOI 10.1016/0047-259X(84)90003-4
   Herbrich R, 2000, ADV NEUR IN, P115
   Hofmann Thomas, 2007, PREDICTING STRUCTURE
   Huber P. J., 2011, ROBUST STAT
   Kadri H., 2013, P INT C MACH LEARN I
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Maxwell Harper F., 2015, ACM T INTERACT INTEL, V5, P19
   Micchelli C. A., 2004, ADV NEURAL INFORM PR, P921
   Mroueh Youssef, 2012, NIPS, P2798
   Schoen Richard, 1994, LECT DIFFERENTIAL GE, V2
   Scholkopf B, 2002, LEARNING KERNELS SUP
   Steinwart I, 2008, INFORM SCI STAT, P1
   Steinwart I, 2011, BERNOULLI, V17, P211, DOI 10.3150/10-BEJ267
   Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453
   Weston J., 2002, NIPS, P873
   Wolpert DH, 1996, NEURAL COMPUT, V8, P1341, DOI 10.1162/neco.1996.8.7.1341
   Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700057
DA 2019-06-15
ER

PT S
AU Coen, MH
   Ansari, MH
   Bendlin, BB
AF Coen, Michael H.
   Ansari, M. Hidayath
   Bendlin, Barbara B.
BE Rish, I
   Langs, G
   Wehbe, L
   Cecchi, G
   Chang, KMK
   Murphy, B
TI Predicting Short-Term Cognitive Change from Longitudinal Neuroimaging
   Analysis
SO MACHINE LEARNING AND INTERPRETATION IN NEUROIMAGING, MLINI 2014
SE Lecture Notes in Artificial Intelligence
LA English
DT Proceedings Paper
CT 4th International Workshop on Machine Learning and Interpretation in
   Neuroimaging (MLINI) Held at Conference on Neural Information Processing
   Systems (NIPS)
CY DEC 13, 2014
CL Montreal, CANADA
AB This paper introduces a framework for analyzing longitudinal neuroimaging datasets. We address the problem of detecting subtle, short-term changes in neural structure that are indicative of cognitive decline and correlate with risk factors for Alzheimer's disease. Previous approaches have focused on separating populations with different risk factors based on gross changes, such as decreasing gray matter volume. In contrast, we introduce a new spatially-sensitive kernel that allows us to characterize individuals, as opposed to populations. We use this for both classification and regression, e.g., to predict changes in a subject's cognitive test scores from neuroimaging data alone. In doing so, this paper presents the first evidence demonstrating that very small changes in white matter structure over a two year period can predict change in cognitive function in healthy adults.
C1 [Coen, Michael H.] Univ Wisconsin, Dept Biostat & Med Informat, Madison, WI 53706 USA.
   [Ansari, M. Hidayath] Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA.
   [Bendlin, Barbara B.] Univ Wisconsin, Dept Med, Madison, WI 53792 USA.
RP Coen, MH (reprint author), Univ Wisconsin, Dept Biostat & Med Informat, Madison, WI 53706 USA.
EM mhcoen@biostat.wisc.edu
CR Bowley MP, 2010, J COMP NEUROL, V518, P3046, DOI 10.1002/cne.22379
   Dyrba M., 2012, LECT NOTES COMPUTER, V7509, P18
   Friedman J, 2010, J STAT SOFTW, V33, P1
   Gretton A., 2009, ADV NEURAL INFORM PR, P673
   Grydeland H., 2012, HUM BRAIN MAPP
   Le Bihan D, 2001, J MAGN RESON IMAGING, V13, P534, DOI 10.1002/jmri.1076
   Misra C, 2009, NEUROIMAGE, V44, P1415, DOI 10.1016/j.neuroimage.2008.10.031
   Rahimi A., 2007, ADV NEURAL INFORM PR, P1177
   Raman P., 2011, P SIAM INT C DAT MIN
   Reitan R.M., 2009, NEUROPSYCHOLOGICAL A, P1
   Sager MA, 2005, J GERIATR PSYCH NEUR, V18, P245, DOI 10.1177/0891988705281882
   Smith SM, 2002, NEUROIMAGE, V17, P479, DOI 10.1006/nimg.2002.1040
   Trenerry MR, 1989, STROOP NEUROPSYCHOLO
   Ziegler DA, 2010, NEUROBIOL AGING, V31, P1912, DOI 10.1016/j.neurobiolaging.2008.10.015
NR 14
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER INT PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
BN 978-3-319-45173-2; 978-3-319-45174-9
J9 LECT NOTES ARTIF INT
PY 2016
VL 9444
BP 107
EP 114
DI 10.1007/978-3-319-45174-9_11
PG 8
WC Computer Science, Artificial Intelligence; Neuroimaging
SC Computer Science; Neurosciences & Neurology
GA BG4FZ
UT WOS:000388723400011
DA 2019-06-15
ER

PT S
AU Colombo, N
   Vlassis, N
AF Colombo, Nicolo
   Vlassis, Nikos
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI A Posteriori Error Bounds for Joint Matrix Decomposition Problems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID SIMULTANEOUS SCHUR DECOMPOSITION; CANONICAL DECOMPOSITION
AB Joint matrix triangularization is often used for estimating the joint eigenstructure of a set M of matrices, with applications in signal processing and machine learning. We consider the problem of approximate joint matrix triangularization when the matrices in M are jointly diagonalizable and real, but we only observe a set M' of noise perturbed versions of the matrices in M. Our main result is a first-order upper bound on the distance between any approximate joint triangularizer of the matrices in M' and any exact joint triangularizer of the matrices in M. The bound depends only on the observable matrices in M' and the noise level. In particular, it does not depend on optimization specific properties of the triangularizer, such as its proximity to critical points, that are typical of existing bounds in the literature. To our knowledge, this is the first a posteriori bound for joint matrix decomposition. We demonstrate the bound on synthetic data for which the ground truth is known.
C1 [Colombo, Nicolo] UCL, Dept Stat Sci, London, England.
   [Vlassis, Nikos] Adobe Res, San Jose, CA USA.
RP Colombo, N (reprint author), UCL, Dept Stat Sci, London, England.
EM nicolo.colombo@ucl.ac.uk; vlassis@adobe.com
CR ABEDMERAIM K, 1998, ACOUST SPEECH SIG PR, P2541
   Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1
   Afsari B, 2008, SIAM J MATRIX ANAL A, V30, P1148, DOI 10.1137/060655997
   Anandkumar A, 2014, J MACH LEARN RES, V15, P2773
   Balle B, 2011, LECT NOTES ARTIF INT, V6911, P156, DOI 10.1007/978-3-642-23780-5_20
   Cardoso J.-F., 1994, 94D023 TEL PAR SIGN
   Cardoso JF, 1996, SIAM J MATRIX ANAL A, V17, P161, DOI 10.1137/S0895479893259546
   Colombo N., 2016, P 33 INT C MACH LEAR
   Corless R. M., 1997, ISSAC 97. Proceedings of the 1997 International Sympsoium on Symbolic and Algebraic Computation, P133
   De Lathauwer L, 2004, SIAM J MATRIX ANAL A, V26, P295, DOI 10.1137/S089547980139786X
   De Lathauwer L, 2006, SIAM J MATRIX ANAL A, V28, P642, DOI 10.1137/040608830
   Fu T, 2006, INT C COMMUN CIRCUIT, P356, DOI 10.1109/ICCCAS.2006.284653
   Haardt M, 1998, IEEE T SIGNAL PROCES, V46, P161, DOI 10.1109/78.651206
   Horn R. A., 2012, MATRIX ANAL
   Iferroudjene R, 2009, APPL MATH COMPUT, V211, P363, DOI 10.1016/j.amc.2009.01.045
   KONSTANTINOV MM, 1994, SIAM J MATRIX ANAL A, V15, P383, DOI 10.1137/S089547989120267X
   Kuleshov V., 2015, 18 INT C ART INT STA
   Luciani X, 2010, LECT NOTES COMPUT SC, V6365, P555, DOI 10.1007/978-3-642-15995-4_69
   PANG JS, 1987, MATH OPER RES, V12, P474, DOI 10.1287/moor.12.3.474
   Podosinnikova A., 2016, P 33 INT C MACH LEAR
   Prudhomme S, 2003, INT J NUMER METH ENG, V56, P1193, DOI 10.1002/nme.609
   Sardouie SH, 2013, INT CONF ACOUST SPEE, P4178, DOI 10.1109/ICASSP.2013.6638446
   Souloumiac A, 2009, IEEE T SIGNAL PROCES, V57, P2222, DOI 10.1109/TSP.2009.2016997
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 8
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703060
DA 2019-06-15
ER

PT S
AU Cormier, Q
   Fard, MM
   Canini, K
   Gupta, MR
AF Cormier, Q.
   Fard, M. Milani
   Canini, K.
   Gupta, M. R.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Launch and Iterate: Reducing Prediction Churn
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID STABILITY
AB Practical applications of machine learning often involve successive training iterations with changes to features and training examples. Ideally, changes in the output of any new model should only be improvements (wins) over the previous iteration, but in practice the predictions may change neutrally for many examples, resulting in extra net-zero wins and losses, referred to as unnecessary churn. These changes in the predictions are problematic for usability for some applications, and make it harder and more expensive to measure if a change is statistically significant positive. In this paper, we formulate the problem and present a stabilization operator to regularize a classifier towards a previous classifier. We use a Markov chain Monte Carlo stabilization operator to produce a model with more consistent predictions without adversely affecting accuracy. We investigate the properties of the proposal with theoretical analysis. Experiments on benchmark datasets for different classification algorithms demonstrate the method and the resulting reduction in churn.
C1 [Cormier, Q.] ENS Lyon, 15 Parvis Rene Descartes, Lyon, France.
   [Fard, M. Milani; Canini, K.; Gupta, M. R.] Google Inc, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA.
RP Cormier, Q (reprint author), ENS Lyon, 15 Parvis Rene Descartes, Lyon, France.
EM quentin.cormier@ens-lyon.fr; mmilanifard@google.com; canini@google.com;
   mayagupta@google.com
CR Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704
   Bousquet O, 2001, ADV NEUR IN, V13, P196
   Candillier L., 2012, P ALRA ACT LEARN REA
   DEVROYE LP, 1979, IEEE T INFORM THEORY, V25, P601, DOI 10.1109/TIT.1979.1056087
   Fernandes K, 2015, LECT NOTES ARTIF INT, V9273, P535, DOI 10.1007/978-3-319-23485-4_53
   Kawala F., 2014, CORIA 2014, P1
   Mukherjee S, 2006, ADV COMPUT MATH, V25, P161, DOI 10.1007/s10444-004-7634-z
   Niculescu-Mizil A., 2005, P 22 INT C MACH LEAR, P625, DOI DOI 10.1145/1102351.1102430
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Platt J, 1999, ADV LARGE MARGIN CLA, V10, P61
   Reinart A., 2015, STAT DONE WRONG WOEF
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Vapnik VN, 1995, NATURE STAT LEARNING
   Zhang L., 2016, INFORM SCI
NR 15
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700017
DA 2019-06-15
ER

PT S
AU Cortes, C
   DeSalvo, G
   Mohri, M
AF Cortes, Corinna
   DeSalvo, Giulia
   Mohri, Mehryar
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Boosting with Abstention
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We present a new boosting algorithm for the key scenario of binary classification with abstention where the algorithm can abstain from predicting the label of a point, at the price of a fixed cost. At each round, our algorithm selects a pair of functions, a base predictor and a base abstention function. We define convex upper bounds for the natural loss function associated to this problem, which we prove to be calibrated with respect to the Bayes solution. Our algorithm benefits from general margin-based learning guarantees which we derive for ensembles of pairs of base predictor and abstention functions, in terms of the Rademacher complexities of the corresponding function classes. We give convergence guarantees for our algorithm along with a linear-time weak-learning algorithm for abstention stumps. We also report the results of several experiments suggesting that our algorithm provides a significant improvement in practice over two confidence-based algorithms.
C1 [Cortes, Corinna] Google Res, New York, NY 10011 USA.
   [DeSalvo, Giulia] Courant Inst, New York, NY 10012 USA.
   [Mohri, Mehryar] Google, New York, NY 10012 USA.
RP Cortes, C (reprint author), Google Res, New York, NY 10011 USA.
EM corinna@google.com; desalvo@cims.nyu.edu; mohri@cims.nyu.edu
FU NSF [CCF-1535987, IIS-1618662]
FX This work was partly funded by NSF CCF-1535987 and IIS-1618662.
CR Bartlett P., 2008, JMLR
   Bounsiar A., 2007, WASET
   Capitaine H. L., 2010, ICPR
   Chaudhuri K., 2014, NIPS
   Chow C., 1957, IEEE T COMPUT
   Chow C., 1970, IEEE T COMPUT
   Cortes C., 2016, ALT
   Dubuisson B., 1993, PR
   El-Yaniv R., 2011, NIPS
   El-Yaniv R., 2010, JMLR
   Elkan  C., 2001, IJCAI
   Fumera G., 2000, ICAPR
   Fumera G., 2002, ICPR
   Grandvalet Y., 2008, NIPS
   I. CVX Research, 2012, CVX MATLAB SOFTWARE
   Koltchinskii V, 2002, ANN STAT, V30, P1
   Landgrebe T., 2005, PRL
   Ledoux M., 1991, PROBABILITY BANACH S
   Littman M., 2008, ICML
   Luo Z.-Q., 1992, J OPTIMIZATION THEOR
   Melvin I, 2008, BMC BIOINFORMATICS, V9, DOI 10.1186/1471-2105-9-389
   Mohri M., 2012, FDN MACHINE LEARNING
   Pedregosa F., 2011, JMLR
   Pietraszek T., 2005, ICML
   Santos-Pereira C., 2005, PRL
   Schapire RE, 2000, MACH LEARN, V39, P135, DOI 10.1023/A:1007649029923
   Tax D., 2008, PATTERN RECOGNITION
   Tortorella F., 2001, ICAPR
   Trapeznikov K., 2013, AISTATS
   Wang J., 2014, JMLR
   Yuan M., 2010, JMLR
   Yuang M., 2011, BERNOULLI
   Zhang C., 2016, COLT
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702083
DA 2019-06-15
ER

PT S
AU Cortes, C
   Kuznetsov, V
   Mohrii, M
   Yang, S
AF Cortes, Corinna
   Kuznetsov, Vitaly
   Mohrii, Mehryar
   Yang, Scott
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Structured Prediction Theory Based on Factor Graph Complexity
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We present a general theoretical analysis of structured prediction with a series of new results. We give new data-dependent margin guarantees for structured prediction for a very wide family of loss functions and a general family of hypotheses, with an arbitrary factor graph decomposition. These are the tightest margin bounds known for both standard multi-class and general structured prediction problems. Our guarantees are expressed in terms of a data-dependent complexity measure, factor graph complexity, which we show can be estimated from data and bounded in terms of familiar quantities for several commonly used hypothesis sets along with a sparsity measure for features and graphs. Our proof techniques include generalizations of Talagrand's contraction lemma that can be of independent interest.
   We further extend our theory by leveraging the principle of Voted Risk Minimization (VRM) and show that learning is possible even with complex factor graphs. We present new learning bounds for this advanced setting, which we use to design two new algorithms, Voted Conditional Random Field (VCRF) and Voted Structured Boosting (StructBoost). These algorithms can make use of complex features and factor graphs and yet benefit from favorable learning guarantees. We also report the results of experiments with VCRF on several datasets to validate our theory.
C1 [Cortes, Corinna; Kuznetsov, Vitaly] Google Res, New York, NY 10011 USA.
   [Mohrii, Mehryar; Yang, Scott] Courant Inst, New York, NY 10012 USA.
   [Mohrii, Mehryar] Google, New York, NY 10012 USA.
RP Cortes, C (reprint author), Google Res, New York, NY 10011 USA.
EM corinna@google.com; vitaly@cims.nyu.edu; mohri@cims.nyu.edu;
   yangs@cims.nyu.edu
FU NSF [CCF-1535987, IIS-1618662, GRFP DGE-1342536]
FX This work was partly funded by NSF CCF-1535987 and IIS-1618662 and NSF
   GRFP DGE-1342536.
CR Bakir G., 2007, PREDICTING STRUCTURE
   Chang Kai-Wei, 2015, ICML
   Collins  M., 2001, P IWPT
   Cortes  C., 2014, ICML
   Cortes C., 2007, PREDICTING STRUCTURE
   Cortes  C., 2015, JMLR
   Daume H, 2009, MACH LEARN, V75, P297, DOI 10.1007/s10994-009-5106-x
   Doppa JR, 2014, J MACH LEARN RES, V15, P1317
   Jurafsky D., 2009, SPEECH LANGUAGE PROC
   Kuznetsov  V., 2014, NIPS
   Lafferty J. O., 2001, ICML
   Lam M., 2015, CVPR
   Lei  Y., 2015, NIPS
   Lucchi  A., 2013, CVPR
   Maurer  A., 2016, ALT
   McAllester  D., 2007, PREDICTING STRUCTURE
   Mohri M., 2012, FDN MACHINE LEARNING
   Nadeau D, 2007, LINGUIST INVESTIG, V30, P3
   Ross Stephane, 2011, AISTATS
   Taskar B, 2003, NIPS
   Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453
   Vinyals O., 2015, NIPS
   Vinyals O, 2015, CVPR
   Zhang  D., 2008, IJCNLP
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704010
DA 2019-06-15
ER

PT S
AU Cutkosky, A
   Boahen, K
AF Cutkosky, Ashok
   Boahen, Kwabena
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Online Convex Optimization with Unconstrained Domains and Losses
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We propose an online convex optimization algorithm (RESCALEDEXP) that achieves optimal regret in the unconstrained setting without prior knowledge of any bounds on the loss functions. We prove a lower bound showing an exponential separation between the regret of existing algorithms that require a known bound on the loss functions and any algorithm that does not require such knowledge. RESCALEDEXP matches this lower bound asymptotically in the number of iterations. RESCALEDEXP is naturally hyperparameter-free and we demonstrate empirically that it matches prior optimization algorithms that require hyperparameter optimization.
C1 [Cutkosky, Ashok] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
   [Boahen, Kwabena] Stanford Univ, Dept Bioengn, Stanford, CA 94305 USA.
RP Cutkosky, A (reprint author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
EM ashokc@cs.stanford.edu; boahen@stanford.edu
CR Abernethy Jacob, 2008, P 19 ANN C COMP LEAR
   Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chang Chih-chung, 2001, P IJCNN IEEE CIT
   Duarte MF, 2004, J PARALLEL DISTR COM, V64, P826, DOI 10.1016/j.jpdc.2004.03.020
   Duchi J., 2010, C LEARN THEOR COLT
   Guyon I., 2004, ADV NEURAL INFORM PR, V4, P545
   Hofmann T, 2008, ANN STAT, V36, P1171, DOI 10.1214/009053607000000677
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kogan S., 2009, P HUM LANG TECHN 200, P272
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lewis DD, 2004, J MACH LEARN RES, V5, P361
   Lichman M., 2013, UCI MACHINE LEARNING
   Littlestone Nick, 2014, P 2 ANN WORKSH COMP, P269
   Mcmahan Brendan, 2012, ADV NEURAL INFORM PR, P2402
   McMahan Brendan, 2013, ADV NEURAL INFORM PR, V26, P2724
   McMahan H. B., 2010, P 23 ANN C LEARN THE
   McMahan H. Brendan, 2014, ARXIV14033465
   Orabona F, 2015, MACH LEARN, V99, P411, DOI 10.1007/s10994-014-5474-8
   Orabona Francesco, 2014, ADV NEURAL INFORM PR, P1116
   Orabona Francesco, 2016, ARXIV160101974
   Orabona Francesco, 2016, C LEARN THEOR
   Orabona Francesco, 2013, ADV NEURAL INFORM PR, P1806
   Shalev-Shwartz S., 2007, THESIS
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Zeiler M.D., 2012, ARXIV12125701
   Zinkevich Martin, 2003, P 20 INT C MACH LEAR, P928
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703007
DA 2019-06-15
ER

PT S
AU Daniely, A
   Frostig, R
   Singer, Y
AF Daniely, Amit
   Frostig, Roy
   Singer, Yoram
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Toward Deeper Understanding of Neural Networks: The Power of
   Initialization and a Dual View on Expressivity
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID SIZE
AB We develop a general duality between neural networks and compositional kernel Hilbert spaces. We introduce the notion of a computation skeleton, an acyclic graph that succinctly describes both a family of neural networks and a kernel space. Random neural networks are generated from a skeleton through node replication followed by sampling from a normal distribution to assign weights. The kernel space consists of functions that arise by compositions, averaging, and non-linear transformations governed by the skeleton's graph topology and activation functions. We prove that random networks induce representations which approximate the kernel space. In particular, it follows that random weight initialization often yields a favorable starting point for optimization despite the worst-case intractability of training neural networks.
C1 [Daniely, Amit; Frostig, Roy; Singer, Yoram] Google Brain, Mountain View, CA 94043 USA.
   [Frostig, Roy] Stanford Univ, Stanford, CA 94305 USA.
RP Daniely, A (reprint author), Google Brain, Mountain View, CA 94043 USA.
CR Andoni A., 2014, P 31 INT C MACH LEAR, P1908
   Anselmi F., 2015, ARXIV150801084
   Anthony  Martin, 1999, NEURAL NETWORK LEARN
   Arora S., 2014, P 31 INT C MACH LEAR, P584
   Bach F., 2014, ARXIV14128690
   Bach F., 2015, EQUIVALENCE KERNEL Q
   BARRON AR, 1993, IEEE T INFORM THEORY, V39, P930, DOI 10.1109/18.256500
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Bartlett PL, 1998, IEEE T INFORM THEORY, V44, P525, DOI 10.1109/18.661502
   Baum EB, 1989, NEURAL COMPUT, V1, P151, DOI 10.1162/neco.1989.1.1.151
   Bo LF, 2011, PROC CVPR IEEE, P1729, DOI 10.1109/CVPR.2011.5995719
   Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230
   Cho Y., 2009, ADV NEURAL INFORM PR, P342, DOI DOI 10.1021/ed028p10
   Choromanska A., 2015, ARTIF INTELL, P192
   Daniely A., 2016, COLT
   Giryes R., 2015, ARXIV150408291
   Grauman K, 2005, IEEE I CONF COMP VIS, P1458
   Hardt M., 2015, ARXIV150901240
   Harris Z. S., 1954, WORD
   Hazan T., 2015, ARXIV150805133
   Kar P., 2012, ARXIV12016530
   Karp R.M., 1980, P 12 ACM S THEOR COM, P302, DOI DOI 10.1145/800141.804678
   Kearns M., 1989, Proceedings of the Twenty First Annual ACM Symposium on Theory of Computing, P433, DOI 10.1145/73007.73049
   Klivans A. R., 2006, FOCS
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Levy O., 2014, ADV NEURAL INFORM PR, V27, P2177, DOI DOI 10.1162/153244303322533223
   Livni R., 2014, ADV NEURAL INFORM PR, V27, P855
   Mairal Julien, 2014, ADV NEURAL INFORM PR, P2627
   Mikolov T., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951
   Neal R. M., 2012, BAYESIAN LEARNING NE, V118
   Neyshabur B., 2015, COLT
   Neyshabur Behnam, 2015, ADV NEURAL INFORM PR, P2413
   ODonnell Ryan, 2014, ANAL BOOLEAN FUNCTIO
   Pennington J., 2015, ADV NEURAL INFORAMTI, P1837
   Rahimi A., 2009, ADV NEURAL INFORM PR, V21, P1313
   Rahimi A., 2007, ADV NEURAL INFORM PR, P1177
   Safran I., 2015, ARXIV151104210
   Saitoh S., 1988, THEORY REPROD KERNEL
   Schoenberg IJ., 1942, DUKE MATH J, V9, P96, DOI [10.1215/S0012-7094-42-00908-6, DOI 10.1215/S0012-7094-42-00908-6]
   Scholkopf B, 1998, ADV NEUR IN, V10, P640
   Sedghi H., 2014, ARXIV14122693
   Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Williams C. K. I., 1997, COMPUTATION INFINITE, P295
NR 45
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703063
DA 2019-06-15
ER

PT S
AU David, O
   Moran, S
   Yehudayoff, A
AF David, Ofir
   Moran, Shay
   Yehudayoff, Amir
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI On statistical learning via the lens of compression
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID SAMPLE COMPRESSION; LEARNABILITY; BOUNDS
AB This work continues the study of the relationship between sample compression schemes and statistical learning, which has been mostly investigated within the framework of binary classification. The central theme of this work is establishing equivalences between learnability and compressibility, and utilizing these equivalences in the study of statistical learning theory. We begin with the setting of multiclass categorization (zero/one loss). We prove that in this case learnability is equivalent to compression of logarithmic sample size, and that uniform convergence implies compression of constant size. We then consider Vapnik's general learning setting: we show that in order to extend the compressibility-learnability equivalence to this case, it is necessary to consider an approximate variant of compression. Finally, we provide some applications of the compressibility-learnability equivalences.
C1 [David, Ofir; Yehudayoff, Amir] Technion Israel Inst Technol, Dept Math, Haifa, Israel.
   [Moran, Shay] Technion Israel Inst Technol, Dept Comp Sci, Haifa, Israel.
RP David, O (reprint author), Technion Israel Inst Technol, Dept Math, Haifa, Israel.
EM ofirdav@tx.technion.ac.il; shaymrn@cs.technion.ac.il;
   amir.yehudayoff@gmail.com
CR Ben-David S, 1998, DISCRETE APPL MATH, V86, P3, DOI 10.1016/S0166-218X(98)00000-6
   BENDAVID S, 1995, J COMPUT SYST SCI, V50, P74, DOI 10.1006/jcss.1995.1008
   BLUMER A, 1989, J ACM, V36, P929, DOI 10.1145/76359.76371
   Chernikov A, 2013, ISR J MATH, V194, P409, DOI 10.1007/s11856-012-0061-9
   Cummings R, 2016, C LEARN THEOR JUN, P772
   Daniely A., 2014, COLT, P287
   Daniely A, 2015, J MACH LEARN RES, V16, P2377
   Floyd S., 1989, Proceedings of the Second Annual Workshop on Computational Learning Theory, P349
   Floyd S, 1995, MACH LEARN, V21, P269
   FREUND Y, 1995, INFORM COMPUT, V121, P256, DOI 10.1006/inco.1995.1136
   Freund  Yoav, 2012, BOOSTING FDN ALGORIT, P2
   Gottlieb  Lee-Ad, 2015, ABS150206208 CORR, P2
   Graepel T, 2005, MACH LEARN, V59, P55, DOI 10.1007/s10994-005-0462-7
   HELMBOLD D, 1992, SIAM J COMPUT, V21, P240, DOI 10.1137/0221019
   Kuzmin D, 2007, J MACH LEARN RES, V8, P2047
   Littlestone  Nick, 1986, RELATING DATA UNPUB, P1
   Livni  Roi, 2013, COLT, P77
   Moran S, 2016, J ACM, V63, DOI 10.1145/2890490
   Natarajan B. K., 1989, Machine Learning, V4, P67, DOI 10.1007/BF00114804
   Rubinstein BIP, 2012, J MACH LEARN RES, V13, P1221
   Rubinstein BIP, 2009, J COMPUT SYST SCI, V75, P37, DOI 10.1016/j.jcss.2008.07.005
   Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN
   Vapnik V. N., 1998, STAT LEARNING THEORY
   VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025
   Warmuth MK, 2003, LECT NOTES ARTIF INT, V2777, P743, DOI 10.1007/978-3-540-45167-9_60
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704015
DA 2019-06-15
ER

PT S
AU Davis, D
   Udell, M
   Edmunds, B
AF Davis, Damek
   Udell, Madeleine
   Edmunds, Brent
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI The Sound of APALM Clapping: Faster Nonsmooth Nonconvex Optimization
   with Stochastic Asynchronous PALM
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We introduce the Stochastic Asynchronous Proximal Alternating Linearized Minimization (SAPALM) method, a block coordinate stochastic proximal-gradient method for solving nonconvex, nonsmooth optimization problems. SAPALM is the first asynchronous parallel optimization method that provably converges on a large class of nonconvex, nonsmooth problems. We prove that SAPALM matches the best known rates of convergence - among synchronous or asynchronous methods - on this problem class. We provide upper bounds on the number of workers for which we can expect to see a linear speedup, which match the best bounds known for less complex problems, and show that in practice SAPALM achieves this linear speedup. We demonstrate state-of-the-art performance on several matrix factorization problems.
C1 [Davis, Damek; Udell, Madeleine] Cornell Univ, Ithaca, NY 14853 USA.
   [Edmunds, Brent] Univ Calif Los Angeles, Los Angeles, CA USA.
RP Davis, D (reprint author), Cornell Univ, Ithaca, NY 14853 USA.
EM dsd95@cornell.edu; mru8@cornell.edu; brent.edmunds@math.ucla.edu
CR Agarwal A, 2012, IEEE DECIS CONTR P, P5451, DOI 10.1109/CDC.2012.6426626
   Bertsekas D. P., PARALLEL DISTRIBUTED, V23
   Bolte J, 2014, MATH PROGRAM, V146, P459, DOI 10.1007/s10107-013-0701-9
   Davis D., 2016, ARXIV160100698
   Davis D., 2016, ARXIV160400526
   Dean J., 2012, ADV NEURAL INFORM PR, P1223
   Ge R., 2015, P 28 C LEARN THEOR, P797
   Ghadimi S, 2016, MATH PROGRAM, V155, P267, DOI 10.1007/s10107-014-0846-1
   Hong M., 2014, ARXIV14126058
   Lian X., 2015, ADV NEURAL INFORM PR, P2719
   Liu J., 2014, ARXIV14014780
   Liu J, 2015, J MACH LEARN RES, V16, P285
   Mania Horia, 2015, ARXIV150706970
   Nesterov Y., 2004, INTRO LECT CONVEX OP
   Niu F., 2011, ADV NEURAL INFORM PR, V24, P693
   Peng Z., 2015, ARXIV150602396
   Rockafellar R. T., 2009, VARIATIONAL ANAL, V317
   Tseng P, 1991, SIAM J OPTIMIZ, V1, P603, DOI 10.1137/0801036
   TSITSIKLIS JN, 1986, IEEE T AUTOMAT CONTR, V31, P803, DOI 10.1109/TAC.1986.1104412
   Udell M., 2014, ARXIV14100342
   Woodworth J., 2015, ARXIV150402923
   Xu YY, 2015, SIAM J OPTIMIZ, V25, P1686, DOI 10.1137/140983938
   Yun H, 2014, PROC VLDB ENDOW, V7, P975, DOI 10.14778/2732967.2732973
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703064
DA 2019-06-15
ER

PT S
AU De, A
   Valera, I
   Ganguly, N
   Bhattacharya, S
   Gomez-Rodriguez, M
AF De, Abir
   Valera, Isabel
   Ganguly, Niloy
   Bhattacharya, Sourangshu
   Gomez-Rodriguez, Manuel
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning and Forecasting Opinion Dynamics in Social Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID MODEL
AB Social media and social networking sites have become a global pinboard for exposition and discussion of news, topics, and ideas, where social media users often update their opinions about a particular topic by learning from the opinions shared by their friends. In this context, can we learn a data-driven model of opinion dynamics that is able to accurately forecast users' opinions? In this paper, we introduce SLANT, a probabilistic modeling framework of opinion dynamics, which represents users' opinions over time by means of marked jump diffusion stochastic differential equations, and allows for efficient model simulation and parameter estimation from historical fine grained event data. We then leverage our framework to derive a set of efficient predictive formulas for opinion forecasting and identify conditions under which opinions converge to a steady state. Experiments on data gathered from Twitter show that our model provides a good fit to the data and our formulas achieve more accurate forecasting than alternatives.
C1 [De, Abir; Ganguly, Niloy; Bhattacharya, Sourangshu] IIT Kharagpur, Kharagpur, W Bengal, India.
   [Valera, Isabel; Gomez-Rodriguez, Manuel] MPI Software Syst, Saarbrucken, Germany.
RP De, A (reprint author), IIT Kharagpur, Kharagpur, W Bengal, India.
EM abir.de@cse.iitkgp.ernet.in; ivalera@mpi-sws.org;
   niloy@cse.iitkgp.ernet.in; sourangshu@cse.iitkgp.ernet.in;
   manuelgr@mpi-sws.org
FU Google India; Humboldt post-doctoral fellowship
FX Abir De is partially supported by Google India under the Google India
   PhD Fellowship Award, and Isabel Valera is supported by a Humboldt
   post-doctoral fellowship.
CR Aalen OO, 2008, STAT BIOL HEALTH, P1
   Al-Mohy AH, 2011, SIAM J SCI COMPUT, V33, P488, DOI 10.1137/100788860
   Axelrod R, 1997, J CONFLICT RESOLUT, V41, P203, DOI 10.1177/0022002797041002001
   Birgin EG, 2000, SIAM J OPTIMIZ, V10, P1196, DOI 10.1137/S1052623497330963
   Blundell C., 2012, ADV NEURAL INFORM PR, P2600
   CLIFFORD P, 1973, BIOMETRIKA, V60, P581, DOI 10.2307/2335008
   Das A., 2014, WSDM
   De A., 2014, CIKM
   DeGroot M. H., 1974, J AM STAT ASSOC, V69
   Farajtabar  M., 2015, NIPS
   Farajtabar M., 2014, NIPS
   Gomez-Rodriguez  M., 2011, ICML
   Hannak Aniko, 2012, ICWSM
   Hanson FB, 2007, ADV DES CONTROL, P1
   Hegselmann R, 2002, JASSS-J ARTIF SOC S, V5
   HINRICHSEN D, 1989, J DIFFER EQUATIONS, V82, P219, DOI 10.1016/0022-0396(89)90132-0
   Holme P, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.056108
   Karppi T., 2015, TC S
   Kim J., 2013, ICWSM
   Leskovec J., 2010, JMLR
   Pang B., 2008, F T INFORM RETRIEVAL, V2
   Raven B. H., 1993, J SOCIAL ISSUES, V49
   SAAD Y, 1986, SIAM J SCI STAT COMP, V7, P856, DOI 10.1137/0907058
   Valera I., 2015, P 2015 IEEE INT C DA
   Wang Yichen, 2016, ARXIV160309021
   Yildiz M. E., 2010, INF THEOR APPL WORKS, P1
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701050
DA 2019-06-15
ER

PT S
AU De Brabandere, B
   Jia, X
   Tuytelaars, T
   Gool, LV
AF De Brabandere, Bert
   Jia, Xu
   Tuytelaars, Tinne
   Luc Van Gool
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Dynamic Filter Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB In a traditional convolutional layer, the learned filters stay fixed after training. In contrast, we introduce a new framework, the Dynamic Filter Network, where filters are generated dynamically conditioned on an input. We show that this architecture is a powerful one, with increased flexibility thanks to its adaptive nature, yet without an excessive increase in the number of model parameters. A wide variety of filtering operations can be learned this way, including local spatial transformations, but also others like selective (de)blurring or adaptive feature extraction. Moreover, multiple such layers can be combined, e.g. in a recurrent architecture.
   We demonstrate the effectiveness of the dynamic filter network on the tasks of video and stereo prediction, and reach state-of-the-art performance on the moving MNIST dataset with a much smaller model. By visualizing the learned filters, we illustrate that the network has picked up flow information by only looking at unlabelled training data. This suggests that the network can be used to pretrain networks for various supervised tasks in an unsupervised way, like optical flow and depth estimation.
C1 [De Brabandere, Bert; Jia, Xu; Tuytelaars, Tinne; Luc Van Gool] Katholieke Univ Leuven, IMinds, ESAT PSI, Leuven, Belgium.
   [Luc Van Gool] Swiss Fed Inst Technol, D ITET, Zurich, Switzerland.
RP De Brabandere, B (reprint author), Katholieke Univ Leuven, IMinds, ESAT PSI, Leuven, Belgium.
EM bert.debrabandere@esat.kuleuven.be; xu.jia@esat.kuleuven.be;
   tinne.tuytelaars@esat.kuleuven.be; vangool@vision.ee.ethz.ch
RI Tuytelaars, Tinne/B-4319-2015
OI Tuytelaars, Tinne/0000-0003-3307-9723
FU FWO [G.0696.12N]; EU FP7 project Europa2; iMinds ICON project Footwork;
   bilateral Toyota project
FX This work was supported by FWO through the project G.0696.12N
   "Representations and algorithms for captation, visualization and
   manipulation of moving 3D objects, subjects and scenes", the EU FP7
   project Europa2, the iMinds ICON project Footwork and bilateral Toyota
   project.
CR Dosovitskiy A., 2015, ICCV
   Finn Chelsea, 2016, NIPS
   Flynn John, 2015, CVPR
   Gomez Faustino J., 2005, ICANN
   Goodfellow I., 2014, NIPS
   Goroshin Ross, 2015, NIPS
   He K., 2015, CORR
   He Kaiming, 2016, CORR
   Jaderberg M., 2015, NIPS
   Klein Benjamin, 2015, CVPR
   Larsen Anders Boesen Lindbo, 2016, ICML
   Mathieu M., 2016, ICLR
   Noh H., 2016, CVPR
   Oh  J., 2015, NIPS
   Patraucean V., 2015, CORR
   Ranzato M., 2014, CORR
   Riegler Gernot, 2015, ICCV
   Shi X., 2015, NIPS
   Srivastava N, 2015, ICML
   Srivastava R. K., 2015, NIPS
   Taylor G., 2009, ICML
   Xie Junyuan, 2016, ECCV
   Yang Jimei, 2015, NIPS
   Yim J., 2015, CVPR
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704102
DA 2019-06-15
ER

PT S
AU Defazio, A
AF Defazio, Aaron
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI A Simple Practical Accelerated Method for Finite Sums
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We describe a novel optimization method for finite sums (such as empirical risk minimization problems) building on the recently introduced SAGA method. Our method achieves an accelerated convergence rate on strongly convex smooth problems. Our method has only one parameter (a step size), and is radically simpler than other accelerated methods for finite sums. Additionally it can be applied when the terms are non-smooth, yielding a method applicable in many areas where operator splitting methods would traditionally be applied.
C1 [Defazio, Aaron] Ambiata, Sydney, NSW, Australia.
RP Defazio, A (reprint author), Ambiata, Sydney, NSW, Australia.
CR Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Defazio A, 2014, ADV NEUR IN, V27
   Defazio Aaron, 2014, P 31 INT C MACH LEAR
   Hofmann Thomas, 2015, ADV NEURAL INFORM PR, P2296
   Johnson R., 2013, NIPS
   Konecny Jakub, 2013, ARXIV E PRINTS
   Lan G., 2015, ARXIV E PRINTS
   Lin Hongzhou, 2015, ADV NEURAL INFORM PR, P3366
   Mairal Julien, 2014, TECHNICAL REPORT
   Nesterov Yu., 1998, INTRO LECT CONVEX PR
   Nitanda A, 2014, NEURAL INF PROCESS S, V27, P1574
   ROCKAFELLAR RT, 1976, SIAM J CONTROL, V14, P877, DOI 10.1137/0314056
   SCHMIDT M., 2013, TECHNICAL REPORT
   Shalev-Shwartz S., 2013, ADV NEURAL INF PROCE, P378
   Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4
   Shalev- Shwartz Shai, 2013, JMLR
   Shalev- Shwartz Shai, 2013, TECHNICAL REPORT
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701011
DA 2019-06-15
ER

PT S
AU Degenne, R
   Perchet, V
AF Degenne, Remy
   Perchet, Vianney
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Combinatorial semi-bandit with known covariance
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB The combinatorial stochastic semi-bandit problem is an extension of the classical multi-armed bandit problem in which an algorithm pulls more than one arm at each stage and the rewards of all pulled arms are revealed. One difference with the single arm variant is that the dependency structure of the arms is crucial. Previous works on this setting either used a worst-case approach or imposed independence of the arms. We introduce a way to quantify the dependency structure of the problem and design an algorithm that adapts to it. The algorithm is based on linear regression and the analysis develops techniques from the linear bandit literature. By comparing its performance to a new lower bound, we prove that it is optimal, up to a poly-logarithmic factor in the number of pulled arms.
C1 [Degenne, Remy] Univ Paris Diderot, LMPA, CMLA, ENS Paris Saclay, Paris, France.
   [Perchet, Vianney] ENS Paris Saclay, CMLA, CRITEO Res, Paris, France.
RP Degenne, R (reprint author), Univ Paris Diderot, LMPA, CMLA, ENS Paris Saclay, Paris, France.
EM degenne@cmla.ens-cachan.fr; perchet@normalesup.org
FU ANR [ANR-13-JS01-0004]; Fondation Mathematiques Jacques Hadamard; EDF
   through the Program Gaspard Monge for Optimization; Irsdi project
   Tecolere
FX The authors would like to acknowledge funding from the ANR under grant
   number ANR-13-JS01-0004 as well as the Fondation Mathematiques Jacques
   Hadamard and EDF through the Program Gaspard Monge for Optimization and
   the Irsdi project Tecolere.
CR Abbasi-Yadkori Y, 2011, 24 ANN C LEARN THEOR, V19, P1
   Audibert JY, 2014, MATH OPER RES, V39, P31, DOI 10.1287/moor.2013.0598
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Carpentier Alexandra, 2012, ADV NEURAL INFORM PR, P251
   Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001
   Chen  W., 2013, P 30 INT C MACH LEAR, P151
   Combes Richard, 2015, NEURAL INFORM PROCES, P1
   Filippi Sarah, 2010, NEURAL INFORM PROCES, P1
   Gai Y, 2012, IEEE ACM T NETWORK, V20, P1466, DOI 10.1109/TNET.2011.2181864
   Garivier A, 2013, 2013 IEEE INFORMATION THEORY WORKSHOP (ITW)
   Komiyama J., 2015, P 32 INT C MACH LEAR
   Kveton B., 2015, P 18 INT C ART INT S
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Pena V. H., 2008, SELF NORMALIZED PROC
   Robbins H., 1985, H ROBBINS SELECTED P, P169
   Rusmevichientong Paat, 1985, MATH OPER RES, P1
NR 16
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700101
DA 2019-06-15
ER

PT S
AU Degraux, K
   Peyre, G
   Fadili, JM
   Jacques, L
AF Degraux, Kevin
   Peyre, Gabriel
   Fadili, Jalal M.
   Jacques, Laurent
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Sparse Support Recovery with Non-smooth Loss Functions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID CONSISTENCY; SELECTION; LASSO
AB In this paper, we study the support recovery guarantees of underdetermined sparse regression using the l(1)-norm as a regularizer and a non-smooth loss function for data fidelity. More precisely, we focus in detail on the cases of l(1) and l(infinity) losses, and contrast them with the usual l(2) loss. While these losses are routinely used to account for either sparse (l(1) loss) or uniform (l(infinity) loss) noise models, a theoretical analysis of their performance is still lacking. In this article, we extend the existing theory from the smooth l(2) case to these non-smooth cases. We derive a sharp condition which ensures that the support of the vector to recover is stable to small additive noise in the observations, as long as the loss constraint size is tuned proportionally to the noise level. A distinctive feature of our theory is that it also explains what happens when the support is unstable. While the support is not stable anymore, we identify an "extended support" and show that this extended support is stable to small additive noise. To exemplify the usefulness of our theory, we give a detailed numerical analysis of the support stability/instability of compressed sensing recovery with these different losses. This highlights different parameter regimes, ranging from total support stability to progressively increasing support instability.
C1 [Degraux, Kevin; Jacques, Laurent] Catholic Univ Louvain, ISPGrp, ICTEAM, FNRS, B-1348 Louvain La Neuve, Belgium.
   [Peyre, Gabriel] Ecole Normale Super, CNRS, DMA, F-75775 Paris, France.
   [Fadili, Jalal M.] Normandie Univ, ENSICAEN, CNRS, GREYC, F-14050 Caen, France.
RP Degraux, K (reprint author), Catholic Univ Louvain, ISPGrp, ICTEAM, FNRS, B-1348 Louvain La Neuve, Belgium.
EM kevin.degraux@uclouvain.be; gabriel.peyre@ens.fr;
   Jalal.Fadili@ensicaen.fr; laurent.jacques@uclouvain.be
FU Belgian F.R.S.-FNRS; Institut Universitaire de France; European Research
   Council (ERC project SIGMA-Vision)
FX KD and LJ are funded by the Belgian F.R.S.-FNRS. JF is partly supported
   by Institut Universitaire de France. GP is supported by the European
   Research Council (ERC project SIGMA-Vision).
CR Bach FR, 2008, J MACH LEARN RES, V9, P1179
   Bach FR, 2008, J MACH LEARN RES, V9, P1019
   Candes E.J., 2006, CALIF I TECHNOL, P1
   Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010
   Duval V., 2015, 01135200 HAL
   Fuchs JJ, 2004, IEEE T INFORM THEORY, V50, P1341, DOI 10.1109/TIT.2004.828141
   Grant M., 2014, CVX MATLAB SOFTWARE
   Grant MC, 2008, LECT NOTES CONTR INF, V371, P95, DOI 10.1007/978-1-84800-155-8_7
   Jacques L., 2013, TRLJ201301
   Jacques L, 2011, IEEE T INFORM THEORY, V57, P559, DOI 10.1109/TIT.2010.2093310
   Nikolova M., 2004, J MATH IMAGING VISIO, V20
   Rockafellar R, 1974, CONJUGATE DUALITY OP, V16
   Studer C, 2012, IEEE T INFORM THEORY, V58, P3115, DOI 10.1109/TIT.2011.2179701
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Tibshirani RJ, 2013, ELECTRON J STAT, V7, P1456, DOI 10.1214/13-EJS815
   Vaiter S., 2014, 00987293 HAL
   Vaiter S, 2013, IEEE T INFORM THEORY, V59, P2001, DOI 10.1109/TIT.2012.2233859
   Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018
   Zhao P, 2006, J MACH LEARN RES, V7, P2541
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704083
DA 2019-06-15
ER

PT S
AU Desir, A
   Goyal, V
   Jagabathula, S
   Segev, D
AF Desir, Antoine
   Goyal, Vineet
   Jagabathula, Srikanth
   Segev, Danny
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Assortment Optimization Under the Mallows model
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID CHOICE; ALGORITHM
AB We consider the assortment optimization problem when customer preferences follow a mixture of Mallows distributions. The assortment optimization problem focuses on determining the revenue/profit maximizing subset of products from a large universe of products; it is an important decision that is commonly faced by retailers in determining what to offer their customers. There are two key challenges: (a) the Mallows distribution lacks a closed-form expression (and requires summing an exponential number of terms) to compute the choice probability and, hence, the expected revenue/profit per customer; and (b) finding the best subset may require an exhaustive search. Our key contributions are an efficiently computable closed-form expression for the choice probability under the Mallows model and a compact mixed integer linear program (MIP) formulation for the assortment problem.
C1 [Desir, Antoine; Goyal, Vineet] Columbia Univ, IEOR Dept, New York, NY 10027 USA.
   [Jagabathula, Srikanth] NYU, Stern Sch Business, IOMS Dept, New York, NY 10003 USA.
   [Segev, Danny] Univ Haifa, Dept Stat, Haifa, Israel.
RP Desir, A (reprint author), Columbia Univ, IEOR Dept, New York, NY 10027 USA.
EM antoine@ieor.columbia.edu; vgoyal@ieor.columbia.edu;
   sjagabat@stern.nyu.edu; segevd@stat.haifa.ac.il
CR Aouad A., 2015, APPROXIMABILITY ASSO
   Blanchet J., 2013, EC, P103
   Brightwell G., 1991, P 23 ANN ACM S THEOR, P175, DOI DOI 10.1145/103418.103441
   COOLEY JW, 1965, MATH COMPUT, V19, P297, DOI 10.2307/2003354
   Davis JM, 2014, OPER RES, V62, P250, DOI 10.1287/opre.2014.1256
   Doignon JP, 2004, PSYCHOMETRIKA, V69, P33, DOI 10.1007/BF02295838
   Dwork C., 2001, P 10 INT C WORLD WID, P613, DOI DOI 10.1145/371920.372165
   Farias VF, 2013, MANAGE SCI, V59, P305, DOI 10.1287/mnsc.1120.1610
   Gallego G, 2014, MANAGE SCI, V60, P2583, DOI 10.1287/mnsc.2014.1931
   Guiver J., 2009, P 26 ANN INT C MACH, P377
   Honhon D, 2012, M&SOM-MANUF SERV OP, V14, P279, DOI 10.1287/msom.1110.0365
   Jagabathula S., 2014, ASSORTMENT OPTIMIZAT
   Lebanon G, 2008, J MACH LEARN RES, V9, P2401
   Lu T., 2011, P 28 INT C MACH LEAR, P145
   Luce R. D., 1959, INDIVIDUAL CHOICE BE
   MALLOWS CL, 1957, BIOMETRIKA, V44, P114, DOI 10.2307/2333244
   Marden J.I., 1995, ANAL MODELING RANK D
   McFadden D, 1978, TRANSPORT RES REC, V672, P72
   Meila M., 2012, ARXIV12065265
   Bront JJM, 2009, OPER RES, V57, P769, DOI 10.1287/opre.1080.0567
   Murphy TB, 2003, COMPUT STAT DATA AN, V41, P645, DOI 10.1016/S0167-9473(02)00165-2
   PLACKETT RL, 1975, ROY STAT SOC C-APP, V24, P193
   Talluri K, 2004, MANAGE SCI, V50, P15, DOI 10.1287/mnsc.1030.0147
   van Ryzin G, 2015, MANAGE SCI, V61, P281, DOI 10.1287/mnsc.2014.2040
   YELLOTT JI, 1977, J MATH PSYCHOL, V15, P109, DOI 10.1016/0022-2496(77)90026-8
NR 25
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701081
DA 2019-06-15
ER

PT S
AU Deza, A
   Eckstein, MP
AF Deza, Arturo
   Eckstein, Miguel P.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Can Peripheral Representations Improve Clutter Metrics on Complex
   Scenes?
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Previous studies have proposed image-based clutter measures that correlate with human search times and/or eye movements. However, most models do not take into account the fact that the effects of clutter interact with the foveated nature of the human visual system: visual clutter further from the fovea has an increasing detrimental influence on perception. Here, we introduce a new foveated clutter model to predict the detrimental effects in target search utilizing a forced fixation search task. We use Feature Congestion (Rosenholtz et al.) as our non foveated clutter model, and we stack a peripheral architecture on top of Feature Congestion for our foveated model. We introduce the Peripheral Integration Feature Congestion (PIFC) coefficient, as a fundamental ingredient of our model that modulates clutter as a non-linear gain contingent on eccentricity. We show that Foveated Feature Congestion (FFC) clutter scores (r(44) = 0.82 +/- 0.04, p < 0.0001) correlate better with target detection (hit rate) than regular Feature Congestion (r(44) = -0.19 +/- 0.13,p = 0.0774) in forced fixation search; and we extend foveation to other clutter models showing stronger correlations in all cases. Thus, our model allows us to enrich clutter perception research by computing fixation specific clutter maps. Code for building peripheral representations is availablel
C1 [Deza, Arturo] UC Santa Barbara, Inst Collaborat Biotechnol, Dynam Neurosci, Santa Barbara, CA 93106 USA.
   [Eckstein, Miguel P.] UC Santa Barbara, Inst Collaborat Biotechnol, Psychol & Brain Sci, Santa Barbara, CA USA.
RP Deza, A (reprint author), UC Santa Barbara, Inst Collaborat Biotechnol, Dynam Neurosci, Santa Barbara, CA 93106 USA.
EM deza@dyns.ucsb.edu; eckstein@psych.ucsb.edu
FU U.S. Army Research Office; Regents of the University of California
   [W911NF-09-0001]
FX We would like to thank Miguel Lago and Aditya Jonnalagadda for useful
   proof-reads and revisions, as well as Mordechai Juni, N.C. Puneeth, and
   Emre Akbas for useful suggestions. This work has been sponsored by the
   U.S. Army Research Office and the Regents of the University of
   California, through Contract Number W911NF-09-0001 for the Institute for
   Collaborative Biotechnologies, and that the content of the information
   does not necessarily reflect the position or the policy of the
   Government or the Regents of the University of California, and no
   official endorsement should be inferred.
CR Achanta R., 2010, TECHNICAL REPORT
   Akbas E., 2014, ARXIV14080814
   Asher MF, 2013, J VISION, V13, DOI 10.1167/13.5.25
   Bravo MJ, 2008, J VISION, V8, DOI 10.1167/8.1.23
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Dubey R., 2014, J VISION, V14, P935
   Eckstein MP, 2011, J VISION, V11, DOI 10.1167/11.5.14
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Freeman J, 2013, NAT NEUROSCI, V16, P974, DOI 10.1038/nn.3402
   Freeman J, 2011, NAT NEUROSCI, V14, P1195, DOI 10.1038/nn.2889
   FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P32, DOI 10.1109/TIT.1975.1055330
   Henderson JM, 2009, J VISION, V9, DOI 10.1167/9.1.32
   Keshvari S, 2016, J VISION, V16, DOI 10.1167/16.3.39
   LANDY MS, 1991, VISION RES, V31, P679, DOI 10.1016/0042-6989(91)90009-T
   Levi DM, 2011, CURR BIOL, V21, pR678, DOI 10.1016/j.cub.2011.07.025
   Levinshtein A, 2009, IEEE T PATTERN ANAL, V31, P2290, DOI 10.1109/TPAMI.2009.96
   Ming-Yu Liu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2097, DOI 10.1109/CVPR.2011.5995323
   Movshon J Anthony, 2014, Cold Spring Harb Symp Quant Biol, V79, P115, DOI 10.1101/sqb.2014.79.024844
   Oliva A., IDENTIFYING PERCEPTU
   Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983
   Pramod RT, 2016, PROC CVPR IEEE, P1601, DOI 10.1109/CVPR.2016.177
   Rosenholtz R, 2005, P SIGCHI C HUM FACT, P761, DOI DOI 10.1145/1054972.1055078
   Rosenholtz R, 2012, J VISION, V12, DOI 10.1167/12.4.14
   Rosenholtz R, 2007, J VISION, V7, DOI 10.1167/7.2.17
   SIMONCELLI EP, 1995, IM PROC INT C, V3, P3444
   van den Berg R, 2009, J VISION, V9, DOI 10.1167/9.4.24
   Yu C.-P., 2013, ADV NEURAL INFORM PR, P118
   Yu CP, 2014, J VISION, V14, DOI 10.1167/14.7.4
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703040
DA 2019-06-15
ER

PT S
AU Dixit, M
   Vasconcelos, N
AF Dixit, Mandar
   Vasconcelos, Nuno
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Object based Scene Representations using Fisher Scores of Local Subspace
   Projections
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID MODELS
AB Several works have shown that deep CNNs can be easily transferred across datasets, e.g. the transfer from object recognition on ImageNet to object detection on Pascal VOC. Less clear, however, is the ability of CNNs to transfer knowledge across tasks. A common example of such transfer is the problem of scene classification, that should leverage localized object detections to recognize holistic visual concepts. While this problems is currently addressed with Fisher vector representations, these are now shown ineffective for the high-dimensional and highly non-linear features extracted by modern CNNs. It is argued that this is mostly due to the reliance on a model, the Gaussian mixture of diagonal covariances, which has a very limited ability to capture the second order statistics of CNN features. This problem is addressed by the adoption of a better model, the mixture of factor analyzers (MFA), which approximates the non-linear data manifold by a collection of local sub-spaces. The Fisher score with respect to the MFA (MFA-FS) is derived and proposed as an image representation for holistic image classifiers. Extensive experiments show that the MFA-FS has state of the art performance for object-to-scene transfer and this transfer actually outperforms the training of a scene CNN from a large scene dataset. The two representations are also shown to be complementary, in the sense that their combination outperforms each of the representations by itself. When combined, they produce a state-of-the-art scene classifier.
C1 [Dixit, Mandar; Vasconcelos, Nuno] Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
RP Dixit, M (reprint author), Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
EM mdixit@ucsd.edu; nvasconcelos@ucsd.edu
CR Cimpoi M., 2015, INT J COMPUTER VISIO
   Dixit Mandar, 2015, IEEE C COMP VIS PATT
   Gao Yang, 2015, CORR
   Ghahramani Z, 1997, TECHNICAL REPORT
   Girshick Ross, 2014, P IEEE C COMP VIS PA
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26
   Jaakkola TS, 1999, ADV NEUR IN, V11, P487
   Karanam S, 2015, IEEE COMPUT SOC CONF
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lin Aruni RoyChowdhury Tsung-Yu, 2015, INT C COMP VIS ICCV
   Liu L., 2014, P ADV NEUR INF PROC, P1143
   Liu Lingqiao, 2016, CORR
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Ren  S., 2015, NEURAL INFORM PROCES
   Sanchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Tanaka Masayuki, 2013, IPSJ T COMPUTER VISI, V5, P50, DOI [10.2197/ipsjtcva.5.50, DOI 10.2197/IPSJTCVA.5.50]
   Torralba Antonio, 2011, CVPR 11
   Verbeek J, 2006, IEEE T PATTERN ANAL, V28, P1236, DOI 10.1109/TPAMI.2006.166
   Vinyals O., 2014, CORR
   Wu Ruobing, 2015, IEEE INT C COMP VIS
   XIAO JX, 2010, PROC CVPR IEEE, P3485, DOI DOI 10.1109/CVPR.2010.5539970
   Zhou  B., 2014, NIPS
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702090
DA 2019-06-15
ER

PT S
AU Djolonga, J
   Jegelka, S
   Tschiatschek, S
   Krause, A
AF Djolonga, Josip
   Jegelka, Stefanie
   Tschiatschek, Sebastian
   Krause, Andreas
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Cooperative Graphical Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID ALGORITHM
AB We study a rich family of distributions that capture variable interactions significantly more expressive than those representable with low-treewidth or pairwise graphical models, or log-supermodular models. We call these cooperative graphical models. Yet, this family retains structure, which we carefully exploit for efficient inference techniques. Our algorithms combine the polyhedral structure of submodular functions in new ways with variational inference methods to obtain both lower and upper bounds on the partition function. While our fully convex upper bound is minimized as an SDP or via tree-reweighted belief propagation, our lower bound is tightened via belief propagation or mean-field algorithms. The resulting algorithms are easy to implement and, as our experiments show, effectively obtain good bounds and marginals for synthetic and real-world examples.
C1 [Djolonga, Josip; Tschiatschek, Sebastian; Krause, Andreas] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
   [Jegelka, Stefanie] MIT, CSAIL, Cambridge, MA 02139 USA.
RP Djolonga, J (reprint author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
EM josipd@inf.ethz.ch; stefje@mit.edu; stschia@inf.ethz.ch;
   krausea@inf.ethz.ch
FU SNSF [CRSII2_147633]; ERC StG [307036]; Microsoft Research; Google; NSF
   CAREER [1553284]
FX This research was supported in part by SNSF grant CRSII2_147633, ERC StG
   307036, a Microsoft Research Faculty Fellowship, a Google European
   Doctoral Fellowship, and NSF CAREER 1553284.
CR Bach F., 2013, FDN TRENDS MACHINE L, V6, P2
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Boykov Y., 2004, PATTERN ANAL MACHINE, V26
   Diamond S., 2016, JMLR
   Djolonga J., 2014, NIPS
   Edmonds J., 1970, COMBINATORIAL STRUCT, P69
   Frank M., 1956, NAVAL RES LOGIST Q
   Fujishige S, 2011, PAC J OPTIM, V7, P3
   Goldberg LA, 2007, COMB PROBAB COMPUT, V16, P43, DOI 10.1017/S096354830600767X
   Hazan T., 2012, ICML
   Heskes T., 2006, JAIR, V26
   Iyer R., 2013, ICML
   Iyer Rishabh, 2015, ARXIV150607329
   Jaggi  M., 2013, ICML
   Jegelka S., 2013, NIPS
   Jegelka S., 2011, ICML
   Jegelka S., 2011, CVPR
   JERRUM M, 1993, SIAM J COMPUT, V22, P1087, DOI 10.1137/0222066
   Kakade S., 2009, TECHNICAL REPORT
   Kohli P., 2013, CVPR
   London B., 2015, ICML
   Meshi O., 2009, UAI
   Mooij JM, 2010, J MACH LEARN RES, V11, P2169
   O'Donoghue B., 2016, J OPTIMIZATION THEOR
   Papandreou G., 2011, ICCV
   Ruozzi N., 2012, NIPS
   Silberman N., 2014, ECCV
   Stobbe P., 2010, NIPS
   Tarlow D., 2012, UAI
   Vilnis L., 2015, UAI
   Vineet  V., 2014, IJCV, V110
   Wainwright M. J., 2006, JMLR, V7
   Wainwright M. J., 2002, UAI
   Wainwright M. J., 2006, SIGNAL PROCESSING IE, V54
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Weller A., 2014, UAI
   Zhang J., 2015, ICCV
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700086
DA 2019-06-15
ER

PT S
AU Djolonga, J
   Tschiatschek, S
   Krause, A
AF Djolonga, Josip
   Tschiatschek, Sebastian
   Krause, Andreas
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Variational Inference in Mixed Probabilistic Submodular Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We consider the problem of variational inference in probabilistic models with both log-submodular and log-supermodular higher-order potentials. These models can represent arbitrary distributions over binary variables, and thus generalize the commonly used pairwise Markov random fields and models with log-supermodular potentials only, for which efficient approximate inference algorithms are known. While inference in the considered models is #P-hard in general, we present efficient approximate algorithms exploiting recent advances in the field of discrete optimization. We demonstrate the effectiveness of our approach in a large set of experiments, where our model allows reasoning about preferences over sets of items with complements and substitutes.
C1 [Djolonga, Josip; Tschiatschek, Sebastian; Krause, Andreas] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
RP Djolonga, J (reprint author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
EM josipd@inf.ethz.ch; tschiats@inf.ethz.ch; krausea@inf.ethz.ch
FU SNSF [CRSII2-147633]; ERC StG [307036]; Microsoft Research; Google
FX The authors acknowledge fruitful discussions with Diego Ballesteros.
   This research was supported in part by SNSF grant CRSII2-147633, ERC StG
   307036, a Microsoft Research Faculty Fellowship and a Google European
   Doctoral Fellowship.
CR Bach F, 2013, FOUND TRENDS MACH LE, V6, P145, DOI 10.1561/2200000039
   DJOLONGA J., 2015, INT C MACH LEARN ICM
   Djolonga Josip, 2014, ADV NEURAL INFORM PR, P244
   Edmonds J, 2003, LECT NOTES COMPUT SC, V2570, P11
   Edmonds J., 1971, MATH PROGRAM, V1, P127, DOI [DOI 10.1007/BF01584082, 10.1007/BF01584082]
   Frank M., 1956, NAVAL RES LOGIST QUA, V3, P95, DOI [10.1002/nav.3800030109, DOI 10.1002/NAV.3800030109]
   Fujishige S, 2005, ANN DISCR MATH, V58, P1
   Gillenwater Jennifer A, 2014, ADV NEURAL INFORM PR
   Gotovos Alkis, 2015, ADV NEURAL INFORM PR, P1936
   Gutmann Michael U, 2012, J MACHINE LEARNING R, V13
   Iyer R., 2012, ARXIV12070560
   Iyer Rishabh, 2015, ARXIV150607329
   Jaggi M., 2013, P 30 INT C MACH LEAR, P427
   Jian Zhang, 2015, INT C COMP VIS ICCV
   Kawahara Yoshinobu, 2015, P 18 INT C ART INT S
   Kulesza A, 2012, FOUND TRENDS MACH LE, V5, P123, DOI 10.1561/2200000044
   Murota K, 2003, DISCRETE CONVEX ANAL
   Narasimhan M., 2012, ARXIV12071404
   Narasimhan Mukund, 2005, UNCERTAINTY ARTIFICI
   Rebeschini Patrick, 2015, P C LEARN THEOR COLT, P1480
   Rother Carsten, 2007, COMP VIS PATT REC 20
   Shioura Akiyoshi, 2014, MATH OPERATIONS RES, V40
   Tschiatschek Sebastian, 2016, P INT C ART INT STAT
   Woodford Oliver, 2009, PATTERN ANAL MACHINE, V31
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701082
DA 2019-06-15
ER

PT S
AU Dohmatob, E
   Mensch, A
   Varoquaux, G
   Thirion, B
AF Dohmatob, Elvis
   Mensch, Arthur
   Varoquaux, Gael
   Thirion, Bertrand
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning brain regions via large-scale online structured sparse
   dictionary-learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We propose a multivariate online dictionary-learning method for obtaining decompositions of brain images with structured and sparse components (aka atoms). Sparsity is to be understood in the usual sense: the dictionary atoms are constrained to contain mostly zeros. This is imposed via an l(1) -norm constraint. By "structured", we mean that the atoms are piece-wise smooth and compact, thus making up blobs, as opposed to scattered patterns of activation. We propose to use a Sobolev (Laplacian) penalty to impose this type of structure. Combining the two penalties, we obtain decompositions that properly delineate brain structures from functional images. This non-trivially extends the online dictionary-learning work of Mairal et al. (2010), at the price of only a factor of 2 or 3 on the overall running time. Just like the Mairal et al. (2010) reference method, the online nature of our proposed algorithm allows it to scale to arbitrarily sized datasets. Preliminary xperiments on brain data show that our proposed method extracts structured and denoised dictionaries that are more intepretable and better capture inter-subject variability in small medium, and large-scale regimes alike, compared to state-of-the-art models.
C1 [Dohmatob, Elvis; Mensch, Arthur; Varoquaux, Gael; Thirion, Bertrand] Univ Paris Saclay, Parietal Team, INRIA CEA, Neurospin, St Aubin, France.
RP Dohmatob, E (reprint author), Univ Paris Saclay, Parietal Team, INRIA CEA, Neurospin, St Aubin, France.
EM elvis.dohmatob@inria.fr; arthur.mensch@inria.fr;
   gael.varoquaux@inria.fr; bertrand.thirion@inria.fr
FU EU [604102]; iConnectome Digiteo
FX This work has been funded by EU FP7/2007-2013 under grant agreement no.
   604102, Human Brain Project (HBP) and the iConnectome Digiteo. We would
   also like to thank the Human Connectome Projection for making their
   wonderful data publicly available.
CR Abraham A., 2013, MICCAI
   Abraham A, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00014
   Baldassarre L., 2012, PRNI
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Beckmann C. F., 2004, T MED IM, V23
   Bertsekas D.P., 1999, NONLINEAR PROGRAMMIN
   Condat L., 2014, MATH PROGRAM
   Dohmatob E., 2014, PRNI
   Duchi J., 2008, ICML
   Friston K. J., 1995, HUM BRAIN MAPP
   Grosenick L, 2013, NEUROIMAGE, V72, P304, DOI 10.1016/j.neuroimage.2012.12.062
   Hebiri M, 2011, ELECTRON J STAT, V5, P1184, DOI 10.1214/11-EJS638
   Hibar D. P., 2013, MICCAI
   Jenatton R., 2010, AISTATS
   Mairal J, 2010, J MACH LEARN RES, V11, P19
   Mensch A., 2016, ICML
   Saxe R, 2006, NEUROIMAGE, V30, P1088, DOI 10.1016/j.neuroimage.2005.12.062
   Smith SM, 2004, NEUROIMAGE, V23, pS208, DOI 10.1016/j.neuroimage.2004.07.051
   van Essen D., 2012, NEUROIMAGE, V62
   Varol E., 2014, MED IMAGE COMPUT COM, V17
   Varoquaux G, 2010, NEUROIMAGE, V51, P288, DOI 10.1016/j.neuroimage.2010.02.010
   Varoquaux G., 2015, ARXIV151206999
   Varoquaux G., 2013, IPMI
   Varoquaux G., 2011, INF P MED IM
   Ying Y., 2006, IEEE T INF THEORY, V52
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702099
DA 2019-06-15
ER

PT S
AU Dolhansky, B
   Bilmes, J
AF Dolhansky, Brian
   Bilmes, Jeff
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Deep Submodular Functions: Definitions & Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We propose and study a new class of submodular functions called deep submodular functions (DSFs). We define DSFs and situate them within the broader context of classes of submodular functions in relationship both to various matroid ranks and sums of concave composed with modular functions (SCMs). Notably, we find that DSFs constitute a strictly broader class than SCMs, thus motivating their use, but that they do not comprise all submodular functions. Interestingly, some DSFs can be seen as special cases of certain deep neural networks (DNNs), hence the name. Finally, we provide a method to learn DSFs in a max-margin framework, and offer preliminary results applying this both to synthetic and real-world data instances.
C1 [Dolhansky, Brian] Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98105 USA.
   [Bilmes, Jeff] Univ Washington, Dept Elect Engn, Seattle, WA 98105 USA.
RP Dolhansky, B (reprint author), Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98105 USA.
EM bdol@cs.washington.edu; bilmes@uw.edu
FU National Science Foundation [IIS-1162606]; National Institutes of Health
   [R01GM103544]; Google; Microsoft; Facebook; Intel research award;
   TerraSwarm, one of six centers of STARnet; Semiconductor Research
   Corporation program - MARCO; DARPA
FX Thanks to Reza Eghbali and Kai Wei for useful discussions. This material
   is based upon work supported by the National Science Foundation under
   Grant No. IIS-1162606, the National Institutes of Health under award
   R01GM103544, and by a Google, a Microsoft, a Facebook, and an Intel
   research award. This work was supported in part by TerraSwarm, one of
   six centers of STARnet, a Semiconductor Research Corporation program
   sponsored by MARCO and DARPA.
CR Badanidiyuru A, 2014, P 20 ACM SIGKDD INT, P671
   Balcan M., 2010, TECHNICAL REPORT
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bilmes Jeffrey, 2017, ABS170108939 ARXIV
   Boyd S., 2004, CONVEX OPTIMIZATION
   Buchbinder N, 2014, P 25 ANN ACM SIAM S, V25, P1433
   Calinescu G, 2011, SIAM J COMPUT, V40, P1740, DOI 10.1137/080733991
   Chekuri C, 2015, LECT NOTES COMPUT SC, V9134, P318, DOI 10.1007/978-3-662-47672-7_26
   CUNNINGHAM WH, 1984, J COMB THEORY B, V36, P161, DOI 10.1016/0095-8956(84)90023-6
   Feldman V., 2013, CORR
   Fujishige S, 2005, ANN DISCR MATH, V58, P1
   Goemans MX, 2009, PROCEEDINGS OF THE TWENTIETH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P535
   Iyer R., 2012, UNCERTAINTY ARTIFICI
   Iyer Rishabh, 2013, INT C MACH LEARN ICM
   Iyer Rishabh, 2013, NEURAL INFORM PROCES
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee J, 2009, ACM S THEORY COMPUT, P323
   Lin H., 2011, CLASS SUBMODULAR FUN
   Lin H., 2012, UNCERTAINTY ARTIFICI
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Nishihara R., 2014, ADV NEURAL INFORM PR, P640
   Pei Wenzhe, 2014, P ACL 14, V1, P293
   Sipos Ruben, 2012, P 13 C EUR CHAPT ASS, P224
   Stobbe P., 2010, NIPS
   Taskar B., 2005, P 22 INT C MACH LEAR, P896, DOI DOI 10.1145/1102351.1102464
   Tschiatschek S., 2014, NEURAL INFORM PROCES
   Vondrak J., 2011, COMMUNICATION
   Wei K., 2014, P IEEE INT C AC SPEE
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702108
DA 2019-06-15
ER

PT S
AU Dosovitskiy, A
   Brox, T
AF Dosovitskiy, Alexey
   Brox, Thomas
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Generating Images with Perceptual Similarity Metrics based on Deep
   Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We propose a class of loss functions, which we call deep perceptual similarity metrics (DeePSiM), allowing to generate sharp high resolution images from compressed abstract representations. Instead of computing distances in the image space, we compute distances between image features extracted by deep neural networks. This metric reflects perceptual similarity of images much better and, thus, leads to better results. We demonstrate two examples of use cases of the proposed loss: (1) networks that invert the AlexNet convolutional network; (2) a modified version of a variational autoencoder that generates realistic high-resolution random images.
C1 [Dosovitskiy, Alexey; Brox, Thomas] Univ Freiburg, Freiburg, Germany.
RP Dosovitskiy, A (reprint author), Univ Freiburg, Freiburg, Germany.
EM dosovits@cs.uni-freiburg.de; brox@cs.uni-freiburg.de
FU ERC Starting Grant VideoLearn [279401]
FX We acknowledge funding by the ERC Starting Grant VideoLearn (279401).
CR Daly Scott, 1993, P179
   Denton E. L., 2015, ADV NEURAL INFORM PR, P1486
   Dosovitskiy A., 2016, CVPR
   Dosovitskiy Alexey, 2015, CVPR
   Gatys L. A., 2016, CVPR
   Goodfellow I., 2014, NIPS
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G.E., 1986, PARALLEL DISTRIBUTED, V1, P282
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jia Y., 2014, ARXIV14085093
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kingma D. P., 2015, ICLR
   Kingma Diederik P, 2014, ICLR
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lamb A., 2016, ARXIV160203220
   Larsen A. B. L, 2016, P 33 INT C MACH LEAR, P1558
   Lee H, 2009, P ANN INT C MACH LEA, V26, P609, DOI DOI 10.1145/1553374.1553453
   Mahendran A., 2015, CVPR
   Mathieu M., 2016, ICLR
   Mirza M., 2014, ARXIV14111784, DOI DOI 10.1029/2009WR008312
   Radford A., 2016, ICLR
   Ridgeway K., 2015, ARXIV151106409
   van den Branden Lambrecht C. J., 1996, ELECT IMAGING SCI TE
   Wang X., 2015, ICCV
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Winkler S., 1998, P SOC PHOTO-OPT INS, P175
NR 26
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701015
DA 2019-06-15
ER

PT S
AU Dubey, A
   Reddi, SJ
   Poczos, B
   Smola, AJ
   Xing, EP
AF Dubey, Avinava
   Reddi, Sashank J.
   Poczos, Barnabas
   Smola, Alexander J.
   Xing, Eric P.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Variance Reduction in Stochastic Gradient Langevin Dynamics
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Stochastic gradient-based Monte Carlo methods such as stochastic gradient Langevin dynamics are useful tools for posterior inference on large scale datasets in many machine learning applications. These methods scale to large datasets by using noisy gradients calculated using a mini-batch or subset of the dataset. However, the high variance inherent in these noisy gradients degrades performance and leads to slower mixing. In this paper, we present techniques for reducing variance in stochastic gradient Langevin dynamics, yielding novel stochastic Monte Carlo methods that improve performance by reducing the variance in the stochastic gradient. We show that our proposed method has better theoretical guarantees on convergence rate than stochastic Langevin dynamics. This is complemented by impressive empirical results obtained on a variety of real world datasets, and on four different machine learning tasks (regression, classification, independent component analysis and mixture modeling). These theoretical and empirical contributions combine to make a compelling case for using variance reduction in stochastic Monte Carlo methods.
C1 [Dubey, Avinava; Reddi, Sashank J.; Poczos, Barnabas; Smola, Alexander J.; Xing, Eric P.] Carnegie Mellon Univ, Dept Machine Learning, Pittsburgh, PA 15213 USA.
RP Dubey, A (reprint author), Carnegie Mellon Univ, Dept Machine Learning, Pittsburgh, PA 15213 USA.
EM akdubey@cs.cmu.edu; sjakkamr@cs.cmu.edu; bapoczos@cs.cmu.edu;
   alex@cs.cmu.edu; epxing@cs.cmu.edu
CR Ahn S., 2012, ICML
   Ahn S., 2014, ICML
   Chen C., 2015, NIPS
   Chen T., 2014, ICML
   Defazio A., 2014, NIPS
   Ding N., 2014, NIPS
   Girolami Mark, 2011, J ROYAL STAT SOC B
   Johnson R., 2013, NIPS
   Ma Y. A., 2015, NIPS
   NEAL R. M., 2010, HDB MARKOV CHAIN MON
   Nesterov Yurii E., 2003, INTRO LECT CONVEX OP
   Patterson S., 2013, NIPS
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Schmidt  M., 2013, ARXIV13092388
   Welling M., 2011, ICML
NR 15
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702040
DA 2019-06-15
ER

PT S
AU Durmus, A
   Simsekli, U
   Moulines, E
   Badeau, R
   Richard, G
AF Durmus, Alain
   Simsekli, Umut
   Moulines, Eric
   Badeau, Roland
   Richard, Gael
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Stochastic Gradient Richardson-Romberg Markov Chain Monte Carlo
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC) algorithms have become increasingly popular for Bayesian inference in large-scale applications. Even though these methods have proved useful in several scenarios, their performance is often limited by their bias. In this study, we propose a novel sampling algorithm that aims to reduce the bias of SG-MCMC while keeping the variance at a reasonable level. Our approach is based on a numerical sequence acceleration method, namely the Richardson-Romberg extrapolation, which simply boils down to running almost the same SG-MCMC algorithm twice in parallel with different step sizes. We illustrate our framework on the popular Stochastic Gradient Langevin Dynamics (SGLD) algorithm and propose a novel SG-MCMC algorithm referred to as Stochastic Gradient Richardson-Romberg Langevin Dynamics (SGRRLD). We provide formal theoretical analysis and show that SGRRLD is asymptotically consistent, satisfies a central limit theorem, and its non-asymptotic bias and the mean squared-error can be bounded. Our results show that SGRRLD attains higher rates of convergence than SGLD in both finite-time and asymptotically, and it achieves the theoretical accuracy of the methods that are based on higher-order integrators. We support our findings using both synthetic and real data experiments.
C1 [Durmus, Alain; Simsekli, Umut; Badeau, Roland; Richard, Gael] Univ Paris Saclay, Telecom ParisTech, CNRS, LTCI, F-75013 Paris, France.
   [Moulines, Eric] Ecole Polytech, UMR 7641, Ctr Math Appl, Palaiseau, France.
RP Durmus, A (reprint author), Univ Paris Saclay, Telecom ParisTech, CNRS, LTCI, F-75013 Paris, France.
FU French National Research Agency (ANR) as a part of the EDISON 3D project
   [ANR-13-CORD-0008-02]
FX This work is partly supported by the French National Research Agency
   (ANR) as a part of the EDISON 3D project (ANR-13-CORD-0008-02).
CR Ahn S., 2012, ICML
   Ahn S., 2015, KDD
   Chen C., 2015, NIPS, P2269
   Chen T., 2014, ICML
   Ding N., 2014, ADV NEURAL INFORM PR, P3203
   Gemulla R., 2011, ACM SIGKDD
   Grenander U., 1983, DIVISION APPL MATH
   Lamberton D, 2002, BERNOULLI, V8, P367
   Lamoerton D., 2003, STOCH DYNAM, V3, P435
   Lemaire  V., 2005, THESIS
   Lemaire V., 2014, ARXIV14011177
   Lemaire V, 2015, ANN I H POINCARE-PR, V51, P1562, DOI 10.1214/13-AIHP591
   Li C., 2016, AAAI C ART INT
   Ma Y. A., 2015, ADV NEURAL INFORM PR, P2899
   Mattingly JC, 2002, STOCH PROC APPL, V101, P185, DOI 10.1016/S0304-4149(02)00150-3
   Mnih A., 2008, P INT C MACH LEARN, V25, P880, DOI DOI 10.1145/1390156.1390267
   Pages G, 2007, MONTE CARLO METHODS, V13, P37, DOI 10.1515/MCMA.2007.003
   Patterson S., 2013, NIPS
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Roberts GO, 1996, BERNOULLI, V2, P341, DOI DOI 10.2307/3318418
   Sato I., 2014, P 31 INT C MACH LEAR, V32, P982
   Shang X., 2015, ADV NEURAL INFORM PR, P37
   Simsekli U., 2016, ICML
   TALAY D, 1990, STOCH ANAL APPL, V8, P483, DOI 10.1080/07362999008809220
   Teh Y. W., 2015, ARXIV150100438
   Vollmer SJ, 2016, J MACH LEARN RES, V17, P1
   Welling M., 2011, P 28 INT C MACH LEAR, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704039
DA 2019-06-15
ER

PT S
AU Dutta, S
   Cadambe, V
   Grover, P
AF Dutta, Sanghamitra
   Cadambe, Viveck
   Grover, Pulkit
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI "Short-Dot": Computing Large Linear Transforms Distributedly Using Coded
   Short Dot Products
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Faced with saturation of Moore's law and increasing size and dimension of data, system designers have increasingly resorted to parallel and distributed computing to reduce computation time of machine-learning algorithms. However, distributed computing is often bottle necked by a small fraction of slow processors called "stragglers" that reduce the speed of computation because the fusion node has to wait for all processors to complete their processing. To combat the effect of stragglers, recent literature proposes introducing redundancy in computations across processors, e.g., using repetition-based strategies or erasure codes. The fusion node can exploit this redundancy by completing the computation using outputs from only a subset of the processors, ignoring the stragglers. In this paper, we propose a novel technique - that we call "Short-Dot" - to introduce redundant computations in a coding theory inspired fashion, for computing linear transforms of long vectors. Instead of computing long dot products as required in the original linear transform, we construct a larger number of redundant and short dot products that can be computed more efficiently at individual processors. Further, only a subset of these short dot products are required at the fusion node to finish the computation successfully. We demonstrate through probabilistic analysis as well as experiments on computing clusters that Short-Dot offers significant speed-up compared to existing techniques. We also derive trade-offs between the length of the dot-products and the resilience to stragglers (number of processors required to finish), for any such strategy and compare it to that achieved by our strategy.
C1 [Dutta, Sanghamitra; Grover, Pulkit] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Cadambe, Viveck] Penn State Univ, University Pk, PA 16802 USA.
RP Dutta, S (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM sanghamd@andrew.cmu.edu; viveck@engr.psu.edu; pgrover@andrew.cmu.edu
FU MARCO; DARPA; NSF [1350314, 1464336, 1553248]; Prabhu and Poonam Goel
   Graduate Fellowship
FX Systems on Nanoscale Information fabriCs (SONIC), one of the six SRC
   STARnet Centers, sponsored by MARCO and DARPA. We also acknowledge NSF
   Awards 1350314, 1464336 and 1553248. S Dutta also received Prabhu and
   Poonam Goel Graduate Fellowship.
CR Ballard G, 2014, COMMUN ACM, V57, P107, DOI 10.1145/2556647.2556660
   Da Wang, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P7, DOI 10.1145/2847220.2847223
   Da Wang, 2014, ACM SIGMETRICS Performance Evaluation Review, V42, P599, DOI 10.1145/2591971.2592042
   Dally  W., 2015, NIPS TUTORIAL
   Dean J, 2013, COMMUN ACM, V56, P74, DOI 10.1145/2408776.2408794
   FOX GC, 1987, PARALLEL COMPUT, V4, P17, DOI 10.1016/0167-8191(87)90060-3
   Herault T, 2015, COMPUT COMMUN NETW S, P1, DOI 10.1007/978-3-319-20943-2
   HUANG KH, 1984, IEEE T COMPUT, V33, P518, DOI 10.1109/TC.1984.1676475
   Joshi G, 2014, IEEE J SEL AREA COMM, V32, P989, DOI 10.1109/JSAC.2014.140518
   Kumar V, 1994, INTRO PARALLEL COMPU
   LeCun  Y., 1998, MNIST DATABASE HANDW
   Lee Kangwook, 2015, NIPS WORKSH LEARN SY
   Li Songze, 2016, ARXIV160901690V1CSIT
   Longbo Huang, 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P2766, DOI 10.1109/ISIT.2012.6284026
   Nahlus I, 2014, I SYMPOS LOW POWER E, P315, DOI 10.1145/2627369.2627664
   Ryan W. E., 2009, CHANNEL CODES CLASSI
   Shvachko K, 2010, IEEE S MASS STOR SYS
   STRASSEN V, 1969, NUMER MATH, V13, P354, DOI 10.1007/BF02165411
   Wang R, 2016, IEEE VLSI TEST SYMP
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702076
DA 2019-06-15
ER

PT S
AU Eberhardt, S
   Cader, J
   Serre, T
AF Eberhardt, Sven
   Cader, Jonah
   Serre, Thomas
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI How Deep is the Feature Analysis underlying Rapid Visual Categorization?
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Rapid categorization paradigms have a long history in experimental psychology: Characterized by short presentation times and speeded behavioral responses, these tasks highlight the efficiency with which our visual system processes natural object categories. Previous studies have shown that feed-forward hierarchical models of the visual cortex provide a good fit to human visual decisions. At the same time, recent work in computer vision has demonstrated significant gains in object recognition accuracy with increasingly deep hierarchical architectures. But it is unclear how well these models account for human visual decisions and what they may reveal about the underlying brain processes.
   We have conducted a large-scale psychophysics study to assess the correlation between computational models and human behavioral responses on a rapid animal vs. non-animal categorization task. We considered visual representations of varying complexity by analyzing the output of different stages of processing in three stateof- the-art deep networks. We found that recognition accuracy increases with higher stages of visual processing (higher level stages indeed outperforming human participants on the same task) but that human decisions agree best with predictions from intermediate stages.
   Overall, these results suggest that human participants may rely on visual features of intermediate complexity and that the complexity of visual representations afforded by modern deep network models may exceed the complexity of those used by human participants during rapid categorization.
C1 [Eberhardt, Sven; Cader, Jonah; Serre, Thomas] Brown Univ, Brown Inst Brain Sci, Dept Cognit Linguist & Psychol Sci, Providence, RI 02818 USA.
RP Eberhardt, S (reprint author), Brown Univ, Brown Inst Brain Sci, Dept Cognit Linguist & Psychol Sci, Providence, RI 02818 USA.
EM sven2@brown.edu; jonah_cader@brown.edu; thomas_serre@brown.edu
FU NSF [IIS-1252951]; DARPA [YFA N66001-14-1-4037]; Center for Computation
   and Visualization (CCV)
FX We would like to thank Matt Ricci for his early contribution to this
   work and further discussions. This work was supported by NSF early
   career award [grant number IIS-1252951] and DARPA young faculty award
   [grant number YFA N66001-14-1-4037]. Additional support was provided by
   the Center for Computation and Visualization (CCV).
CR Cauchoix M, 2016, NEUROIMAGE, V125, P280, DOI 10.1016/j.neuroimage.2015.10.012
   Crouzet SM, 2011, FRONT PSYCHOL, V2, DOI 10.3389/fpsyg.2011.00326
   Crump MJC, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0057410
   DENG J, 2009, PROC CVPR IEEE, P248, DOI DOI 10.1109/CVPR.2009.5206848
   DiCarlo JJ, 2012, NEURON, V73, P415, DOI 10.1016/j.neuron.2012.01.010
   Fabre-Thorpe M, 2011, FRONT PSYCHOL, V2, DOI 10.3389/fpsyg.2011.00243
   Gilbert CD, 2013, NAT REV NEUROSCI, V14, P350, DOI 10.1038/nrn3476
   He K., 2015, DELVING DEEP RECTIFI
   He Kaiming, 2016, ABS160305027 CORR
   Jia Y., 2014, P 2014 ACM C MULT MM, P10005
   Krizhevsky A, 2012, NEURAL INF PROCESS S
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   McDonnell J., 2012, PSITURK VERSION 1 02
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Potter MC, 2012, FRONT PSYCHOL, V3, DOI 10.3389/fpsyg.2012.00032
   Serre T, 2007, P NATL ACAD SCI USA, V104, P6424, DOI 10.1073/pnas.0700622104
   Simonyan K., 2014, TECHNICAL REPORT
   Sofer I, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004456
   Vanrullen Rufin, 2008, Adv Cogn Psychol, V3, P167, DOI 10.2478/v10053-008-0022-3
   Yamins DLK, 2016, NAT NEUROSCI, V19, P356, DOI 10.1038/nn.4244
   Yu W., 2014, ARXIV14126631
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701075
DA 2019-06-15
ER

PT S
AU Eghbali, R
   Fazel, M
AF Eghbali, Reza
   Fazel, Maryam
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Designing smoothing functions for improved worst-case competitive ratio
   in online optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID PRIMAL-DUAL ALGORITHMS
AB Online optimization covers problems such as online resource allocation, online bipartite matching, adwords (a central problem in e-commerce and advertising), and adwords with separable concave returns. We analyze the worst case competitive ratio of two primal-dual algorithms for a class of online convex (conic) optimization problems that contains the previous examples as special cases defined on the positive orthant. We derive a sufficient condition on the objective function that guarantees a constant worst case competitive ratio (greater than or equal to 1/2) for monotone objective functions. We provide new examples of online problems on the positive orthant that satisfy the sufficient condition. We show how smoothing can improve the competitive ratio of these algorithms, and in particular for separable functions, we show that the optimal smoothing can be derived by solving a convex optimization problem. This result allows us to directly optimize the competitive ratio bound over a class of smoothing functions, and hence design effective smoothing customized for a given cost function.
C1 [Eghbali, Reza; Fazel, Maryam] Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA.
RP Eghbali, R (reprint author), Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA.
EM eghbali@uw.edu; mfazel@uw.edu
CR Abernethy Jacob, 2008, P 21 ANN C LEARN THE, P263
   Agrawal Shipra, 2014, ARXIV14107596
   Azar Yossi, 2014, ARXIV14123507
   Buchbinder N, 2007, LECT NOTES COMPUT SC, V4698, P253
   Buchbinder N, 2009, MATH OPER RES, V34, P270, DOI 10.1287/moor.1080.0363
   Buchbinder Niv, 2014, ARXIV14128347
   Chan TH, 2015, ARXIV150201802
   Devanur N. R., 2011, P 12 ACM C EL COMM, P29
   Devanur Nikhil R., 2012, P 44 S THEOR COMP, P137
   Eghbali R., 2016, 55 IEEE C DEC CONTR
   Eghbali Reza, 2014, ARXIV14107171
   Gupta Anupam, 2014, ARXIV14075298
   Kalyanasundaram B, 2000, THEOR COMPUT SCI, V233, P319, DOI 10.1016/S0304-3975(99)00140-1
   KARP RM, 1990, PROCEEDINGS OF THE TWENTY SECOND ANNUAL ACM SYMPOSIUM ON THEORY OF COMPUTING, P352, DOI 10.1145/100216.100262
   Kleinberg R, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P630
   Mehta A, 2007, J ACM, V54, DOI 10.1145/1284320.1284321
   Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5
   Nesterov Y, 2009, MATH PROGRAM, V120, P221, DOI 10.1007/s10107-007-0149-x
   Shalev-Shwartz S., 2007, ONLINE LEARNING THEO
   Shalev-Shwartz S, 2007, MACH LEARN, V69, P115, DOI 10.1007/s10994-007-5014-x
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700037
DA 2019-06-15
ER

PT S
AU Eldridge, J
   Belkin, M
   Wang, YS
AF Eldridge, Justin
   Belkin, Mikhail
   Wang, Yusu
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Graphons, mergeons, and so on!
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB In this work we develop a theory of hierarchical clustering for graphs. Our modeling assumption is that graphs are sampled from a graphon, which is a powerful and general model for generating graphs and analyzing large networks. Graphons are a far richer class of graph models than stochastic blockmodels, the primary setting for recent progress in the statistical theory of graph clustering. We define what it means for an algorithm to produce the "correct" clustering, give sufficient conditions in which a method is statistically consistent, and provide an explicit algorithm satisfying these properties.
C1 [Eldridge, Justin; Belkin, Mikhail; Wang, Yusu] Ohio State Univ, Columbus, OH 43210 USA.
RP Eldridge, J (reprint author), Ohio State Univ, Columbus, OH 43210 USA.
EM eldridge@cse.ohio-state.edu; mbelkin@cse.ohio-state.edu;
   yusu@cse.ohio-state.edu
FU NSF [IIS-1550757]
FX This work was supported by NSF grant IIS-1550757.
CR Abbe E, 2016, IEEE T INFORM THEORY, V62, P471, DOI 10.1109/TIT.2015.2490670
   Airoldi EM, 2013, ADV NEURAL INFORM PR, V26, P692
   Ash R., 2000, PROBABILITY MEASURE
   Balakrishnan S., 2011, ADV NEURAL INFORM PR, P954
   Borgs C, 2008, ADV MATH, V219, P1801, DOI 10.1016/j.aim.2008.07.008
   Borgs Christian, 2016, ARXIV160107134
   Caron F., 2014, ARXIV14011137
   CHAN S. H., 2014, J MACH LEARN RES C P, P208
   CHAUDHURI K., 2010, ADV NEURAL INFORM PR, P343
   ELDRIDGE J., 2015, P 28 C LEARN THEOR, P588
   Girvan M, 2002, P NATL ACAD SCI USA, V99, P7821, DOI 10.1073/pnas.122653799
   HARTIGAN JA, 1981, J AM STAT ASSOC, V76, P388, DOI 10.2307/2287840
   Janson Svante, 2008, ARXIV08023795
   JWolfe Patrick, 2013, ARXIV13095936
   KPOTUFE S., 2011, P 28 INT C MACH LEAR, P225
   Lovasz L., 2012, LARGE NETWORKS GRAPH, V60
   Lovasz L, 2006, J COMB THEORY B, V96, P933, DOI 10.1016/j.jctb.2006.05.002
   McSherry F, 2001, ANN IEEE SYMP FOUND, P529, DOI 10.1109/SFCS.2001.959929
   Rohe K, 2011, ANN STAT, V39, P1878, DOI 10.1214/11-AOS887
   STEINWART I., 2011, P 24 ANN C LEARN THE, P703
   Zhang Yuan, 2015, ARXIV150908588
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700053
DA 2019-06-15
ER

PT S
AU Elhamifar, E
AF Elhamifar, E.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI High-Rank Matrix Completion and Clustering under Self-Expressive Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We propose efficient algorithms for simultaneous clustering and completion of incomplete high-dimensional data that lie in a union of low-dimensional subspaces. We cast the problem as finding a completion of the data matrix so that each point can be reconstructed as a linear or affine combination of a few data points. Since the problem is NP-hard, we propose a lifting framework and reformulate the problem as a group-sparse recovery of each incomplete data point in a dictionary built using incomplete data, subject to rank-one constraints. To solve the problem efficiently, we propose a rank pursuit algorithm and a convex relaxation. The solution of our algorithms recover missing entries and provides a similarity matrix for clustering. Our algorithms can deal with both low-rank and high-rank matrices, does not suffer from initialization, does not need to know dimensions of subspaces and can work with a small number of data points. By extensive experiments on synthetic data and real problems of video motion segmentation and completion of motion capture data, we show that when the data matrix is low-rank, our algorithm performs on par with or better than low-rank matrix completion methods, while for high-rank data matrices, our method significantly outperforms existing algorithms.
C1 [Elhamifar, E.] Northeastern Univ, Coll Comp & Informat Sci, Boston, MA 02115 USA.
RP Elhamifar, E (reprint author), Northeastern Univ, Coll Comp & Informat Sci, Boston, MA 02115 USA.
EM eelhami@ccs.neu.edu
CR Balzano L., 2012, IEEE STAT SIGN PROC
   Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153
   Bhojanapalli S., 2013, INT C MACH LEARN ICM
   Boyd  S., 2010, FDN TRENDS MACHINE L, V3
   Candes E. J., 2014, ROBUST SUBSPAC UNPUB
   Candes E. J., 2008, FDN COMPUTATIONAL MA, V9
   Candes E. J., 2009, P IEEE
   Chen G., 2009, INT J COMPUTER VISIO, V81
   Chen Y., 2011, INT C MACH LEARN ICM
   Chiang K. Y., 2015, NEURAL INFORM PROCES
   Costeira J., 1998, INT J COMPUTER VISIO, V29
   Elhamifar  E., 2013, IEEE T PATTERN ANAL
   Elhamifar E., 2016, IEEE T PATTERN ANAL
   Elhamifar E., 2009, IEEE C COMP VIS PATT
   Elhamifar E, 2011, PROC CVPR IEEE
   Eriksson B., 2012, INT C ART INT STAT
   Gabay D., 1976, COMP MATH APPL, V2
   Ganti R., 2015, NEURAL INFORM PROCES
   Ghahramani Z., 1996, CRGTR961 U TOR DEP C
   Gruber A., 2004, IEEE C COMP VIS PATT
   Hastie T., 1998, STAT SCI
   Jenatton R., 2011, J MACHINE LEARNING R, V12
   Kanatani K., 2001, IEEE INT C COMP VIS, V2
   Keshavan R. H., 2010, IEEE T INFORM THEORY
   Knott M., 1999, LATENT VARIABLE MODE
   Mairal J, 2009, INT C MACH LEARN
   Ng A., 2001, NEURAL INFORM PROCES
   Park D., 2015, INT C MACH LEARN ICM
   Soltanolkotabi  M., 2014, ANN STAT
   Tipping M., 1999, NEURAL COMPUTATION, V11
   Tipping M., 1999, J ROYAL STAT SOC, V61
   Tomasi C., 1992, INT J COMPUTER VISIO, V9
   Tropp J., 2009, ACM SIAM S DISCR ALG
   Vidal R., 2008, INT J COMPUTER VISIO, V79
   Yan J., 2006, EUR C COMP VIS
   Yang C., 2015, INT C MACH LEARN ICM
   Zhang A., 2012, UNCERTAINTY ARTIFICI
   Zhao B., 2009, ANN STAT, V37
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702104
DA 2019-06-15
ER

PT S
AU Ellis, K
   Solar-Lezama, A
   Tenenbaum, JB
AF Ellis, Kevin
   Solar-Lezama, Armando
   Tenenbaum, Joshua B.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Sampling for Bayesian Program Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Towards learning programs from data, we introduce the problem of sampling programs from posterior distributions conditioned on that data. Within this setting, we propose an algorithm that uses a symbolic solver to efficiently sample programs. The proposal combines constraint-based program synthesis with sampling via random parity constraints. We give theoretical guarantees on how well the samples approximate the true posterior, and have empirical results showing the algorithm is efficient in practice, evaluating our approach on 22 program learning problems in the domains of text editing and computer-aided programming.
C1 [Ellis, Kevin; Tenenbaum, Joshua B.] MIT, Brain & Cognit Sci, Cambridge, MA 02139 USA.
   [Solar-Lezama, Armando] MIT, CSAIL, Cambridge, MA 02139 USA.
RP Ellis, K (reprint author), MIT, Brain & Cognit Sci, Cambridge, MA 02139 USA.
EM ellisk@mit.edu; asolar@csail.mit.edu; jbt@mit.edu
FU AFOSR [FA9550-16-1-0012];  [NSF-1161775]
FX We are grateful for feedback from Adam Smith, Kuldeep Meel, and our
   anonymous reviewers. Work supported by NSF-1161775 and AFOSR award
   FA9550-16-1-0012.
CR Achlioptas Dimitris, 2015, STOCHASTIC INTEGRATI
   Chakraborty Supratik, 2013, Computer Aided Verification. 25th International Conference, CAV 2013. Proceedings. LNCS 8044, P608, DOI 10.1007/978-3-642-39799-8_40
   Chakraborty Supratik, 2014, AAAI C ART INT
   Ellis K., 2015, ADV NEURAL INFORM PR, P973
   Ermon S., 2014, P 31 INT C MACH LEAR, P271
   Ermon S., 2013, ADV NEURAL INFORM PR, P2085
   Gomes C. P., 2006, ADV NEURAL INFORM PR, V19, P481
   Gomes Carla P, 2006, AAAI C ART INT
   Graves A., 2014, ARXIV14105401
   Gulwani S, 2011, ACM SIGPLAN NOTICES, V46, P317, DOI 10.1145/1925844.1926423
   Jha S., 2010, P ICSE, P215, DOI DOI 10.1145/1806799.1806833
   Katz Yarden, 2008, COGSCI, P71
   Koza J. R., 1993, GENETIC PROGRAMMING
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   Lezama Armando Solar, 2008, THESIS
   Liang P., 2011, HLT 11, P590
   Liang P., 2010, P 27 INT C MACH LEAR, P639
   Lin DH, 2014, FRONT ARTIF INTEL AP, V263, P525, DOI 10.3233/978-1-61499-419-0-525
   Menon A., 2013, P 30 INT C MACH LEAR, P187
   RAYCHEV V, 2016, POPL, V51, P761, DOI DOI 10.1145/2837614.2837671
   Reed Scott, 2015, ABS151106279 CORR
   Schkufza E, 2013, ACM SIGPLAN NOTICES, V48, P305, DOI 10.1145/2499368.2451150
   Singh R, 2013, ACM SIGPLAN NOTICES, V48, P15, DOI 10.1145/2499370.2462195
   Valiant L. G., 1985, P 17 ANN ACM S THEOR, P458
NR 24
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700046
DA 2019-06-15
ER

PT S
AU Erdogdu, MA
   Bayati, M
   Dicker, LH
AF Erdogdu, Murat A.
   Bayati, Mohsen
   Dicker, Lee H.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Scaled Least Squares Estimator for GLMs in Large-Scale Problems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We study the problem of efficiently estimating the coefficients of generalized linear models (GLMs) in the large-scale setting where the number of observations n is much larger than the number of predictors p, i.e. n >> p >> 1. We show that in GLMs with random (not necessarily Gaussian) design, the GLM coefficients are approximately proportional to the corresponding ordinary least squares (OLS) coefficients. Using this relation, we design an algorithm that achieves the same accuracy as the maximum likelihood estimator (MLE) through iterations that attain up to a cubic convergence rate, and that are cheaper than any batch optimization algorithm by at least a factor of 0(p). We provide theoretical guarantees for our algorithm, and analyze the convergence behavior in terms of data dimensions. Finally, we demonstrate the performance of our algorithm through extensive numerical studies on large-scale real and synthetic datasets, and show that it achieves the highest performance compared to several other widely used optimization algorithms.
C1 [Erdogdu, Murat A.] Stanford Univ, Dept Stat, Stanford, CA 94305 USA.
   [Bayati, Mohsen] Stanford Univ, Grad Sch Business, Stanford, CA 94305 USA.
   [Dicker, Lee H.] Rutgers State Univ, Dept Stat & Biostat, New Brunswick, NJ USA.
   [Dicker, Lee H.] Amazon, Seattle, WA USA.
RP Erdogdu, MA (reprint author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.
EM erdogdu@stanford.edu; bayati@stanford.edu; ldicker@stat.rutgers.edu
CR Baldi P, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5308
   Bayati Mohsen, 2013, ADV NEURAL INFORM PR, P944
   Bishop C M, 1995, NEURAL NETWORKS PATT
   Blackard JA, 1999, COMPUT ELECTRON AGR, V24, P131, DOI 10.1016/S0168-1699(99)00046-0
   Boyd S., 2004, CONVEX OPTIMIZATION
   Brillinger D. R, 1982, FESTSCHRIFT EL LEHMA, P97
   Chen LHY, 2011, PROBAB APPL SER, P1, DOI 10.1007/978-3-642-15007-4
   Dhillon P., 2013, ADV NEURAL INFORM PR, P360
   Erdogdu M. A., 2015, ADV NEURAL INFORM PR, V28, P1216
   Erdogdu M. A., 2016, J MACHINE LEARNING R
   Erdogdu Murat A, 2015, ADV NEURAL INFORM PR, V28, P3034
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   Goldstein L, 2007, ANN PROBAB, V35, P1888, DOI 10.1214/009117906000001123
   Goldstein L, 1997, ANN APPL PROBAB, V7, P935
   HESTENES MR, 1952, J RES NAT BUR STAND, V49, P409, DOI 10.6028/jres.049.044
   Koller D., 2009, PROBABILISTIC GRAPHI
   LI KC, 1989, ANN STAT, V17, P1009, DOI 10.1214/aos/1176347254
   Martens J., 2010, P 27 INT C MACH LEAR, P735, DOI DOI 10.1155/2011/176802
   Mccullagh P, 2004, INTRO LECT CONVEX OP
   McCullagh P., 1989, GEN LINEAR MODELS
   Nesterov Y., 1983, SOV MATH DOKL, V27, P372
   PAIGE CC, 1975, SIAM J NUMER ANAL, V12, P617, DOI 10.1137/0712047
   Plan Y., 2015, ARXIV150204071
   Rokhlin V, 2008, P NATL ACAD SCI USA, V105, P13212, DOI 10.1073/pnas.0804869105
   Thrampoulidis C., 2015, ADV NEURAL INFORM PR, P3402
   Vershynin  Roman, 2010, ARXIV10113027
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704047
DA 2019-06-15
ER

PT S
AU Esfandiari, H
   Korula, N
   Mirrokni, V
AF Esfandiari, Hossein
   Korula, Nitish
   Mirrokni, Vahab
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Bi-Objective Online Matching and Submodular Allocations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID APPROXIMATIONS
AB Online allocation problems have been widely studied due to their numerous practical applications (particularly to Internet advertising), as well as considerable theoretical interest. The main challenge in such problems is making assignment decisions in the face of uncertainty about future input; effective algorithms need to predict which constraints are most likely to bind, and learn the balance between short-term gain and the value of long-term resource availability.
   In many important applications, the algorithm designer is faced with multiple objectives to optimize. In particular, in online advertising it is fairly common to optimize multiple metrics, such as clicks, conversions, and impressions, as well as other metrics which may be largely uncorrelated such as 'share of voice', and 'buyer surplus'. While there has been considerable work on multi-objective offline optimization (when the entire input is known in advance), very little is known about the online case, particularly in the case of adversarial input. In this paper, we give the first results for bi-objective online submodular optimization, providing almost matching upper and lower bounds for allocating items to agents with two submodular value functions. We also study practically relevant special cases of this problem related to Internet advertising, and obtain improved results. All our algorithms are nearly best possible, as well as being efficient and easy to implement in practice.
C1 [Esfandiari, Hossein] Univ Maryland, College Pk, MD 20740 USA.
   [Korula, Nitish; Mirrokni, Vahab] Google Res, New York, NY 10011 USA.
RP Esfandiari, H (reprint author), Univ Maryland, College Pk, MD 20740 USA.
EM hossein@cs.umd.edu; nitish@google.com; mirrokni@google.com
CR Aggarwal G, 2014, LECT NOTES COMPUT SC, V8877, P218, DOI 10.1007/978-3-319-13129-0_16
   Agrawal Shipra, 2009, COMPUTING RES REPOSI
   BUCHBINDER N, 2007, ESA, V4698, P253
   Ciocan DF, 2012, MATH OPER RES, V37, P501, DOI 10.1287/moor.1120.0548
   Devanur N. R., 2011, P 12 ACM C EL COMM, P29
   Devanur NR, 2009, 10TH ACM CONFERENCE ON ELECTRONIC COMMERCE - EC 2009, P71
   Devanur Nikhil R., 2013, EC 2013 ACM, P305
   Esfandiari Hossein, 2015, EC
   FELDMAN J, 2010, ESA, V6346, P182
   Feldman J, 2009, WINE
   FISHER ML, 1978, MATH PROGRAM STUD, V8, P73
   Kapralov M, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1216
   Korula Nitish, 2013, Web and Internet Economics. 9th International Conference, WINE 2013. Proceedings: LNCS 8289, P305, DOI 10.1007/978-3-642-45046-4_25
   Korula Nitish, 2015, P 47 ANN ACM S THEOR, P889
   Lehmann B, 2006, GAME ECON BEHAV, V55, P270, DOI 10.1016/j.geb.2005.02.006
   Mahdian M, 2011, ACM S THEORY COMPUT, P597
   Mehta A, 2007, J ACM, V54, DOI 10.1145/1284320.1284321
   Mirrokni V, 2008, EC'08: PROCEEDINGS OF THE 2008 ACM CONFERENCE ON ELECTRONIC COMMERCE, P70
   Mirrokni Vahab S., 2012, P 23 ANN ACM SIAM S, P1690, DOI DOI 10.1137/1.9781611973099.134
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Papadimitriou CH, 2000, ANN IEEE SYMP FOUND, P86
   Tan B, 2011, IEEE DECIS CONTR P, P4504, DOI 10.1109/CDC.2011.6161009
   Vee Erik, 2010, ACM C EL COMM, P109
   Vondrak J, 2008, ACM S THEORY COMPUT, P67
   Wei Kai, 2015, ADV NEURAL INFORM PR, P2233
   Yannakakis Mihalis, 2001, WADS, P1
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700049
DA 2019-06-15
ER

PT S
AU Eslami, SMA
   Heess, N
   Weber, T
   Tassa, Y
   Szepesvari, D
   Kavukcuoglu, K
   Hinton, GE
AF Eslami, S. M. Ali
   Heess, Nicolas
   Weber, Theophane
   Tassa, Yuval
   Szepesvari, David
   Kavukcuoglu, Koray
   Hinton, Geoffrey E.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Attend, Infer, Repeat: Fast Scene Understanding with Generative Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We present a framework for efficient inference in structured image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps. We use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects - counting, locating and classifying the elements of a scene without any supervision, e.g., decomposing 3D images with various numbers of objects in a single forward pass of a neural network at unprecedented speed. We further show that the networks produce accurate inferences when compared to supervised counterparts, and that their structure leads to improved generalization.
C1 [Eslami, S. M. Ali; Heess, Nicolas; Weber, Theophane; Tassa, Yuval; Szepesvari, David; Kavukcuoglu, Koray; Hinton, Geoffrey E.] Google DeepMind, London, England.
RP Eslami, SMA (reprint author), Google DeepMind, London, England.
EM aeslami@google.com; heess@google.com; theophane@google.com;
   tassa@google.com; dsz@google.com; korayk@google.com;
   geoffhinton@google.com
CR Ba  Jimmy, 2015, ICLR
   Graves Alex, 2016, ABS160308983
   Gregor K., 2015, ICML
   Grenander Ulf, 1976, PATTERN SYNTHESIS LE
   HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831
   JADERBERG M, 2015, SPATIAL TRANSFORMER
   Jampani Varun, 2015, CVIU
   Kendall A, 2015, ICCV
   Kingma D. P., 2013, ARXIV13126114
   Krizhevsky Alex, 2012, NIPS 25
   Kulkarni Tejas D, 2015, CVPR
   Kulkarni Tejas D, 2015, NIPS 28
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   Lempitsky Victor, 2010, NIPS 23
   Loper Matthew M., 2014, ECCV, V8695
   Mansinghka Vikash, 2013, NIPS 26
   MILCH B, 2005, IJCAI, P1352
   Mnih A., 2014, ICML
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Mnih Volodymyr, 2014, NIPS 27
   Rezende D. J., 2014, ICML
   Salakhutdinov R., 2009, AISTATS
   Schulman John, 2015, NIPS 28
   Tang Y., 2013, ICML
   Tang Yichuan, 2014, NIPS 27
   Todorov Emanuel, 2012, ICIRS
   Zhang Jianming, 2015, CVPR
   Zhu Song-Chun, 2006, FDN TRENDS COMPUTER, V2
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701087
DA 2019-06-15
ER

PT S
AU Falahatgar, M
   Ohannessian, MI
   Orlitsky, A
AF Falahatgar, Moein
   Ohannessian, Mesrob I.
   Orlitsky, Alon
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Near-Optimal Smoothing of Structured Conditional Probability Matrices
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Utilizing the structure of a probabilistic model can significantly increase its learning speed. Motivated by several recent applications, in particular bigram models in language processing, we consider learning low-rank conditional probability matrices under expected KL-risk. This choice makes smoothing, that is the careful handling of low-probability elements, paramount. We derive an iterative algorithm that extends classical non-negative matrix factorization to naturally incorporate additive smoothing and prove that it converges to the stationary points of a penalized empirical risk. We then derive sample-complexity bounds for the global minimzer of the penalized risk and show that it is within a small factor of the optimal sample complexity. This framework generalizes to more sophisticated smoothing techniques, including absolute-discounting.
C1 [Falahatgar, Moein; Orlitsky, Alon] Univ Calif San Diego, San Diego, CA 92103 USA.
   [Ohannessian, Mesrob I.] Toyota Technol Inst Chicago, Chicago, IL USA.
RP Falahatgar, M (reprint author), Univ Calif San Diego, San Diego, CA 92103 USA.
EM moein@ucsd.edu; mesrob@ttic.edu; alon@ucsd.edu
FU NSF [1065622, 1564355]
FX We would like to thank Venkatadheeraj Pichapati and Ananda Theertha
   Suresh for many helpful discussions. This work was supported in part by
   NSF grants 1065622 and 1564355.
CR Abe, 1991, COLT
   Acharya, 2013, COLT
   Agarwal, 2013, ARXIV13107991
   Arora, 2015, ARXIV150300778
   Ben Hamou, 2017, BERNOULLI
   Bhojanapalli, 2015, ARXIV150903917
   Blei, 2003, JMLR
   Borade, 2008, IEEE INT ZUR SEM COM
   Brants, 2007, EMNLP
   Buntine W., 2004, P 20 C UNC ART INT, P59
   Chen SF, 1999, COMPUT SPEECH LANG, V13, P359, DOI 10.1006/csla.1999.0128
   Hofmann, 1999, ACM SIGIR
   Huang, 2016, ARXIV160206586
   Hutchinson, 2015, IEEE T AUDIO SPEECH
   Hutchinson, 2011, IEEE SPL
   Kamath, 2015, COLT
   Kneser, 1995, ICASSP
   Lee, 2001, NIPS
   Levy, 2014, NIPS
   Mikolov, 2011, ICASSP
   Ohannessian, 2012, COLT
   Orlitsky, 2015, NIPS
   Papadimitriou, 1998, ACM SIGACT SIGMOD SI
   Parikh, 2013, ARXIV13127077
   Shazeer, 2014, ARXIV14121454
   Valiant, 2015, ARXIV150405321
   Vapnik, 1998, STAT LEARNING THEORY
   Williams, 2015, ARXIV150200512
   Zhu, 2013, IM AN
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701056
DA 2019-06-15
ER

PT S
AU Farajtabar, M
   Ye, XJ
   Harati, S
   Song, L
   Zha, HY
AF Farajtabar, Mehrdad
   Ye, Xiaojing
   Harati, Sahar
   Song, Le
   Zha, Hongyuan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Multistage Campaigning in Social Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We consider the problem of how to optimize multi-stage campaigning over social networks. The dynamic programming framework is employed to balance the high present reward and large penalty on low future outcome in the presence of extensive uncertainties. In particular, we establish theoretical foundations of optimal campaigning over social networks where the user activities are modeled as a multivariate Hawkes process, and we derive a time dependent linear relation between the intensity of exogenous events and several commonly used objective functions of campaigning. We further develop a convex dynamic programming framework for determining the optimal intervention policy that prescribes the required level of external drive at each stage for the desired campaigning result. Experiments on both synthetic data and the real-world MemeTracker dataset show that our algorithm can steer the user activities for optimal campaigning much more accurately than baselines.
C1 [Farajtabar, Mehrdad; Song, Le; Zha, Hongyuan] Georgia Inst Technol, Atlanta, GA 30332 USA.
   [Ye, Xiaojing] Georgia State Univ, Atlanta, GA 30303 USA.
   [Harati, Sahar] Emory Univ, Atlanta, GA 30322 USA.
RP Farajtabar, M (reprint author), Georgia Inst Technol, Atlanta, GA 30332 USA.
EM mehrdad@gatech.edu; xye@gsu.edu; sahar.harati@emory.edu;
   lsong@cc.gatech.edu; zha@cc.gatech.edu
FU NSF/NIH BIGDATA [R01 GM108341]; NSF [IIS-1639792, DMS-1620345,
   DMS-1620342]
FX The work is supported in part by NSF/NIH BIGDATA R01 GM108341, NSF
   IIS-1639792, NSF DMS-1620345, and NSF DMS-1620342.
CR Aalen OO, 2008, STAT BIOL HEALTH, P1
   Al-Mohy AH, 2011, SIAM J SCI COMPUTING
   Bertsekas Dimitri P, DYNAMIC PROGRAMMING, V1
   Bloembergen D, 2014, ECAI
   Blundell C., 2012, NIPS
   Bracewell R, 1965, FOURIER TRANSFORM II, V5
   Chen Pin-Yu, 2014, SYSTEMS MAN CYBERN A
   Daley D. J., 2007, INTRO THEORY POINT P
   De A., 2015, ARXIV150605474
   Farajtabar M., 2014, NIPS
   Folland G. B, 2013, REAL ANAL MODERN TEC
   Hanson F. B., 2007, APPL STOCHASTIC PROC, V13
   Hawkes A. G., 1971, BIOMETRIKA
   Hijab O, 2007, INTRO CALCULUS CLASS
   Iwata T., 2013, SIGKDD
   Kandhway K, 2015, CAMPAIGNING HETEROGE
   Kempe D., 2003, SIGKDD
   Leskovec J, 2009, SIGKDD
   Lian W, 2015, ICML
   Linderman S., 2014, ICML
   Parikh AP, 2012, UAI
   Perry PO, 2013, J ROYAL STAT SOC
   Vergeer M, 2013, PARTY POLITICS
   Wang Yichen, 2016, ARXIV160309021
   WEST D. M, 2013, AIR WARS TELEVISION
   Yang SH, 2013, ICML
   Zhou K., 2013, AISTATS
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700066
DA 2019-06-15
ER

PT S
AU Farnia, F
   Tse, D
AF Farnia, Farzan
   Tse, David
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI A Minimax Approach to Supervised Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID MAXIMUM-ENTROPY; REGRESSION; SELECTION
AB Given a task of predicting Y from X, a loss function L, and a set of probability distributions F on (X, Y), what is the optimal decision rule minimizing the worst case expected loss over F? In this paper, we address this question by introducing a generalization of the maximum entropy principle. Applying this principle to sets of distributions with marginal on X constrained to be the empirical marginal, we provide a minimax interpretation of the maximum likelihood problem over generalized linear models as well as some popular regularization schemes. For quadratic and logarithmic loss functions we revisit well-known linear and logistic regression models. Moreover, for the 0-1 loss we derive a classifier which we call the minimax SVM. The minimax SVM minimizes the worst-case expected 0-1 loss over the proposed F by solving a tractable optimization problem. We perform several numerical experiments to show the power of the minimax SVM in outperforming the SVM.
C1 [Farnia, Farzan; Tse, David] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
RP Farnia, F (reprint author), Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
EM farnia@stanford.edu; dntse@stanford.edu
FU Stanford University; Center for Science of Information (CSoI), an NSF
   Science and Technology Center [CCF-0939370]
FX We are grateful to Stanford University providing a Stanford Graduate
   Fellowship, and the Center for Science of Information (CSoI), an NSF
   Science and Technology Center under grant agreement CCF-0939370, for the
   support during this research.
CR Altun Yasemin, 2006, LEARNING THEORY
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Berger AL, 1996, COMPUT LINGUIST, V22, P39
   Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010
   CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Cover T. M., 2012, ELEMENTS INFORM THEO
   Dawid Philip, 1998, 139 U COLL LOND
   Dudik M, 2007, J MACH LEARN RES, V8, P1217
   Eban Elad, 2014, P 31 INT C MACH LEAR, P1233
   Erkan Ayse, 2010, INT C ART INT STAT A, P209
   Feldman V, 2012, SIAM J COMPUT, V41, P1558, DOI 10.1137/120865094
   Friedman J., 2001, ELEMENTS STAT LEARNI, V1
   Globerson Amir, 2004, P 20 C UNC ART INT, P193
   Grunwald PD, 2004, ANN STAT, V32, P1367, DOI 10.1214/009053604000000553
   HOERL AE, 1970, TECHNOMETRICS, V12, P55
   Huber P. J., 1981, ROBUST STAT
   Jacob L, 2009, P 26 ANN INT C MACH, P433, DOI DOI 10.1145/1553374.1553431
   JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620
   Lanckriet G. R. G., 2003, Journal of Machine Learning Research, V3, P555, DOI 10.1162/153244303321897726
   McCullagh P, 1989, GEN LINEAR MODELS, V37
   Razaviyayn Meisam, 2015, ADV NEURAL INFORM PR, V28, P3258
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Vapnik V., 2013, NATURE STAT LEARNING
   Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701104
DA 2019-06-15
ER

PT S
AU Fathony, R
   Liu, AQ
   Asif, K
   Ziebart, BD
AF Fathony, Rizal
   Liu, Anqi
   Asif, Kaiser
   Ziebart, Brian D.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Adversarial Multiclass Classification: A Risk Minimization Perspective
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID SUPPORT VECTOR MACHINES; CONSISTENCY
AB Recently proposed adversarial classification methods have shown promising results for cost sensitive and multivariate losses. In contrast with empirical risk minimization (ERM) methods, which use convex surrogate losses to approximate the desired non-convex target loss function, adversarial methods minimize non-convex losses by treating the properties of the training data as being uncertain and worst case within a minimax game. Despite this difference in formulation, we recast adversarial classification under zero-one loss as an ERM method with a novel prescribed loss function. We demonstrate a number of theoretical and practical advantages over the very closely related hinge loss ERM methods. This establishes adversarial classification under the zero-one loss as a method that fills the long standing gap in multiclass hinge loss classification, simultaneously guaranteeing Fisher consistency and universal consistency, while also providing dual parameter sparsity and high accuracy predictions in practice.
C1 [Fathony, Rizal; Liu, Anqi; Asif, Kaiser; Ziebart, Brian D.] Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA.
RP Fathony, R (reprint author), Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA.
EM rfatho2@uic.edu; aliu33@uic.edu; kasif2@uic.edu; bziebart@uic.edu
FU Future of Life Institute FLI-RFP-AI1 program [2016-158710]; NSF
   [1526379]
FX This research was supported as part of the Future of Life Institute
   (futureoflife.org) FLI-RFP-AI1 program, grant#2016-158710 and by NSF
   grant RI-#1526379.
CR Asif Kaiser, 2015, P C UNC ART INT
   Bartlett Peter L, 2003, ADV NEURAL INFORM PR, P1173
   Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907
   Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628
   Dalvi Nilesh, 2004, P 10 ACM SIGKDD INT, P99, DOI DOI 10.1145/1014052.1014066
   Deng N.Y., 2012, SUPPORT VECTOR MACHI
   Dogan U, 2016, J MACH LEARN RES, V17
   Farnia Farzan, 2016, ADV NEURAL INFORM PR, P4233
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Grunwald PD, 2004, ANN STAT, V32, P1367, DOI 10.1214/009053604000000553
   Igel C, 2008, J MACH LEARN RES, V9, P993
   Joachims Thorsten, 2005, P 22 INT C MACH LEAR, P377
   Lanckriet G. R. G., 2003, Journal of Machine Learning Research, V3, P555, DOI 10.1162/153244303321897726
   Lee YK, 2004, J AM STAT ASSOC, V99, P67, DOI 10.1198/016214504000000098
   Lin Y, 2002, DATA MIN KNOWL DISC, V6, P259, DOI 10.1023/A:1015469627679
   Liu A., 2014, ADV NEURAL INFORM PR, P37
   Liu Yufeng, 2007, AISTATS, P291
   McCullagh P, 1989, GEN LINEAR MODELS, V37
   Micchelli CA, 2006, J MACH LEARN RES, V7, P2651
   Steinwart I, 2005, IEEE T INFORM THEORY, V51, P128, DOI 10.1109/TIT.2004.839514
   Steinwart I, 2002, J COMPLEXITY, V18, P768, DOI 10.1006/jcom.2002.0642
   Steinwart I, 2008, INFORM SCI STAT, P1
   Tewari A, 2007, J MACH LEARN RES, V8, P1007
   TOPSOE F, 1979, KYBERNETIKA, V15, P8
   Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453
   VAPNIK V, 1992, ADV NEUR IN, V4, P831
   Wang Hong, 2015, ADV NEURAL INFORM PR, P2710
   Weston J., 1999, 7th European Symposium on Artificial Neural Networks. ESANN'99. Proceedings, P219
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700052
DA 2019-06-15
ER

PT S
AU Fawzi, A
   Moosayi-Dezfooli, SM
   Frossard, P
AF Fawzi, Alhussein
   Moosayi-Dezfooli, Seyed-Mohsen
   Frossard, Pascal
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Robustness of classifiers: from adversarial to random noise
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Several recent works have shown that state-of-the-art classifiers are vulnerable to worst-case (i.e., adversarial) perturbations of the datapoints. On the other hand, it has been empirically observed that these same classifiers are relatively robust to random noise. In this paper, we propose to study a semi-random noise regime that generalizes both the random and worst-case noise regimes. We propose the first quantitative analysis of the robustness of nonlinear classifiers in this general noise regime. We establish precise theoretical bounds on the robustness of classifiers in this general regime, which depend on the curvature of the classifier's decision boundary. Our bounds confirm and quantify the empirical observations that classifiers satisfying curvature constraints are robust to random noise. Moreover, we quantify the robustness of classifiers in terms of the subspace dimension in the semi-random noise regime, and show that our bounds remarkably interpolate between the worst-case and random noise regimes. We perform experiments and show that the derived bounds provide very accurate estimates when applied to various state-of-the-art deep neural networks and datasets. This result suggests bounds on the curvature of the classifiers' decision boundaries that we support experimentally, and more generally offers important insights onto the geometry of high dimensional classification problems.
C1 [Fawzi, Alhussein; Moosayi-Dezfooli, Seyed-Mohsen; Frossard, Pascal] Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
RP Fawzi, A (reprint author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
EM alhussein.fawzi@epfl.ch; seyed.moosavi@epfl.ch; pascal.frossard@epfl.ch
FU Hasler Foundation, Switzerland, in the framework of the CORA project
FX We would like to thank the anonymous reviewers for their helpful
   comments. We thank Omar Fawzi and Louis Merlin for the fruitful
   discussions. We also gratefully acknowledge the support of NVIDIA
   Corporation with the donation of the Tesla K40 GPU used for this
   research. This work has been partly supported by the Hasler Foundation,
   Switzerland, in the framework of the CORA project.
CR Caramanis C, 2012, OPTIMIZATION FOR MACHINE LEARNING, P369
   Chatfield K., 2014, BRIT MACH VIS C
   Fawzi A., 2015, ABS150202590 CORR
   Fawzi A., 2015, BRIT MACH VIS C BMVC
   Goodfellow I. J., 2015, INT C LEARN REPR ICL
   Gu S., 2014, ARXIV14125068
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Huang R., 2015, ABS151103034 CORR
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lanckriet G. R. G., 2003, Journal of Machine Learning Research, V3, P555, DOI 10.1162/153244303321897726
   Lee J. M., 2009, MANIFOLDS DIFFERENTI, V107
   Luo Y., 2015, ARXIV151106292
   Moosavi-Dezfooli S.-M., 2016, IEEE C COMP VIS PATT
   Sabour  S., 2016, INT C LEARN REPR ICL
   Shaham  U., 2015, ARXIV151105432
   Simonyan K., 2014, INT C LEARN REPR ICL
   Szegedy  C., 2014, INT C LEARN REPR ICL
   Tabacof P., 2016, IEEE INT JOINT C NEU
   Xu H, 2009, J MACH LEARN RES, V10, P1485
   Zhao Q., 2016, ARXIV160305145
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702078
DA 2019-06-15
ER

PT S
AU Feichtenhofer, C
   Pinz, A
   Wildes, RP
AF Feichtenhofer, Christoph
   Pinz, Axel
   Wildes, Richard P.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Spatiotemporal Residual Networks for Video Action Recognition
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Two-stream Convolutional Networks (ConvNets) have shown strong performance for human action recognition in videos. Recently, Residual Networks (ResNets) have arisen as a new technique to train extremely deep architectures. In this paper, we introduce spatiotemporal ResNets as a combination of these two approaches. Our novel architecture generalizes ResNets for the spatiotemporal domain by introducing residual connections in two ways. First, we inject residual connections between the appearance and motion pathways of a two-stream architecture to allow spatiotemporal interaction between the two streams. Second, we transform pretrained image ConvNets into spatiotemporal networks by equipping them with learnable convolutional filters that are initialized as temporal residual connections and operate on adjacent feature maps in time. This approach slowly increases the spatiotemporal receptive field as the depth of the model increases and naturally integrates image ConvNet design principles. The whole model is trained end-to-end to allow hierarchical learning of complex spatiotemporal features. We evaluate our novel spatiotemporal ResNet using two widely used action recognition benchmarks where it exceeds the previous state-of-the-art.
C1 [Feichtenhofer, Christoph; Pinz, Axel] Graz Univ Technol, Graz, Austria.
   [Wildes, Richard P.] York Univ, Toronto, ON, Canada.
RP Feichtenhofer, C (reprint author), Graz Univ Technol, Graz, Austria.
EM feichtenhofer@tugraz.at; axel.pinz@tugraz.at; wildes@cse.yorku.ca
FU Austrian Science Fund (FWF) [P27076]; NSERC; DOC Fellowship of the
   Austrian Academy of Sciences at the Institute of Electrical Measurement
   and Measurement Signal Processing, Graz University of Technology
FX This work was supported by the Austrian Science Fund (FWF) under project
   P27076 and NSERC. The GPUs used for this research were donated by
   NVIDIA. Christoph Feichtenhofer is a recipient of a DOC Fellowship of
   the Austrian Academy of Sciences at the Institute of Electrical
   Measurement and Measurement Signal Processing, Graz University of
   Technology.
CR Ballas Nicolas, 2016, P ICLR
   Bilen H., 2016, P CVPR
   BORN RT, 1992, NATURE, V357, P497, DOI 10.1038/357497a0
   Donahue J., 2015, P CVPR
   Feichtenhofer Christoph, P CVPR
   GOODALE MA, 1992, TRENDS NEUROSCI, V15, P20, DOI 10.1016/0166-2236(92)90344-8
   Goroshin Ross, 2015, P ICCV
   He K., 2016, ARXIV160305027
   He  K., 2015, ARXIV151203385
   Ioannou Yani, 2016, P ICLR
   Ioffe Sergey, 2015, P ICML
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Karpathy A., 2014, P CVPR
   Krizhevsky A., 2012, NIPS
   Kuehne Hildegard, 2011, P ICCV
   Le Quoc V, 2011, P CVPR
   Mahasseni Behrooz, REGULARIZING LONG SH
   Ng Joe Yue-Hei, 2015, P CVPR
   Sharma Shikhar, 2015, NIPS WORKSHOP TIME S
   Simonyan K., 2014, NIPS
   Simonyan K., 2014, P ICLR
   Soomro K., 2012, CRCVTR1201
   Szegedy C., 2016, ARXIV160207261
   Szegedy C., 2015, ARXIV151200567
   Taylor G. W., 2010, P ECCV
   Tran D., 2015, P ICCV
   VANESSEN DC, 1994, NEURON, V13, P1, DOI 10.1016/0896-6273(94)90455-3
   Vedaldi A., 2015, P ACM INT C MULT
   Wang Heng, 2013, P ICCV
   Wang Limin, 2015, P CVPR
   Wang Xiaolong, 2016, P CVPR
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703069
DA 2019-06-15
ER

PT S
AU Feldman, M
   Koren, T
   Livni, R
   Mansour, Y
   Zohar, A
AF Feldman, Michal
   Koren, Tomer
   Livni, Roi
   Mansour, Yishay
   Zohar, Aviv
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Online Pricing with Strategic and Patient Buyers
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID MECHANISM DESIGN
AB We consider a seller with an unlimited supply of a single good, who is faced with a stream of T buyers. Each buyer has a window of time in which she would like to purchase, and would buy at the lowest price in that window, provided that this price is lower than her private value (and otherwise, would not buy at all). In this setting, we give an algorithm that attains O(T-2/3) regret over any sequence of T buyers with respect to the best fixed price in hindsight, and prove that no algorithm can perform better in the worst case.
C1 [Feldman, Michal; Mansour, Yishay] Tel Aviv Univ, Tel Aviv, Israel.
   [Feldman, Michal] MSR Herzliya, Herzliyya, Israel.
   [Koren, Tomer] Google Brain, Mountain View, CA USA.
   [Livni, Roi] Princeton Univ, Princeton, NJ 08544 USA.
   [Zohar, Aviv] Hebrew Univ Jerusalem, Jerusalem, Israel.
   [Koren, Tomer; Livni, Roi; Mansour, Yishay; Zohar, Aviv] Microsoft Res, Herzliyya, Israel.
RP Feldman, M (reprint author), Tel Aviv Univ, Tel Aviv, Israel.; Feldman, M (reprint author), MSR Herzliya, Herzliyya, Israel.
EM michal.feldman@cs.tau.ac.il; tkoren@google.com; rlivni@cs.princeton.edu;
   mansour@tau.ac.il; avivz@cs.huji.ac.il
CR Amin Kareem, 2013, ADV NEURAL INF PROCE, P1169
   Arora R., 2012, ARXIV12066400
   Auer P, 2003, SIAM J COMPUT, V32, P48
   Balcan MF, 2008, J COMPUT SYST SCI, V74, P1245, DOI 10.1016/j.jcss.2007.08.002
   Chawla S, 2010, ACM S THEORY COMPUT, P311
   Chawla S, 2007, EC'07: PROCEEDINGS OF THE EIGHTH ANNUAL CONFERENCE ON ELECTRONIC COMMERCE, P243
   Chawla Shuchi, 2010, 11 ACM C EL COMM EC
   Cole Richard, 2014, P 46 ANN ACM S THEOR, P243
   Dekel O., 2014, P STOC, P459
   Huang  Zhiyi, 2015, P 16 ACM C EC COMP E, P45
   Kleinberg R, 2003, ANN IEEE SYMP FOUND, P594, DOI 10.1109/SFCS.2003.1238232
   Mohri M., 2014, ADV NEURAL INFORM PR, P1871
   Mohri Mehryar, 2015, ADV NEURAL INFORM PR, P2530
NR 13
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703051
DA 2019-06-15
ER

PT S
AU Feldman, V
AF Feldman, Vitaly
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Generalization of ERM in Stochastic Convex Optimization: The Dimension
   Strikes Back
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID STABILITY
AB In stochastic convex optimization the goal is to minimize a convex function F(x) (=) over dot Ef similar to D [f (x)] over a convex set K subset of R-d where D is some unknown distribution and each f (.) in the support of D is convex over K. The optimization is commonly based on i.i.d. samples f(1), f(2),, f(n) from D. A standard approach to such problems is empirical risk minimization (ERM) that optimizes F-s(x) (=) over dot 1/n Sigma(i <= n) f(i)(x). Here we consider the question of how many samples are necessary for ERM to succeed and the closely related question of uniform convergence of F-S to F over K. We demonstrate that in the standard l(p)/l(p) setting of Lipschitz-bounded functions over a K of bounded radius, ERM requires sample size that scales linearly with the dimension d. This nearly matches standard upper bounds and improves on Omega (log d) dependence proved for l(2)/l(2) setting in [18]. In stark contrast, these problems can be solved using dimension-independent number of samples for l(2)/l(2) setting and log d dependence for l(1)/l(infinity) setting using other approaches.
   We further show that our lower bound applies even if the functions in the support of D are smooth and efficiently computable and even if an l(1) regularization term is added. Finally, we demonstrate that for a more general class of bounded-range (but not Lipschitz-bounded) stochastic convex programs an infinite gap appears already in dimension 2.
C1 [Feldman, Vitaly] IBM Res Almaden, San Jose, CA 95120 USA.
RP Feldman, V (reprint author), IBM Res Almaden, San Jose, CA 95120 USA.
CR Bach F., 2013, ADV NEURAL INFORM PR, V26, P773
   Bassily R, 2016, STOC'16: PROCEEDINGS OF THE 48TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P1046, DOI 10.1145/2897518.2897566
   Belloni Alexandre, 2015, P 28 C LEARN THEOR, P240
   Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704
   Bubeck S., 2015, MACH LEARN, V8, P231, DOI DOI 10.1561/2200000050
   Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339
   Dwork C., 2015, ABS1506 CORR
   Dwork C, 2014, ABS14112664 CORR
   Feldman V., 2016, ABS160804414 CORR
   Feldman V., 2015, ABS151209170 CORR
   Hardt M, 2016, P 33 INT C MACH LEAR, P1225
   JUSTESEN J, 1972, IEEE T INFORM THEORY, V18, P652, DOI 10.1109/TIT.1972.1054893
   Kakade S. M., 2008, NIPS, P793
   Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277
   Rakhlin A., 2015, ABS150107340 CORR
   Rakhlin Alexander, 2012, ICML
   Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN
   Shalev-Shwartz S, 2010, J MACH LEARN RES, V11, P2635
   Shalev-Shwartz Shai, 2009, COLT
   Shamir O., 2013, INT C MACH LEARN, V28, P71
   Shapiro A., 2005, CONTINUOUS OPTIMIZAT, P144
   Spielman DA, 1996, IEEE T INFORM THEORY, V42, P1723, DOI 10.1109/18.556668
   Vapnik V. N., 1998, STAT LEARNING THEORY
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703103
DA 2019-06-15
ER

PT S
AU Fernandez, T
   Rivera, N
   Teh, YW
AF Fernandez, Tamara
   Rivera, Nicolas
   Teh, Yee Whye
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Gaussian Processes for Survival Analysis
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We introduce a semi-parametric Bayesian model for survival analysis. The model is centred on a parametric baseline hazard, and uses a Gaussian process to model variations away from it nonparametrically, as well as dependence on covariates. As opposed to many other methods in survival analysis, our framework does not impose unnecessary constraints in the hazard rate or in the survival function. Furthermore, our model handles left, right and interval censoring mechanisms common in survival analysis. We propose a MCMC algorithm to perform inference and an approximation scheme based on random Fourier features to make computations faster. We report experimental results on synthetic and real data, showing that our model performs better than competing models such as Cox proportional hazards, ANOVA-DDP and random survival forests.
C1 [Fernandez, Tamara; Teh, Yee Whye] Univ Oxford, Dept Stat, Oxford, England.
   [Rivera, Nicolas] Kings Coll London, Dept Informat, London, England.
RP Fernandez, T (reprint author), Univ Oxford, Dept Stat, Oxford, England.
EM fernandez@stats.ox.ac.uk; nicolas.rivera@kcl.ac.uk;
   y.w.teh@stats.ox.ac.uk
FU European Research Council under the European Union's Seventh Framework
   Programme (FP7/2007-2013) ERC grant [617071]; Becas CHILE
FX YWT's research leading to these results has received funding from the
   European Research Council under the European Union's Seventh Framework
   Programme (FP7/2007-2013) ERC grant agreement no. 617071. Tamara
   Fernandez and Nicolas Rivera were supported by funding from Becas CHILE.
CR Adams R, 2009, P 26 ANN INT C MACH, P9, DOI DOI 10.1145/1553374.1553376
   Barrett J. E., 2013, ARXIV13121591
   COX DR, 1972, J R STAT SOC B, V34, P187
   De Iorio M, 2009, BIOMETRICS, V65, P762, DOI 10.1111/j.1541-0420.2008.01166.x
   DOKSUM K, 1974, ANN PROBAB, V2, P183, DOI 10.1214/aop/1176996703
   Duvenaud D., 2011, ADV NEURAL INFORM PR, P226
   DYKSTRA RL, 1981, ANN STAT, V9, P356, DOI 10.1214/aos/1176345401
   FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360
   Hjort NL, 2010, BAYESIAN NONPARAMETR, V28
   Ishwaran H, 2008, ANN APPL STAT, V2, P841, DOI 10.1214/08-AOAS169
   Joensuu H, 2014, RADIOLOGY, V271, P96, DOI 10.1148/radiol.13131040
   Joensuu H, 2012, LANCET ONCOL, V13, P265, DOI 10.1016/S1470-2045(11)70299-6
   KAPLAN EL, 1958, J AM STAT ASSOC, V53, P457, DOI 10.2307/2281868
   Martino S, 2011, SCAND J STAT, V38, P514, DOI 10.1111/j.1467-9469.2010.00715.x
   Murray I., 2010, ADV NEURAL INFORM PR, V23, P1732
   Murray I., 2010, JMLR W CP, V9, P541
   Rahimi A., 2007, ADV NEURAL INFORM PR, P1177
   Rao Vinayak, 2011, ADV NEURAL INFORM PR, P2474
   Therneau T. M., 2015, PACKAGE SURVIVAL
   Walker S, 1997, ANN STAT, V25, P1762
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703079
DA 2019-06-15
ER

PT S
AU Figurnov, M
   Ibraimova, A
   Vetrov, D
   Kohli, P
AF Figurnov, Michael
   Ibraimova, Aijan
   Vetrov, Dmitry
   Kohli, Pushmeet
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI PerforatedCNNs: Acceleration through Elimination of Redundant
   Convolutions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We propose a novel approach to reduce the computational cost of evaluation of convolutional neural networks, a factor that has hindered their deployment in low-power devices such as mobile phones. Inspired by the loop perforation technique from source code optimization, we speed up the bottleneck convolutional layers by skipping their evaluation in some of the spatial positions. We propose and analyze several strategies of choosing these positions. We demonstrate that perforation can accelerate modern convolutional networks such as AlexNet and VGG-16 by a factor of 2x - 4x. Additionally, we show that perforation is complementary to the recently proposed acceleration method of Zhang et al. [28].
C1 [Figurnov, Michael; Vetrov, Dmitry] Natl Res Univ, Higher Sch Econ, Moscow, Russia.
   [Figurnov, Michael] Lomonosov Moscow State Univ, Moscow, Russia.
   [Vetrov, Dmitry] Yandex, Moscow, Russia.
   [Ibraimova, Aijan] Skolkovo Inst Sci & Technol, Moscow, Russia.
   [Kohli, Pushmeet] Microsoft Res, Redmond, WA USA.
RP Figurnov, M (reprint author), Natl Res Univ, Higher Sch Econ, Moscow, Russia.; Figurnov, M (reprint author), Lomonosov Moscow State Univ, Moscow, Russia.
EM michael@figurnov.ru; aijan.ibraimova@gmail.com; vetrovd@yandex.ru;
   pkohli@microsoft.com
FU RFBR [15-31-20596 (mol-a-ved)]; Microsoft: Moscow State University Joint
   Research Center [RPD 1053945]
FX We would like to thank Alexander Kirillov and Dmitry Kropotov for
   helpful discussions, and Yandex for providing computational resources
   for this project. This work was supported by RFBR project No.
   15-31-20596 (mol-a-ved) and by Microsoft: Moscow State University Joint
   Research Center (RPD 1053945).
CR Ba J., 2015, NIPS
   Chen T., 2015, MATRIX SHADOW LIB
   Chetlur S., 2014, CUDNN EFFICIENT PRIM
   Collins M. D., 2014, MEMORY BOUNDED DEEP
   Courbariaux M., 2015, ICLR
   Denton E., 2014, NIPS
   Graham  B., 2014, FRACTIONAL MAX POOLI
   Graham B., 2014, SPATIALLY SPARSE CON
   Gupta S., 2015, ICML
   Jaderberg M., 2015, NIPS
   Jaderberg M., 2014, BMVC
   Jia Y., 2014, ACM ICM
   Krizhevsky A., 2012, NIPS
   Krizhevsky A., 2014, CUDA CONVNET2
   Lebedev V., 2015, ICLR
   Lebedev V, 2016, CVPR
   Lin M., 2014, ICLR
   Misailovic S., 2011, STAT ANAL
   Misailovic S., 2010, ICSE
   Mnih V., 2014, NIPS
   Novikov A., 2015, NIPS
   Ovtcharov K., 2015, MICROSOFT RES WHITEP
   Samadi M., 2014, ASPLOS
   Sidiroglou-Douskos S., 2011, ACM SIGSOFT
   Simonyan Karen, 2015, ICLR
   Vedaldi A., 2014, MATCONVNET CONVOLUTI
   Yang Zichao, 2015, ICCV
   Zhang X., 2015, ACCELERATING VERY DE
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703099
DA 2019-06-15
ER

PT S
AU Finn, C
   Goodfellow, I
   Levine, S
AF Finn, Chelsea
   Goodfellow, Ian
   Levine, Sergey
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Unsupervised Learning for Physical Interaction through Video Prediction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB A core challenge for an agent learning to interact with the world is to predict how its actions affect objects in its environment. Many existing methods for learning the dynamics of physical interactions require labeled object information. However, to scale real-world interaction learning to a variety of scenes and objects, acquiring labeled data becomes increasingly impractical. To learn about physical object motion without labels, we develop an action-conditioned video prediction model that explicitly models pixel motion, by predicting a distribution over pixel motion from previous frames. Because our model explicitly predicts motion, it is partially invariant to object appearance, enabling it to generalize to previously unseen objects. To explore video prediction for real-world interactive agents, we also introduce a dataset of 59,000 robot interactions involving pushing motions, including a test set with novel objects. In this dataset, accurate prediction of videos conditioned on the robot's future actions amounts to learning a "visual imagination" of different futures based on different courses of action. Our experiments show that our proposed method produces more accurate video predictions both quantitatively and qualitatively, when compared to prior methods.
C1 [Finn, Chelsea; Levine, Sergey] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Goodfellow, Ian] OpenAI, San Francisco, CA USA.
   [Finn, Chelsea; Levine, Sergey] Google Brain, Mountain View, CA USA.
RP Finn, C (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM cbfinn@eecs.berkeley.edu; ian@openai.com; slevine@google.com
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Battaglia P. W., 2013, P NATL ACAD SCI USA, V110
   Bengio S., 2015, ADV NEURAL INFORM PR, P1171
   Boots B., 2014, INT C ROB AUT ICAR
   Brubaker M. A., 2009, INT C COMP VIS ICCV
   De Brabandere B, 2016, NEURAL INFORM PROCES
   Eslami S., 2016, NEURAL INFORM PROCES
   Geiger  A., 2013, INT J ROBOTICS RES I
   Huang D. - A., 2014, EUR C COMP VIS ECCV
   Ionescu C., 2014, PAMI, V36
   Jaderberg  M., 2015, NEURAL INFORM PROCES
   Karpathy A., 2014, COMPUER VISION PATTE
   Kinga D, 2015, INT C LEARN REPR ICL
   Lange S, 2012, IEEE IJCNN
   Lerer A., 2016, INT C MACH LEARN ICM
   Lotter W., 2016, ARXIV160508104
   Mathieu M., 2016, INT C LEARN REPR ICL
   Mottaghi R., 2015, COMPUTER VISION PATT
   Mottaghi R., 2016, EUR C COMP VIS ECCV
   Oh J., 2015, NEURAL INFORM PROCES
   Ranzato M, 2014, ARXIV14126604
   Srivastava N., 2015, INT C MACH LEARN ICM
   Vondrick C., 2016, NEURAL INFORM PROCES
   Vondrick C., 2015, ABS150408023 CORR
   Walker J., 2016, EUR C COMP VIS ECCV
   Walker J., 2014, COMPUTER VISION PATT
   Watter M., 2015, NEURAL INFORM PROCES
   Xingjian S., 2015, NEURAL INFORM PROCES
   Xue T., 2016, NEURAL INFORM PROCES
   Yuen J., 2010, EUR C COMP VIS ECCV
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701018
DA 2019-06-15
ER

PT S
AU Flamary, R
   Fevotte, C
   Courty, N
   Emiya, V
AF Flamary, Remi
   Fevotte, Cedric
   Courty, Nicolas
   Emiya, Valentin
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Optimal spectral transportation with application to music transcription
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Many spectral unmixing methods rely on the non-negative decomposition of spectral data onto a dictionary of spectral templates. In particular, state-of-the-art music transcription systems decompose the spectrogram of the input signal onto a dictionary of representative note spectra. The typical measures of fit used to quantify the adequacy of the decomposition compare the data and template entries frequency-wise. As such, small displacements of energy from a frequency bin to another as well as variations of timbre can disproportionally harm the fit. We address these issues by means of optimal transportation and propose a new measure of fit that treats the frequency distributions of energy holistically as opposed to frequency-wise. Building on the harmonic nature of sound, the new measure is invariant to shifts of energy to harmonically-related frequencies, as well as to small and local displacements of energy. Equipped with this new measure of fit, the dictionary of note templates can be considerably simplified to a set of Dirac vectors located at the target fundamental frequencies (musical pitch values). This in turns gives ground to a very fast and simple decomposition algorithm that achieves state-of-the-art performance on real musical data.
C1 [Flamary, Remi] Univ Cote dAzur, CNRS, OCA, Nice, France.
   [Fevotte, Cedric] IRIT, CNRS, Toulouse, France.
   [Courty, Nicolas] Univ Bretagne Sud, CNRS, IRISA, Lorient, France.
   [Emiya, Valentin] Aix Marseille Univ, CNRS, LIF, Marseille, France.
RP Flamary, R (reprint author), Univ Cote dAzur, CNRS, OCA, Nice, France.
EM remi.flamary@unice.fr; cedric.fevotte@irit.fr; courty@univ-ubs.fr;
   valentin.emiya@lif.univ-mrs.fr
FU European Research Council (ERC) under the European Union's Horizon 2020
   research & innovation programme (project FACTORY); French ANR JCJC
   program MAD [ANR-14-CE27-0002]
FX This work is supported in part by the European Research Council (ERC)
   under the European Union's Horizon 2020 research & innovation programme
   (project FACTORY) and by the French ANR JCJC program MAD
   (ANR-14-CE27-0002). Many thanks to Antony Schutz for generating &
   providing some of the musical data.
CR Benamou JD, 2015, SIAM J SCI COMPUT, V37, pA1111, DOI 10.1137/141000439
   Boulanger-Lewandowski N., 2012, P INT SOC MUS INF RE
   Courty N., 2014, P EUR C MACH LEARN P
   Cuturi M, 2013, ADV NEURAL INFORM PR
   Daniel A., 2008, P INT SOC MUS INF RE
   Emiya V, 2010, IEEE T AUDIO SPEECH, V18, P1643, DOI 10.1109/TASL.2009.2038819
   Hofmann T, 2001, MACH LEARN, V42, P177, DOI 10.1023/A:1007617005950
   Huang J, 2009, BIOMETRIKA, V96, P339, DOI 10.1093/biomet/asp020
   Oudre L, 2011, IEEE T AUDIO SPEECH, V19, P2222, DOI 10.1109/TASL.2011.2139205
   Rigaud F, 2013, J ACOUST SOC AM, V133, P3107, DOI 10.1121/1.4799806
   Rolet A., 2016, P INT C ART INT STAT
   Rubner Y., 1998, P INT C COMP VIS ICC
   Sandler R, 2011, IEEE T PATTERN ANAL, V33, P1590, DOI 10.1109/TPAMI.2011.18
   Smaragdis P., 2006, P NIPS WORKSH ADV MO
   Smaragdis P., 2003, P IEEE WORKSH APPL S
   Typke R., 2004, P ACM INT C MULT
   Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5
   Vincent E, 2010, IEEE T AUDIO SPEECH, V18, P528, DOI 10.1109/TASL.2009.2034186
   Zen G., 2014, P INT C PATT REC ICP
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704004
DA 2019-06-15
ER

PT S
AU Foerster, JN
   Assael, YM
   de Freitas, N
   Whiteson, S
AF Foerster, Jakob N.
   Assael, Yannis M.
   de Freitas, Nando
   Whiteson, Shimon
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning to Communicate with Deep Multi-Agent Reinforcement Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.
C1 [Foerster, Jakob N.; Assael, Yannis M.; de Freitas, Nando; Whiteson, Shimon] Univ Oxford, Oxford, England.
   [de Freitas, Nando] Canadian Inst Adv Res, CIFAR NCAP Program, London, England.
   [de Freitas, Nando] Google DeepMind, London, England.
RP Foerster, JN (reprint author), Univ Oxford, Oxford, England.
EM jakob.foerster@cs.ox.ac.uk; yannis.assael@cs.ox.ac.uk;
   nandodefreitas@google.com; shimon.whiteson@cs.ox.ac.uk
RI Assael, Yannis M/E-8160-2013
OI Assael, Yannis M/0000-0001-7408-3847
CR Cho K., 2014, ARXIV14091259
   Chung J., 2014, CORR
   Courbariaux M, 2016, ARXIV160202830
   Giles CL, 2002, LECT NOTES ARTIF INT, V2564, P377
   Gregor K., 2015, ARXIV150204623
   Hausknecht M.J., 2015, ARXIV150706527
   Hinton G, 2011, TOP COGN SCI, V3, P74, DOI 10.1111/j.1756-8765.2010.01109.x
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Kasai Tatsuya, 2008, 2008 IEEE Conference on Soft Computing in Industrial Applications. SMCia/08, P1, DOI 10.1109/SMCIA.2008.5045926
   Kraemer L, 2016, NEUROCOMPUTING, V190, P82, DOI 10.1016/j.neucom.2016.01.031
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Melo F. S., 2011, MULTIAGENT SYSTEMS, P189
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Narasimhan  K., 2015, ARXIV150608941
   Oliehoek FA, 2008, J ARTIF INTELL RES, V32, P289, DOI 10.1613/jair.2447
   Panait L, 2005, AUTON AGENT MULTI-AG, V11, P387, DOI 10.1007/s10458-005-2631-2
   Shoham Y., 2009, MULTIAGENT SYSTEMS A
   Studdert-Kennedy M., 2005, LANGUAGE ORIGINS PER
   Sukhbaatar S., 2016, ARXIV160507736
   Sutton R., 1998, INTRO REINFORCEMENT
   Tampuu Ardi, 2015, ARXIV151108779
   Tan M., 1993, ICML
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P5
   Wu W., 2002, TECHNICAL REPORT
   Zawadzki E., 2014, 14018074 ARXIV
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700007
DA 2019-06-15
ER

PT S
AU Foster, DJ
   Li, ZY
   Lykouris, T
   Sridharan, K
   Tardos, E
AF Foster, Dylan J.
   Li, Zhiyuan
   Lykouris, Thodoris
   Sridharan, Karthik
   Tardos, Eva
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning in Games: Robustness of Fast Convergence
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID BOUNDS
AB We show that learning algorithms satisfying a low approximate regret property experience fast convergence to approximate optimality in a large class of repeated games. Our property, which simply requires that each learner has small regret compared to a (1 + epsilon)-multiplicative approximation to the best action in hindsight, is ubiquitous among learning algorithms; it is satisfied even by the vanilla Hedge forecaster. Our results improve upon recent work of Syrgkanis et al. [28] in a number of ways. We require only that players observe payoffs under other players' realized actions, as opposed to expected payoffs. We further show that convergence occurs with high probability, and show convergence under bandit feedback. Finally, we improve upon the speed of convergence by a factor of n, the number of players. Both the scope of settings and the class of algorithms for which our analysis provides fast convergence are considerably broader than in previous work.
   Our framework applies to dynamic population games via a low approximate regret property for shifting experts. Here we strengthen the results of Lykouris et al. [19] in two ways: We allow players to select learning algorithms from a larger class, which includes a minor variant of the basic Hedge algorithm, and we increase the maximum churn in players for which approximate optimality is achieved.
   In the bandit setting we present a new algorithm which provides a "small loss"-type bound with improved dependence on the number of actions in utility settings, and is both simple and efficient. This result may be of independent interest.
C1 [Foster, Dylan J.; Lykouris, Thodoris; Sridharan, Karthik; Tardos, Eva] Cornell Univ, Ithaca, NY 14853 USA.
   [Li, Zhiyuan] Tsinghua Univ, Beijing, Peoples R China.
RP Foster, DJ (reprint author), Cornell Univ, Ithaca, NY 14853 USA.
EM djfoster@cs.cornell.edu; lizhiyuan13@mails.tsinghua.edu.cn;
   teddlyk@cs.cornell.edu; sridharan@cs.cornell.edu; eva@cs.cornell.edu
FU NSF [CDSE-MSS 1521544, CCF-1563714]; ONR [N00014-08-1-0031]; Google
   faculty research award; NDSEG fellowship
FX Work supported in part under NSF grants CDS&E-MSS 1521544, CCF-1563714,
   ONR grant N00014-08-1-0031, a Google faculty research award, and an
   NDSEG fellowship.
CR Abernethy  J., 2008, P 21 ANN C LEARN THE
   Allenberg Chamy, 2006, HANNAN CONSISTENCY O, P229
   Audibert JY, 2010, J MACH LEARN RES, V11, P2785
   Auer P, 2002, J COMPUT SYST SCI, V64, P48, DOI 10.1006/jcss.2001.1795
   Bartlett P.L., 2008, P 21 ANN C LEARN THE, P335
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Cesa-Bianchi N, 2007, MACH LEARN, V66, P321, DOI 10.1007/s10994-006-5001-7
   Christodoulou G., 2005, STOC, P67, DOI DOI 10.1145/1060590.1060600
   Daskalakis C, 2015, GAME ECON BEHAV, V92, P327, DOI 10.1016/j.geb.2014.01.003
   Foster D. J., 2015, ADV NEURAL INFORM PR, P3357
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Hazan E, 2010, MACH LEARN, V80, P165, DOI 10.1007/s10994-010-5175-x
   Hazan Elad, 2009, P 26 ANN INT C MACH, V382, P393
   Hazan Elad, 2016, INTRO ONLINE CONVEX
   Kakade SM, 2009, SIAM J COMPUT, V39, P1088, DOI 10.1137/070701704
   Koolen W. M., 2015, COLT, V40, P1155
   Koutsoupias E, 2009, COMPUT SCI REV, V3, P65, DOI 10.1016/j.cosrev.2009.04.003
   Luo Haipeng, 2015, P 28 C LEARN THEOR, P1286
   Lykouris T., 2016, P 27 ANN ACM SIAM S, P120, DOI DOI 10.1137/1.9781611974331.CH9
   Neu  G., 2015, COLT, V40, P1360
   Rakhlin Alexander, 2013, COLT, P993
   Rakhlin S., 2013, ADV NEURAL INFORM PR, P3066
   Roughgarden T, 2002, J ACM, V49, P236, DOI 10.1145/506147.506153
   Roughgarden T, 2015, J ACM, V62, DOI 10.1145/2806883
   Roughgarden Tim, 2016, PRICE ANARCHY AUCTIO
   Steinhardt Jacob, 2014, P 31 INT C MACH LEAR, P1593
   Stoltz G., 2005, THESIS
   Syrgkanis V., 2013, P 45 ANN ACM S THEOR, P211, DOI DOI 10.1145/2488608.2488635
   Syrgkanis Vasilis, 2015, ADV NEURAL INFORM PR, P2989
   Yaroshinsky R, 2004, MACH LEARN, V55, P271, DOI 10.1023/B:MACH.0000027784.72823.e4
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700097
DA 2019-06-15
ER

PT S
AU Fraccaro, M
   Sonderby, SK
   Paquet, U
   Winther, O
AF Fraccaro, Marco
   Sonderby, Soren Kaae
   Paquet, Ulrich
   Winther, Ole
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Sequential Neural Models with Stochastic Layers
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB How can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks? This paper introduces stochastic recurrent neural networks which glue a deterministic recurrent neural network and a state space model together to form a stochastic and sequential neural generative model. The clear separation of deterministic and stochastic layers allows a structured variational inference network to track the factorization of the model's posterior distribution. By retaining both the nonlinear recursive structure of a recurrent neural network and averaging over the uncertainty in a latent path, like a state space model, we improve the state of the art results on the Blizzard and TIMIT speech modeling data sets by a large margin, while achieving comparable performances to competing methods on polyphonic music modeling.
C1 [Fraccaro, Marco; Winther, Ole] Tech Univ Denmark, Lyngby, Denmark.
   [Sonderby, Soren Kaae; Winther, Ole] Univ Copenhagen, Copenhagen, Denmark.
   [Paquet, Ulrich] Google DeepMind, London, England.
RP Fraccaro, M (reprint author), Tech Univ Denmark, Lyngby, Denmark.
OI Winther, Ole/0000-0002-1966-3205
FU Microsoft Research
FX We thank Casper Kaae Sonderby and Lars Maaloe for many fruitful
   discussions, and NVIDIA Corporation for the donation of TITAN X and
   Tesla K40 GPUs. Marco Fraccaro is supported by Microsoft Research
   through its PhD Scholarship Programme.
CR Archer E., 2015, ARXIV151107367
   Bastien F, 2012, ARXIV12115590
   Bayer Justin, 2014, ARXIV14117610
   Boulanger-Lewandowski Nicolas, 2012, ARXIV12066392
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Chung J., 2014, CORR
   Chung Junyoung, 2015, ADV NEURAL INFORM PR, P2962
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Dieleman S, 2015, LASAGNE 1 RELEASE
   Doucet A., 2001, SEQUENTIAL MONTE CAR
   Fabius Otto, 2014, ARXIV14126581
   Gan Z., 2015, NIPS, P2458
   GEIGER D, 1990, NETWORKS, V20, P507, DOI 10.1002/net.3230200504
   Gregor K., 2015, ICML
   Gu S., 2015, NIPS, P2611
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   King S., 2013, 9 ANN BLIZZARD CHALL
   Kingma Diederik P, 2014, ICLR
   Krishnan Rahul G, 2015, ARXIV151105121
   Mnih Andriy, 2014, ARXIV14020030
   Paisley J. W., 2012, ICML
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   Roweis S, 1999, NEURAL COMPUT, V11, P305, DOI 10.1162/089976699300016674
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700004
DA 2019-06-15
ER

PT S
AU Fraser, M
AF Fraser, Maia
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Multi-step learning and underlying structure in statistical models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID MANIFOLD REGULARIZATION
AB In multi-step learning, where a final learning task is accomplished via a sequence of intermediate learning tasks, the intuition is that successive steps or levels transform the initial data into representations more and more "suited" to the final learning task. A related principle arises in transfer-learning where Baxter (2000) proposed a theoretical framework to study how learning multiple tasks transforms the inductive bias of a learner. The most widespread multi-step learning approach is semi-supervised learning with two steps: unsupervised, then supervised. Several authors (Castelli-Cover, 1996; Balcan-Blum, 2005; Niyogi, 2008; Ben-David et al, 2008; Urner et al, 2011) have analyzed SSL, with Balcan-Blum (2005) proposing a version of the PAC learning framework augmented by a "compatibility function" to link concept class and unlabeled data distribution. We propose to analyze SSL and other multi-step learning approaches, much in the spirit of Baxter's framework, by defining a learning problem generatively as a joint statistical model on X.Y. This determines in a natural way the class of conditional distributions that are possible with each marginal, and amounts to an abstract form of compatibility function. It also allows to analyze both discrete and non-discrete settings. As tool for our analysis, we define a notion of gamma-uniform shattering for statistical models. We use this to give conditions on the marginal and conditional models which imply an advantage for multi-step learning approaches. In particular, we recover a more general version of a result of Poggio et al (2012): under mild hypotheses a multi-step approach which learns features invariant under successive factors of a finite group of invariances has sample complexity requirements that are additive rather than multiplicative in the size of the subgroups.
C1 [Fraser, Maia] Univ Ottawa, Dept Math & Stat, Brain & Mind Res Inst, Ottawa, ON K1N 6N5, Canada.
RP Fraser, M (reprint author), Univ Ottawa, Dept Math & Stat, Brain & Mind Res Inst, Ottawa, ON K1N 6N5, Canada.
EM mfrase8@uottawa.ca
CR Ahissar M, 2004, TRENDS COGN SCI, V8, P457, DOI 10.1016/j.tics.2004.08.011
   Alain G., 2012, TECHNICAL REPORT
   Balcan MF, 2005, LECT NOTES COMPUT SC, V3559, P111, DOI 10.1007/11503415_8
   Baxter J, 2000, J ARTIF INTELL RES, V12, P149, DOI 10.1613/jair.731
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Ben-David  Shai, 2008, P 21 ANN C LEARN THE, P33
   Bourne JA, 2006, CEREB CORTEX, V16, P405, DOI 10.1093/cercor/bhi119
   Castelli V, 1996, IEEE T INFORM THEORY, V42, P2102, DOI 10.1109/18.556600
   Devroye L., 1996, PROBABILISTIC THEORY, V31
   Haussler D., 1989, GEN PAC MODEL SAMPLE, P40
   Kearns M. J., 1994, INTRO COMPUTATIONAL
   KEARNS MJ, 1994, J COMPUT SYST SCI, V48, P464, DOI 10.1016/S0022-0000(05)80062-5
   Koltchinskii V, 2001, IEEE T INFORM THEORY, V47, P1902, DOI 10.1109/18.930926
   Mallat S., 2011, ABS11012286 CORR
   Mohri M., 2012, FDN MACHINE LEARNING
   Niyogi P, 2013, J MACH LEARN RES, V14, P1229
   Poggio T., 2012, TECHNICAL REPORT
   Urner R., 2011, ICML
   VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701054
DA 2019-06-15
ER

PT S
AU Friedrich, J
   Paninski, L
AF Friedrich, Johannes
   Paninski, Liam
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Fast Active Set Methods for Online Spike Inference from Calcium Imaging
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID DECONVOLUTION; POPULATIONS; CA2+
AB Fluorescent calcium indicators are a popular means for observing the spiking activity of large neuronal populations. Unfortunately, extracting the spike train of each neuron from raw fluorescence calcium imaging data is a nontrivial problem. We present a fast online active set method to solve this sparse nonnegative deconvolution problem. Importantly, the algorithm progresses through each time series sequentially from beginning to end, thus enabling real-time online spike inference during the imaging session. Our algorithm is a generalization of the pool adjacent violators algorithm (PAVA) for isotonic regression and inherits its linear-time computational complexity. We gain remarkable increases in processing speed: more than one order of magnitude compared to currently employed state of the art convex solvers relying on interior point methods. Our method can exploit warm starts; therefore optimizing model hyperparameters only requires a handful of passes through the data. The algorithm enables real-time simultaneous deconvolution of O(10(5)) traces of whole-brain zebrafish imaging data on a laptop.
C1 [Friedrich, Johannes; Paninski, Liam] Columbia Univ, Grossman Ctr, New York, NY 10027 USA.
   [Friedrich, Johannes; Paninski, Liam] Columbia Univ, Dept Stat, New York, NY 10027 USA.
   [Friedrich, Johannes] Janelia Res Campus, Ashburn, VA 20147 USA.
RP Friedrich, J (reprint author), Columbia Univ, Grossman Ctr, New York, NY 10027 USA.; Friedrich, J (reprint author), Columbia Univ, Dept Stat, New York, NY 10027 USA.; Friedrich, J (reprint author), Janelia Res Campus, Ashburn, VA 20147 USA.
EM j.friedrich@columbia.edu; liam@stat.columbia.edu
FU Swiss National Science Foundation [P300P2_158428]; Simons Foundation
   Global Brain Research Awards [325171, 365002]; NIH BRAIN Initiative [R01
   EB22913, R21 EY027592]; DARPA [N66001-15-C-4032]; Google Faculty
   Research award; Intelligence Advanced Research Projects Activity (IARPA)
   via Department of Interior/Interior Business Center (DoI/IBC)
   [D16PC00003]; ARO MURI [W911NF-12-1-0594]
FX Funding for this research was provided by Swiss National Science
   Foundation Research Award P300P2_158428, Simons Foundation Global Brain
   Research Awards 325171 and 365002, ARO MURI W911NF-12-1-0594, NIH BRAIN
   Initiative R01 EB22913 and R21 EY027592, DARPA N66001-15-C-4032
   (SIMPLEX), and a Google Faculty Research award; in addition, this work
   was supported by the Intelligence Advanced Research Projects Activity
   (IARPA) via Department of Interior/Interior Business Center (DoI/IBC)
   contract number D16PC00003. The U.S. Government is authorized to
   reproduce and distribute reprints for Governmental purposes
   notwithstanding any copyright annotation thereon. Disclaimer: The views
   and conclusions contained herein are those of the authors and should not
   be interpreted as necessarily representing the official policies or
   endorsements, either expressed or implied, of IARPA, DoI/IBC, or the
   U.S. Government.
CR Ahrens MB, 2013, NAT METHODS, V10, P413, DOI [10.1038/nmeth.2434, 10.1038/NMETH.2434]
   Andersen E.D., 2000, HIGH PERFORMANCE OPT, P197, DOI DOI 10.1007/978-1-4757-3216-0_8
   AYER M, 1955, ANN MATH STAT, V26, P641, DOI 10.1214/aoms/1177728423
   Barlow R, 1972, STAT INFERENCE ORDER
   Brent Richard P, 1973, ALGORITHMS MINIMIZAT
   Chen TW, 2013, NATURE, V499, P295, DOI 10.1038/nature12354
   Clancy KB, 2014, NAT NEUROSCI, V17, P807, DOI 10.1038/nn.3712
   Diamond S, 2016, J MACH LEARN RES, V17
   Domahidi Alexander, 2013, 2013 European Control Conference (ECC), P3071
   Friedrich J, 2016, ARXIV160900639
   Grewe BF, 2010, NAT METHODS, V7, P399, DOI 10.1038/nmeth.1453
   Grienberger C, 2012, NEURON, V73, P862, DOI 10.1016/j.neuron.2012.02.011
   Grosenick L, 2015, NEURON, V86, P106, DOI 10.1016/j.neuron.2015.03.034
   Gurobi Optimization Inc, 2015, GUROBI OPTIMIZER REF
   Holekamp TF, 2008, NEURON, V57, P661, DOI 10.1016/j.neuron.2008.01.011
   Lewi J, 2009, NEURAL COMPUT, V21, P619, DOI 10.1162/neco.2008.08-07-594
   O'Donoghue B, 2016, J OPTIMIZATION THEOR, P1
   Packer AM, 2015, NAT METHODS, V12, P140, DOI 10.1038/nmeth.3217
   Park M, 2012, ADV NEURAL INFORM PR, P2348
   Pnevmatikakis EA, 2013, AS C SIGN SYST COMP
   Pnevmatikakis EA, 2016, NEURON, V89, P285, DOI 10.1016/j.neuron.2015.11.037
   Pologruto TA, 2004, J NEUROSCI, V24, P9572, DOI 10.1523/JNEUROSCI.2854-04.2004
   Rickgauer JP, 2014, NAT NEUROSCI, V17, P1816, DOI 10.1038/nn.3866
   Sasaki T, 2008, J NEUROPHYSIOL, V100, P1668, DOI 10.1152/jn.00084.2008
   Shababo B, 2013, NIPS, P1304
   Theis L, 2016, NEURON, V90, P471, DOI 10.1016/j.neuron.2016.04.014
   Vladimirov N, 2014, NAT METHODS
   Vogelstein JT, 2010, J NEUROPHYSIOL, V104, P3691, DOI 10.1152/jn.01073.2009
   Vogelstein JT, 2009, BIOPHYS J, V97, P636, DOI 10.1016/j.bpj.2008.08.005
   Yaksi E, 2006, NAT METHODS, V3, P377, DOI 10.1038/NMETH874
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704030
DA 2019-06-15
ER

PT S
AU Fujii, K
   Kashima, H
AF Fujii, Kaito
   Kashima, Hisashi
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Budgeted stream-based active learning via adaptive submodular
   maximization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Active learning enables us to reduce the annotation cost by adaptively selecting unlabeled instances to be labeled. For pool-based active learning, several effective methods with theoretical guarantees have been developed through maximizing some utility function satisfying adaptive submodularity. In contrast, there have been few methods for stream-based active learning based on adaptive submodularity. In this paper, we propose a new class of utility functions, policy-adaptive submodular functions, which includes many existing adaptive submodular functions appearing in real world problems. We provide a general framework based on policy-adaptive submodularity that makes it possible to convert existing pool-based methods to stream-based methods and give theoretical guarantees on their performance. In addition we empirically demonstrate their effectiveness by comparing with existing heuristics on common benchmark datasets.
C1 [Fujii, Kaito] Kyoto Univ, JST, ERATO, Kawarabayashi Large Graph Project, Kyoto, Japan.
   [Kashima, Hisashi] Kyoto Univ, Kyoto, Japan.
RP Fujii, K (reprint author), Kyoto Univ, JST, ERATO, Kawarabayashi Large Graph Project, Kyoto, Japan.
EM fujii@ml.ist.i.kyoto-u.ac.jp; kashima@i.kyoto-u.ac.jp
CR Badanidiyuru A, 2014, P 20 ACM SIGKDD INT, P671
   Balcan M.-F., 2006, ICML, P65, DOI DOI 10.1145/1143844.1143853]
   Bateni M, 2013, ACM T ALGORITHMS, V9, DOI 10.1145/2500121
   Beygelzimer A., 2009, P 26 INT C MACH LEAR, P49
   Chakrabarti A, 2015, MATH PROGRAM, V154, P225, DOI 10.1007/s10107-015-0900-7
   Chekuri C, 2015, LECT NOTES COMPUT SC, V9134, P318, DOI 10.1007/978-3-662-47672-7_26
   Chen Y., 2013, INT C MACH LEARN ICM, P160
   Cuong N. V., 2014, UNCERTAINTY ARTIFICI
   Cuong N. V., 2013, ADV NEURAL INFORM PR, V26, P1457
   Dagan I., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P150
   Das J, 2015, INT J ROBOT RES, V34, P1435, DOI 10.1177/0278364915587723
   Dasgupta S., 2004, ADV NEURAL INFORM PR, V17, P337
   Dynkin E. B., 1963, SOV MATH DOKL, V4, P627
   Feige U, 2011, SIAM J COMPUT, V40, P1133, DOI 10.1137/090779346
   Feldman Moran, 2011, Approximation, Randomization, and Combinatorial Optimization Algorithms and Techniques. Proceedings 14th International Workshop, APPROX 2011 and 15th International Workshop, RANDOM 2011, P218, DOI 10.1007/978-3-642-22935-0_19
   Freund Y, 1997, MACH LEARN, V28, P133, DOI 10.1023/A:1007330508534
   Fujishige S, 2005, ANN DISCR MATH, V58, P1
   Gabillon Victor, 2013, P ADV NEUR INF PROC, P2697
   Golovin D., 2010, ADV NEURAL INFORM PR, P766
   Golovin D, 2011, J ARTIF INTELL RES, V42, P427
   Gonen A, 2013, J MACH LEARN RES, V14, P2583
   Gotovos A, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P1996
   Loy C. C., 2012, P IEEE C COMP VIS PA
   Sabato Sivan, 2016, ANN C LEARN THEOR, P1419
   Sculley D., 2007, P 4 C EM ANT CEAS
   Smailovic J, 2014, INFORM SCIENCES, V285, P181, DOI 10.1016/j.ins.2014.04.034
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700003
DA 2019-06-15
ER

PT S
AU Ganapathiraman, V
   Zhang, XH
   Yu, YL
   Wen, JF
AF Ganapathiraman, Vignesh
   Zhang, Xinhua
   Yu, Yaoliang
   Wen, Junfeng
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Convex Two-Layer Modeling with Latent Structure
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Unsupervised learning of structured predictors has been a long standing pursuit in machine learning. Recently a conditional random field auto-encoder has been proposed in a two-layer setting, allowing latent structured representation to be automatically inferred. Aside from being nonconvex, it also requires the demanding inference of normalization. In this paper, we develop a convex relaxation of two-layer conditional model which captures latent structure and estimates model parameters, jointly and optimally. We further expand its applicability by resorting to a weaker form of inference-maximum a-posteriori. The flexibility of the model is demonstrated on two structures based on total unimodularity-graph matching and linear chain. Experimental results confirm the promise of the method.
C1 [Ganapathiraman, Vignesh; Zhang, Xinhua] Univ Illinois, Chicago, IL 60607 USA.
   [Yu, Yaoliang] Univ Waterloo, Waterloo, ON, Canada.
   [Wen, Junfeng] Univ Alberta, Edmonton, AB, Canada.
RP Ganapathiraman, V (reprint author), Univ Illinois, Chicago, IL 60607 USA.
EM vganap2@uic.edu; zhangx@uic.edu; yaoliang.yu@uwaterloo.ca;
   junfengwen@gmail.com
CR Ammar W., 2014, NIPS
   Arora Sanjeev, 2014, ICML
   Aslan  O., 2014, NIPS
   Banerjee A, 2005, J MACH LEARN RES, V6, P1705
   Bengio Y., 2005, NIPS
   Chandrasekaran V, 2012, FOUND COMPUT MATH, V12, P805, DOI 10.1007/s10208-012-9135-7
   Chang M.-W., 2010, ICML
   Chang M.-W., 2010, NAACL
   Chen L.-C., 2015, ICML
   Chen N, 2012, IEEE T PATTERN ANAL, V34, P2365, DOI 10.1109/TPAMI.2012.64
   Collobert R., 2011, ICML
   Daume III H., 2009, ICML
   Druck G., 2007, KDD
   Gane A., 2014, AISTATS
   Goldwasser D., 2008, EMNLP
   Gotovos A., 2015, NIPS
   Haffari G., 2007, UAI
   Hazan T., 2012, ICML
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jaggi  M., 2013, ICML
   Larochelle H., 2008, ICML
   Livni R., 2014, ARXIV13047045V2
   Meshi O., 2015, NIPS
   Mnih V., 2011, AISTATS
   Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5
   Pataki G, 1998, MATH OPER RES, V23, P339, DOI 10.1287/moor.23.2.339
   Ratajczak M., 2014, WORKSH LEARN TRACT P
   Smith N., 2005, ACL
   Sohn K., 2015, NIPS
   Taskar B., 2005, ICML
   Xu L., 2009, ICML
   Xu L., 2006, ICML
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702061
DA 2019-06-15
ER

PT S
AU Gao, SY
   Steeg, GV
   Galstyan, A
AF Gao, Shuyang
   Steeg, Greg Ver
   Galstyan, Aram
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Variational Information Maximization for Feature Selection
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID MUTUAL INFORMATION
AB Feature selection is one of the most fundamental problems in machine learning. An extensive body of work on information-theoretic feature selection exists which is based on maximizing mutual information between subsets of features and class labels. Practical methods are forced to rely on approximations due to the difficulty of estimating mutual information. We demonstrate that approximations made by existing methods are based on unrealistic assumptions. We formulate a more flexible and general class of assumptions based on variational distributions and use them to tractably generate lower bounds for mutual information. These bounds define a novel information-theoretic framework for feature selection, which we prove to be optimal under tree graphical models with proper choice of variational distributions. Our experiments demonstrate that the proposed method strongly outperforms existing information-theoretic feature selection approaches.
C1 [Gao, Shuyang; Steeg, Greg Ver; Galstyan, Aram] Univ Southern Calif, Informat Sci Inst, Los Angeles, CA 90089 USA.
RP Gao, SY (reprint author), Univ Southern Calif, Informat Sci Inst, Los Angeles, CA 90089 USA.
EM gaos@usc.edu; gregv@isi.edu; galstyan@isi.edu
CR BACHE K., 2013, UCI MACHINE LEARNING
   Balagani KS, 2010, IEEE T PATTERN ANAL, V32, P1342, DOI 10.1109/TPAMI.2010.62
   Barber D, 2004, ADV NEUR IN, V16, P201
   BATTITI R, 1994, IEEE T NEURAL NETWOR, V5, P537, DOI 10.1109/72.298224
   Brown G, 2012, J MACH LEARN RES, V13, P27
   Cheng HR, 2011, ETRI J, V33, P210, DOI 10.4218/etrij.11.0110.0237
   Cover T. M., 2012, ELEMENTS INFORM THEO
   Das A., 2011, P 28 INT C MACH LEAR, P1057
   Dash M, 1997, INTELL DATA ANAL, V1, P131, DOI DOI 10.1016/S1088-467X(97)00008-5
   Fleuret F, 2004, J MACH LEARN RES, V5, P1531
   Gao Shuyang, VARIATIONAL FEATURE
   Gentleman R, 2005, STAT BIOL HEALTH, P189
   Guyon I., 2003, Journal of Machine Learning Research, V3, P1157, DOI 10.1162/153244303322753616
   Kohavi R, 1997, ARTIF INTELL, V97, P273, DOI 10.1016/S0004-3702(97)00043-X
   LEWIS DD, 1992, SPEECH AND NATURAL LANGUAGE, P212
   Lin DH, 2006, LECT NOTES COMPUT SC, V3951, P68
   Liu H., 2012, FEATURE SELECTION KN, V454
   Mohamed S., 2015, ADV NEURAL INFORM PR, V28, P2116
   Nguyen X. V., 2014, P 20 ACM SIGKDD INT, P512, DOI 10.1145/2623330.2623611
   Nguyen XL, 2010, IEEE T INFORM THEORY, V56, P5847, DOI 10.1109/TIT.2010.2068870
   Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159
   Rodriguez-Lujan I, 2010, J MACH LEARN RES, V11, P1491
   Ross BC, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0087357
   Vinh N. X., 2015, PATTERN RECOGNITION
   Yang H. H., 1999, ADV NEURAL INFORM PR, P687
   Zhou Yingbo, 2014, ADV NEURAL INFORM PR, V27, P3554
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703080
DA 2019-06-15
ER

PT S
AU Gao, WH
   Oh, S
   Viswanath, P
AF Gao, Weihao
   Oh, Sewoong
   Viswanath, Pramod
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Breaking the Bandwidth Barrier: Geometrical Adaptive Entropy Estimation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID DENSITIES
AB Estimators of information theoretic measures such as entropy and mutual information are a basic workhorse for many downstream applications in modern data science. State of the art approaches have been either geometric (nearest neighbor (NN) based) or kernel based (with a globally chosen bandwidth). In this paper, we combine both these approaches to design new estimators of entropy and mutual information that outperform state of the art methods. Our estimator uses local bandwidth choices of k-NN distances with a finite k, independent of the sample size. Such a local and data dependent choice improves performance in practice, but the bandwidth is vanishing at a fast rate, leading to a non-vanishing bias. We show that the asymptotic bias of the proposed estimator is universal; it is independent of the underlying distribution. Hence, it can be precomputed and subtracted from the estimate. As a byproduct, we obtain a unified way of obtaining both kernel and NN estimators. The corresponding theoretical contribution relating the asymptotic geometry of nearest neighbors to order statistics is of independent mathematical interest.
C1 [Gao, Weihao; Oh, Sewoong; Viswanath, Pramod] Univ Illinois, Coordinated Sci Lab, Urbana, IL 61801 USA.
   [Gao, Weihao; Viswanath, Pramod] Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA.
   [Oh, Sewoong] Univ Illinois, Dept Ind & Enterprise Syst Engn, Urbana, IL 61801 USA.
RP Gao, WH (reprint author), Univ Illinois, Coordinated Sci Lab, Urbana, IL 61801 USA.; Gao, WH (reprint author), Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA.
EM wgao9@illinois.edu; swoh@illinois.edu; pramodv@illinois.edu
FU NSF SaTC award [CNS-1527754]; NSF CISE award [CCF-1553452, CCF-1617745]
FX This work is supported by NSF SaTC award CNS-1527754, NSF CISE award
   CCF-1553452, NSF CISE award CCF-1617745. We thank the anonymous
   reviewers for their constructive feedback.
CR Biau G., 2016, LECT NEAREST NEIHBOR
   Galstyan A, 2014, P ADV NEUR INF PROC, P577
   Gao S., 2015, INT C ART INT STAT A
   Gao S., 2015, 31 C UNC ART INT UAI
   Gao W., 2016, ARXIV160403006
   Goria MN, 2005, J NONPARAMETR STAT, V17, P277, DOI 10.1080/104852504200026815
   Hjort NL, 1996, ANN STAT, V24, P1619
   JOE H, 1989, ANN I STAT MATH, V41, P683, DOI 10.1007/BF00057735
   Kandasamy K., 2015, ADV NEURAL INFORM PR, P397
   Kozachenko L. F., 1987, PROBL PEREDACHI INF, V23, P9
   Kraskov A, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.066138
   Krishnaswamy S, 2014, SCIENCE, V346, P1079, DOI 10.1126/science.1250689
   Leonenko N, 2008, ANN STAT, V36, P2153, DOI 10.1214/07-AOS539
   Liu  H., 2012, ADV NEURAL INFORM PR, P2537
   Loader C., 2006, LOCAL REGRESSION LIK
   Loader CR, 1996, ANN STAT, V24, P1602
   Lombardi D, 2016, PHYS REV E, V93, DOI 10.1103/PhysRevE.93.013310
   Manning C. D., 2008, INTRO INFORM RETRIEV, V1
   Pal D, 2010, ADV NEURAL INFORM PR, V23, P1849
   Reiss  R.-D., 2012, APPROXIMATE DISTRIBU
   Reshef DN, 2011, SCIENCE, V334, P1518, DOI 10.1126/science.1205438
   Sheather SJ, 2004, STAT SCI, V19, P588, DOI 10.1214/088342304000000297
   Singh Shashank, 2016, ARXIV160308578
   Steeg G. Ver, 2016, ICML
   Tsybakov AB, 1996, SCAND J STAT, V23, P75
   Vincent P., 2003, LOCALLY WEIGHTED FUL
   Wang Q., 2009, FDN TRENDS COMMUN IN, V5, P265, DOI [DOI 10.1561/0100000021, 10.1561/0100000021]
   Wang Q, 2009, IEEE T INFORM THEORY, V55, P2392, DOI 10.1109/TIT.2009.2016060
   Wasserman L, 2006, ALL NONPARAMETRIC ST
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702046
DA 2019-06-15
ER

PT S
AU Gao, YJ
   Archer, E
   Paninski, L
   Cunningham, JP
AF Gao, Yuanjun
   Archer, Evan
   Paninski, Liam
   Cunningham, John P.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Linear dynamical neural population models through nonlinear embeddings
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB A body of recent work in modeling neural activity focuses on recovering low-dimensional latent features that capture the statistical structure of large-scale neural populations. Most such approaches have focused on linear generative models, where inference is computationally tractable. Here, we propose fLDS, a general class of nonlinear generative models that permits the firing rate of each neuron to vary as an arbitrary smooth function of a latent, linear dynamical state. This extra flexibility allows the model to capture a richer set of neural variability than a purely linear model, but retains an easily visualizable low-dimensional latent space. To fit this class of non-conjugate models we propose a variational inference scheme, along with a novel approximate posterior capable of capturing rich temporal correlations across time. We show that our techniques permit inference in a wide class of generative models. We also show in application to two neural datasets that, compared to state-of-the-art neural population models, fLDS captures a much larger proportion of neural variability with a small number of latent dimensions, providing superior predictive performance and interpretability.
C1 [Gao, Yuanjun; Archer, Evan; Paninski, Liam; Cunningham, John P.] Columbia Univ, Dept Stat, New York, NY 10027 USA.
   [Archer, Evan; Paninski, Liam; Cunningham, John P.] Columbia Univ, Grossman Ctr, New York, NY USA.
RP Gao, YJ (reprint author), Columbia Univ, Dept Stat, New York, NY 10027 USA.
EM yg2312@columbia.edu; evan@stat.columbia.edu; liam@stat.columbia.edu;
   jpc2181@columbia.edu
CR Archer E., 2015, ARXIV151107367
   Archer E. W., 2014, ADV NEURAL INFORM PR, V27, P343
   Bastien F, 2012, ARXIV12115590
   Bergstra J, 2010, P PYTH SCI COMP C SC, P3
   Buesing L., 2014, ADV NEURAL INFORM PR, V27, P3500
   Burda Y., 2015, ARXIV150900519
   Churchland MM, 2012, NATURE, V487, P51, DOI 10.1038/nature11129
   Ecker AS, 2014, NEURON, V82, P235, DOI 10.1016/j.neuron.2014.02.006
   Gao Y., 2015, NIPS, P2035
   Ghahramani Z, 2014, ADV NEURAL INFORM PR, P3680
   Goris RLT, 2014, NAT NEUROSCI, V17, P858, DOI 10.1038/nn.3711
   Graf ABA, 2011, NAT NEUROSCI, V14, P239, DOI 10.1038/nn.2733
   Hafting T, 2005, NATURE, V436, P801, DOI 10.1038/nature03721
   Johnson Matthew J, 2016, ARXIV160306277
   Khan M. E., 2013, INT C MACH LEARN, P951
   Kingma D.P., 2013, ARXIV13126114
   Krishnan Rahul G, 2015, ARXIV151105121
   Macke J. H., 2011, ADV NEURAL INF PROCE, V24, P1350
   Pfau D., 2013, ADV NEURAL INF PROCE, V26, P2391
   Ranganath R., 2013, ARXIV14010118
   Rezende D. J, 2014, ARXIV14014082
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Titsias M., 2014, P 31 INT C MACH LEAR, P1971
   Yu BM, 2009, J NEUROPHYSIOL, V102, P614, DOI 10.1152/jn.90941.2008
   Zeiler M.D., 2012, ARXIV12125701
   Zhao Yuan, 2016, ARXIV160403053
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703066
DA 2019-06-15
ER

PT S
AU Garber, D
   Meshi, O
AF Garber, Dan
   Meshi, Ofer
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Linear-Memory and Decomposition-Invariant Linearly Convergent
   Conditional Gradient Algorithm for Structured Polytopes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Recently, several works have shown that natural modifications of the classical conditional gradient method (aka Frank-Wolfe algorithm) for constrained convex optimization, provably converge with a linear rate when: i) the feasible set is a polytope, and ii) the objective is smooth and strongly-convex. However, all of these results suffer from two significant shortcomings:
   1. large memory requirement due to the need to store an explicit convex decomposition of the current iterate, and as a consequence, large running-time overhead per iteration
   2. the worst case convergence rate depends unfavorably on the dimension In this work we present a new conditional gradient variant and a corresponding analysis that improves on both of the above shortcomings. In particular:
   1. both memory and computation overheads are only linear in the dimension
   2. in case the optimal solution is sparse, the new convergence rate replaces a factor which is at least linear in the dimension in previous work, with a linear dependence on the number of non-zeros in the optimal solution
   At the heart of our method and corresponding analysis, is a novel way to compute decomposition-invariant away-steps. While our theoretical guarantees do not apply to any polytope, they apply to several important structured polytopes that capture central concepts such as paths in graphs, perfect matchings in bipartite graphs, marginal distributions that arise in structured prediction tasks, and more. Our theoretical findings are complemented by empirical evidence which shows that our method delivers state-of-the-art performance.
C1 [Garber, Dan] Toyota Technol Inst, Chicago, IL 60637 USA.
   [Meshi, Ofer] Google, Mountain View, CA USA.
RP Garber, D (reprint author), Toyota Technol Inst, Chicago, IL 60637 USA.
EM dgarber@ttic.edu; meshi@google.com
CR Ahipasaoglu SD, 2008, OPTIM METHOD SOFTW, V23, P5, DOI 10.1080/10556780701589669
   Beck A, 2004, MATH METHOD OPER RES, V59, P235, DOI 10.1007/s001860300327
   Beck Amir, 2015, ARXIV150405002
   Dudik M., 2012, AISTATS, P327
   Frank M., 1956, NAV RES LOG, V3, P149
   Gaidon A, 2011, PROC CVPR IEEE
   Garber D., 2015, P 32 INT C MACH LEAR, V37, P541
   Garber D, 2016, SIAM J OPTIMIZ, V26, P1493, DOI 10.1137/140985366
   Garber Dan, 2016, ARXIV160506203
   GueLat Jacques, 1986, MATH PROGRAMMING, V35
   Hazan Elad, 2016, CORR
   Hazan  Elad, 2012, P 29 INT C MACH LEAR
   Jaggi Martin, 2010, P 27 INT C MACH LEAR
   Jaggi Martin, 2013, P 30 INT C MACH LEAR
   Joulin A, 2014, LECT NOTES COMPUT SC, V8694, P253, DOI 10.1007/978-3-319-10599-4_17
   Lacoste-Julien Simon, 2013, P 30 INT C MACH LEAR
   Lacoste-Julien Simon, 2015, ADV NEURAL INFORM PR, V28, P496
   Lan Guanghui, 2013, CORR
   Laue Soren, 2012, P 29 INT C MACH LEAR
   Osokin Anton, 2016, INT C MACH LEARN ICM
   Schrijver A., 2003, COMBINATORIAL OPTIMI
   Shalev-Shwartz Shai, 2011, P 28 INT C MACH LEAR
   Taskar B., 2003, ADV NEURAL INFORM PR
   Wainwright M, 2008, GRAPHICAL MODELS EXP
   Ying YM, 2012, J MACH LEARN RES, V13, P1
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700079
DA 2019-06-15
ER

PT S
AU Garber, D
AF Garber, Dan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Faster Projection-free Convex Optimization over the Spectrahedron
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID EIGENVALUE
AB Minimizing a convex function over the spectrahedron, i.e., the set of all d x d positive semidefinite matrices with unit trace, is an important optimization task with many applications in optimization, machine learning, and signal processing. It is also notoriously difficult to solve in large-scale since standard techniques require to compute expensive matrix decompositions. An alternative is the conditional gradient method (aka Frank-Wolfe algorithm) that regained much interest in recent years, mostly due to its application to this specific setting. The key benefit of the CG method is that it avoids expensive matrix decompositions all together, and simply requires a single eigenvector computation per iteration, which is much more efficient. On the downside, the CG method, in general, converges with an inferior rate. The error for minimizing a beta-smooth function after t iterations scales like beta/t. This rate does not improve even if the function is also strongly convex. In this work we present a modification of the CG method tailored for the spectrahedron. The per-iteration complexity of the method is essentially identical to that of the standard CG method: only a single eigenvector computation is required. For minimizing an a-strongly convex and beta-smooth function, the expected error of the method after t iterations is:
   O(min{beta/t, (beta root rank(X*)/alpha(1/4)t)(4/3), (beta/root alpha lambda(min) (X*)t)(2)}),
   where rank(X*), lambda(min) (X*) are the rank of the optimal solution and smallest non-zero eigenvalue, respectively. Beyond the significant improvement in convergence rate, it also follows that when the optimum is low-rank, our method provides better accuracy-rank tradeoff than the standard CG method. To the best of our knowledge, this is the first result that attains provably faster convergence rates for a CG variant for optimization over the spectrahedron. We also present encouraging preliminary empirical results.
C1 [Garber, Dan] Toyota Technol Inst Chicago, Chicago, IL 60637 USA.
RP Garber, D (reprint author), Toyota Technol Inst Chicago, Chicago, IL 60637 USA.
EM dgarber@ttic.edu
CR Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Dudik M., 2012, AISTATS, P327
   Frank M., 1956, NAV RES LOG, V3, P149
   Freund Robert M, 2015, ARXIV151102204
   Garber D., 2015, P 32 INT C MACH LEAR, V37, P541
   Garber D., 2015, ARXIV150905647
   Garber D., 2016, INT C MACH LEARN, P2626
   Garber D, 2016, SIAM J OPTIMIZ, V26, P1493, DOI 10.1137/140985366
   Gonen M, 2011, J MACH LEARN RES, V12, P2211
   Hazan Elad, 2008, 8 LAT AM THEOR INF S
   Hazan  Elad, 2012, P 29 INT C MACH LEAR
   Jaggi Martin, 2010, P 27 INT C MACH LEAR
   Jaggi Martin, 2013, P 30 INT C MACH LEAR
   KUCZYNSKI J, 1992, SIAM J MATRIX ANAL A, V13, P1094, DOI 10.1137/0613066
   Lacoste-Julien Simon, 2015, ADV NEURAL INFORM PR, V28, P496
   Lanckriet GRG, 2004, J MACH LEARN RES, V5, P27
   Laue Soren, 2012, P 29 INT C MACH LEAR
   Negahban S., 2009, ADV NEURAL INFORM PR, P1348
   Shalev-Shwartz Shai, 2011, P 28 INT C MACH LEAR
   Shamir Ohad, 2015, P 32 INT C MACH LEAR
   Xing E. P., 2003, ADV NEURAL INFORM PR, P505
   Ying YM, 2012, J MACH LEARN RES, V13, P1
   Zhang X., 2012, ADV NEURAL INFORM PR, V25, P2906
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703033
DA 2019-06-15
ER

PT S
AU Garg, VK
   Jaakkola, T
AF Garg, Vikas K.
   Jaakkola, Tommi
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning Tree Structured Potential Games
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Many real phenomena, including behaviors, involve strategic interactions that can be learned from data. We focus on learning tree structured potential games where equilibria are represented by local maxima of an underlying potential function. We cast the learning problem within a max margin setting and show that the problem is NP-hard even when the strategic interactions form a tree. We develop a variant of dual decomposition to estimate the underlying game and demonstrate with synthetic and real decision/voting data that the game theoretic perspective (carving out local maxima) enables meaningful recovery.
C1 [Garg, Vikas K.; Jaakkola, Tommi] MIT, CSAIL, Cambridge, MA 02139 USA.
RP Garg, VK (reprint author), MIT, CSAIL, Cambridge, MA 02139 USA.
EM vgarg@csail.mit.edu; tommi@csail.mit.edu
CR Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Bradley J. K., 2010, ICML
   Dubey P, 2006, GAME ECON BEHAV, V54, P77, DOI 10.1016/j.geb.2004.10.007
   Hoefer M., 2012, INTERNET NETWORK EC, P364
   Honorio J, 2015, J MACH LEARN RES, V16, P1157
   Irfan MT, 2014, ARTIF INTELL, V215, P79, DOI 10.1016/j.artint.2014.06.004
   Kearns M. J., 2001, UAI
   Lafferty J. O., 2001, ICML
   Martins A. F. T., 2011, EMNLP
   Martins A. F. T., 2010, NIPS
   Meshi O., 2013, UAI
   Monderer D, 1996, GAME ECON BEHAV, V14, P124, DOI 10.1006/game.1996.0044
   Nowozin S., 2011, FDN TRENDS COMPUTER
   Rush A. M., 2010, EMNLP
   Rush AM, 2012, J ARTIF INTELL RES, V45, P305, DOI 10.1613/jair.3680
   Samdani R., 2012, ICML
   Song Y., 2011, MOBICOM
   Sontag D., 2010, NIPS
   Taskar B, 2003, NIPS
   Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453
   Ui T, 2001, ECONOMETRICA, V69, P1373, DOI 10.1111/1468-0262.00246
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701009
DA 2019-06-15
ER

PT S
AU Garivier, A
   Kaufmann, E
   Lattimore, T
AF Garivier, Aurelien
   Kaufmann, Emilie
   Lattimore, Tor
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI On Explore-Then-Commit Strategies
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID MULTIARMED BANDIT; ALLOCATION; BOUNDS
AB We study the problem of minimising regret in two-armed bandit problems with Gaussian rewards. Our objective is to use this simple setting to illustrate that strategies based on an exploration phase (up to a stopping time) followed by exploitation are necessarily suboptimal. The results hold regardless of whether or not the difference in means between the two arms is known. Besides the main message, we also refine existing deviation inequalities, which allow us to design fully sequential strategies with finite-time regret guarantees that are (a) asymptotically optimal as the horizon grows and (b) order-optimal in the minimax sense. Furthermore we provide empirical evidence that the theory also holds in practice and discuss extensions to non-gaussian and multiple-armed case.
C1 [Garivier, Aurelien] Inst Math Toulouse, F-31062 Toulouse 9, France.
   [Garivier, Aurelien] Univ Toulouse, UMR5219, F-31062 Toulouse 9, France.
   [Garivier, Aurelien] CNRS, UPS IMT, F-31062 Toulouse 9, France.
   [Kaufmann, Emilie] Univ Lille, CNRS, Cent Lille,Inria SequeL,UMR 9189, CRIStAL Ctr Rech Informat Signal & Automat Lille, F-59000 Lille, France.
   [Lattimore, Tor] Univ Alberta, 116 St & 85 Ave, Edmonton, AB T6G 2R3, Canada.
RP Garivier, A (reprint author), Inst Math Toulouse, F-31062 Toulouse 9, France.; Garivier, A (reprint author), Univ Toulouse, UMR5219, F-31062 Toulouse 9, France.; Garivier, A (reprint author), CNRS, UPS IMT, F-31062 Toulouse 9, France.
EM aurelien.garivier@math.univ-toulouse.fr; emilie.kaufmann@univ-lille1.fr;
   tor.lattimore@gmail.com
FU CIMI (Centre International de Mathematiques et d'Informatique)
   Excellence program; French Agence Nationale de la Recherche (ANR)
   [ANR-13-BS01-0005, ANR-13-CORD-0020]
FX This work was partially supported by the CIMI (Centre International de
   Mathematiques et d'Informatique) Excellence program while Emilie
   Kaufmann visited Toulouse in November 2015. The authors acknowledge the
   support of the French Agence Nationale de la Recherche (ANR), under
   grants ANR-13-BS01-0005 (project SPADRO) and ANR-13-CORD-0020 (project
   ALICIA).
CR Audibert J.-Y., 2009, COLT, P217
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Auer P, 2010, PERIOD MATH HUNG, V61, P55, DOI 10.1007/s10998-010-3055-6
   Bubeck S, 2013, ADV NEURAL INFORM PR, P638
   Bubeck Sebastien, 2013, P 26 C LEARN THEOR, P122
   Cappe O, 2013, ANN STAT, V41, P1516, DOI 10.1214/13-AOS1119
   Even-Dar E, 2006, J MACH LEARN RES, V7, P1079
   Garivier Aurelien, 2016, P 29 C LEARN THEOR
   Garivier Aurelien, 2016, ARXIV160207182
   Hoorfar A., 2008, J INEQUAL PURE APPL, V9, P5
   KATEHAKIS MN, 1995, P NATL ACAD SCI USA, V92, P8584, DOI 10.1073/pnas.92.19.8584
   Kaufmann Emilie, 2014, P 27 C LEARN THEOR
   LAI TL, 1987, ANN STAT, V15, P1091, DOI 10.1214/aos/1176350495
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Lai Tze Leung, 1983, RECENT ADV STAT, P51
   Lattimore Tor, 2015, TECHNICAL REPORT
   Morters Peter, 2010, BROWNIAN MOTION, V30
   Perchet Vianney, 2015, P 28 C LEARN THEOR
   Perchet Vianney, 2013, ANN STAT
   Siegmund D, 1985, SEQUENTIAL ANAL
   Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.2307/2332286
   WALD A, 1945, ANN MATH STAT, V16, P117, DOI 10.1214/aoms/1177731118
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701036
DA 2019-06-15
ER

PT S
AU Gautier, A
   Nguyen, Q
   Hein, M
AF Gautier, A.
   Nguyen, Q.
   Hein, M.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Globally Optimal Training of Generalized Polynomial Neural Networks with
   Nonlinear Spectral Methods
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB The optimization problem behind neural networks is highly non-convex. Training with stochastic gradient descent and variants requires careful parameter tuning and provides no guarantee to achieve the global optimum. In contrast we show under quite weak assumptions on the data that a particular class of feedforward neural networks can be trained globally optimal with a linear convergence rate with our nonlinear spectral method. Up to our knowledge this is the first practically feasible method which achieves such a guarantee. While the method can in principle be applied to deep networks, we restrict ourselves for simplicity in this paper to one and two hidden layer networks. Our experiments confirm that these models are rich enough to achieve good performance on a series of real-world datasets.
C1 [Gautier, A.; Nguyen, Q.; Hein, M.] Saarland Univ, Dept Math & Comp Sci, Saarland Informat Campus, Saarbrucken, Germany.
RP Gautier, A (reprint author), Saarland Univ, Dept Math & Comp Sci, Saarland Informat Campus, Saarbrucken, Germany.
FU ERC starting grant NOLEPRO [307793]
FX The authors acknowledge support by the ERC starting grant NOLEPRO
   307793.
CR Anthony  Martin, 1999, NEURAL NETWORK LEARN
   Arora Sanjeev, 2014, ICML
   Berman A., 1994, NONNEGATIVE MATRICES
   Bertsekas D.P., 1999, NONLINEAR PROGRAMMIN
   Choromanska Anna, 2015, AISTATS
   Daniely A, 2016, ARXIV160205897V1
   Gautier A., 2016, PERRON FROBENI UNPUB
   Haeffele B. D., 2015, ARXIV150607540V1
   Hardt M., 2016, ICML
   Horn R. A., 2013, MATRIX ANAL
   Janzamin M., 2015, ARXIV150608473V3
   Kirk W.A., 2001, INTRO METRIC SPACES
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lemmens B., 2012, NONLINEAR PERRON FRO
   Livni R., 2014, ADV NEURAL INFORM PR, V27, P855
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Sima J, 2002, NEURAL COMPUT, V14, P2709, DOI 10.1162/089976602760408035
   Thompson A., 1963, P AM MATH SOC, V14, P438
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701095
DA 2019-06-15
ER

PT S
AU Ge, R
   Lee, JD
   Ma, TY
AF Ge, Rong
   Lee, Jason D.
   Ma, Tengyu
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Matrix Completion has No Spurious Local Minimum
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Matrix completion is a basic machine learning problem that has wide applications, especially in collaborative filtering and recommender systems. Simple non-convex optimization algorithms are popular and effective in practice. Despite recent progress in proving various non-convex algorithms converge from a good initial point, it remains unclear why random or arbitrary initialization suffices in practice. We prove that the commonly used non-convex objective function for positive semidefinite matrix completion has no spurious local minima - all local minima must also be global. Therefore, many popular optimization algorithms such as (stochastic) gradient descent can provably solve positive semidefinite matrix completion with arbitrary initialization in polynomial time. The result can be generalized to the setting when the observed entries contain noise. We believe that our main proof strategy can be useful for understanding geometric properties of other statistical problems involving partial or noisy observations.
C1 [Ge, Rong] Duke Univ, 308 Res Dr, Durham, NC 27708 USA.
   [Lee, Jason D.] Univ Southern Calif, 3670 Trousdale Pkwy, Los Angeles, CA 90089 USA.
   [Ma, Tengyu] Princeton Univ, 35 Olden St, Princeton, NJ 08540 USA.
RP Ge, R (reprint author), Duke Univ, 308 Res Dr, Durham, NC 27708 USA.
EM rongge@cs.duke.edu; jasonlee@marshall.usc.edu; tengyu@cs.princeton.edu
CR Amit Y., 2007, P INT C MACH LEARN, P17
   Bandeira AS, 2016, ARXIV160204426
   Bhojanapalli S., 2016, ARXIV E PRINTS
   Burer S, 2003, MATH PROGRAM, V95, P329, DOI 10.1007/s10107-002-0352-8
   Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061
   Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Chen Y., 2015, ARXIV150903025
   Davenport MA, 2014, INF INFERENCE, V3, P189, DOI 10.1093/imaiai/iau006
   Ge R., 2015, ARXIV150302101
   Hardt M., 2014, P 27 C LEARN THEOR, P638
   Hardt Moritz, 2014, FOCS 2014
   Hsu D., 2012, ELECTRON COMMUN PROB, V17, P6
   Jain P., 2013, P 45 ANN ACM S THEOR, P665, DOI DOI 10.1145/2488608.2488693
   Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205
   Koren Yehuda, 2009, NETFLIX PRIZE DOCUME, V81
   Li Yuanzhi, 2016, ARXIV160202262
   Loh PL, 2015, J MACH LEARN RES, V16, P559
   Loh Po-Ling, 2014, ARXIV14125632
   Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8
   Oliveira R. Imbuzeiro, 2010, ARXIV E PRINTS
   PEMANTLE R, 1990, ANN PROBAB, V18, P698, DOI 10.1214/aop/1176990853
   Recht B, 2011, J MACH LEARN RES, V12, P3413
   Rennie J. D. M., 2005, P 22 INT C MACH LEAR, P713, DOI [10.1145/1102351.1102441, DOI 10.1145/1102351.1102441]
   Simchowitz Max, 2016, U CALIFORNIA BERKELE, V1050, P16
   Sun J., 2015, ARXIV151006096
   Sun RY, 2015, ANN IEEE SYMP FOUND, P270, DOI 10.1109/FOCS.2015.25
   Zhao Tuo, 2015, Adv Neural Inf Process Syst, V28, P559
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700013
DA 2019-06-15
ER

PT S
AU Genevay, A
   Cuturi, M
   Peyre, G
   Bach, F
AF Genevay, Aude
   Cuturi, Marco
   Peyre, Gabriel
   Bach, Francis
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Stochastic Optimization for Large-scale Optimal Transport
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID DISTANCE
AB Optimal transport (OT) defines a powerful framework to compare probability distributions in a geometrically faithful way. However, the practical impact of OT is still limited because of its computational burden. We propose a new class of stochastic optimization algorithms to cope with large-scale OT problems. These methods can handle arbitrary distributions (either discrete or continuous) as long as one is able to draw samples from them, which is the typical setup in high-dimensional learning problems. This alleviates the need to discretize these densities, while giving access to provably convergent methods that output the correct distance without discretization error. These algorithms rely on two main ideas: (a) the dual OT problem can be re-cast as the maximization of an expectation; (b) the entropic regularization of the primal OT problem yields a smooth dual optimization which can be addressed with algorithms that have a provably faster convergence. We instantiate these ideas in three different setups: (i) when comparing a discrete distribution to another, we show that incremental stochastic optimization schemes can beat Sinkhorn's algorithm, the current state-of-the-art finite dimensional OT solver; (ii) when comparing a discrete distribution to a continuous density, a semi-discrete reformulation of the dual program is amenable to averaged stochastic gradient descent, leading to better performance than approximately solving the problem by discretization; (iii) when dealing with two continuous densities, we propose a stochastic gradient descent over a reproducing kernel Hilbert space (RKHS). This is currently the only known method to solve this problem, apart from computing OT on finite samples. We backup these claims on a set of discrete, semi-discrete and continuous benchmark problems.
C1 [Genevay, Aude] Univ Paris 09, CEREMADE, INRIA, Mokaplan Project Team, Paris, France.
   [Cuturi, Marco] Univ Paris Saclay, CREST, ENSAE, Paris, France.
   [Peyre, Gabriel] Ecole Normale Super, CNRS, INRIA, Mokaplan Project Team, Paris, France.
   [Peyre, Gabriel] Ecole Normale Super, DMA, INRIA, Mokaplan Project Team, Paris, France.
   [Bach, Francis] INRIA, Sierra Project Team, DI, ENS, Villers Les Nancy, France.
RP Genevay, A (reprint author), Univ Paris 09, CEREMADE, INRIA, Mokaplan Project Team, Paris, France.
EM genevay@ceremade.dauphine.fr; marco.cuturi@ensae.fr;
   gabriel.peyre@ens.fr; francis.bach@inria.fr
FU European Research Council (ERC SIGMA-Vision); Region Ile-de-France; JSPS
   [26700002]
FX GP was supported by the European Research Council (ERC SIGMA-Vision); AG
   by Region Ile-de-France; MC by JSPS grant 26700002.
CR Aurenhammer F, 1998, ALGORITHMICA, V20, P61, DOI 10.1007/PL00009187
   Bassetti F, 2006, STAT PROBABIL LETT, V76, P1298, DOI 10.1016/j.spl.2006.02.001
   Burkard R., 2009, ASSIGNMENT PROBLEMS
   Carlier  G., 2015, ARXIV151202783
   COMINETTI R, 1994, MATH PROGRAM, V67, P169, DOI 10.1007/BF01582220
   CUTURI M., 2013, ADV NEURAL INFORM PR, V26, P2292
   Dieuleveut  A., 2014, ARXIV14080361
   FRANKLIN J, 1989, LINEAR ALGEBRA APPL, V114, P717, DOI 10.1016/0024-3795(89)90490-4
   Frogner C., 2015, NIPS, P2044
   Kantorovich L. V., 1942, DOKL AKAD NAUK SSSR, V37, P227
   Kusner M. J., 2015, ICML
   Merigot Q, 2011, COMPUT GRAPH FORUM, V30, P1583, DOI 10.1111/j.1467-8659.2011.02032.x
   Montavon  G., 2016, ADV NEURAL INFORM PR
   Pennington J., 2014, EMNLP, P1532, DOI DOI 10.3115/V1/D14-1162
   POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046
   Rahimi A., 2007, ADV NEURAL INFORM PR, P1177
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Santambrogio F., 2015, OPTIMAL TRANSPORT AP
   Schmidt  M., 2016, MATH PROGRAMMING
   SINKHORN R, 1964, ANN MATH STAT, V35, P876, DOI 10.1214/aoms/1177703591
   Solomon J., 2015, ACM T GRAPHIC, V34, P66
   Sriperumbudur BK, 2012, ELECTRON J STAT, V6, P1550, DOI 10.1214/12-EJS722
   Steinwart I, 2008, INFORM SCI STAT, P1
   Villani C., 2003, TOPICS OPTIMAL TRANS
   Wu G., 2006, P 12 ACM SIGKDD INT, P760
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704090
DA 2019-06-15
ER

PT S
AU Georgogiannis, A
AF Georgogiannis, Alexandros
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Robust k-means: a Theoretical Revisit
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Over the last years, many variations of the quadratic k-means clustering procedure have been proposed, all aiming to robustify the performance of the algorithm in the presence of outliers. In general terms, two main approaches have been developed: one based on penalized regularization methods, and one based on trimming functions. In this work, we present a theoretical analysis of the robustness and consistency properties of a variant of the classical quadratic k-means algorithm, the robust k-means, which borrows ideas from outlier detection in regression. We show that two outliers in a dataset are enough to breakdown this clustering procedure. However, if we focus on "well-structured" datasets, then robust k-means can recover the underlying cluster structure in spite of the outliers. Finally, we show that, with slight modifications, the most general non-asymptotic results for consistency of quadratic k-means remain valid for this robust variant.
C1 [Georgogiannis, Alexandros] Tech Univ Crete, Sch Elect & Comp Engn, Khania, Greece.
RP Georgogiannis, A (reprint author), Tech Univ Crete, Sch Elect & Comp Engn, Khania, Greece.
EM alexandrosgeorgogiannis@gmail.com
CR Antoniadis Anestis, 2011, J AM STAT ASS
   Ben-David S., 2014, P 31 INT C MACH JMLR, P280
   Chawla Sanjay, K MEANS UNIFIED APPR
   Devroye L., 1997, STOCHASTIC MODELLING
   Forero PA, 2012, IEEE T SIGNAL PROCES, V60, P4163, DOI 10.1109/TSP.2012.2196696
   Gallegos MT, 2005, ANN STAT, V33, P347, DOI 10.1214/009053604000000940
   Garcia-Escudero LA, 1999, J AM STAT ASSOC, V94, P956, DOI 10.2307/2670010
   Garey M. R, 1979, COMPUTERS INTRACTABI
   Hampel F. R., 2011, ROBUST STAT APPROACH, V114
   Hennig C, 2012, TRIMCLUSTER CLUSTER
   Linder T, 2002, CISM COURSES LECT, P163
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Mazumder Rahul, 2012, J AM STAT ASS
   Melnykov V, 2012, J STAT SOFTW, V51, P1
   POLLARD D, 1981, ANN STAT, V9, P135, DOI 10.1214/aos/1176345339
   Pollard D., 1984, CONVERGENCE STOCHAST
   RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239
   Ritter G., 2014, CHAPMAN HALL CRC MON
   Rockafellar R. T., 2009, VARIATIONAL ANAL, V317
   She YY, 2009, ELECTRON J STAT, V3, P384, DOI 10.1214/08-EJS348
   Teboulle M, 2007, J MACH LEARN RES, V8, P65
   Tseng P, 2001, J OPTIMIZ THEORY APP, V109, P475, DOI 10.1023/A:1017501703105
   Van De Geer Sara, 2003, COWL WORKSH YAL U
   Witten DM, 2013, STAT INTERFACE, V6, P211
   Wright SJ, 2015, MATH PROGRAM, V151, P3, DOI 10.1007/s10107-015-0892-3
   Yu Yaoliang, 2015, AISTATS
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700090
DA 2019-06-15
ER

PT S
AU Gerchinovitz, S
   Lattimore, T
AF Gerchinovitz, Sebastien
   Lattimore, Tor
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Refined Lower Bounds for Adversarial Bandits
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We provide new lower bounds on the regret that must be suffered by adversarial bandit algorithms. The new results show that recent upper bounds that either (a) hold with high-probability or (b) depend on the total loss of the best arm or (c) depend on the quadratic variation of the losses, are close to tight. Besides this we prove two impossibility results. First, the existence of a single arm that is optimal in every round cannot improve the regret in the worst case. Second, the regret cannot scale with the effective range of the losses. In contrast, both results are possible in the full-information setting.
C1 [Gerchinovitz, Sebastien] Univ Toulouse 3 Paul Sabatier, Inst Math Toulouse, F-31062 Toulouse, France.
   [Lattimore, Tor] Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.
RP Gerchinovitz, S (reprint author), Univ Toulouse 3 Paul Sabatier, Inst Math Toulouse, F-31062 Toulouse, France.
EM sebastien.gerchinovitz@math.univ-toulouse.fr; tor.lattimore@gmail.com
FU CIMI (Centre International de Mathematiques et d'Informatique)
   Excellence program; French Agence Nationale de la Recherche (ANR)
   [ANR-13-BS01-0005, ANR-13-CORD-0020]
FX The authors would like to thank Aurelien Garivier and Emilie Kaufmann
   for insightful discussions. This work was partially supported by the
   CIMI (Centre International de Mathematiques et d'Informatique)
   Excellence program. The authors acknowledge the support of the French
   Agence Nationale de la Recherche (ANR), under grants ANR-13-BS01-0005
   (project SPADRO) and ANR-13-CORD-0020 (project ALICIA).
CR Allenberg C, 2006, LECT NOTES ARTIF INT, V4264, P229
   Audibert J.-Y., 2009, COLT, P217
   Auer P, 2003, SIAM J COMPUT, V32, P48
   Auer P, 1995, AN S FDN CO, P322, DOI 10.1109/SFCS.1995.492488
   Boucheron S., 2013, CONCENTRATION INEQUA
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Bubeck Sebastien, 2013, P 26 C LEARN THEOR, P122
   Cesa-Bianchi N, 2007, MACH LEARN, V66, P321, DOI 10.1007/s10994-006-5001-7
   de Rooij S, 2014, J MACH LEARN RES, V15, P1281
   Gaillard P., 2014, P 27 C LEARN THEOR C
   Hazan E., 2011, P 24 C LEARN THEOR, P817
   Hazan E, 2011, J MACH LEARN RES, V12, P1287
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Neu  G., 2015, COLT, V40, P1360
   Neu G, 2015, ADV NEUR IN, V28
   Rakhlin Alexander, 2013, COLT, P993
   Stoltz G., 2005, THESIS
   Tsybakov A. B., 2008, INTRO NONPARAMETRIC
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702024
DA 2019-06-15
ER

PT S
AU Goh, G
   Cotter, A
   Gupta, M
   Friedlander, M
AF Goh, Gabriel
   Cotter, Andrew
   Gupta, Maya
   Friedlander, Michael
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Satisfying Real-world Goals with Dataset Constraints
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB The goal of minimizing misclassification error on a training set is often just one of several real-world goals that might be defined on different datasets. For example, one may require a classifier to also make positive predictions at some specified rate for some subpopulation (fairness), or to achieve a specified empirical recall. Other real-world goals include reducing churn with respect to a previously deployed model, or stabilizing online training In this paper we propose handling multiple goals on multiple datasets by training with dataset constraints, using the ramp penalty to accurately quantify costs, and present an efficient algorithm to approximately optimize the resulting non-convex constrained optimization problem. Experiments on both benchmark and real-world industry datasets demonstrate the effectiveness of our approach.
C1 [Goh, Gabriel] Univ Calif Davis, Dept Math, Davis, CA 95616 USA.
   [Cotter, Andrew; Gupta, Maya] Google Inc, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA.
   [Friedlander, Michael] Univ British Columbia, Dept Comp Sci, Vancouver, BC V6T 1Z4, Canada.
RP Goh, G (reprint author), Univ Calif Davis, Dept Math, Davis, CA 95616 USA.
EM ggoh@math.ucdavis.edu; acotter@google.com; mayagupta@google.com;
   mpf@cs.ubc.ca
CR Ball K., 1997, MATH SCI RES I PUBL, V31, P1, DOI DOI 10.2977/PRIMS/1195164788.MR1491097
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Biddle D. A., 2005, ADVERSE IMPACT TEST
   BLAND RG, 1981, OPER RES, V29, P1039, DOI 10.1287/opre.29.6.1039
   Boyd S., 2011, STANFORD EE 364B LEC
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Collobert R., 2006, ICML
   Cotter A., 2013, P 30 INT C MACH LEAR, P266
   Davenport M., 2010, IEEE T PATTERN ANAL
   Eban E. E., 2016, LARGE SCALE LEARNING
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Fard M. M., 2016, NIPS
   Gasso G., 2011, ACM T INTELLIGENT SY
   Grunbaum B., 1960, PAC J MATH, V10, P1257
   Hardt M., 2016, NIPS
   Joachims T., 2005, ICML
   Mann G. S., 2007, ICML
   Miettinen K. M., 2012, NONLINEAR MULTIOBJEC, V12
   Narasimhan H., 2015, ICML
   Nemirovski A., 1994, LECT NOTES EFFICIENT
   Rademacher L. A., 2007, S COMP GEOM, P302
   Rockafellar R, 2000, J RISK, V2, P21, DOI DOI 10.21314/JOR.2000.038
   Scott C. D., 2005, IEEE T INFORM THEORY
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4
   Vuolo M. S., 2013, NEW YORK LAW J
   Zafar M. B., 2015, ICML WORKSH FAIRN AC
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702063
DA 2019-06-15
ER

PT S
AU Golkov, V
   Skwark, MJ
   Golkov, A
   Dosovitskiy, A
   Brox, T
   Meiler, J
   Cremers, D
AF Golkov, Vladimir
   Skwark, Marcin J.
   Golkov, Antonij
   Dosovitskiy, Alexey
   Brox, Thomas
   Meiler, Jens
   Cremers, Daniel
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Protein contact prediction from amino acid co-evolution using
   convolutional networks for graph-valued images
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Proteins are responsible for most of the functions in life, and thus are the central focus of many areas of biomedicine. Protein structure is strongly related to protein function, but is difficult to elucidate experimentally, therefore computational structure prediction is a crucial task on the way to solve many biological questions. A contact map is a compact representation of the three-dimensional structure of a protein via the pairwise contacts between the amino acids constituting the protein. We use a convolutional network to calculate protein contact maps from detailed evolutionary coupling statistics between positions in the protein sequence. The input to the network has an image-like structure amenable to convolutions, but every "pixel" instead of color channels contains a bipartite undirected edge-weighted graph. We propose several methods for treating such "graph-valued images" in a convolutional network. The proposed method outperforms state-of-the-art methods by a considerable margin.
C1 [Golkov, Vladimir; Cremers, Daniel] Tech Univ Munich, Munich, Germany.
   [Skwark, Marcin J.; Meiler, Jens] Vanderbilt Univ, 221 Kirkland Hall, Nashville, TN 37235 USA.
   [Golkov, Antonij] Univ Augsburg, Augsburg, Germany.
   [Dosovitskiy, Alexey; Brox, Thomas] Univ Freiburg, Freiburg, Germany.
RP Golkov, V (reprint author), Tech Univ Munich, Munich, Germany.
EM golkov@cs.tum.edu; marcin@skwark.pl;
   antonij.golkov@student.uni-augsburg.de; dosovits@cs.uni-freiburg.de;
   brox@cs.uni-freiburg.de; jens.meiler@vanderbilt.edu; cremers@tum.de
FU Deutsche Telekom Foundation; ERC Starting Grant "VideoLearn"; ERC
   Consolidator Grant "3DReloaded"
FX Deutsche Telekom Foundation, ERC Consolidator Grant "3DReloaded", ERC
   Starting Grant "VideoLearn".
CR Bruna J., 2014, INT C LEARN REPR
   Choi IG, 2006, P NATL ACAD SCI USA, V103, P14056, DOI 10.1073/pnas.0606239103
   Choromanska A., 2015, ARTIF INTELL, P192
   Dosovitskiy A, 2016, IEEE C COMP VIS PATT
   Dunn SD, 2008, BIOINFORMATICS, V24, P333, DOI 10.1093/bioinformatics/btm604
   Duvenaud D. K, 2015, ADV NEURAL INFORM PR, P2215
   Ekeberg M, 2013, PHYS REV E, V87, DOI 10.1103/PhysRevE.87.012707
   Feinauer C, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003847
   Golkov V, 2016, IEEE T MED IMAGING, V35, P1344, DOI 10.1109/TMI.2016.2551324
   Gromiha MM, 1999, BIOPHYS CHEM, V77, P49, DOI 10.1016/S0301-4622(99)00010-1
   Henaff Mikael, 2015, ARXIV150605163
   Jones DT, 2015, BIOINFORMATICS, V31, P999, DOI 10.1093/bioinformatics/btu791
   Kingma D. P., 2015, INT C LEARN REPR
   Kosciolek T, 2016, PROTEINS, V84, P145, DOI 10.1002/prot.24863
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Leinonen R, 2004, BIOINFORMATICS, V20, P3236, DOI 10.1093/bioinformatics/bth191
   Morcos F, 2011, P NATL ACAD SCI USA, V108, pE1293, DOI 10.1073/pnas.1111471108
   Ronneberger O., 2015, INT C MED IM COMP CO, P234, DOI [DOI 10.1007/978-3-319-24574-4_28, 10.1007/978-3-319-24574-4_28]
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704013
DA 2019-06-15
ER

PT S
AU Goyal, A
   Lamb, A
   Zhang, Y
   Zhang, SZ
   Courville, A
   Bengio, Y
AF Goyal, Anirudh
   Lamb, Alex
   Zhang, Ying
   Zhang, Saizheng
   Courville, Aaron
   Bengio, Yoshua
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Professor Forcing: A New Algorithm for Training Recurrent Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB The Teacher Forcing algorithm trains recurrent networks by supplying observed sequence values as inputs during training and using the network's own one-step-ahead predictions to do multi-step sampling. We introduce the Professor Forcing algorithm, which uses adversarial domain adaptation to encourage the dynamics of the recurrent network to be the same when training the network and when sampling from the network over multiple time steps. We apply Professor Forcing to language modeling, vocal synthesis on raw waveforms, handwriting generation, and image generation. Empirically we find that Professor Forcing acts as a regularizer, improving test likelihood on character level Penn Treebank and sequential MNIST. We also find that the model qualitatively improves samples, especially when sampling for a large number of time steps. This is supported by human evaluation of sample quality. Trade-offs between Professor Forcing and Scheduled Sampling are discussed. We produce T-SNEs showing that Professor Forcing successfully makes the dynamics of the network during training and sampling more similar.
C1 [Goyal, Anirudh; Lamb, Alex; Zhang, Ying; Zhang, Saizheng; Courville, Aaron] Univ Montreal, MILA, Montreal, PQ, Canada.
   [Bengio, Yoshua] CIFAR, Toronto, ON, Canada.
RP Goyal, A (reprint author), Univ Montreal, MILA, Montreal, PQ, Canada.
EM anirudhgoyal9119@gmail.com; alex6200@gmail.com; ying.zhlisa@gmail.com;
   saizhenglisa@gmail.com; aaron.courville@gmail.com;
   yoshua.umontreal@gmail.com
CR Ajakan H., 2014, ARXIV E PRINTS
   Al-Rfou R., 2016, CORR
   Bahdanau D., 2014, ARXIV14090473
   Bahdanau D., 2016, ARXIV E PRINTS
   Bahdanau  Dzmitry, 2015, ARXIV150804395
   Bengio S., 2015, ADV NEURAL INFORM PR, P1171
   Bengio Y., 2013, ARXIV E PRINTS
   Brebisson A., 2016, CONDITIONAL HANDWRIT
   Chen X, 2015, PROC CVPR IEEE, P2422, DOI 10.1109/CVPR.2015.7298856
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Chorowski JK, 2015, ADV NEURAL INFORM PR, P577
   Daume III H., 2009, ARXIV E PRINTS
   Ganin Y., 2015, ARXIV E PRINTS
   Germain M., 2015, ARXIV150203509
   Goodfellow I. J., 2014, NIPS 2014
   Graves A., 2013, ARXIV E PRINTS
   Graves A., 2012, STUDIES COMPUTATIONA
   Graves A., 2013, TECHNICAL REPORT
   Gregor K., 2015, ARXIV150204623
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Huszar F., 2015, ARXIV E PRINTS
   Kingma D. P., 2014, ARXIV14126980
   Larochelle H., 2011, NEURAL AUTOREGRESSIV
   Liwicki M, 2005, EIGHTH INTERNATIONAL CONFERENCE ON DOCUMENT ANALYSIS AND RECOGNITION, VOLS 1 AND 2, PROCEEDINGS, P956, DOI 10.1109/ICDAR.2005.132
   Mikolov T, 2010, RECURRENT NEURAL NET
   Mikolov T, 2012, CONTEXT DEPENDENT RE
   Murray I., 2009, ADV NEURAL INFORM PR, V21, P1137
   Raiko T., 2014, P ADV NEUR INF PROC, P325
   Ross S., 2010, ARXIV E PRINTS
   Salimans T, 2014, ARXIV14106460
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Theis L., 2015, ARXIV E PRINTS
   van den Oord A., 2016, ARXIV E PRINTS
   Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270
   Xu K, 2015, ARXIV150203044
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700063
DA 2019-06-15
ER

PT S
AU Greff, K
   Rasmus, A
   Berglund, M
   Hao, TH
   Schmidhuber, J
   Valpola, H
AF Greff, Klaus
   Rasmus, Antti
   Berglund, Mathias
   Hao, Tele Hotloo
   Schmidhuber, Jurgen
   Valpola, Harri
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Tagger: Deep Unsupervised Perceptual Grouping
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID COMPETITION; ATTENTION; BINDING; MODEL
AB We present a framework for efficient perceptual inference that explicitly reasons about the segmentation of its inputs and features. Rather than being trained for any specific segmentation, our framework learns the grouping process in an unsupervised manner or alongside any supervised task. We enable a neural network to group the representations of different objects in an iterative manner through a differentiable mechanism. We achieve very fast convergence by allowing the system to amortize the joint iterative inference of the groupings and their representations. In contrast to many other recently proposed methods for addressing multi-object scenes, our system does not assume the inputs to be images and can therefore directly handle other modalities. We evaluate our method on multi-digit classification of very cluttered images that require texture segmentation. Remarkably our method achieves improved classification performance over convolutional networks despite being fully connected, by making use of the grouping mechanism. Furthermore, we observe that our system greatly improves upon the semi-supervised result of a baseline Ladder network on our dataset. These results are evidence that grouping is a powerful tool that can help to improve sample efficiency.
C1 [Rasmus, Antti; Berglund, Mathias; Hao, Tele Hotloo; Valpola, Harri] Curious AI Co, Helsinki, Finland.
   [Greff, Klaus; Schmidhuber, Jurgen] IDSIA, Manno, Switzerland.
RP Greff, K (reprint author), IDSIA, Manno, Switzerland.
EM klaus@idsia.ch; antti@cai.fi; mathias@cai.fi; hotloo@cai.fi;
   juergen@idsia.ch; harri@cai.fi
FU EU [687795]
FX The authors wish to acknowledge useful discussions with Theofanis
   Karaletsos, Jaakko Sarela, Tapani Raiko, and Soren Kaae Sonderby. And
   further acknowledge Rinu Boney, Timo Haanpaa and the rest of the Curious
   AI Company team for their support, computational infrastructure, and
   human testing. This research was supported by the EU project "INPUT"
   (H2020-ICT-2015 grant no. 687795).
CR Ba J. L., 2016, ARXIV160706450CSSTAT
   Bahdanau D., 2014, ARXIV14090473
   Deco G, 2001, LECT NOTES ARTIF INT, V2036, P114
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Eslami SM, 2016, ARXIV160308575
   Gallinari P., 1987, CESTA AFCET
   Goodfellow I. J., 2013, ARXIV13126082
   Greff K., 2015, ARXIV151106418CS
   Gregor K., 2010, P 27 INT C MACH LEAR, P399
   Hochreiter S, 2001, LECT NOTES COMPUT SC, V2130, P87
   Huang J., 2015, ARXIV151106362
   Hyvarinen A, 2006, IEEE IJCNN, P4167
   Ioffe S., 2015, ARXIV150203167
   Kingma D., 2015, METHOD STOCHASTIC OP
   Le Roux N, 2011, NEURAL COMPUT, V23, P593, DOI 10.1162/NECO_a_00086
   Lecun Y., 1987, THESIS
   Meier M, 2014, NEUROCOMPUTING, V141, P76, DOI 10.1016/j.neucom.2014.02.011
   RAO AR, 2008, NEURAL NETWORKS IEEE, V19, P168, DOI DOI 10.1109/TNN.2007.905852
   Rasmus A, 2015, ADV NEURAL INFORM PR, P3532
   Reichert D. P., 2013, ARXIV13126115CSQBIOS
   Reichert DP, 2011, LECT NOTES COMPUT SC, V6791, P18, DOI 10.1007/978-3-642-21735-7_3
   Ross DA, 2006, J MACH LEARN RES, V7, P2369
   SAUND E, 1995, NEURAL COMPUT, V7, P51, DOI 10.1162/neco.1995.7.1.51
   SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P234, DOI 10.1162/neco.1992.4.2.234
   Schmidhuber J., 1991, International Journal of Neural Systems, V2, P125, DOI 10.1142/S012906579100011X
   SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P131, DOI 10.1162/neco.1992.4.1.131
   Schmidhuber J., 1993, ICANN93, P460
   Schmidhuber J., 1993, ICANN 93, P446
   Sohn K, 2013, P 30 INT C MACH LEAR, P217
   Springenberg Jost Tobias, 2014, ARXIV14126806
   Srikumar V, 2012, P 2012 JOINT C EMP M, P1114
   Tang YC, 2012, PROC CVPR IEEE, P2264, DOI 10.1109/CVPR.2012.6247936
   Team The Theano Development, 2016, ARXIV160502688CS
   Vincent P., 2008, P 25 INT C MACH LEAR, V2008, P1096, DOI DOI 10.1145/1390156.1390294
   Vinh NX, 2010, J MACH LEARN RES, V11, P2837
   von der Malsburg C., 1981, CORRELATION THEORY B
   VONDERMALSBURG C, 1995, CURR OPIN NEUROBIOL, V5, P520, DOI 10.1016/0959-4388(95)80014-X
   Wersing H, 2001, NEURAL COMPUT, V13, P357, DOI 10.1162/089976601300014574
   Xu K, 2015, ARXIV150203044
   Yli-Krekola A, 2009, LECT NOTES COMPUT SC, V5769, P285, DOI 10.1007/978-3-642-04277-5_29
NR 40
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700031
DA 2019-06-15
ER

PT S
AU Grill, JB
   Valko, M
   Munos, R
AF Grill, Jean-Bastien
   Valko, Michal
   Munos, Remi
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Blazing the trails before beating the path: Sample-efficient Monte-Carlo
   planning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB You are a robot and you live in a Markov decision process (MDP) with a finite or an infinite number of transitions from state-action to next states. You got brains and so you plan before you act. Luckily, your roboparents equipped you with a generative model to do some Monte-Carlo planning. The world is waiting for you and you have no time to waste. You want your planning to be efficient. Sample-efficient. Indeed, you want to exploit the possible structure of the MDP by exploring only a subset of states reachable by following near-optimal policies. You want guarantees on sample complexity that depend on a measure of the quantity of near-optimal states. You want something, that is an extension of Monte-Carlo sampling (for estimating an expectation) to problems that alternate maximization (over actions) and expectation (over next states). But you do not want to StOP with exponential running time, you want something simple to implement and computationally efficient. You want it all and you want it now. You want TrailBlazer.
C1 [Grill, Jean-Bastien; Valko, Michal] INRIA Lille Nord Europe, SequeL Team, Lille, France.
   [Munos, Remi] Google DeepMind, London, England.
RP Grill, JB (reprint author), INRIA Lille Nord Europe, SequeL Team, Lille, France.
EM jean-bastien.grill@inria.fr; michal.valko@inria.fr; munos@google.com
FU French Ministry of Higher Education and Research; Nord-Pas-de-Calais
   Regional Council; Ecole Normale Superieure in Paris; Inria; Carnegie
   Mellon University; French National Research Agency [ANR-14-CE24-0010-01,
   ANR-16-CE23-0003]
FX The research presented in this paper was supported by French Ministry of
   Higher Education and Research, Nord-Pas-de-Calais Regional Council, a
   doctoral grant of Ecole Normale Superieure in Paris, Inria and Carnegie
   Mellon University associated-team project EduBand, and French National
   Research Agency projects ExTra-Learn (n.ANR-14-CE24-0010-01) and BoB
   (n.ANR-16-CE23-0003)
CR Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   BELLMAN R, 1957, DYNAMIC PROGRAMMING
   Bertsekas D.P., 1996, NEURODYNAMIC PROGRAM
   Browne CB, 2012, IEEE T COMP INTEL AI, V4, P1, DOI 10.1109/TCIAIG.2012.2186810
   Bubeck S, 2010, C LEARN THEOR
   Busoniu Lucian, 2012, INT C ART INT STAT
   Coulom R, 2007, LECT NOTES COMPUT SC, V4630, P72
   Gelly S., 2006, TECHNICAL REPORT
   Guez Arthur, 2012, NEURAL INFORM PROCES
   Hren Jean- Francois, 2008, EUR WORKSH REINF LEA
   Kearns Michael, 1999, INT C ART INT STAT
   Kocsis Levente, 2006, EUR C MACH LEARN
   Munos R, 2014, FOUND TRENDS MACH LE, V7, P1, DOI 10.1561/2200000038
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Silver David, 2010, NEURAL INFORM PROCES
   Szorenyi Balazs, 2014, NEURAL INFORM PROCES
   Walsh Thomas J, 2010, AAAI C ART INT
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701110
DA 2019-06-15
ER

PT S
AU Grinchuk, O
   Lebedev, V
   Lempitsky, V
AF Grinchuk, Oleg
   Lebedev, Vadim
   Lempitsky, Victor
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learnable Visual Markers
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We propose a new approach to designing visual markers (analogous to QR-codes, markers for augmented reality, and robotic fiducial tags) based on the advances in deep generative networks. In our approach, the markers are obtained as color images synthesized by a deep network from input bit strings, whereas another deep network is trained to recover the bit strings back from the photos of these markers. The two networks are trained simultaneously in a joint backpropagation process that takes characteristic photometric and geometric distortions associated with marker fabrication and marker scanning into account. Additionally, a stylization loss based on statistics of activations in a pretrained classification network can be inserted into the learning in order to shift the marker appearance towards some texture prototype. In the experiments, we demonstrate that the markers obtained using our approach are capable of retaining bit strings that are long enough to be practical. The ability to automatically adapt markers according to the usage scenario and the desired capacity as well as the ability to combine information encoding with artistic stylization are the unique properties of our approach. As a byproduct, our approach provides an insight on the structure of patterns that are most suitable for recognition by ConvNets and on their ability to distinguish composite patterns.
C1 [Grinchuk, Oleg; Lebedev, Vadim; Lempitsky, Victor] Skolkovo Inst Sci & Technol, Moscow, Russia.
   [Lebedev, Vadim] Yandex, Moscow, Russia.
RP Grinchuk, O (reprint author), Skolkovo Inst Sci & Technol, Moscow, Russia.
CR Belussi LFF, 2013, J MATH IMAGING VIS, V45, P277, DOI 10.1007/s10851-012-0355-x
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bergamasco F, 2013, MACH VISION APPL, V24, P1295, DOI 10.1007/s00138-012-0469-6
   Chih-Chung Lo, 1995, 1995 International IEEE/IAS Conference on Industrial Automation and Control: Emerging Technologies (Cat. No.95TH8070), P485, DOI 10.1109/IACET.1995.527607
   Claus D, 2004, LECT NOTES COMPUT SC, V2034, P469
   Dosovitskiy A., 2015, C COMP VIS PATT REC
   Fiala M, 2005, PROC CVPR IEEE, P590
   Gatys L, 2015, ADV NEURAL INFORM PR, P262
   Gatys L. A., 2016, P IEEE C COMP VIS PA
   Hara M., 1998, US Patent, Patent No. [5,726,435, 5726435]
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Jaderberg M, 2015, ADV NEURAL INFORM PR, P2008
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kaltenbrunner M., 2007, P 1 INT C TANG EMB I, P69, DOI [DOI 10.1145/1226969.1226983, 10.1145/1226969.1226983]
   Kingma D. P., 2015, INT C LEARN REPR
   Kingma Diederik, 2014, INT C LEARN REPR
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Longacre Jr A., 1997, US Patent, Patent No. [5,591,956, 5591956]
   MacKay D. J, 2003, INFORM THEORY INFERE
   Mahendran Aravindh, 2015, C COMP VIS PATT REC
   Mooser J, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1301, DOI 10.1109/ICME.2006.262777
   Nguyen A., 2015, C COMP VIS PATT REC
   Olson Edwin, 2011, 2011 IEEE International Conference on Robotics and Automation, P3400
   Petitcolas FAP, 1999, P IEEE, V87, P1062, DOI 10.1109/5.771065
   Richardson A, 2013, IEEE INT CONF ROBOT, P631, DOI 10.1109/ICRA.2013.6630639
   Scharstein D, 2001, IMAGE VISION COMPUT, V19, P763, DOI 10.1016/S0262-8856(00)00105-0
   Simonyan K., 2013, 13126034 ARXIV
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Ulyanov Dmitry, 2016, INT C MACH LEARN ICM
   Vincent P., 2008, INT C MACH LEARN ICM
   Woodland N. J., 1952, US Patent, Patent No. [2612994, 2,612,994]
   Yahyanejad S., 2010, CVPR, P41
   Zeiler M. D., 2014, EUR C COMP VIS, P818, DOI DOI 10.1007/978-3-319-10590-1_53
   Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702070
DA 2019-06-15
ER

PT S
AU Grosse, RB
   Ancha, S
   Roy, DM
AF Grosse, Roger B.
   Ancha, Siddharth
   Roy, Daniel M.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Measuring the reliability of MCMC inference with bidirectional Monte
   Carlo
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID CONVERGENCE
AB Markov chain Monte Carlo (MCMC) is one of the main workhorses of probabilistic inference, but it is notoriously hard to measure the quality of approximate posterior samples. This challenge is particularly salient in black box inference methods, which can hide details and obscure inference failures. In this work, we extend the recently introduced bidirectional Monte Carlo [GGA15] technique to evaluate MCMC-based posterior inference algorithms. By running annealed importance sampling (AIS) chains both from prior to posterior and vice versa on simulated data, we upper bound in expectation the symmetrized KL divergence between the true posterior distribution and the distribution of approximate samples. We integrate our method into two probabilistic programming languages, WebPPL [GS] and Stan [CGHL+ p], and validate it on several models and datasets. As an example of how our method be used to guide the design of inference algorithms, we apply it to study the effectiveness of different model representations in WebPPL and Stan.
C1 [Grosse, Roger B.; Ancha, Siddharth] Univ Toronto, Dept Comp Sci, Toronto, ON, Canada.
   [Roy, Daniel M.] Univ Toronto, Dept Stat, Toronto, ON, Canada.
RP Grosse, RB (reprint author), Univ Toronto, Dept Comp Sci, Toronto, ON, Canada.
CR [Anonymous], STAN MOD LANG US GUI
   Brooks SP, 1998, J COMPUT GRAPH STAT, V7, P434, DOI 10.2307/1390675
   Burda Y., 2015, ARTIFICIAL INTELLIGE
   Carpenter B., J STAT SOFTWARE
   Cowles MK, 1996, J AM STAT ASSOC, V91, P883, DOI 10.2307/2291683
   Cusumano-Towner M. F., 2016, ARXIV160600068
   Del Moral P, 2006, J R STAT SOC B, V68, P411, DOI 10.1111/j.1467-9868.2006.00553.x
   Efron B., 1998, INTRO BOOTSTRAP
   Gelman A, 1992, STAT SCI, V7, P457, DOI DOI 10.1214/SS/1177011136
   Gogate V., 2007, C UNC AI
   Goodman N. D., 2008, C UNC AI
   Goodman N. D., DESIGN IMPLEMENTATIO
   Gorham J., 2015, NEURAL INFORM PROCES
   Grosse R., 2013, NEURAL INFORM PROCES
   Grosse RB, 2015, ARXIV151102543
   Hoffman MD, 2014, J MACH LEARN RES, V15, P1593
   Jarzynski C, 1997, PHYS REV E, V56, P5018, DOI 10.1103/PhysRevE.56.5018
   Lunn DJ, 2000, STAT COMPUT, V10, P325, DOI 10.1023/A:1008929526011
   Neal RM, 2011, CH CRC HANDB MOD STA, P113
   Neal RM, 1996, STAT COMPUT, V6, P353, DOI 10.1007/BF00143556
   Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028
   Roberts GO, 2001, STAT SCI, V16, P351, DOI 10.1214/ss/1015346320
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702037
DA 2019-06-15
ER

PT S
AU Grover, A
   Ermon, S
AF Grover, Aditya
   Ermon, Stefano
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Variational Bayes on Monte Carlo Steroids
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Variational approaches are often used to approximate intractable posteriors or normalization constants in hierarchical latent variable models. While often effective in practice, it is known that the approximation error can be arbitrarily large. We propose a new class of bounds on the marginal log-likelihood of directed latent variable models. Our approach relies on random projections to simplify the posterior. In contrast to standard variational methods, our bounds are guaranteed to be tight with high probability. We provide a new approach for learning latent variable models based on optimizing our new bounds on the log-likelihood. We demonstrate empirical improvements on benchmark datasets in vision and language for sigmoid belief networks, where a neural network is used to approximate the posterior.
C1 [Grover, Aditya; Ermon, Stefano] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
RP Grover, A (reprint author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
EM adityag@cs.stanford.edu; ermon@cs.stanford.edu
FU NSF [1649208]; Future of Life Institute [2016-158687]
FX This work was supported by grants from the NSF (grant 1649208) and
   Future of Life Institute (grant 2016-158687).
CR Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Bouchard-Cote A., 2009, UAI
   Ermon S., 2013, ICML
   Ermon S., 2014, ICML
   Ermon S., 2013, UAI
   Gan Z., 2015, AISTATS
   Gershman Samuel J, 2014, P 36 ANN C COGN SCI
   Gregor K., 2014, ICML
   Hadjis S., 2014, UAI
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Hsu L.-K., 2016, AISTATS
   Kingma D. P., 2015, ICLR
   Kingma Diederik P, 2014, ICLR
   Lee H., 2009, ICML
   Mnih A., 2016, ICML
   Mnih A., 2014, ICML
   NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6
   Poon H., 2011, UAI
   Ranganath R., 2013, AISTATS
   Rezende D. J., 2015, ICML
   Salakhutdinov R., 2016, ICLR
   Salimans T., 2015, ICML
   Titsias M., 2015, NIPS
   Vadhan S. P., 2011, FDN TRENDS TCS
   Zhu M., 2015, ICML
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702006
DA 2019-06-15
ER

PT S
AU Gruslys, A
   Munos, R
   Danihelka, I
   Lanctot, M
   Graves, A
AF Gruslys, Audrunas
   Munos, Remi
   Danihelka, Ivo
   Lanctot, Marc
   Graves, Alex
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Memory-Efficient Backpropagation Through Time
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We propose a novel approach to reduce memory consumption of the backpropagation through time (BPTT) algorithm when training recurrent neural networks (RNNs). Our approach uses dynamic programming to balance a trade-off between caching of intermediate results and recomputation. The algorithm is capable of tightly fitting within almost any user-set memory budget while finding an optimal execution policy minimizing the computational cost. Computational devices have limited memory capacity and maximizing a computational performance given a fixed memory budget is a practical use-case. We provide asymptotic computational upper bounds for various regimes. The algorithm is particularly effective for long sequences. For sequences of length 1000, our algorithm saves 95% of memory usage while using only one third more time per iteration than the standard BPTT.
C1 [Gruslys, Audrunas; Munos, Remi; Danihelka, Ivo; Lanctot, Marc; Graves, Alex] Google DeepMind, London, England.
RP Gruslys, A (reprint author), Google DeepMind, London, England.
EM audrunas@google.com; munos@google.com; danihelka@google.com;
   lanctot@google.com; gravesa@google.com
CR Chen Tianqi, 2016, ARXIV160406174
   Dauvergne B, 2006, LECT NOTES COMPUT SC, V3994, P566
   Eck Douglas, 2002, I DALLE MOLLE STUDI, V103
   Graves A., 2012, STUDIES COMPUTATIONA
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Graves Alex, 2016, NATURE
   Grefenstette E., 2015, ADV NEURAL INFORM PR, V29, P1819
   Gregor K., 2015, ARXIV150204623
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Mnih  V., 2016, INT C MACH LEARN, V48, P1928
   Rumelhart David E, 1985, TECHNICAL REPORT
   Sorokin Ivan, 2015, ARXIV151201693
   Sutskever I, 2011, P 28 INT C MACH LEAR, P1017
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
NR 14
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701078
DA 2019-06-15
ER

PT S
AU Gu, Q
   Banerjee, A
AF Gu, Qilong
   Banerjee, Arindam
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI High Dimensional Structured Superposition Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID MATRIX DECOMPOSITION
AB High dimensional superposition models characterize observations using parameters which can be written as a sum of multiple component parameters, each with its own structure, e.g., sum of low rank and sparse matrices, sum of sparse and rotated sparse vectors, etc. In this paper, we consider general superposition models which allow sum of any number of component parameters, and each component structure can be characterized by any norm. We present a simple estimator for such models, give a geometric condition under which the components can be accurately estimated, characterize sample complexity of the estimator, and give high probability non-asymptotic bounds on the componentwise estimation error. We use tools from empirical processes and generic chaining for the statistical analysis, and our results, which substantially generalize prior work on superposition models, are in terms of Gaussian widths of suitable sets.
C1 [Gu, Qilong; Banerjee, Arindam] Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.
RP Gu, Q (reprint author), Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.
EM guxxx396@cs.umn.edu; banerjee@cs.umn.edu
FU NSF [IIS-1563950, IIS-1447566, IIS-1447574, IIS-1422557, CCF-1451986,
   CNS-1314560, IIS-0953274, IIS-1029711]; NASA [NNX12AQ39A]
FX The research was also supported by NSF grants IIS-1563950, IIS-1447566,
   IIS-1447574, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274,
   IIS-1029711, NASA grant NNX12AQ39A, and gifts from Adobe, IBM, and
   Yahoo.
CR Agarwal A, 2012, ANN STAT, V40, P1171, DOI 10.1214/12-AOS1000
   Argyriou A., 2012, ADV NEURAL INFORM PR
   Banerjee A., 2014, ADV NEURAL INFORM PR
   Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620
   Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9
   Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Chandrasekaran V, 2012, ANN STAT, V40, P1935, DOI 10.1214/11-AOS949
   Chandrasekaran V, 2012, FOUND COMPUT MATH, V12, P805, DOI 10.1007/s10208-012-9135-7
   Chandrasekaran V, 2011, SIAM J OPTIMIZ, V21, P572, DOI 10.1137/090761793
   Donoho DL, 2001, IEEE T INFORM THEORY, V47, P2845, DOI 10.1109/18.959265
   Foygel R, 2014, IEEE T INFORM THEORY, V60, P1223, DOI 10.1109/TIT.2013.2293654
   Hsu D, 2011, IEEE T INFORM THEORY, V57, P7221, DOI 10.1109/TIT.2011.2158250
   McCoy M. B., 2013, ACHIEVABLE PERFORMAN
   Mendelson S, 2015, J ACM, V62, DOI 10.1145/2699439
   Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400
   Oymak S., 2015, ARXIV E PRINTS
   Raskutti G, 2010, J MACH LEARN RES, V11, P2241
   Rockafellar R T, 1970, CONVEX ANAL
   Talagrand M., 2014, SERIES MODERN SURVEY
   Tropp J. A., 2014, CONVEX RECOVERY STRU
   VERSHYNIN R., 2012, COMPRESSED SENSING T, P210, DOI DOI 10.1017/CBO9780511794308.006
   Vershynin R, 2015, APPL NUMER HARMON AN, P3, DOI 10.1007/978-3-319-19749-4_1
   Wright J., 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P1276, DOI 10.1109/ISIT.2012.6283062
   Yang E., 2012, ADV NEURAL INFORM PR, P1
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700043
DA 2019-06-15
ER

PT S
AU Guclu, U
   Thielen, J
   Hanke, M
   van Gerven, MAJ
AF Guclu, Umut
   Thielen, Jordy
   Hanke, Michael
   van Gerven, Marcel A. J.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Brains on Beats
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID INFORMATION; RESPONSES
AB We developed task-optimized deep neural networks (DNNs) that achieved state-of-the-art performance in different evaluation scenarios for automatic music tagging. These DNNs were subsequently used to probe the neural representations of music. Representational similarity analysis revealed the existence of a representational gradient across the superior temporal gyrus (STG). Anterior STG was shown to be more sensitive to low-level stimulus features encoded in shallow DNN layers whereas posterior STG was shown to be more sensitive to high-level stimulus features encoded in deep DNN layers.
C1 [Guclu, Umut; Thielen, Jordy; van Gerven, Marcel A. J.] Radboud Univ Nijmegen, Donders Inst Brain Cognit & Behav, Nijmegen, Netherlands.
   [Hanke, Michael] Otto von Guericke Univ, Ctr Behav Brain Sci, Magdeburg, Germany.
RP Guclu, U (reprint author), Radboud Univ Nijmegen, Donders Inst Brain Cognit & Behav, Nijmegen, Netherlands.
EM u.guclu@donders.ru.nl; j.thielen@psych.ru.nl; michael.hanke@ovgu.de;
   m.vangerven@donders.ru.nl
RI Hanke, Michael/A-7726-2013; Thielen, Jordy/Q-8599-2018
OI Hanke, Michael/0000-0001-6398-6370; 
FU German federal state of Saxony-Anhalt; European Regional Development
   Fund (ERDF), project: Center for Behavioral Brain Sciences; Netherlands
   Organization for Scientific Research (NWO) [639.072.513]
FX supported by the German federal state of Saxony-Anhalt and the European
   Regional Development Fund (ERDF), project: Center for Behavioral Brain
   Sciences.; supported by VIDI grant 639.072.513 of the Netherlands
   Organization for Scientific Research (NWO).
CR Agrawal P., 2014, ARXIV14075104
   Alluri V, 2013, NEUROIMAGE, V83, P627, DOI 10.1016/j.neuroimage.2013.06.064
   Alluri V, 2012, NEUROIMAGE, V59, P3677, DOI 10.1016/j.neuroimage.2011.11.019
   Cadieu CF, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003963
   Casey M., 2011, MLINI
   Chollet F., 2015, KERAS
   Cichy R. M., 2016, ARXIV160102970
   Cichy RM, 2016, NEUROIMAGE
   Dieleman S., 2014, ICASSP
   Dieleman S., 2013, ISMIR
   Fuster J.M., 2003, CORTEX MIND UNIFYING
   Glorot X., 2010, AISTATS
   Guclu U., 2015, NEUROIMAGE
   Guclu U, 2015, J NEUROSCI, V35, P10005, DOI 10.1523/JNEUROSCI.5023-14.2015
   Hanke M., 2015, F1000RESEARCH
   Horikawa T., 2015, ARXIV151006479
   Ioffe S., 2015, ARXIV150203167
   Kay KN, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00247
   Kell A., 2016, COSYNE
   Khaligh-Razavi SM, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003915
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kriegeskorte N, 2006, P NATL ACAD SCI USA, V103, P3863, DOI 10.1073/pnas.0600244103
   Kriegeskorte N., 2008, FRONTIERS SYSTEMS NE
   Krizhevsky A., 2012, NIPS
   Law E., 2009, ISMIR
   McFarland JM, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003143
   Moerel M, 2015, SCI REP-UK, V5, DOI 10.1038/srep17048
   Patterson RD, 2002, NEURON, V36, P767, DOI 10.1016/S0896-6273(02)01060-7
   Santoro R, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003412
   Schwartz B. L., 2015, SENSATION PERCEPTION
   Seibert D., 2016, BIORXIV
   Staeren N, 2009, CURR BIOL, V19, P498, DOI 10.1016/j.cub.2009.01.066
   Toiviainen P, 2014, NEUROIMAGE, V88, P170, DOI 10.1016/j.neuroimage.2013.11.017
   van den Oord A, 2014, ISMIR
   Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701079
DA 2019-06-15
ER

PT S
AU Gunasekar, S
   Koyejo, O
   Ghosh, J
AF Gunasekar, Suriya
   Koyejo, Oluwasanmi
   Ghosh, Joydeep
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Preference Completion from Partial Rankings
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We propose a novel and efficient algorithm for the collaborative preference completion problem, which involves jointly estimating individualized rankings for a set of entities over a shared set of items, based on a limited number of observed affinity values. Our approach exploits the observation that while preferences are often recorded as numerical scores, the predictive quantity of interest is the underlying rankings. Thus, attempts to closely match the recorded scores may lead to over-fitting and impair generalization performance. Instead, we propose an estimator that directly fits the underlying preference order, combined with nuclear norm constraints to encourage low-rank parameters. Besides (approximate) correctness of the ranking order, the proposed estimator makes no generative assumption on the numerical scores of the observations. One consequence is that the proposed estimator can fit any consistent partial ranking over a subset of the items represented as a directed acyclic graph (DAG), generalizing standard techniques that can only fit preference scores. Despite this generality, for supervision representing total or blockwise total orders, the computational complexity of our algorithm is within a log factor of the standard algorithms for nuclear norm regularization based estimates for matrix completion. We further show promising empirical results for a novel and challenging application of collaboratively ranking of the associations between brain-regions and cognitive neuroscience terms.
C1 [Gunasekar, Suriya; Ghosh, Joydeep] Univ Texas Austin, Austin, TX 78712 USA.
   [Koyejo, Oluwasanmi] Univ Illinois, Urbana, IL 61801 USA.
RP Gunasekar, S (reprint author), Univ Texas Austin, Austin, TX 78712 USA.
EM suriya@utexas.edu; sanmi@illinois.edu; ghosh@ece.utexas.edu
FU NSF [IIS-1421729, SCH 1418511]
FX SG and JG acknowledge funding from NSF grants IIS-1421729 and SCH
   1418511.
CR Acharyya S., 2013, UAI
   Acharyya S., 2012, UAI
   Bartlett P. L., 2003, JMLR
   Best M. J., 1990, MATH PROGRAM
   Cai J. F., 2010, SIAM J OPTIM
   Candes E. J., 2009, FOCM
   Candes E. J., 2006, IEEE T INF THEORY
   Candes E. J., 2010, P IEEE
   Cao Z., 2007, ICML
   Chi E., 2013, GENOME RES
   Cremonesi P., 2010, RECSYS
   Duchi John C., 2010, ICML
   Ganti R., 2015, NIPS
   Goldberg D., 1992, COMMUN ACM
   Grotzinger S. J., 1984, APPL MATH OPTIM
   Herbrich  R., 1999, NIPS
   Horowitz J. L., 2009, SEMIPARAMETRIC NONPA, V12
   Joachims T., 2002, SIGKDD
   Kakade S. M., 2011, NIPS
   Kalai Adam Tauman, 2009, COLT
   Keshavan R. H., 2010, IEEE T IT
   Koren Y., 2009, IEEE COMPUTER
   Koyejo O., 2013, RECSYS
   Li P., 2007, NIPS
   Liu T. Y., 2009, FDN TRENDS IR
   Lu Y., 2015, ANN ALL C COMM CONTR
   Ma S., 2011, MATH PROGRAM
   Mnih  Andriy, 2007, NIPS
   Oh S., 2015, NIPS
   Parikh Neal, 2014, FDN TRENDS OPTIMIZAT
   Park D., 2015, ICML
   Recht B., 2011, JMLR
   Steck H., 2010, KDD
   Stout Q. F., 2013, ALGORITHMICA
   Weimer M., 2008, NIPS
   Yarkoni T., 2011, NAT METHODS
   Zhou Y., 2008, LNCS, V5034
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702019
DA 2019-06-15
ER

PT S
AU Guo, YW
   Yao, AB
   Chen, YR
AF Guo, Yiwen
   Yao, Anbang
   Chen, Yurong
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Dynamic Network Surgery for Efficient DNNs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Deep learning has become a ubiquitous technology to improve machine intelligence. However, most of the existing deep models are structurally very complex, making them difficult to be deployed on the mobile platforms with limited computational power. In this paper, we propose a novel network compression method called dynamic network surgery, which can remarkably reduce the network complexity by making on-the-fly connection pruning. Unlike the previous methods which accomplish this task in a greedy way, we properly incorporate connection splicing into the whole process to avoid incorrect pruning and make it as a continual network maintenance. The effectiveness of our method is proved with experiments. Without any accuracy loss, our method can efficiently compress the number of parameters in LeNet-5 and AlexNet by a factor of 108x and 17:7x respectively, proving that it outperforms the recent pruning method by considerable margins. Code and some models are available at https://github.com/yiwenguo/Dynamic-Network-Surgery.
C1 [Guo, Yiwen; Yao, Anbang; Chen, Yurong] Intel Labs China, Beijing, Peoples R China.
RP Guo, YW (reprint author), Intel Labs China, Beijing, Peoples R China.
EM yiwen.guo@intel.com; anbang.yao@intel.com; yurong.chen@intel.com
CR Chen  Wenlin, 2015, ICML
   Courbariaux M, 2016, ARXIV160202830V3
   David J., 2015, NIPS
   Denil  Misha, 2013, NIPS
   Denton E., 2014, NIPS
   Gong Y., 2014, ARXIV14126115
   Han S., 2016, ISCA
   Han S, 2016, ICLR
   Han S., 2015, NIPS
   Hassibi Babak, 1993, NIPS
   He K., 2016, CVPR
   Jia Y., 2014, ACM MM
   Krizhevsky A., 2012, NIPS
   Lebedev V., 2015, ICLR
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y., 1989, NIPS
   Lodish H, 2000, MOL CELL BIOL NEUROT
   Mathieu Michael, 2013, ARXIV13125851
   Simonyan K., 2014, ICLR
   Vanhoucke Vincent, 2011, NIPS WORKSH
   Yang Zichao, 2015, ICCV
   Zhang X., 2015, CVPR
NR 22
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701022
DA 2019-06-15
ER

PT S
AU Gupta, R
   Kumar, R
   Vassilvitskii, S
AF Gupta, Rishi
   Kumar, Ravi
   Vassilvitskii, Sergei
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI On Mixtures of Markov Chains
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID MAXIMUM-LIKELIHOOD
AB We study the problem of reconstructing a mixture of Markov chains from the trajectories generated by random walks through the state space. Under mild non-degeneracy conditions, we show that we can uniquely reconstruct the underlying chains by only considering trajectories of length three, which represent triples of states. Our algorithm is spectral in nature, and is easy to implement.
C1 [Gupta, Rishi] Stanford Univ, Stanford, CA 94305 USA.
   [Gupta, Rishi; Kumar, Ravi] Google Res, Mountain View, CA USA.
   [Vassilvitskii, Sergei] Google Res, New York, NY 10011 USA.
RP Gupta, R (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM rishig@cs.stanford.edu; ravi.k53@gmail.com; sergeiv@google.com
CR Allman ES, 2009, ANN STAT, V37, P3099, DOI 10.1214/09-AOS689
   Anandkumar A., 2012, C LEARN THEOR COLT
   Anandkumar A, 2014, J MACH LEARN RES, V15, P2773
   Caron R., 2005, WSMR REPORT
   Chaudhuri K., 2008, COLT, P9
   Chierichetti F., 2012, P 21 INT C WORLD WID, P609
   Cohen S.B., 2013, P NAACL HLT, P148
   Dasgupta S., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P634, DOI 10.1109/SFFCS.1999.814639
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Hsu D, 2012, J COMPUT SYST SCI, V78, P1460, DOI 10.1016/j.jcss.2011.12.025
   Moitra A, 2010, ANN IEEE SYMP FOUND, P93, DOI 10.1109/FOCS.2010.15
   REDNER RA, 1984, SIAM REV, V26, P195, DOI 10.1137/1026034
   Subakan C., 2014, NIPS, P2249
   Subakan Y. C., 2011, THESIS
   Vempala S, 2004, J COMPUT SYST SCI, V68, P841, DOI 10.1016/j.jess.2003.11.008
NR 15
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700042
DA 2019-06-15
ER

PT S
AU Gutin, E
   Farias, VF
AF Gutin, Eli
   Farias, Vivek F.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Optimistic Gittins Indices
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID ALLOCATION
AB Starting with the Thomspon sampling algorithm, recent years have seen a resurgence of interest in Bayesian algorithms for the Multi-armed Bandit (MAB) problem. These algorithms seek to exploit prior information on arm biases and while several have been shown to be regret optimal, their design has not emerged from a principled approach. In contrast, if one cared about Bayesian regret discounted over an infinite horizon at a fixed, pre-specified rate, the celebrated Gittins index theorem offers an optimal algorithm. Unfortunately, the Gittins analysis does not appear to carry over to minimizing Bayesian regret over all sufficiently large horizons and computing a Gittins index is onerous relative to essentially any incumbent index scheme for the Bayesian MAB problem.
   The present paper proposes a sequence of 'optimistic' approximations to the Gittins index. We show that the use of these approximations in concert with the use of an increasing discount factor appears to offer a compelling alternative to state-of-the-art index schemes proposed for the Bayesian MAB problem in recent years by offering substantially improved performance with little to no additional computational overhead. In addition, we prove that the simplest of these approximations yields frequentist regret that matches the Lai-Robbins lower bound, including achieving matching constants.
C1 [Gutin, Eli] MIT, Operat Res Ctr, Cambridge, MA 02142 USA.
   [Farias, Vivek F.] MIT, Sloan Sch Management, Cambridge, MA 02142 USA.
RP Gutin, E (reprint author), MIT, Operat Res Ctr, Cambridge, MA 02142 USA.
EM gutin@mit.edu; vivekf@mit.edu
CR AGRAWAL R, 1995, ADV APPL PROBAB, V27, P1054, DOI 10.2307/1427934
   Agrawal S, 2013, P 16 INT C ART INT S, P99
   AGRAWAL S., P 25 C LEARN THEOR
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Berry D. A., 1985, MONOGRAPHS STAT APPL
   Bertsimas D, 1996, MATH OPER RES, V21, P257, DOI 10.1287/moor.21.2.257
   Chapelle O, 2011, ADV NEURAL INFORM PR, P2249
   Cover T. M., 2012, ELEMENTS INFORM THEO
   GARIVIER A, 2011, COLT
   GITTINS JC, 1979, J ROY STAT SOC B MET, V41, P148
   JOGDEO K, 1968, ANN MATH STAT, V39, P1191, DOI 10.1214/aoms/1177698243
   Kaufmann Emilie, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P199, DOI 10.1007/978-3-642-34106-9_18
   Korda N, 2013, ADV NEURAL INFORM PR, P1448
   LAI TL, 1987, ANN STAT, V15, P1091, DOI 10.1214/aos/1176350495
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   LATTIMORE T, 2016, P 29 C LEARN THEOR, P1
   Nino-Mora J, 2011, INFORMS J COMPUT, V23, P254, DOI 10.1287/ijoc.1100.0398
   Powell W. B., 2012, OPTIMAL LEARNING, V841
   Russo D., 2014, ADV NEURAL INFORM PR, P1583
   Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.2307/2332286
   Tsitsiklis JN, 1994, ANN APPL PROBAB, V4, P194, DOI 10.1214/aoap/1177005207
   Weber R, 1992, ANN APPL PROBAB, V2, P1024
   WHITTLE P, 1980, J ROY STAT SOC B MET, V42, P143
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700001
DA 2019-06-15
ER

PT S
AU Haarnoja, T
   Ajay, A
   Levine, S
   Abbeel, P
AF Haarnoja, Tuomas
   Ajay, Anurag
   Levine, Sergey
   Abbeel, Pieter
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Backprop KF: Learning Discriminative Deterministic State Estimators
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Generative state estimators based on probabilistic filters and smoothers are one of the most popular classes of state estimators for robots and autonomous vehicles. However, generative models have limited capacity to handle rich sensory observations, such as camera images, since they must model the entire distribution over sensor readings. Discriminative models do not suffer from this limitation, but are typically more complex to train as latent variable models for state estimation. We present an alternative approach where the parameters of the latent state distribution are directly optimized as a deterministic computation graph, resulting in a simple and effective gradient descent algorithm for training discriminative state estimators. We show that this procedure can be used to train state estimators that use complex input, such as raw camera images, which must be processed using expressive nonlinear function approximators such as convolutional neural networks. Our model can be viewed as a type of recurrent neural network, and the connection to probabilistic filtering allows us to design a network architecture that is particularly well suited for state estimation. We evaluate our approach on synthetic tracking task with raw image inputs and on the visual odometry task in the KITTI dataset. The results show significant improvement over both standard generative approaches and regular recurrent neural networks.
C1 [Haarnoja, Tuomas; Ajay, Anurag; Levine, Sergey; Abbeel, Pieter] Univ Calif Berkeley, Dept Comp Sci, Berkeley, CA 94720 USA.
RP Haarnoja, T (reprint author), Univ Calif Berkeley, Dept Comp Sci, Berkeley, CA 94720 USA.
EM haarnoja@berkeley.edu; anuragajay@berkeley.edu; svlevine@berkeley.edu;
   pabbeel@berkeley.edu
FU ONR; Army Research Office; Berkeley DeepDrive Center
FX This research was funded in part by ONR through a Young Investigator
   Program award, by the Army Research Office through the MAST program, and
   by the Berkeley DeepDrive Center.
CR Abbeel P., 2005, ROBOTICS SCI SYSTEMS
   Ba J. L., 2016, ARXIV160706450
   Baccouche Moez, 2011, Human Behavior Unterstanding. Proceedings Second International Workshop, HBU 2011, P29, DOI 10.1007/978-3-642-25446-8_4
   Bobrowski O., 2007, ADV NEURAL INFORM PR
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Do T. M. T., 2010, INT C ART INT STAT, P177
   Geiger  A., 2013, INT J ROBOTICS RES I
   Hess R, 2009, PROC CVPR IEEE, P240, DOI 10.1109/CVPRW.2009.5206801
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Ioffe S, 2015, INT C MACH LEARN
   Kim M., 2007, COMP VIS 2007 ICCV 2, P1
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Ko J, 2009, AUTON ROBOT, V27, P75, DOI 10.1007/s10514-009-9119-x
   Krishnan Rahul G, 2015, ARXIV151105121
   Lafferty J., 2001, CONDITIONAL RANDOM F
   Limketkai B., 2007, INT C ROB AUT
   Morency L., 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383299
   Ondruska  P., 2016, ARXIV160200991
   Ross D. A., 2006, P 23 INT C MACH LEAR, P761
   Sminchisescu C, 2005, PROC CVPR IEEE, P390
   Thrun S., 2005, PROBABILISTIC ROBOTI
   Wilson R., 2009, NEURAL INFORM PROCES, V22, P2062
   Yadaiah N., 2006, INT JOINT C NEUR NET
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700054
DA 2019-06-15
ER

PT S
AU Hadfield-Menell, D
   Dragan, A
   Abbeel, P
   Russell, S
AF Hadfield-Menell, Dylan
   Dragan, Anca
   Abbeel, Pieter
   Russell, Stuart
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Cooperative Inverse Reinforcement Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.
C1 [Hadfield-Menell, Dylan; Dragan, Anca; Abbeel, Pieter; Russell, Stuart] Univ Calif Berkeley, Elect Engn & Comp Sci, Berkeley, CA 94709 USA.
RP Hadfield-Menell, D (reprint author), Univ Calif Berkeley, Elect Engn & Comp Sci, Berkeley, CA 94709 USA.
EM dhm@cs.berkeley.edu; anca@cs.berkeley.edu; pabbeel@cs.berkeley.edu;
   russell@cs.berkeley.edu
FU DARPA Simplifying Complexity in Scientific Discovery (SIMPLEX) program;
   Berkeley Deep Drive Center; Center for Human Compatible AI; Future of
   Life Institute; Defense Sciences Office [N66001-15-2-4048]; NSF Graduate
   Research Fellowship
FX This work was supported by the DARPA Simplifying Complexity in
   Scientific Discovery (SIMPLEX) program, the Berkeley Deep Drive Center,
   the Center for Human Compatible AI, the Future of Life Institute, and
   the Defense Sciences Office contract N66001-15-2-4048. Dylan
   Hadfield-Menell is also supported by a NSF Graduate Research Fellowship.
CR Abbeel P., 2004, ICML
   Balbach F, 2009, LANGUAGE AUTOMATA TH
   Bernstein D, 2000, UAI
   Bostrom N., 2014, SUPERINTELLIGENCE PA
   Boutilier C, 1999, IJCAI-99: PROCEEDINGS OF THE SIXTEENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 & 2, P478
   Cakmak M., 2012, AAAI
   Dragan Anca, 2013, ROBOTICS SCI SYSTEMS
   Fern A, 2014, J ARTIF INTELL RES, V50, P71, DOI 10.1613/jair.4213
   Gibbons R., 1998, TECHNICAL REPORT
   GOLDMAN SA, 1993, SIAM J COMPUT, V22, P1006, DOI 10.1137/0222062
   GOLDMAN SA, 1995, J COMPUT SYST SCI, V50, P20, DOI 10.1006/jcss.1995.1003
   Golland D., 2010, P 2010 C EMP METH NA, P410
   JENSEN MC, 1976, J FINANC ECON, V3, P305, DOI 10.1016/0304-405X(76)90026-X
   KERR S, 1975, ACAD MANAGE J, V18, P769, DOI 10.2307/255378
   Kuleshov V, 2015, WEB INTERNET EC
   LEVESON NG, 1993, COMPUTER, V26, P18, DOI 10.1109/MC.1993.274940
   Natarajan S, 2010, INT C MACH LEARN APP
   Nayyar A, 2013, IEEE T AUTOMAT CONTR, V58, P1644, DOI 10.1109/TAC.2013.2239000
   Ng A., 2000, ICML
   Ramachandran D., 2007, IJCAI
   Ratliff N, 2006, ICML
   Russell S., 2010, ARTIFICIAL INTELLIGE
   Russell Stuart J., 1998, COLT
   Waugh K, 2011, ICML
   WIENER N, 1960, SCIENCE, V131, P1355, DOI 10.1126/science.131.3410.1355
   Ziebart Brian D, 2008, AAAI
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703056
DA 2019-06-15
ER

PT S
AU Hajinezhad, D
   Hong, MY
   Zhao, T
   Wang, ZR
AF Hajinezhad, Davood
   Hong, Mingyi
   Zhao, Tuo
   Wang, Zhaoran
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI NESTT: A Nonconvex Primal-Dual Splitting Method for Distributed and
   Stochastic Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID CONVERGENCE; REGRESSION
AB We study a stochastic and distributed algorithm for nonconvex problems whose objective consists of a sum of N nonconvex L-i/N-smooth functions, plus a non-smooth regularizer. The proposed NonconvEx primal-dual SpliTTing (NESTT) algorithm splits the problem into N subproblems, and utilizes an augmented Lagrangian based primal-dual scheme to solve it in a distributed and stochastic manner. With a special non-uniform sampling, a version of NESTT achieves epsilon-stationary solution using O ((Sigma(N)(i=1) root L-i/N)(2)/epsilon) gradient evaluations, which can be up to O (N) times better than the (proximal) gradient descent methods. It also achieves Q-linear convergence rate for nonconvex l(1) penalized quadratic problems with polyhedral constraints. Further, we reveal a fundamental connection between primal-dual based methods and a few primal only methods such as IAG/SAG/SAGA.
C1 [Hajinezhad, Davood; Hong, Mingyi] Iowa State Univ, Dept Ind & Mfg Syst Engn, Ames, IA 50011 USA.
   [Hajinezhad, Davood; Hong, Mingyi] Iowa State Univ, Dept Elect & Comp Engn, Ames, IA 50011 USA.
   [Zhao, Tuo] Georgia Inst Technol, Sch Ind & Syst Engn, Atlanta, GA 30332 USA.
   [Wang, Zhaoran] Princeton Univ, Dept Operat Res, Princeton, NJ 08544 USA.
RP Hong, MY (reprint author), Iowa State Univ, Dept Ind & Mfg Syst Engn, Ames, IA 50011 USA.; Hong, MY (reprint author), Iowa State Univ, Dept Elect & Comp Engn, Ames, IA 50011 USA.
EM dhaji@iastate.edu; mingyi@iastate.edu; tourzhao@gatech.edu;
   zhaoran@princeton.edu
CR Antoniadis A, 2011, ANN I STAT MATH, V63, P585, DOI 10.1007/s10463-009-0242-4
   Bertsekas D., 2000, 2848 LIDS
   Bjornson E., 2013, FDN TRENDS COMMUNICA, V9
   Blatt D, 2007, SIAM J OPTIMIZ, V18, P29, DOI 10.1137/040615961
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Cevher V, 2014, IEEE SIGNAL PROC MAG, V31, P32, DOI 10.1109/MSP.2014.2329397
   Chang TH, 2015, IEEE T SIGNAL PROCES, V63, P482, DOI 10.1109/TSP.2014.2367458
   Defazio A., 2014, P NIPS
   Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811
   Hajinezhad D., 2015, P GLOBALSIPT
   Hajinezhad D, 2016, INT CONF ACOUST SPEE, P4742, DOI 10.1109/ICASSP.2016.7472577
   Hazan E., 2016, PREPRINT
   Hong MY, 2016, SIAM J OPTIMIZ, V26, P337, DOI 10.1137/140990309
   Johnson R., 2013, P NEUR INF PROC NIPS
   Lan G., 2015, OPTIMAL RANDOMIZED I
   Loh PL, 2012, ANN STAT, V40, P1637, DOI 10.1214/12-AOS1018
   Lorenzo P. D., 2016, NEXT IN NETWORK NONC
   LUO ZQ, 1992, SIAM J CONTROL OPTIM, V30, P408, DOI 10.1137/0330025
   Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400
   Razaviyayn M., 2014, P NIPS
   Reddi S. J., 2016, PREPRINT
   SCHMIDT M., 2013, TECHNICAL REPORT
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Sra S, 2012, ADV NEURAL INFORM PR
   Tseng P, 2009, MATH PROGRAM, V117, P387, DOI 10.1007/s10107-007-0170-0
   Wang ZR, 2014, ANN STAT, V42, P2164, DOI 10.1214/14-AOS1238
   Zlobec S, 2005, J GLOBAL OPTIM, V32, P401, DOI 10.1007/s10898-004-3134-4
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704031
DA 2019-06-15
ER

PT S
AU Kasaei, SH
   Tome, AM
   Lopes, LS
AF Hamidreza Kasaei, S.
   Tome, Ana Maria
   Lopes, Luis Seabra
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Hierarchical Object Representation for Open-Ended Object Category
   Learning and Recognition
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID MODEL
AB Most robots lack the ability to learn new objects from past experiences. To migrate a robot to a new environment one must often completely re-generate the knowledge-base that it is running with. Since in open-ended domains the set of categories to be learned is not predefined, it is not feasible to assume that one can pre-program all object categories required by robots. Therefore, autonomous robots must have the ability to continuously execute learning and recognition in a concurrent and interleaved fashion. This paper proposes an open-ended 3D object recognition system which concurrently learns both the object categories and the statistical features for encoding objects. In particular, we propose an extension of Latent Dirichlet Allocation to learn structural semantic features (i.e. topics) from low-level feature co-occurrences for each category independently. Moreover, topics in each category are discovered in an unsupervised fashion and are updated incrementally using new object views. The approach contains similarities with the organization of the visual cortex and builds a hierarchy of increasingly sophisticated representations. Results show the fulfilling performance of this approach on different types of objects. Moreover, this system demonstrates the capability of learning from few training examples and competes with state-of-the-art systems.
C1 [Hamidreza Kasaei, S.; Tome, Ana Maria; Lopes, Luis Seabra] Univ Aveiro, IEETA, P-3810193 Aveiro, Portugal.
RP Kasaei, SH (reprint author), Univ Aveiro, IEETA, P-3810193 Aveiro, Portugal.
EM seyed.hamidreza@ua.pt; ana@ua.pt; lsl@ua.pt
RI Lopes, Luis Seabra/A-6012-2012
OI Lopes, Luis Seabra/0000-0002-5719-5019
FU FCT [PEst-OE/EEI/UI0127/2016]; FCT scholarship [SFRH/BD/94183/2013]
FX This work was funded by National Funds through FCT project
   PEst-OE/EEI/UI0127/2016 and FCT scholarship SFRH/BD/94183/2013.
CR Banerjee A., 2007, P SIAM INT C DAT MIN, V7, P437
   Blei David M., 2008, ADV NEURAL INFORM PR, V7, P121
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Canini K. R., 2009, INT C ART INT STAT, P65
   Chauhan A, 2011, COGN PROCESS, V12, P341, DOI 10.1007/s10339-011-0407-y
   Collet A, 2015, INT J ROBOT RES, V34, P3, DOI 10.1177/0278364914546030
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Hofmann T, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P50, DOI 10.1145/312624.312649
   Hyun Lim  Gi, 2014, ROB HUM INT COMM 23
   Jeong S, 2012, NEURAL NETWORKS, V25, P130, DOI 10.1016/j.neunet.2011.06.020
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Kasaei SH, 2015, J INTELL ROBOT SYST, V80, P537, DOI 10.1007/s10846-015-0189-z
   Kim JG, 2009, VISION RES, V49, P2297, DOI 10.1016/j.visres.2009.06.020
   Lai K, 2011, IEEE INT CONF ROBOT, P1817
   Oliveira M, 2016, ROBOT AUTON SYST, V75, P614, DOI 10.1016/j.robot.2015.09.019
   Ramage D, 2009, P 2009 C EMP METH NA, V1, DOI DOI 10.3115/1699510.1699543
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019
   Schwarz M, 2015, IEEE INT CONF ROBOT, P1329, DOI 10.1109/ICRA.2015.7139363
   Sivic J, 2005, IEEE I CONF COMP VIS, P370
   WANG CL, 2009, COMP VIS PATT REC 20, V57, P1903, DOI DOI 10.1109/TCOMM.2009.07.070156
   Wang Y, 2007, LECT NOTES COMPUT SC, V4814, P240
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704063
DA 2019-06-15
ER

PT S
AU Han, J
   Liu, Q
AF Han, Jun
   Liu, Qiang
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Bootstrap Model Aggregation for Distributed Statistical Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB In distributed, or privacy-preserving learning, we are often given a set of probabilistic models estimated from different local repositories, and asked to combine them into a single model that gives efficient statistical estimation. A simple method is to linearly average the parameters of the local models, which, however, tends to be degenerate or not applicable on non-convex models, or models with different parameter dimensions. One more practical strategy is to generate bootstrap samples from the local models, and then learn a joint model based on the combined bootstrap set. Unfortunately, the bootstrap procedure introduces additional noise and can significantly deteriorate the performance. In this work, we propose two variance reduction methods to correct the bootstrap noise, including a weighted M-estimator that is both statistically efficient and practically powerful. Both theoretical and empirical analysis is provided to demonstrate our methods.
C1 [Han, Jun; Liu, Qiang] Dartmouth Coll, Dept Comp Sci, Hanover, NH 03755 USA.
RP Han, J (reprint author), Dartmouth Coll, Dept Comp Sci, Hanover, NH 03755 USA.
EM jun.han.gr@dartmouth.edu; qiang.liu@dartmouth.edu
FU NSF CRII [1565796]
FX This work is supported in part by NSF CRII 1565796.
CR Boyd S., 2011, FDN TRENDS MACHINE L, V3
   Dekel O., 2012, JMLR
   Henmi M, 2007, BIOMETRIKA, V94, P985, DOI 10.1093/biomet/asm076
   Hirano K., 2003, ECONOMETRICA, V71
   Huang C., 2015, ARXIV151101443
   Kawakita M., 2013, MACHINE LEARNING, V91
   Korattikara A., 2015, ARXIV150604416
   Li L., 2015, AISTATS
   Liu Q., 2014, NIPS
   Merugu S, 2003, THIRD IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P211
   Nelson B. L., 1987, COMPUTERS OPERATIONS, V14
   Rosenblatt J., 2014, ARXIV14072724
   Shamir O., 2014, ICML
   Sokolovska N., 2008, ICML
   Wilson J. R., 1984, AM J MATH MANAGEMENT, V4
   Zhang Y., 2013, NIPS
   Zhang Yichuan, 2012, NIPS
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700014
DA 2019-06-15
ER

PT S
AU Han, SZ
   Meng, ZB
   Khan, AS
   Tong, Y
AF Han, Shizhong
   Meng, Zibo
   Khan, Ahmed Shehab
   Tong, Yan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Incremental Boosting Convolutional Neural Network for Facial Action Unit
   Recognition
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Recognizing facial action units (AUs) from spontaneous facial expressions is still a challenging problem. Most recently, CNNs have shown promise on facial AU recognition. However, the learned CNNs are often overfitted and do not generalize well to unseen subjects due to limited AU-coded training images. We proposed a novel Incremental Boosting CNN (IB-CNN) to integrate boosting into the CNN via an incremental boosting layer that selects discriminative neurons from the lower layer and is incrementally updated on successive mini-batches. In addition, a novel loss function that accounts for errors from both the incremental boosted classifier and individual weak classifiers was proposed to fine-tune the IB-CNN. Experimental results on four benchmark AU databases have demonstrated that the IB-CNN yields significant improvement over the traditional CNN and the boosting CNN without incremental learning, as well as outperforming the state-of-the-art CNN-based methods in AU recognition. The improvement is more impressive for the AUs that have the lowest frequencies in the databases.
C1 [Han, Shizhong; Meng, Zibo; Khan, Ahmed Shehab; Tong, Yan] Univ South Carolina, Dept Comp Sci & Engn, Columbia, SC 29208 USA.
RP Han, SZ (reprint author), Univ South Carolina, Dept Comp Sci & Engn, Columbia, SC 29208 USA.
EM han38@email.sc.edu; mengz@email.sc.edu; akhan@email.sc.edu;
   tongy@cse.sc.edu
RI Han, Shizhong/G-4483-2019
OI Han, Shizhong/0000-0002-3381-6992
FU National Science Foundation under CAREER Award [IIS-1149787]
FX This work is supported by National Science Foundation under CAREER Award
   IIS-1149787.
CR Asthana A, 2013, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2013.442
   Baltruaitis T, 2015, 2015 11 IEEE INT C W, V6, P1, DOI DOI 10.1109/FG.2015.7284869
   BARTLETT MS, 2005, CVPR, P568
   Ekman P., 2002, FACIAL ACTION CODING
   Fasel B, 2002, FOURTH IEEE INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, PROCEEDINGS, P529, DOI 10.1109/ICMI.2002.1167051
   Ghosh S., 2015, ACII
   Gudi A., 2015, FG
   Han S., 2014, ICIP
   Hinton G.E., 2012, IMPROVING NEURAL NET
   Jaiswal S., 2016, WACV
   Jia Y., 2014, P 22 ACM INT C MULT, P675, DOI DOI 10.1145/2647868.2654889
   Jiang BH, 2014, INT C PATT RECOG, P1776, DOI 10.1109/ICPR.2014.312
   Jung H, 2015, IEEE I CONF COMP VIS, P2983, DOI 10.1109/ICCV.2015.341
   Kanade T., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P46, DOI 10.1109/AFGR.2000.840611
   Liu M., 2014, ACCV
   Liu P, 2014, CVPR
   LUCEY S, 2007, FACE RECOGNITION BOO
   Mavadati SM, 2013, IEEE T AFFECT COMPUT, V4, P151, DOI 10.1109/T-AFFC.2013.4
   Medera D, 2009, IJCCI 2009: PROCEEDINGS OF THE INTERNATIONAL JOINT CONFERENCE ON COMPUTATIONAL INTELLIGENCE, P547
   Nagi J, 2012, 2012 11TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2012), VOL 1, P27, DOI 10.1109/ICMLA.2012.14
   Petrou M., 2010, P IEEE C COMP VIS PA, P32
   Rifai S, 2012, LECT NOTES COMPUT SC, V7577, P808, DOI 10.1007/978-3-642-33783-3_58
   Sariyanidi E, 2015, IEEE T PATTERN ANAL, V37, P1113, DOI 10.1109/TPAMI.2014.2366127
   Sayette MA, 2001, J NONVERBAL BEHAV, V25, P167, DOI 10.1023/A:1010671109788
   Song Y., 2015, FG
   Tang Y., 2013, ICML
   Tong Y, 2007, IEEE T PATTERN ANAL, V29, P1683, DOI 10.1109/TPAMI.2007.1094
   Valstar M., 2015, FG
   Valstar MF, 2012, IEEE T SYST MAN CY B, V42, P966, DOI 10.1109/TSMCB.2012.2200675
   Yang P, 2009, PATTERN RECOGN LETT, V30, P132, DOI 10.1016/j.patrec.2008.03.014
   Yuce A., 2015, FG
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zhao G., 2007, TPAMI, V29, P915
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702005
DA 2019-06-15
ER

PT S
AU Harris, KD
   Mihalas, S
   Shea-Brown, E
AF Harris, Kameron Decker
   Mihalas, Stefan
   Shea-Brown, Eric
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI High resolution neural connectivity from incomplete tracing data using
   nonnegative spline regression
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID AREA
AB Whole-brain neural connectivity data are now available from viral tracing experiments, which reveal the connections between a source injection site and elsewhere in the brain. These hold the promise of revealing spatial patterns of connectivity throughout the mammalian brain. To achieve this goal, we seek to fit a weighted, nonnegative adjacency matrix among 100 mu m brain "voxels" using viral tracer data. Despite a multi-year experimental effort, injections provide incomplete coverage, and the number of voxels in our data is orders of magnitude larger than the number of injections, making the problem severely underdetermined. Furthermore, projection data are missing within the injection site because local connections there are not separable from the injection signal.
   We use a novel machine-learning algorithm to meet these challenges and develop a spatially explicit, voxel-scale connectivity map of the mouse visual system. Our method combines three features: a matrix completion loss for missing data, a smoothing spline penalty to regularize the problem, and (optionally) a low rank factorization. We demonstrate the consistency of our estimator using synthetic data and then apply it to newly available Allen Mouse Brain Connectivity Atlas data for the visual system. Our algorithm is significantly more predictive than current state of the art approaches which assume regions to be homogeneous. We demonstrate the efficacy of a low rank version on visual cortex data and discuss the possibility of extending this to a whole-brain connectivity matrix at the voxel scale.
C1 [Harris, Kameron Decker] Univ Washington, Appl Math, Seattle, WA 98195 USA.
   [Mihalas, Stefan; Shea-Brown, Eric] Univ Washington, Appl Math, Allen Inst Brain Sci, Seattle, WA 98195 USA.
RP Harris, KD (reprint author), Univ Washington, Appl Math, Seattle, WA 98195 USA.
EM kamdh@uw.edu; stefanm@alleninstitute.org; etsb@uw.edu
FU UW NIH; Boeing Scholarship; NSF [DMS-1122106, 1514743]; Simons
   Fellowship in Mathematics
FX We acknowledge the support of the UW NIH Training Grant in Big Data for
   Neuroscience and Genetics (KDH), Boeing Scholarship (KDH), NSF Grant
   DMS-1122106 and 1514743 (ESB & KDH), and a Simons Fellowship in
   Mathematics (ESB). We thank Liam Paninski for helpful insights at the
   outset of this project. We wish to thank the Allen Institute founders,
   Paul G. Allen and Jody Allen, for their vision, encouragement, and
   support. This work was facilitated though the use of advanced
   computational, storage, and networking infrastructure provided by the
   Hyak supercomputer system at the University of Washington.
CR Argyriou A, 2009, J MACH LEARN RES, V10, P2507
   Bock DD, 2011, NATURE, V471, P177, DOI 10.1038/nature09802
   Boyd S., 2004, CONVEX OPTIMIZATION
   BYRD RH, 1995, SIAM J SCI COMPUT, V16, P1190, DOI 10.1137/0916069
   Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061
   Chaplin TA, 2013, J COMP NEUROL, V521, P1001, DOI 10.1002/cne.23215
   Felleman DJ, 1991, CEREB CORTEX, V1, P1, DOI 10.1093/cercor/1.1.1
   Garrett ME, 2014, J NEUROSCI, V34, P12587, DOI 10.1523/JNEUROSCI.1124-14.2014
   Glickfeld L. L., 2013, NATURE NEUROSCIENCE, V16
   GOODMAN CS, 1993, CELL, V72, P77, DOI 10.1016/S0092-8674(05)80030-3
   Hubei DH, 1962, J PHYSL, V160
   Jenett A, 2012, CELL REP, V2, P991, DOI 10.1016/j.celrep.2012.09.011
   Jonas E, 2015, ELIFE, V4, DOI 10.7554/eLife.04250
   Kleinfeld D, 2011, J NEUROSCI, V31, P16125, DOI 10.1523/JNEUROSCI.4077-11.2011
   Kuan L, 2015, METHODS, V73, P4, DOI 10.1016/j.ymeth.2014.12.013
   Lin CJ, 2007, NEURAL COMPUT, V19, P2756, DOI 10.1162/neco.2007.19.10.2756
   LYNCH RE, 1964, B AM MATH SOC, V70, P378, DOI 10.1090/S0002-9904-1964-11105-8
   Oh SW, 2014, NATURE, V508, P207, DOI 10.1038/nature13186
   Peng H., 2014, NATURE COMMUNICATION, V5
   Rosa MGP, 2005, PHILOS T R SOC B, V360, P665, DOI 10.1098/rstb.2005.1626
   Sporns O., 2010, NETWORKS BRAIN
   UDIN SB, 1988, ANNU REV NEUROSCI, V11, P289, DOI 10.1146/annurev.ne.11.030188.001445
   Van Loan CF, 2000, J COMPUT APPL MATH, V123, P85, DOI 10.1016/S0377-0427(00)00393-9
   Wahba G., 1990, SPLINE MODELS OBSERV
   Wang QX, 2007, J COMP NEUROL, V502, P339, DOI 10.1002/cne.21286
   WHITE JG, 1986, PHILOS T R SOC B, V314, P1, DOI 10.1098/rstb.1986.0056
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701101
DA 2019-06-15
ER

PT S
AU Hartford, J
   Wright, JR
   Leyton-Brown, K
AF Hartford, Jason
   Wright, James R.
   Leyton-Brown, Kevin
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Deep Learning for Predicting Human Strategic Behavior
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Predicting the behavior of human participants in strategic settings is an important problem in many domains. Most existing work either assumes that participants are perfectly rational, or attempts to directly model each participant's cognitive processes based on insights from cognitive psychology and experimental economics. In this work, we present an alternative, a deep learning approach that automatically performs cognitive modeling without relying on such expert knowledge. We introduce a novel architecture that allows a single network to generalize across different input and output dimensions by using matrix units rather than scalar units, and show that its performance significantly outperforms that of the previous state of the art, which relies on expert-constructed features.
C1 [Hartford, Jason; Wright, James R.; Leyton-Brown, Kevin] Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada.
RP Hartford, J (reprint author), Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada.
EM jasonhar@cs.ubc.ca; jrwright@cs.ubc.ca; kevinlb@cs.ubc.ca
CR Bengio Y, 2013, IEEE T PATTERN ANAL, V35
   Camerer C, 2003, BEHAV GAME THEORY EX
   Camerer C. F., 2004, Q J EC, V119
   Clark C., 2015, P 32 INT C MACH LEAR
   Costa-Gomes M, 2001, ECONOMETRICA, V69, P1193, DOI 10.1111/1468-0262.00239
   Edelman B., 2007, AM ECON REV, V97, P1, DOI DOI 10.1257/AER.97.1.242
   Goldstein A., 1964, B AM MATH SOC, V70
   Kingma  D.P., 2015, INT C LEARN REPR ICL
   Lecun Y., 2015, NATURE
   Lin M., 2014, INT C LEARN REPR
   Long  J., 2015, CVPR
   MCKELVEY RD, 1995, GAME ECON BEHAV, V10, P6, DOI 10.1006/game.1995.1023
   Milgrom P., 2014, P 15 ACM C EC COMP
   Parkes DC, 2015, SCIENCE, V349, P267, DOI 10.1126/science.aaa8403
   Schmidhuber  J., 2015, NEURAL NETWORKS
   Shoham  Y., 2008, MULTIAGENT SYSTEMS A
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Stahl D. O., 1994, JEBO, V25
   Tambe M., 2011, SECURITY GAME THEORY
   Varian H. R., 2007, INT J IND ORG, V25
   Wright J. R., 2012, P 11 INT C AUT AG MU, V2, P921
   Wright J. R., 2010, AAAI
   Wright J.R., 2014, P 15 ACM C EC COMP, P857
   Yang R., 2013, ARTIFICIAL INTELLIGE
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704034
DA 2019-06-15
ER

PT S
AU Harwath, D
   Torralba, A
   Glass, JR
AF Harwath, David
   Torralba, Antonio
   Glass, James R.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Unsupervised Learning of Spoken Language with Visual Context
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Humans learn to speak before they can read or write, so why can't computers do the same? In this paper, we present a deep neural network model capable of rudimentary spoken language acquisition using untranscribed audio training data, whose only supervision comes in the form of contextually relevant visual images. We describe the collection of our data comprised of over 120,000 spoken audio captions for the Places image dataset and evaluate our model on an image search and annotation task. We also provide some visualizations which suggest that our model is learning to recognize meaningful words within the caption spectrograms.
C1 [Harwath, David; Torralba, Antonio; Glass, James R.] MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02115 USA.
RP Harwath, D (reprint author), MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02115 USA.
EM dharwath@csail.mit.edu; torralba@csail.mit.edu; jrg@csail.mit.edu
CR Barnard K., 2003, J MACHINE LEARNING R
   Chopra S., 2005, P CVPR
   Dredze Mark, 2010, P EMNLP
   Fang H., 2015, P CVPR
   Frome A., 2013, P NEUR INF PROC SOC
   Garofolo J., 1993, TIMIT ACOUSTIC PHONE
   Girshick R., 2013, P CVPR
   Glass James, 2012, ISSPA KEYNOTE
   Goldwater S, 2009, COGNITION, V112, P21, DOI 10.1016/j.cognition.2009.03.008
   Harwath D., 2012, P ICASSP
   Harwath D., 2015, P IEEE WORKSHOP AUTO
   Jansen A., 2010, P INTERSPEECH
   Jansen A., 2011, P IEEE WORKSH AUT SP
   Johnson M., 2008, P ACL SIG COMP MORPH
   Karpathy A., 2015, P 2015 C COMP VIS PA
   Karpathy A., 2014, P NEUR INF PROC SOC
   Lee C., 2012, P 2012 M ASS COMP LI
   Lee C., 2015, T ASS COMPUTATIONAL
   Lin T., 2015, ARXIV14050312
   Malioutov I., 2007, P ASS COMP LING ACL
   Park AS, 2008, IEEE T AUDIO SPEECH, V16, P186, DOI 10.1109/TASL.2007.909282
   Povey Daniel, 2011, IEEE 2011 WORKSH AUT
   Rashtchian C., 2010, P NAACL HLT 2010 WOR
   Saylor P., 2015, THESIS
   Simons G., 2016, ETHNOLOGUE LANGUAGES
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Socher R., 2014, T ASS COMPUTATIONAL
   Socher R., 2010, P CVPR
   van der Maaten  Laurens, 2008, J MACHINE LEARNING R
   Vinyals O., 2015, P 2015 C COMP VIS PA
   Young P., 2014, T ASS COMPUTATIONAL
   Zhang  Y., 2009, P ASRU
   Zhou B., 2014, P NEUR INF PROC SOC
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701043
DA 2019-06-15
ER

PT S
AU Hayashi, K
   Yoshida, Y
AF Hayashi, Kohei
   Yoshida, Yuichi
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Minimizing Quadratic Functions in Constant Time
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID APPROXIMATION ALGORITHMS
AB A sampling-based optimization method for quadratic functions is proposed. Our method approximately solves the following n-dimensional quadratic minimization problem in constant time, which is independent of n : z* = min(v is an element of Rn) < v, Av > + n < v, diag(d)v > + n < b, v >, where A is an element of R-nxn is a matrix and d, b is an element of R-n are vectors. Our theoretical analysis specifies the number of samples k(delta, epsilon) such that the approximated solution z satisfies vertical bar z - z*vertical bar = O(epsilon n(2)) with probability 1-delta. The empirical performance (accuracy and runtime) is positively confirmed by numerical experiments.
C1 [Hayashi, Kohei] Natl Inst Adv Ind Sci & Technol, Tokyo, Japan.
   [Yoshida, Yuichi] Natl Inst Informat & Preferred Infrastruct Inc, Tokyo, Japan.
RP Hayashi, K (reprint author), Natl Inst Adv Ind Sci & Technol, Tokyo, Japan.
EM hayashi.kohei@gmail.com; yyoshida@nii.ac.jp
FU MEXT KAKENHI [15K16055]; MEXT [24106001]; JST, CREST; Foundations of
   Innovative Algorithms for Big Data; JST, ERATO
FX We would like to thank Makoto Yamada for suggesting a motivating problem
   of our method. K. H. is supported by MEXT KAKENHI 15K16055. Y.Y. is
   supported by MEXT Grant-in-Aid for Scientific Research on Innovative
   Areas (No. 24106001), JST, CREST, Foundations of Innovative Algorithms
   for Big Data, and JST, ERATO, Kawarabayashi Large Graph Project.
CR Alon N., 2002, STOC, P232
   Alon N, 2009, SIAM J COMPUT, V39, P143, DOI 10.1137/060667177
   Borgs C, 2008, ADV MATH, V219, P1801, DOI 10.1016/j.aim.2008.07.008
   Borgs C., 2006, STOC'06. Proceedings of the 38th Annual ACM Symposium on Theory of Computing, P261
   Bottou L, 2004, LECT NOTES ARTIF INT, V3176, P146
   Brattka V, 1998, J COMPLEXITY, V14, P490, DOI 10.1006/jcom.1998.0488
   Clarkson KL, 2012, J ACM, V59, DOI 10.1145/2371656.2371658
   Frieze A, 1996, AN S FDN CO, P12, DOI 10.1109/SFCS.1996.548459
   Goldreich O, 1998, J ACM, V45, P653, DOI 10.1145/285055.285060
   Lovasz L., 2012, LARGE NETWORKS GRAPH
   Lovasz L, 2013, COMB PROBAB COMPUT, V22, P749, DOI 10.1017/S0963548313000205
   Lovasz L, 2006, J COMB THEORY B, V96, P933, DOI 10.1016/j.jctb.2006.05.002
   MATHIEU C, 2008, SODA, P176
   Murphy KP, 2012, MACHINE LEARNING PRO
   Nguyen HN, 2008, ANN IEEE SYMP FOUND, P327, DOI 10.1109/FOCS.2008.81
   Onak K., 2012, P 23 ANN ACM SIAM S, P1123
   Rubinfeld R, 1996, SIAM J COMPUT, V25, P252, DOI 10.1137/S0097539793255151
   Sugiyama M, 2012, DENSITY RATIO ESTIMATION IN MACHINE LEARNING, P1, DOI 10.1017/CBO9781139035613
   Suzuki T, 2011, NEURAL COMPUT, V23, P284, DOI 10.1162/NECO_a_00062
   Williams Christopher KI, 2001, NIPS
   Yamada M., 2011, NIPS
   Yoshida Y., 2016, SODA, P1391
   Yoshida Y., 2014, STOC, P154
   Yoshida Y, 2012, SIAM J COMPUT, V41, P1074, DOI 10.1137/110828691
   Yoshida Y, 2011, ACM S THEORY COMPUT, P665
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700009
DA 2019-06-15
ER

PT S
AU Hazan, E
   Ma, TY
AF Hazan, Elad
   Ma, Tengyu
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI A Non-generative Framework and Convex Relaxations for Unsupervised
   Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID COMPRESSION
AB We give a novel formal theoretical framework for unsupervised learning with two distinctive characteristics. First, it does not assume any generative model and based on a worst-case performance metric. Second, it is comparative, namely performance is measured with respect to a given hypothesis class. This allows to avoid known computational hardness results and improper algorithms based on convex relaxations. We show how several families of unsupervised learning models, which were previously only analyzed under probabilistic assumptions and are otherwise provably intractable, can be efficiently learned in our framework by convex optimization.
C1 [Hazan, Elad; Ma, Tengyu] Princeton Univ, 35 Olden St, Princeton, NJ 08540 USA.
RP Hazan, E (reprint author), Princeton Univ, 35 Olden St, Princeton, NJ 08540 USA.
EM ehazan@cs.princeton.edu; tengyu@cs.princeton.edu
CR Acharya J, 2013, IEEE INT SYMP INFO, P2875, DOI 10.1109/ISIT.2013.6620751
   Aharon M., 2005, P SPARS, P9, DOI DOI 10.1109/TSP.2006.881199
   Anandkumar A, 2014, J MACH LEARN RES, V15, P2773
   Arora  S., 2015, COLT, P113
   Arora S, 2012, ANN IEEE SYMP FOUND, P1, DOI 10.1109/FOCS.2012.49
   Arora  Sanjeev, 2013, ARXIV13086273
   Balcan MF, 2008, ACM S THEORY COMPUT, P671
   Barak B., 2015, P 47 ANN ACM S THEOR, P143, DOI DOI 10.1145/2746539.2746605
   Ben-David S., 2009, P ADV NEUR INF PROC, P121
   Berger T., 1971, RATE DISTORTION THEO
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Cover T. M., 2006, WILEY SERIES TELECOM
   Donoho DL, 2001, IEEE T INFORM THEORY, V47, P2845, DOI 10.1109/18.959265
   Hazan  Elad, 2012, P 29 INT C MACH LEAR
   Hsu Daniel, 2013, P 4 C INN THEOR COMP, P11, DOI DOI 10.1145/2422436.2422439
   Jevtic N, 2005, THEOR COMPUT SCI, V332, P293, DOI 10.1016/j.tcs.2004.10.038
   Kleinberg J., 2003, ADV NEURAL INFORM PR, P463
   Livni  Roi, 2013, P 30 INT C MACH LEAR
   Mohri M., 2012, FDN MACHINE LEARNING
   NATARAJAN BK, 1995, SIAM J COMPUT, V24, P227, DOI 10.1137/S0097539792240406
   Orlitsky A, 2004, IEEE T INFORM THEORY, V50, P1469, DOI 10.1109/TIT.2004.830761
   Paskov Hristo S, 2013, NIPS, P2931
   RISSANEN J, 1978, AUTOMATICA, V14, P465, DOI 10.1016/0005-1098(78)90005-5
   Salakhutdinov R., 2009, THESIS
   Shamir  Ohad, 2008, LEARNING GEN INFORM, P92
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Tishby  Naftali, 2000, PHYS0004057 CORR
   VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Vu V, 2011, RANDOM STRUCT ALGOR, V39, P526, DOI 10.1002/rsa.20367
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704057
DA 2019-06-15
ER

PT S
AU He, D
   Xia, YC
   Qin, T
   Wang, LW
   Yu, NH
   Liu, TY
   Ma, WY
AF He, Di
   Xia, Yingce
   Qin, Tao
   Wang, Liwei
   Yu, Nenghai
   Liu, Tie-Yan
   Ma, Wei-Ying
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Dual Learning for Machine Translation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB While neural machine translation (NMT) is making good progress in the past two years, tens of millions of bilingual sentence pairs are needed for its training. However, human labeling is very costly. To tackle this training data bottleneck, we develop a dual-learning mechanism, which can enable an NMT system to automatically learn from unlabeled data through a dual-learning game. This mechanism is inspired by the following observation: any machine translation task has a dual task, e.g., English-to-French translation (primal) versus French-to-English translation (dual); the primal and dual tasks can form a closed loop, and generate informative feedback signals to train the translation models, even if without the involvement of a human labeler. In the dual-learning mechanism, we use one agent to represent the model for the primal task and the other agent to represent the model for the dual task, then ask them to teach each other through a reinforcement learning process. Based on the feedback signals generated during this process (e.g., the language-model likelihood of the output of a model, and the reconstruction error of the original sentence after the primal and dual translations), we can iteratively update the two models until convergence (e.g., using the policy gradient methods). We call the corresponding approach to neural machine translation dual-NMT. Experiments show that dual-NMT works very well on English <-> French translation; especially, by learning from monolingual data (with 10% bilingual data for warm start), it achieves a comparable accuracy to NMT trained from the full bilingual data for the French-to-English translation task.
C1 [He, Di; Wang, Liwei] Peking Univ, Key Lab Machine Percept MOE, Sch EECS, Beijing, Peoples R China.
   [Xia, Yingce; Yu, Nenghai] Univ Sci & Technol China, Hefei, Anhui, Peoples R China.
   [Qin, Tao; Liu, Tie-Yan; Ma, Wei-Ying] Microsoft Res, Redmond, WA USA.
   [Xia, Yingce] Microsoft Res Asia, Beijing, Peoples R China.
RP He, D (reprint author), Peking Univ, Key Lab Machine Percept MOE, Sch EECS, Beijing, Peoples R China.
EM dih@cis.pku.edu.cn; xiayingc@mail.ustc.edu.cn; taoqin@microsoft.com;
   wanglw@cis.pku.edu.cn; ynh@ustc.edu.cn; tie-yan.liu@microsoft.com;
   wyma@microsoft.com
FU National Basic Research Program of China (973 Program) [2015CB352502];
   NSFC [61573026]; MOE-Microsoft Key Laboratory of Statistics and Machine
   Learning, Peking University
FX This work was partially supported by National Basic Research Program of
   China (973 Program) (grant no. 2015CB352502), NSFC (61573026) and the
   MOE-Microsoft Key Laboratory of Statistics and Machine Learning, Peking
   University. We would like to thank Yiren Wang, Fei Tian, Li Zhao and Wei
   Chen for helpful discussions, and the anonymous reviewers for their
   valuable comments on our paper.
CR Bandanau D., 2015, ICLR
   Brants T., 2007, P JOINT C EMP METH N
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Gulcehre C., 2015, ARXIV150303535
   Jean S, 2015, P 53 ANN M ASS COMP, P1
   Koehn Philipp, 2003, P 2003 C N AM CHAPT, P48, DOI DOI 10.3115/1073445.1073462
   Mikolov T, 2010, NTFRSPEECH, V2, P3
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311
   Ranzato M., 2015, ARXIV151106732
   Rush A.M., 2015, P 2015 C EMP METH NA, P379, DOI DOI 10.18653/V1/D15-1044
   Sennrich R., 2016, ACL
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   SUTTON R.S., 1999, NIPS, V99, P1057
   Ueffing N., 2008, MACHINE TRANSLATION
   Zeiler M.D., 2012, ARXIV12125701
NR 15
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703105
DA 2019-06-15
ER

PT S
AU He, K
   Wang, Y
   Hopcroft, J
AF He, Kun
   Wang, Yan
   Hopcroft, John
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI A Powerful Generative Model Using Random Weights for the Deep Image
   Representation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB To what extent is the success of deep visualization due to the training? Could we do deep visualization using untrained, random weight networks? To address this issue, we explore new and powerful generative models for three popular deep visualization tasks using untrained, random weight convolutional neural networks. First we invert representations in feature spaces and reconstruct images from white noise inputs. The reconstruction quality is statistically higher than that of the same method applied on well trained networks with the same architecture. Next we synthesize textures using scaled correlations of representations in multiple layers and our results are almost indistinguishable with the original natural texture and the synthesized textures based on the trained network. Third, by recasting the content of an image in the style of various artworks, we create artistic images with high perceptual quality, highly competitive to the prior work of Gatys et al. on pretrained networks. To our knowledge this is the first demonstration of image representations using untrained deep neural networks. Our work provides a new and fascinating tool to study the representation of deep network architecture and sheds light on new understandings on deep visualization. It may possibly lead to a way to compare network architectures without training.
C1 [He, Kun; Wang, Yan] Huazhong Univ Sci & Technol, Dept Comp Sci & Technol, Wuhan 430074, Hubei, Peoples R China.
   [Hopcroft, John] Cornell Univ, Dept Comp Sci, Ithaca, NY 14850 USA.
RP Wang, Y (reprint author), Huazhong Univ Sci & Technol, Dept Comp Sci & Technol, Wuhan 430074, Hubei, Peoples R China.
EM brooklet60@hust.edu.cn; yanwang@hust.edu.cn; jeh@cs.cornell.edu
FU US Army Research Office [W911NF-14-1-0477]; National Science Foundation
   of China [61472147]; National Science Foundation of Hubei Province
   [2015CFB566]
FX This research work was supported by US Army Research
   Office(W911NF-14-1-0477) and National Science Foundation of
   China(61472147) and National Science Foundation of Hubei
   Province(2015CFB566).
CR Dosovitskiy A, 2016, PROC CVPR IEEE, P4829, DOI 10.1109/CVPR.2016.522
   Dosovitskiy Alexey, 2016, NIPS
   Erhan Dumitru, 2009, 4323 U MONTR
   Gardner J. R., 2015, ARXIV151106421
   Gatys L, 2015, ADV NEURAL INFORM PR, P262
   Gatys L. A., 2015, ARXIV150507376
   Gatys Leon A., 2015, ARXIV150806576
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   He K., 2016, CVPR
   Jia Y., 2014, P 22 ACM INT C MULT, P675, DOI DOI 10.1145/2647868.2654889
   Johnson J., 2016, ECCV
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155
   Nguyen A., 2015, CVPR
   Nguyen Anh Mai, 2016, ARXIV 1602 03616
   Nikulin Yaroslav, 2016, ARXIV160207188
   Russalcovsky O., 2015, INT J COMPUT VISION, V115, P211, DOI [DOI 10.1007/S11263-015-0816-Y, DOI 10.1007/s11263-015-0816-y]
   Saxe A., 2011, P 28 INT C MACH LEAR, P1089
   Simonyan K., 2014, ICLR
   Simonyan Karen, 2015, ICLR
   Ulyanov D., 2016, ICML
   Wei D., 2015, ARXIV150702379
   Yosinski J., 2015, DEEP LEARN WORKSH IC
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704092
DA 2019-06-15
ER

PT S
AU He, XR
   Xu, K
   Kempe, D
   Liu, Y
AF He, Xinran
   Xu, Ke
   Kempe, David
   Liu, Yan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning Influence Functions from Incomplete Observations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We study the problem of learning influence functions under incomplete observations of node activations. Incomplete observations are a major concern as most (online and real-world) social networks are not fully observable. We establish both proper and improper PAC learnability of influence functions under randomly missing observations. Proper PAC learnability under the Discrete-Time Linear Threshold (DLT) and Discrete-Time Independent Cascade (DIC) models is established by reducing incomplete observations to complete observations in a modified graph. Our improper PAC learnability result applies for the DLT and DIC models as well as the Continuous-Time Independent Cascade (CIC) model. It is based on a parametrization in terms of reachability features, and also gives rise to an efficient and practical heuristic. Experiments on synthetic and real-world datasets demonstrate the ability of our method to compensate even for a fairly large fraction of missing observations.
C1 [He, Xinran; Xu, Ke; Kempe, David; Liu, Yan] Univ Southern Calif, Los Angeles, CA 90089 USA.
RP He, XR (reprint author), Univ Southern Calif, Los Angeles, CA 90089 USA.
EM xinranhe@usc.edu; xuk@usc.edu; dkempe@usc.edu; yanliu.cs@usc.edu
FU NSF [IIS-1254206]; U.S. Defense Advanced Research Projects Agency
   (DARPA) under the Social Media in Strategic Communication (SMISC)
   program [W911NF-12-1-0034]
FX We would like to thank anonymous reviewers for useful feedback. The
   research was sponsored in part by NSF research grant IIS-1254206 and by
   the U.S. Defense Advanced Research Projects Agency (DARPA) under the
   Social Media in Strategic Communication (SMISC) program, Agreement
   Number W911NF-12-1-0034. The views and conclusions are those of the
   authors and should not be interpreted as representing the official
   policies of the funding agency or the U.S. Government.
CR Amin Kareem, 2014, P 31 INT C MACH LEAR, V32, P1845
   Chierichetti F., 2011, ADV NEURAL INFORM PR, P792
   Du N., 2012, ADV NEURAL INFORM PR, P2780
   Du Nan, 2014, Proc Int Conf Mach Learn, V2014, P2016
   Du Nan, 2013, Adv Neural Inf Process Syst, V26, P3147
   Gomez-Rodriguez M., 2011, P 28 INT C MACH LEAR, P561
   Gomez-Rodriguez M, 2012, ACM T KNOWL DISCOV D, V5, DOI 10.1145/2086737.2086741
   Goyal A., 2010, P 3 ACM INT C WEB SE, P241, DOI DOI 10.1145/1718487.1718518
   Kempe D., 2003, P 9 ACM SIGKDD INT C, P137, DOI DOI 10.1145/956750.956769
   Leskovec J, 2010, J MACH LEARN RES, V11, P985
   Leskovec J, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P497
   Lokhov A. Y, 2016, P 29 C NEUR INF PROC, P3459
   Myers S. A., 2012, P 18 ACM SIGKDD INT, P33, DOI DOI 10.1145/2339530.2339540
   Myers S.A., 2010, ADV NEURAL INFORM PR, P1741
   Narasimhan H., 2015, P 27 NIPS, P3168
   Natarajan N., 2013, ADV NEURAL INFORM PR, P1196
   Netrapalli Praneeth, 2012, Performance Evaluation Review, V40, P211, DOI 10.1145/2318857.2254783
   Quang Duong, 2011, Proceedings of the 2011 IEEE Third International Conference on Privacy, Security, Risk and Trust and IEEE Third International Conference on Social Computing (PASSAT/SocialCom 2011), P362, DOI 10.1109/PASSAT/SocialCom.2011.50
   Rosenfeld N, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P563, DOI 10.1145/2835776.2835802
   Sadikov E, 2011, P 4 ACM INT C WEB SE, P55, DOI DOI 10.1145/1935826.1935844
   VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972
   Wu X., 2013, P 29 IJCAI, P2923
   Yang S. H., 2013, P 30 INT C MACH LEAR, V28, P1
   Zhou K., 2013, P 16 INT C ART INT S, P641
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701038
DA 2019-06-15
ER

PT S
AU Hefny, A
   Kass, RE
   Khanna, S
   Smith, M
   Gordon, GJ
AF Hefny, Ahmed
   Kass, Robert E.
   Khanna, Sanjeev
   Smith, Matthew
   Gordon, Geoffrey J.
BE Rish, I
   Langs, G
   Wehbe, L
   Cecchi, G
   Chang, KMK
   Murphy, B
TI Fast and Improved SLEX Analysis of High-Dimensional Time Series
SO MACHINE LEARNING AND INTERPRETATION IN NEUROIMAGING, MLINI 2014
SE Lecture Notes in Artificial Intelligence
LA English
DT Proceedings Paper
CT 4th International Workshop on Machine Learning and Interpretation in
   Neuroimaging (MLINI) Held at Conference on Neural Information Processing
   Systems (NIPS)
CY DEC 13, 2014
CL Montreal, CANADA
DE SLEX; Time series
AB We address the problem of segmenting a multi-dimensional time series into stationary blocks by improving AutoSLEX [1], which has been successfully used for this purpose. AutoSLEX finds the best basis in a library of smoothed localized exponentials (SLEX) basis functions that are orthogonal and localized in both time and frequency. We introduce DynamicSLEX, a variant of AutoSLEX that relaxes the dyadic intervals constraint of AutoSLEX, allowing for more flexible segmentation while maintaining tractability. Then, we introduce RandSLEX, which uses random projections to scale-up SLEX-based segmentation to high dimensional inputs and to establish a notion of strength of splitting points in the segmentation. We demonstrate the utility of the proposed improvements on synthetic and real data.
C1 [Hefny, Ahmed; Kass, Robert E.; Gordon, Geoffrey J.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Khanna, Sanjeev; Smith, Matthew] Univ Pittsburgh, Pittsburgh, PA USA.
RP Hefny, A (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM ahefny@cs.cmu.edu
CR BINGHAM E, 2001, P 7 ACM SIGKDD INT C, P245, DOI DOI 10.1145/502512.502546
   Cranstoun SD, 2002, IEEE T BIO-MED ENG, V49, P988, DOI 10.1109/TBME.2002.802015
   Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806
   Ombao H, 2005, J AM STAT ASSOC, V100, P519, DOI 10.1198/016214504000001448
   Ombao HC, 2001, J AM STAT ASSOC, V96, P543, DOI 10.1198/016214501753168244
   RABINER LR, 1969, BELL SYST TECH J, V48, P1249, DOI 10.1002/j.1538-7305.1969.tb04268.x
   Rosen O., 2012, ADAPTSPEC ADAPTIVE S
   Wickerhauser M.V., 1995, ADAPTIVE WAVELET ANA
NR 8
TC 0
Z9 0
U1 0
U2 2
PU SPRINGER INT PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
BN 978-3-319-45173-2; 978-3-319-45174-9
J9 LECT NOTES ARTIF INT
PY 2016
VL 9444
BP 94
EP 103
DI 10.1007/978-3-319-45174-9_10
PG 10
WC Computer Science, Artificial Intelligence; Neuroimaging
SC Computer Science; Neurosciences & Neurology
GA BG4FZ
UT WOS:000388723400010
DA 2019-06-15
ER

PT S
AU Hegde, C
   Indyk, P
   Schmidt, L
AF Hegde, Chinmay
   Indyk, Piotr
   Schmidt, Ludwig
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Fast recovery from a union of subspaces
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID ALGORITHMS
AB We address the problem of recovering a high-dimensional but structured vector from linear observations in a general setting where the vector can come from an arbitrary union of subspaces. This setup includes well-studied problems such as compressive sensing and low-rank matrix recovery. We show how to design more efficient algorithms for the union-of-subspace recovery problem by using approximate projections. Instantiating our general framework for the low-rank matrix recovery problem gives the fastest provable running time for an algorithm with optimal sample complexity. Moreover, we give fast approximate projections for 2D histograms, another well-studied low-dimensional model of data. We complement our theoretical results with experiments demonstrating that our framework also leads to improved time and sample complexity empirically.
C1 [Hegde, Chinmay] Iowa State Univ, Ames, IA 50011 USA.
   [Indyk, Piotr; Schmidt, Ludwig] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
RP Hegde, C (reprint author), Iowa State Univ, Ames, IA 50011 USA.
CR Baraniuk RG, 2010, IEEE T INFORM THEORY, V56, P1982, DOI 10.1109/TIT.2010.2040894
   Becker Stephen, 2013, SAMPTA C SAMPL THEOR
   Bhojanapalli Srinadh, 2015, 150903917 ARXIV
   Blumensath T, 2011, IEEE T INFORM THEORY, V57, P4660, DOI 10.1109/TIT.2011.2146550
   Blumensath T, 2009, APPL COMPUT HARMON A, V27, P265, DOI 10.1016/j.acha.2009.04.002
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Chen Y., 2015, ARXIV150903025
   Cormode G, 2011, FOUND TRENDS DATABAS, V4, P1, DOI 10.1561/1900000004
   Davenport Mark, 2016, 160106422 ARXIV
   Gilbert A. C., 2001, Proceedings of the 27th International Conference on Very Large Data Bases, P79
   Gilbert Anna C., 2002, STOC
   Giryes R, 2015, APPL COMPUT HARMON A, V39, P1, DOI 10.1016/j.acha.2014.07.004
   Hegde C, 2015, IEEE T INFORM THEORY, V61, P5129, DOI 10.1109/TIT.2015.2457939
   Ioannidis Y., 2003, VLDB, V29, P19
   Jain Prateek, 2010, NIPS
   Larsen Rasmus M., PROPACK
   Musco Cameron, 2015, NIPS
   Muthukrishnan S, 1999, LECT NOTES COMPUT SC, V1540, P236
   Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835
   SAAD Y, 1980, SIAM J NUMER ANAL, V17, P687, DOI 10.1137/0717059
   Stephen Tu, 2016, ICML
   Thaper Nitin, 2002, SIGMOD
   Zhao Tuo, NONCONVEX LOW RANK M
   Zheng Qinqing, 2015, NIPS
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704009
DA 2019-06-15
ER

PT S
AU Heller, R
   Heller, Y
AF Heller, Ruth
   Heller, Yair
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Multivariate tests of association based on univariate tests
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB For testing two vector random variables for independence, we propose testing whether the distance of one vector from an arbitrary center point is independent from the distance of the other vector from another arbitrary center point by a univariate test. We prove that under minimal assumptions, it is enough to have a consistent univariate independence test on the distances, to guarantee that the power to detect dependence between the random vectors increases to one with sample size. If the univariate test is distribution-free, the multivariate test will also be distribution-free. If we consider multiple center points and aggregate the center-specific univariate tests, the power may be further improved, and the resulting multivariate test may have a distribution-free critical value for specific aggregation methods (if the univariate test is distribution free). We show that certain multivariate tests recently proposed in the literature can be viewed as instances of this general approach. Moreover, we show in experiments that novel tests constructed using our approach can have better power and computational time than competing approaches.
C1 [Heller, Ruth] Tel Aviv Univ, Dept Stat & Operat Res, IL-6997801 Tel Aviv, Israel.
RP Heller, R (reprint author), Tel Aviv Univ, Dept Stat & Operat Res, IL-6997801 Tel Aviv, Israel.
EM ruheller@gmail.com; heller.yair@gmail.com
CR Baringhaus L, 2004, J MULTIVARIATE ANAL, V88, P190, DOI 10.1016/S0047-259X(03)00079-4
   Benjamini Y, 2001, ANN STAT, V29, P1165
   Chwialkowski K, 2015, ADV NEUR IN, V28
   Cuesta-Albertos JA, 2006, B BRAZ MATH SOC, V37, P477, DOI 10.1007/s00574-006-0023-0
   Gretton  A., 2012, ADV NEURAL INFORM PR, P1205
   GRETTON A., 2007, ADV NEURAL INFORM PR, V19
   Gretton A., 2008, ADV NEURAL INFORM PR, V20, P585
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Hall P, 2002, BIOMETRIKA, V89, P359, DOI 10.1093/biomet/89.2.359
   Heller R, 2016, J MACH LEARN RES, V17
   Heller R, 2013, BIOMETRIKA, V100, P503, DOI 10.1093/biomet/ass070
   HENZE N, 1988, ANN STAT, V16, P772, DOI 10.1214/aos/1176350835
   HOEFFDING W, 1948, ANN MATH STAT, V19, P546, DOI 10.1214/aoms/1177730150
   HOEFFDING W, 1948, ANN MATH STAT, V19, P293, DOI 10.1214/aoms/1177730196
   Hommel G, 1983, BIOMETR J, V25, P423
   Hotelling H, 1931, ANN MATH STAT, V2, P360, DOI 10.1214/aoms/1177732979
   Kolmogoroff A, 1941, ANN MATH STAT, V12, P461, DOI 10.1214/aoms/1177731684
   Maa JF, 1996, ANN STAT, V24, P1069
   PETTITT AN, 1976, BIOMETRIKA, V63, P161, DOI 10.2307/2335097
   Rawat R, 2000, J FOURIER ANAL APPL, V6, P343, DOI 10.1007/BF02511160
   Rosenbaum PR, 2005, J ROY STAT SOC B, V67, P515, DOI 10.1111/j.1467-9868.2005.00513.x
   SCHILLING MF, 1986, J AM STAT ASSOC, V81, P799, DOI 10.2307/2289012
   Sejdinovic D, 2013, ANN STAT, V41, P2263, DOI 10.1214/13-AOS1140
   Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517
   SZEKELY G., 2004, INTERSTAT
   Szekely GJ, 2007, ANN STAT, V35, P2769, DOI 10.1214/009053607000000505
   Thas O, 2004, COMMUN STAT-SIMUL C, V33, P711, DOI 10.1081/SAC-200033335
   Wei S, 2016, J COMPUT GRAPH STAT, V25, P549, DOI 10.1080/10618600.2015.1027773
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701077
DA 2019-06-15
ER

PT S
AU Herbster, M
   Pasteris, S
   Pontil, M
AF Herbster, Mark
   Pasteris, Stephen
   Pontil, Massimiliano
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Mistake Bounds for Binary Matrix Completion
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We study the problem of completing a binary matrix in an online learning setting. On each trial we predict a matrix entry and then receive the true entry. We propose a Matrix Exponentiated Gradient algorithm [1] to solve this problem. We provide a mistake bound for the algorithm, which scales with the margin complexity [2, 3] of the underlying matrix. The bound suggests an interpretation where each row of the matrix is a prediction task over a finite set of objects, the columns. Using this we show that the algorithm makes a number of mistakes which is comparable up to a logarithmic factor to the number of mistakes made by the Kernel Perceptron with an optimal kernel in hindsight. We discuss applications of the algorithm to predicting as well as the best biclustering and to the problem of predicting the labeling of a graph without knowing the graph in advance.
C1 [Herbster, Mark; Pasteris, Stephen; Pontil, Massimiliano] UCL, Dept Comp Sci, London WC1E 6BT, England.
   [Pontil, Massimiliano] Ist Italiano Tecnol, I-16163 Genoa, Italy.
RP Herbster, M (reprint author), UCL, Dept Comp Sci, London WC1E 6BT, England.
EM m.herbster@cs.ucl.ac.uk; s.pasteris@cs.ucl.ac.uk; m.pontil@cs.ucl.ac.uk
FU EPSRC [EP/P009069/1, EP/M006093/1]; U.S. Army Research Laboratory; U.K.
   Defence Science and Technology Laboratory [W911NF-16-3-0001]
FX We wish to thank the anonymous reviewers for their useful comments. This
   work was supported in part by EPSRC Grants EP/P009069/1, EP/M006093/1,
   and by the U.S. Army Research Laboratory and the U.K. Defence Science
   and Technology Laboratory and was accomplished under Agreement Number
   W911NF-16-3-0001. The views and conclusions contained in this document
   are those of the authors and should not be interpreted as representing
   the official policies, ether expressed or implied, of the U.S. Army
   Research Laboratory, the U.S. Government, the U.K. Defence Science and
   Technology Laboratory or the U.K. Government. The U.S. and U.K.
   Governments are authorized to reproduce and distribute reprints for
   Government purposes notwithstanding any copyright notation herein.
CR Alquier P., 2016, PREPRINT
   Arora S, 2007, ACM S THEORY COMPUT, P227, DOI 10.1145/1250790.1250823
   Balcan M. -F., 2015, C LEARN THEOR, P191
   Belkin M, 2004, MACH LEARN, V56, P209, DOI 10.1023/B:MACH.0000033120.25363.1e
   Ben-David S., 2003, Journal of Machine Learning Research, V3, P441, DOI 10.1162/153244303321897681
   Bhatia R., 1997, MATRIX ANAL
   Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061
   Cesa-Bianchi N., 2009, P 22 ANN C LEARN THE
   Cesa-Bianchi N, 2011, ADV NEURAL INFORM PR, P343
   Cesa-Bianchi N, 2013, J MACH LEARN RES, V14, P1251
   Cesa-Bianchi N, 2011, THEOR COMPUT SCI, V412, P1791, DOI 10.1016/j.tcs.2010.12.056
   Gentile C., 2013, P 26 ANN C LEARN THE
   Goldman S. A., 1993, Proceeding of the Sixth Annual ACM Conference on Computational Learning Theory, P453, DOI 10.1145/168304.168396
   Goldman S. A., 1993, SIAM J COMPUT, V22
   Hazan E., 2012, P 23 ANN C LEARN THE, V23
   Herbster M., 2005, P 22 INT C MACH LEAR, P305
   Herbster M., 2006, ADV NEURAL INFORM PR, P577
   Herbster M, 2015, J MACH LEARN RES, V16, P2003
   Linial N, 2007, COMBINATORICA, V27, P439, DOI 10.1007/s00493-007-2160-5
   Maurer A., 2013, C LEARNING THEORY CO, V30, P55
   Nie JZ, 2013, LECT NOTES ARTIF INT, V8139, P98
   Novikoff A., 1962, P S MATH THEOR AUT, VXII, P615
   Srebro N, 2005, LECT NOTES COMPUT SC, V3559, P545, DOI 10.1007/11503415_37
   Tsuda K, 2005, J MACH LEARN RES, V6, P995
   Warmuth M. K., 2007, P 24 INT C MACH LEAR, P999
   Warmuth MK, 2012, MACH LEARN, V87, P1, DOI 10.1007/s10994-011-5269-0
   Wullf S., 2013, P 30 INT C MACH LEAR, V28, P145
   Zhu X., 2003, INT C MACH LEARN, V20, P912
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704091
DA 2019-06-15
ER

PT S
AU Herreros-Alonso, I
   Arsiwalla, XD
   Verschure, PFMJ
AF Herreros-Alonso, Ivan
   Arsiwalla, Xerxes D.
   Verschure, Paul F. M. J.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI A Forward Model at Purkinje Cell Synapses Facilitates Cerebellar
   Anticipatory Control
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID LEARNING CONTROL; MOVEMENT; TIME
AB How does our motor system solve the problem of anticipatory control in spite of a wide spectrum of response dynamics from different musculo-skeletal systems, transport delays as well as response latencies throughout the central nervous system? To a great extent, our highly-skilled motor responses are a result of a reactive feedback system, originating in the brain-stem and spinal cord, combined with a feed-forward anticipatory system, that is adaptively fine-tuned by sensory experience and originates in the cerebellum. Based on that interaction we design the counterfactual predictive control (CFPC) architecture, an anticipatory adaptive motor control scheme in which a feed-forward module, based on the cerebellum, steers an error feedback controller with counterfactual error signals. Those are signals that trigger reactions as actual errors would, but that do not code for any current or forthcoming errors. In order to determine the optimal learning strategy, we derive a novel learning rule for the feed-forward module that involves an eligibility trace and operates at the synaptic level. In particular, our eligibility trace provides a mechanism beyond co-incidence detection in that it convolves a history of prior synaptic inputs with error signals. In the context of cerebellar physiology, this solution implies that Purkinje cell synapses should generate eligibility traces using a forward model of the system being controlled. From an engineering perspective, CFPC provides a general-purpose anticipatory control architecture equipped with a learning rule that exploits the full dynamics of the closed-loop system.
C1 [Herreros-Alonso, Ivan; Arsiwalla, Xerxes D.] Univ Pompeu Fabra, SPECS Lab, Barcelona, Spain.
   [Verschure, Paul F. M. J.] UPF, SPECS, Catalan Inst Res & Adv Studies ICREA, Barcelona, Spain.
RP Herreros-Alonso, I (reprint author), Univ Pompeu Fabra, SPECS Lab, Barcelona, Spain.
EM ivan.herreros@upf.edu
FU European Commission's Horizon 2020 socSMC project
   [socSMC-641321H2020-FETPROACT-2014]; European Research Council's CDAC
   project [ERC-2013-ADG 341196]
FX The research leading to these results has received funding from the
   European Commission's Horizon 2020 socSMC project
   (socSMC-641321H2020-FETPROACT-2014) and by the European Research
   Council's CDAC project (ERC-2013-ADG 341196).
CR ALBUS J S, 1971, Mathematical Biosciences, V10, P25, DOI 10.1016/0025-5564(71)90051-4
   Amann N, 1996, IEE P-CONTR THEOR AP, V143, P217, DOI 10.1049/ip-cta:19960244
   Apps R, 2005, NAT REV NEUROSCI, V6, P297, DOI 10.1038/nrn1646
   Astrom KJ, 2012, FEEDBACK SYSTEMS INT
   BARTO AG, 1983, IEEE T SYST MAN CYB, V13, P834, DOI 10.1109/TSMC.1983.6313077
   Bastian AJ, 2006, CURR OPIN NEUROBIOL, V16, P645, DOI 10.1016/j.conb.2006.08.016
   Bengtsson F, 2006, CEREBELLUM, V5, P7, DOI 10.1080/14734220500462757
   Benna M. K., 2016, NATURE NEUROSCIENCE
   Boyd S., 2008, ONLINE LECT NOTES
   De Zeeuw CI, 2005, CURR OPIN NEUROBIOL, V15, P667, DOI 10.1016/j.conb.2005.10.008
   Dean P, 2010, NAT REV NEUROSCI, V11, P30, DOI 10.1038/nrn2756
   Eccles J. C. M., 1967, CEREBELLUM NEURONAL
   FUJITA M, 1982, BIOL CYBERN, V45, P195, DOI 10.1007/BF00336192
   Gormezano I., 1983, 20 YEARS CLASSICAL C
   Herreros I, 2013, NEURAL NETWORKS, V47, P64, DOI 10.1016/j.neunet.2013.01.026
   Hesslow Germund, 2002, P86
   Hofstotter C, 2002, EUR J NEUROSCI, V16, P1361, DOI 10.1046/j.1460-9568.2002.02182.x
   Jordan M. I., 1996, HDB PERCEPTION ACTIO, V2, P71, DOI DOI 10.1016/S1874-5822(06)80005-8
   KAWATO M, 1987, BIOL CYBERN, V57, P169, DOI 10.1007/BF00364149
   Kettner RE, 1997, J NEUROPHYSIOL, V77, P2115
   Lahiri S, 2013, ADV NEURAL INF PROCE, P1034
   LISBERGER SG, 1987, ANNU REV NEUROSCI, V10, P97, DOI 10.1146/annurev.ne.10.030187.000525
   MARR D, 1969, J PHYSIOL-LONDON, V202, P437, DOI 10.1113/jphysiol.1969.sp008820
   MASSION J, 1992, PROG NEUROBIOL, V38, P35, DOI 10.1016/0301-0082(92)90034-C
   McKinstry JL, 2006, P NATL ACAD SCI USA, V103, P3387, DOI 10.1073/pnas.0511281103
   Porrill J, 2007, NEURAL COMPUT, V19, P170, DOI 10.1162/neco.2007.19.1.170
   Shibata T, 2001, IROS 2001: PROCEEDINGS OF THE 2001 IEEE/RJS INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P278, DOI 10.1109/IROS.2001.973371
   Suvrathan A, 2016, NEURON, V92, P959, DOI 10.1016/j.neuron.2016.10.022
   Wang SSH, 2000, NAT NEUROSCI, V3, P1266
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701008
DA 2019-06-15
ER

PT S
AU Hjelm, RD
   Cho, K
   Chung, JY
   Salakhutdinov, R
   Calhoun, V
   Jojic, N
AF Hjelm, R. Devon
   Cho, Kyunghyun
   Chung, Junyoung
   Salakhutdinov, Russ
   Calhoun, Vince
   Jojic, Nebojsa
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Iterative Refinement of the Approximate Posterior for Directed Belief
   Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Variational methods that rely on a recognition network to approximate the posterior of directed graphical models offer better inference and learning than previous methods. Recent advances that exploit the capacity and flexibility in this approach have expanded what kinds of models can be trained. However, as a proposal for the posterior, the capacity of the recognition network is limited, which can constrain the representational power of the generative model and increase the variance of Monte Carlo estimates. To address these issues, we introduce an iterative refinement procedure for improving the approximate posterior of the recognition network and show that training with the refined posterior is competitive with state-of-the-art methods. The advantages of refinement are further evident in an increased effective sample size, which implies a lower variance of gradient estimates.
C1 [Hjelm, R. Devon; Calhoun, Vince] Univ New Mexico, Albuquerque, NM 87131 USA.
   [Hjelm, R. Devon; Calhoun, Vince] Mind Res Network, Albuquerque, NM 87106 USA.
   [Cho, Kyunghyun] NYU, Courant Inst, New York, NY 10003 USA.
   [Cho, Kyunghyun] NYU, Ctr Data Sci, New York, NY 10003 USA.
   [Chung, Junyoung] Univ Montreal, Montreal, PQ, Canada.
   [Salakhutdinov, Russ] Carnegie Melon Univ, Pittsburgh, PA USA.
   [Jojic, Nebojsa] Microsoft Res, Redmond, WA USA.
RP Hjelm, RD (reprint author), Univ New Mexico, Albuquerque, NM 87131 USA.; Hjelm, RD (reprint author), Mind Res Network, Albuquerque, NM 87106 USA.
EM dhjelm@mrn.org; kyunghyun.cho@nyu.edu; junyoung.chung@umontreal.ca;
   rsalakhu@cs.toronto.edu; vcalhoun@mrn.org; jojic@microsoft.com
FU Microsoft Research; NIH [P20GM103472]; R01 grant [REB020407]; NSF
   [1539067]; ONR [N000141512791]; ADeLAIDE grant [FA8750-16C-0130-001];
   Facebook; Google; NVidia (GPU Center of Excellence); PIBBS
FX This work was supported by Microsoft Research to RDH under NJ; NIH
   P20GM103472, R01 grant REB020407, and NSF grant 1539067 to VDC; and ONR
   grant N000141512791 and ADeLAIDE grant FA8750-16C-0130-001 to RS. KC was
   supported in part by Facebook, Google (Google Faculty Award 2016) and
   NVidia (GPU Center of Excellence 2015-2016), and RDH was supported in
   part by PIBBS.
CR Bornschein Jorg, 2014, ARXIV14062751
   Burda Y., 2015, ARXIV150900519
   DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889
   Dempster Arthur P, 1977, J ROYAL STAT SOC B
   Doucet A, 2001, STAT ENG IN, P3
   GORDON NJ, 1993, IEE PROC-F, V140, P107, DOI 10.1049/ip-f-2.1993.0015
   Gregor K., 2013, ARXIV13108499
   Gu S., 2015, NIPS, P2611
   Gu Shixiang, 2015, ARXIV151105176
   Hinton G, 2012, NEURAL NETWORKS MACH
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Kingma D.P., 2013, ARXIV13126114
   Mnih A., 2014, P 31 INT C MACH LEAR, P1791
   Mnih Andriy, 2016, ARXIV160206725
   Neal R., 1992, ARTIFICIAL INTELLIGE, V56
   Neal RM, 1998, NATO ADV SCI I D-BEH, V89, P355
   Oh M.-S., 1992, J STAT COMPUT SIM, V41, P143
   Paige Brooks, 2016, ARXIV160206701
   Raiko T., 2014, ARXIV14062989
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   Rezende Danilo Jimenez, 2015, ARXIV150505770
   Salakhutdinov R., 2010, INT C ART INT STAT, P693
   Salakhutdinov R., 2008, P 25 INT C MACH LEAR, P872, DOI DOI 10.1145/1390156.1390266
   Salimans T., 2015, P 32 INT C MACH LEAR, V37, P1218
   Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251
   Tang Y., 2013, ADV NEURAL INFORM PR, P530
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702058
DA 2019-06-15
ER

PT S
AU Ho, CJ
   Frongillo, R
   Chen, YL
AF Ho, Chien-Ju
   Frongillo, Rafael
   Chen, Yiling
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Eliciting Categorical Data for Optimal Aggregation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID PREDICTION
AB Models for collecting and aggregating categorical data on crowdsourcing platforms typically fall into two broad categories: those assuming agents honest and consistent but with heterogeneous error rates, and those assuming agents strategic and seek to maximize their expected reward. The former often leads to tractable aggregation of elicited data, while the latter usually focuses on optimal elicitation and does not consider aggregation. In this paper, we develop a Bayesian model, wherein agents have differing quality of information, but also respond to incentives. Our model generalizes both categories and enables the joint exploration of optimal elicitation and aggregation. This model enables our exploration, both analytically and experimentally, of optimal aggregation of categorical data and optimal multiple-choice interface design.
C1 [Ho, Chien-Ju] Cornell Univ, Ithaca, NY 14853 USA.
   [Frongillo, Rafael] CU Boulder, Boulder, CO USA.
   [Chen, Yiling] Harvard Univ, Cambridge, MA 02138 USA.
RP Ho, CJ (reprint author), Cornell Univ, Ithaca, NY 14853 USA.
EM ch624@cornell.edu; raf@colorado.edu; yiling@seas.harvard.edu
FU NSF [CCF-1512964, CCF-1301976]; ONR [N00014-15-1-2335]
FX thank the anonymous reviewers for their helpful comments. This research
   was partially supported by NSF grant CCF-1512964, NSF grant CCF-1301976,
   and ONR grant N00014-15-1-2335.
CR AURENHAMMER F, 1987, SIAM J COMPUT, V16, P78, DOI 10.1137/0216006
   Brier GW., 1950, MONTHLY WEATHER REVI, V75, P1, DOI DOI 10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2
   Cholleti S. R., 2008, P 20 IEEE INT C TOOL
   DAWID AP, 1979, APPL STAT, V28, P20, DOI DOI 10.2307/2346806
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Frongillo R., 2015, J MACH LEARN RES WOR, P1
   Frongillo R. M., 2015, 29 AAAI C ART INT
   Frongillo R, 2014, LECT NOTES COMPUT SC, V8877, P354, DOI 10.1007/978-3-319-13129-0_29
   Gneiting T, 2007, J AM STAT ASSOC, V102, P359, DOI 10.1198/016214506000001437
   Ho C., 2013, 30 INT C MACH LEARN
   Ipeirotis P., 2014, DATA MINING KNOWLEDG
   Jin R., 2003, ADV NEURAL INFORM PR, V15, P897
   Karger D. R., 2011, P 49 ANN C COMM CONT
   Karger D. R., 2011, 25 ANN C NEUR INF PR
   Lambert N. S., 2009, P 10 ACM C EL COMM E, P109
   Lambert N, 2008, EC'08: PROCEEDINGS OF THE 2008 ACM CONFERENCE ON ELECTRONIC COMMERCE, P129
   Miller N, 2005, MANAGE SCI, V51, P1359, DOI 10.1287/ninsc.1050.0379
   Prelec D, 2004, SCIENCE, V306, P462, DOI 10.1126/science.1102081
   Raykar VC, 2010, J MACH LEARN RES, V11, P1297
   SAVAGE LJ, 1971, J AM STAT ASSOC, V66, P783, DOI 10.2307/2284229
   Shah N. B., 2015, NEURAL INFORM PROCES
   Sheng V., 2008, ACM SIGKDD C KNOWL D
   Whitehill J., 2009, ADV NEURAL INFORM PR, P2035
   Witkowski J., 2012, P 26 AAAI C ART INT
   Zou J., 2012, P WORKSH MACH LEARN
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701094
DA 2019-06-15
ER

PT S
AU Ho, MK
   Littman, ML
   MacGlashan, J
   Cushman, F
   Austerweil, JL
AF Ho, Mark K.
   Littman, Michael L.
   MacGlashan, James
   Cushman, Fiery
   Austerweil, Joseph L.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Showing versus Doing: Teaching by Demonstration
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB People often learn from others' demonstrations, and inverse reinforcement learning (IRL) techniques have realized this capacity in machines. In contrast, teaching by demonstration has been less well studied computationally. Here, we develop a Bayesian model for teaching by demonstration. Stark differences arise when demonstrators are intentionally teaching (i.e. showing) a task versus simply performing (i.e. doing) a task. In two experiments, we show that human participants modify their teaching behavior consistent with the predictions of our model. Further, we show that even standard IRL algorithms benefit when learning from showing versus doing.
C1 [Ho, Mark K.] Brown Univ, Dept Cognit Linguist & Psychol Sci, Providence, RI 02912 USA.
   [Littman, Michael L.; MacGlashan, James] Brown Univ, Dept Comp Sci, Providence, RI 02912 USA.
   [Cushman, Fiery] Harvard Univ, Dept Psychol, Cambridge, MA 02138 USA.
   [Austerweil, Joseph L.] Univ Wisconsin, Dept Psychol, 1202 W Johnson St, Madison, WI 53706 USA.
RP Ho, MK (reprint author), Brown Univ, Dept Cognit Linguist & Psychol Sci, Providence, RI 02912 USA.
EM mark_ho@brown.edu; mlittman@cs.brown.edu; james_macglashan@brown.edu;
   cushman@fas.harvard.edu; austerweil@wisc.edu
FU NSF GRFP [DGE-1058262]; DARPA SIMPLEX program [14-46-FP-097,
   N00014-14-1-0800]; Office of Naval Research
FX MKH was supported by the NSF GRFP under Grant No. DGE-1058262. JLA and
   MLL were supported by DARPA SIMPLEX program Grant No. 14-46-FP-097. FC
   was supported by grant N00014-14-1-0800 from the Office of Naval
   Research.
CR Abbeel P., 2004, P 21 INT C MACH LEAR, P1, DOI DOI 10.1145/1015330.1015430
   Argall BD, 2009, ROBOT AUTON SYST, V57, P469, DOI 10.1016/j.robot.2008.10.024
   Austerweil JL, 2013, PSYCHOL REV, V120, P817, DOI 10.1037/a0034194
   Babes M., 2011, P 28 INT C MACH LEAR, P897
   Baker CL, 2009, COGNITION, V113, P329, DOI 10.1016/j.cognition.2009.07.005
   Buchsbaum D, 2011, COGNITION, V120, P331, DOI 10.1016/j.cognition.2010.12.001
   Butler LP, 2014, COGNITION, V130, P116, DOI 10.1016/j.cognition.2013.10.002
   Cakmak M., 2012, AAAI C ART INT AAAI
   Dragan AD, 2013, ACMIEEE INT CONF HUM, P301, DOI 10.1109/HRI.2013.6483603
   Frank MC, 2012, SCIENCE, V336, P998, DOI 10.1126/science.1218633
   Hadfield-Menell D., 2016, ADV NEURAL INFORM PR, V28
   MacGlashan J, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3692
   Ramachandran D, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2586
   Shafto P, 2014, COGNITIVE PSYCHOL, V71, P55, DOI 10.1016/j.cogpsych.2013.12.004
   Sutton R. S., 1998, REINFORCEMENT LEARNI
NR 15
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703049
DA 2019-06-15
ER

PT S
AU Hoiles, W
   van der Schaar, M
AF Hoiles, William
   van der Schaar, Mihaela
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI A Non-parametric Learning Method for Confidently Estimating Patient's
   Clinical State and Dynamics
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Estimating patient's clinical state from multiple concurrent physiological streams plays an important role in determining if a therapeutic intervention is necessary and for triaging patients in the hospital. In this paper we construct a non-parametric learning algorithm to estimate the clinical state of a patient. The algorithm addresses several known challenges with clinical state estimation such as eliminating the bias introduced by therapeutic intervention censoring, increasing the timeliness of state estimation while ensuring a sufficient accuracy, and the ability to detect anomalous clinical states. These benefits are obtained by combining the tools of non-parametric Bayesian inference, permutation testing, and generalizations of the empirical Bernstein inequality. The algorithm is validated using real-world data from a cancer ward in a large academic hospital.
C1 [Hoiles, William; van der Schaar, Mihaela] Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90024 USA.
RP Hoiles, W (reprint author), Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90024 USA.
EM whoiles@ucla.edu; mihaela@ee.ucla.edu
FU Airforce DDDAS program [NSF ECCS 1462245]
FX This research was supported by: NSF ECCS 1462245, and the Airforce DDDAS
   program.
CR Bartlett MS, 1937, PROC R SOC LON SER-A, V160, P0268, DOI 10.1098/rspa.1937.0109
   Basso D., 2009, PERMUTATION TESTS
   Beal M., 2001, ADV NEURAL INFORM PR, P577
   Bilodeau M, 2008, THEORY MULTIVARIATE
   Brockwell P. J., 2013, TIME SERIES THEORY M
   Bryant M., 2012, NIPS, P2699
   Campbell Trevor, 2015, ADV NEURAL INFORM PR, P280
   FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360
   Fox E., 2009, ADV NEURAL INFORM PR, V22, P549
   Fox E. B., 2008, P 25 INT C MACH LEAR, P312, DOI DOI 10.1145/1390156.1390196
   Gelman A., 2014, BAYESIAN DATA ANAL, V2
   Johnson AEW, 2016, P IEEE, V104, P444, DOI 10.1109/JPROC.2015.2501978
   Maurer Andreas, 2009, COLT
   Montanez G., 2015, AAAI C ART INT, P1819
   Muirhead R. G., 1982, ASPECTS MULTIVARIATE
   Paxton C., 2012, ANN S P AMIA S AMIA, V2013, P1109
   Pesarin F., 2010, PERMUTATION TESTS CO
   Rothman MJ, 2013, J BIOMED INFORM, V46, P837, DOI 10.1016/j.jbi.2013.06.011
   Saria S., 2010, P NEUR INF PROC SYST
   Schott JR, 2007, COMPUT STAT DATA AN, V51, P6535, DOI 10.1016/j.csda.2007.03.004
   Subbe CP, 2001, QJM-MON J ASSOC PHYS, V94, P521, DOI 10.1093/qjmed/94.10.521
   Teh Y. W., 2012, J AM STAT ASS
   Tenreiro C, 2011, COMPUT STAT DATA AN, V55, P1980, DOI 10.1016/j.csda.2010.12.004
   Timm N., 2002, APPL MULTIVARIATE AN, V1
   Van Gael J., 2008, P 25 INT C MACH LEAR, V25, P1088, DOI DOI 10.1145/1390156.1390293
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703090
DA 2019-06-15
ER

PT S
AU Hosseini, MJ
   Lee, SI
AF Hosseini, Mohammad Javad
   Lee, Su-In
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning Sparse Gaussian Graphical Models with Overlapping Blocks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID REGULATORS
AB We present a novel framework, called GRAB (GRaphical models with overlApping Blocks), to capture densely connected components in a network estimate. GRAB takes as input a data matrix of p variables and n samples and jointly learns both a network of the p variables and densely connected groups of variables (called 'blocks'). GRAB has four major novelties as compared to existing network estimation methods: 1) It does not require blocks to be given a priori. 2) Blocks can overlap. 3) It can jointly learn a network structure and overlapping blocks. 4) It solves a joint optimization problem with the block coordinate descent method that is convex in each step. We show that GRAB reveals the underlying network structure substantially better than four state-of-the-art competitors on synthetic data. When applied to cancer gene expression data, GRAB outperforms its competitors in revealing known functional gene sets and potentially novel cancer driver genes.
C1 [Hosseini, Mohammad Javad; Lee, Su-In] Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA.
   [Lee, Su-In] Univ Washington, Dept Genome Sci, Seattle, WA 98195 USA.
RP Hosseini, MJ (reprint author), Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA.
EM hosseini@cs.washington.edu; suinlee@cs.washington.edu
FU National Science Foundation [DBI-1355899]; American Cancer Society
   [127332-RSG-15-097-01-TBG]
FX We give warm thanks to Reza Eghbali and Amin Jalali for many useful
   discussions. This work was supported by the National Science Foundation
   grant DBI-1355899 and the American Cancer Society Research Scholar Award
   127332-RSG-15-097-01-TBG.
CR Ambroise C, 2009, ELECTRON J STAT, V3, P205, DOI 10.1214/08-EJS314
   Celik S., 2014, ICML
   Duchi J., 2008, UAI
   Fang Y, 2015, EUR REV MED PHARMACO, V19, P4811
   Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045
   Grechkin M., 2015, PATHWAY GRAPHICAL LA
   Haferlach T, 2010, J CLIN ONCOL, V28, P2529, DOI 10.1200/JCO.2009.23.4732
   HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7
   Hsieh C.-J., 2011, ADV NEURAL INFORM PR, P2330
   Lasorella A, 2014, NAT REV CANCER, V14, P77, DOI 10.1038/nrc3638
   Lauritzen SL, 1996, GRAPHICAL MODELS
   Lee S. - I., 2003, ADV NEURAL INFORM PR, V16
   Liberzon A, 2011, BIOINFORMATICS, V27, P1739, DOI 10.1093/bioinformatics/btr260
   Liu Q., 2011, INT C ART INT STAT, P40
   Marlin B. M., 2009, SPARSE GAUSSIAN GRAP, P705
   Miyoshi N, 2010, J SURG ONCOL, V101, P156, DOI 10.1002/jso.21459
   Mohan K., 2012, ADV NEURAL INFORM PR, V25, P620
   Newman MEJ, 2006, P NATL ACAD SCI USA, V103, P8577, DOI 10.1073/pnas.0601602103
   Rothman AJ, 2008, ELECTRON J STAT, V2, P494, DOI 10.1214/08-EJS176
   Rubie C., 2010, RES CCL20 CCR6 EXPRE
   Segal E, 2003, NAT GENET, V34, P166, DOI 10.1038/ng1165
   Shen Y, 2011, BLOOD, V118, P5593, DOI 10.1182/blood-2011-03-343988
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Tamura K, 2009, CANCER RES, V69, P8133, DOI 10.1158/0008-5472.CAN-09-0775
   Tan KM, 2015, COMPUT STAT DATA AN, V85, P23, DOI 10.1016/j.csda.2014.11.015
   Tseng P, 2001, J OPTIMIZ THEORY APP, V109, P475, DOI 10.1023/A:1017501703105
   Witten DM, 2011, J COMPUT GRAPH STAT, V20, P892, DOI 10.1198/jcgs.2011.11051a
   Yuan M, 2007, BIOMETRIKA, V94, P19, DOI 10.1093/biomet/asm018
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700061
DA 2019-06-15
ER

PT S
AU Houthooft, R
   Chen, X
   Duan, Y
   Schulman, J
   De Turck, F
   Abbeel, P
AF Houthooft, Rein
   Chen, Xi
   Duan, Yan
   Schulman, John
   De Turck, Filip
   Abbeel, Pieter
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI VIME: Variational Information Maximizing Exploration
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID REINFORCEMENT; CURIOSITY
AB Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as epsilon-greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.
C1 [Houthooft, Rein; Chen, Xi; Duan, Yan; Schulman, John; Abbeel, Pieter] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
   [Houthooft, Rein; De Turck, Filip] Univ Ghent, IMEC, Dept Informat Technol, Ghent, Belgium.
   [Houthooft, Rein; Chen, Xi; Duan, Yan; Schulman, John; Abbeel, Pieter] OpenAI, San Francisco, CA 94110 USA.
RP Houthooft, R (reprint author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.; Houthooft, R (reprint author), Univ Ghent, IMEC, Dept Informat Technol, Ghent, Belgium.
FU DARPA; Berkeley Vision and Learning Center (BVLC); Berkeley Artificial
   Intelligence Research (BAIR) laboratory; ONR through a PECASE award;
   Ph.D. Fellowship of the Research Foundation - Flanders (FWO); Berkeley
   AI Research lab Fellowship; Huawei Fellowship; Berkeley Deep Drive (BDD)
FX This work was supported in part by DARPA, the Berkeley Vision and
   Learning Center (BVLC), the Berkeley Artificial Intelligence Research
   (BAIR) laboratory, Berkeley Deep Drive (BDD), and ONR through a PECASE
   award. Rein Houthooft is supported by a Ph.D. Fellowship of the Research
   Foundation - Flanders (FWO). Xi Chen was also supported by a Berkeley AI
   Research lab Fellowship. Yan Duan was also supported by a Berkeley AI
   Research lab Fellowship and a Huawei Fellowship.
CR Auer P., 2003, Journal of Machine Learning Research, V3, P397, DOI 10.1162/153244303321897663
   Auer P., 2009, ADV NEURAL INFORM PR, V21, P89
   Blundell C., 2015, ICML
   Brafman RI, 2003, J MACH LEARN RES, V3, P213, DOI 10.1162/153244303765208377
   Duan Y., 2016, ICML
   Ghavamzadeh M, 2015, FOUND TRENDS MACH LE, V8, P360, DOI 10.1561/2200000049
   Graves A., 2011, ADV NEURAL INFORM PR, P2348
   Guez A., 2014, NIPS, P451
   Hester T., 2015, ARTIFICIAL INTELLIGE
   Hinton G.E., 1993, P 6 ANN C COMP LEARN, P5, DOI DOI 10.1145/168304.168306
   Itti L., 2005, ADV NEURAL INFORM PR, V19, P547
   Kearns M, 2002, MACH LEARN, V49, P209, DOI 10.1023/A:1017984413808
   Kingma D. P., 2015, ADV NEURAL INFORM PR, P2575
   Kolter J. Z., 2009, P 26 ANN INT C MACH, P513
   Langford J., 2003, P 20 INT C MACH LEAR, V3, P306
   Little D. Y., 2014, CLOSING LOOP NEURAL, P295
   Lopes M., 2012, ADV NEURAL INFORM PR, P206
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Mohamed S., 2015, ADV NEURAL INFORM PR, V28, P2116
   Montufar G., 2016, ARXIV160509735
   Oh J., 2015, NIPS, V28, P2845
   Osband I., 2016, ICML
   Osband Ian, 2014, ARXIV14020635
   Oudeyer Pierre-Yves, 2007, Front Neurorobot, V1, P6, DOI 10.3389/neuro.12.006.2007
   Pazis Jason, 2013, AAAI
   Peters J, 2007, P 24 INT C MACH LEAR, P745
   Peters J., 2009, ADV NEURAL INFORM PR, V21, P849
   Salguero CP, 2014, ENCOUNTERS ASIA, P67
   SCHMIDHUBER J, 1991, IEEE IJCNN, P1458, DOI 10.1109/IJCNN.1991.170605
   Schmidhuber J, 2007, LECT NOTES ARTIF INT, V4755, P26
   Schmidhuber J, 2010, IEEE T AUTON MENT DE, V2, P230, DOI 10.1109/TAMD.2010.2056368
   Schossau J, 2016, ENTROPY-SWITZ, V18, DOI 10.3390/e18010006
   Schulman John, 2015, ICML
   Stadie BC, 2015, ARXIV150700814
   Still S, 2012, THEOR BIOSCI, V131, P139, DOI 10.1007/s12064-011-0142-z
   Storck J., 1995, ICANN '95. International Conference on Artificial Neural Networks. Neuronimes '95 Scientific Conference, P159
   Subramanian K., 2016, AAMAS
   Sutton R. S., INTRO REINFORCEMENT
   Thrun S. B., 1992, TECH REP
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Yi Sun, 2011, Artificial General Intelligence. Proceedings 4th International Conference, AGI 2011, P41, DOI 10.1007/978-3-642-22887-2_5
   Zahedi K, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00801
NR 42
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973705005
DA 2019-06-15
ER

PT S
AU Hsu, WS
   Poupart, P
AF Hsu, Wei-Shou
   Poupart, Pascal
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Online Bayesian Moment Matching for Topic Modeling with Unknown Number
   of Topics
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Latent Dirichlet Allocation (LDA) is a very popular model for topic modeling as well as many other problems with latent groups. It is both simple and effective. When the number of topics (or latent groups) is unknown, the Hierarchical Dirichlet Process (HDP) provides an elegant non-parametric extension; however, it is a complex model and it is difficult to incorporate prior knowledge since the distribution over topics is implicit. We propose two new models that extend LDA in a simple and intuitive fashion by directly expressing a distribution over the number of topics. We also propose a new online Bayesian moment matching technique to learn the parameters and the number of topics of those models based on streaming data. The approach achieves higher log-likelihood than batch and online HDP with fixed hyperparameters on several corpora. The code is publicly available at https://github.com/whsu/bmm.
C1 [Hsu, Wei-Shou; Poupart, Pascal] Univ Waterloo, David R Cheriton Sch Comp Sci, Wateroo, ON N2L 3G1, Canada.
RP Hsu, WS (reprint author), Univ Waterloo, David R Cheriton Sch Comp Sci, Wateroo, ON N2L 3G1, Canada.
EM wwhsu@uwaterloo.ca; ppoupart@uwaterloo.ca
CR Anandkumar Anima, 2012, ADV NEURAL INFORM PR, V25, P926
   ARORA S, 2012, FDN COMPUTER SCI, P1, DOI DOI 10.1109/FOCS.2012.49
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Bryant M., 2012, NIPS, P2699
   Ge Rong, 2015, C LEARN THEOR
   Goldwater S., 2005, NIPS, V18, P459
   Griffiths  T., 2002, GIBBS SAMPLING GENER
   Hoffman M., 2010, ADV NEURAL INFORM PR, P856
   Hsu Daniel, 2013, P 4 C INN THEOR COMP, P11, DOI DOI 10.1145/2422436.2422439
   Huang FR, 2015, J MACH LEARN RES, V16, P2797
   Huang Furong, 2013, ARXIV13090787
   Minka T.P., 2001, UNCERTAINTY ARTIFICI, V17, P362
   Minka Thomas, 2002, P 18 C UNC ART INT, P352
   Omar Farheen, 2016, THESIS
   Porteous I., 2008, P 14 ACM SIGKDD INT, P569, DOI DOI 10.1145/1401890.1401960
   Rehurek Radim, 2010, P LREC 2010 WORKSH N, P45, DOI DOI 10.13140/2.1.2393.1847
   Sato I., 2012, P 18 ACM SIGKDD INT, P105
   Teh Y.W., 2006, ADV NEURAL INFORM PR, P1353
   Teh Y.W., 2007, ADV NEURAL INFORM PR, P1481
   Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302
   Teh YW, 2006, COLING/ACL 2006, VOLS 1 AND 2, PROCEEDINGS OF THE CONFERENCE, P985
   Wang C., 2011, J MACH LEARN RES PRO, P752
   Wang C., 2012, ADV NEURAL INFORM PR, P413
   Wang C., 2011, AISTATS, V15
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700041
DA 2019-06-15
ER

PT S
AU Huang, C
   Loy, CC
   Tang, XO
AF Huang, Chen
   Loy, Chen Change
   Tang, Xiaoou
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Local Similarity-Aware Deep Feature Embedding
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID CLASSIFICATION
AB Existing deep embedding methods in vision tasks are capable of learning a compact Euclidean space from images, where Euclidean distances correspond to a similarity metric. To make learning more effective and efficient, hard sample mining is usually employed, with samples identified through computing the Euclidean feature distance. However, the global Euclidean distance cannot faithfully characterize the true feature similarity in a complex visual feature space, where the intraclass distance in a high-density region may be larger than the interclass distance in low-density regions. In this paper, we introduce a Position-Dependent Deep Metric (PDDM) unit, which is capable of learning a similarity metric adaptive to local feature structure. The metric can be used to select genuinely hard samples in a local neighborhood to guide the deep embedding learning in an online and robust manner. The new layer is appealing in that it is pluggable to any convolutional networks and is trained end-to-end. Our local similarity-aware feature embedding not only demonstrates faster convergence and boosted performance on two complex image retrieval datasets, its large margin nature also leads to superior generalization results under the large and open set scenarios of transfer learning and zero-shot learning on ImageNet 2010 and ImageNet-10K datasets.
C1 [Huang, Chen; Loy, Chen Change; Tang, Xiaoou] Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Peoples R China.
RP Huang, C (reprint author), Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Peoples R China.
EM chuang@ie.cuhk.edu.hk; ccloy@ie.cuhk.edu.hk; xtang@ie.cuhk.edu.hk
FU SenseTime Group Limited; Hong Kong Innovation and Technology Support
   Programme
FX This work is supported by SenseTime Group Limited and the Hong Kong
   Innovation and Technology Support Programme.
CR BELL S, 2015, TOG, V34
   Bendale A., 2015, CVPR
   Cui Y., 2016, CVPR
   Dalal  N., 2005, CVPR
   Deng J., 2010, ECCV
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Frome A., 2013, NIPS
   Frome A., 2007, ICCV
   Fu Z., 2015, CVPR
   Goldberger J., 2005, NIPS
   Hoi S. C. H., 2006, CVPR
   Huang C., 2016, CVPR
   Krause J., 2013, ICCVW
   Krizhevsky A., 2012, NIPS
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Le Q. V., 2012, ICML
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Martinez AM, 2001, IEEE T PATTERN ANAL, V23, P228, DOI 10.1109/34.908974
   Mensink T, 2013, IEEE T PATTERN ANAL, V35, P2624, DOI 10.1109/TPAMI.2013.83
   Norouzi M., 2014, ICLR
   Perronnin F., 2012, CVPR
   Rohrbach M., 2011, CVPR
   Rohrbach  Marcus, 2013, NIPS
   Sanchez J., 2011, CVPR
   Schroff F., 2015, CVPR
   Simo-Serra E., 2015, ARXIV14126537V2
   Song H. Oh, 2016, CVPR
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Szegedy C., 2015, CVPR
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wah C., 2011, CNSTR2011001 CALTECH
   Wang J, 2014, CVPR
   Wang X., 2015, ICCV
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Xing E. P., 2003, NIPS
   Xiong C., 2012, SIGKDD
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703004
DA 2019-06-15
ER

PT S
AU Huang, CD
   Sun, XW
   Xiong, JC
   Yao, Y
AF Huang, Chendi
   Sun, Xinwei
   Xiong, Jiechao
   Yao, Yuan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Split LBI: An Iterative Regularization Path with Structural Sparsity
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID ALGORITHMS; REGRESSION; SELECTION
AB An iterative regularization path with structural sparsity is proposed in this paper based on variable splitting and the Linearized Bregman Iteration, hence called Split LBI. Despite its simplicity, Split LBI outperforms the popular generalized Lasso in both theory and experiments. A theory of path consistency is presented that equipped with a proper early stopping, Split LBI may achieve model selection consistency under a family of Irrepresentable Conditions which can be weaker than the necessary and sufficient condition for generalized Lasso. Furthermore, some l(2) error bounds are also given at the minimax optimal rates. The utility and benefit of the algorithm are illustrated by applications on both traditional image denoising and a novel example on partial order ranking.
C1 [Huang, Chendi; Sun, Xinwei; Xiong, Jiechao; Yao, Yuan] Peking Univ, Beijing, Peoples R China.
   [Yao, Yuan] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
RP Huang, CD (reprint author), Peking Univ, Beijing, Peoples R China.
EM cdhuang@pku.edu.cn; sxwxiaoxiaohehe@pku.edu.cn; xiongjiechao@pku.edu.cn;
   yuany@ust.hk
FU National Basic Research Program of China [2012CB825501, 2015CB856000];
   NSFC [61071157, 11421110001]
FX The authors were supported in part by National Basic Research Program of
   China under grants 2012CB825501 and 2015CB856000, as well as NSFC grants
   61071157 and 11421110001.
CR Arnold TB, 2016, J COMPUT GRAPH STAT, V25, P1, DOI 10.1080/10618600.2015.1008638
   Bo W., 2012, IFAC P, V45, P83, DOI DOI 10.3182/20120711-3-BE-2027.00310
   Buhlmann P, 2003, J AM STAT ASSOC, V98, P324, DOI 10.1198/016214503000125
   Efron B, 2004, ANN STAT, V32, P407
   Goldstein T, 2009, SIAM J IMAGING SCI, V2, P323, DOI 10.1137/080725891
   Hoefling H, 2010, J COMPUT GRAPH STAT, V19, P984, DOI 10.1198/jcgs.2010.09208
   LEE J. D., 2013, ADV NEURAL INFORM PR, V26, P342
   Liu Ji, 2013, P 30 INT C MACH LEAR, P91
   Moeller Michael, 2012, THESIS
   Osher S, 2016, APPL COMPUT HARMON A, V41, P436, DOI 10.1016/j.acha.2016.01.002
   Ramdas A, 2016, J COMPUT GRAPH STAT, V25, P839, DOI 10.1080/10618600.2015.1054033
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Sharpnack J, 2012, P INT C ART INT STAT, P1028
   Tibshirani R, 2005, J R STAT SOC B, V67, P91, DOI 10.1111/j.1467-9868.2005.00490.x
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Tibshirani RJ, 2011, ANN STAT, V39, P1335, DOI 10.1214/11-AOS878
   Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793
   Vaiter S, 2013, IEEE T INFORM THEORY, V59, P2001, DOI 10.1109/TIT.2012.2233859
   Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2
   Ye GB, 2011, COMPUT STAT DATA AN, V55, P1552, DOI 10.1016/j.csda.2010.10.021
   Yin WT, 2008, SIAM J IMAGING SCI, V1, P143, DOI 10.1137/070703983
   Zhang F.Z., 2006, SCHUR COMPLEMENT ITS
   Zhao P, 2006, J MACH LEARN RES, V7, P2541
   Zhu YZ, 2017, J COMPUT GRAPH STAT, V26, P195, DOI 10.1080/10618600.2015.1114491
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702035
DA 2019-06-15
ER

PT S
AU Huang, G
   Guo, C
   Kusner, MJ
   Sun, Y
   Weinberger, KQ
   Sha, F
AF Huang, Gao
   Guo, Chuan
   Kusner, Matt J.
   Sun, Yu
   Weinberger, Kilian Q.
   Sha, Fei
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Supervised Word Mover's Distance
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Recently, a new document metric called the word mover's distance (WMD) has been proposed with unprecedented results on k NN-based document classification. The WMD elevates high-quality word embeddings to a document metric by formulating the distance between two documents as an optimal transport problem between the embedded words. However, the document distances are entirely unsupervised and lack a mechanism to incorporate supervision when available. In this paper we propose an efficient technique to learn a supervised metric, which we call the Supervised-WMD (S-WMD) metric. The supervised training minimizes the stochastic leave-one-out nearest neighbor classification error on a per-document level by updating an affine transformation of the underlying word embedding space and a word-imporance weight vector. As the gradient of the original WMD distance would result in an inefficient nested optimization problem, we provide an arbitrarily close approximation that results in a practical and efficient update rule. We evaluate S-WMD on eight real-world text classification tasks on which it consistently outperforms almost all of our 26 competitive baselines.
C1 [Huang, Gao; Guo, Chuan; Sun, Yu; Weinberger, Kilian Q.] Cornell Univ, Ithaca, NY 14853 USA.
   [Kusner, Matt J.] Univ Warwick, Alan Turing Inst, Coventry, W Midlands, England.
   [Sha, Fei] Univ Calif Los Angeles, Los Angeles, CA 90024 USA.
RP Huang, G (reprint author), Cornell Univ, Ithaca, NY 14853 USA.
EM gh349@cornell.edu; cg563@cornell.edu; mkusner@turing.ac.uk;
   ys646@cornell.edu; kqw4@cornell.edu; feisha@cs.ucla.edu
FU National Science Foundation [III-1618134, III-1526012, IIS-1149882];
   Bill and Melinda Gates Foundation
FX The authors are supported in part by the, III-1618134, III-1526012,
   IIS-1149882 grants from the National Science Foundation and the Bill and
   Melinda Gates Foundation. We also thank Dor Kedem for many insightful
   discussions.
CR Bertsimas D, 1997, INTRO LINEAR OPTIMIZ
   Blei D. M., 2003, JMLR
   Cardoso-Cachopo A, 2007, THESIS
   Chen Minmin, 2012, ICML
   Collobert R., 2008, P 25 ICML, P160, DOI [10.1145/1390156.1390177, DOI 10.1145/1390156.1390177]
   Cuturi M., 2014, INT C MACH LEARN, P685
   Cuturi M., 2014, JMLR
   CUTURI M., 2013, ADV NEURAL INFORM PR, V26, P2292
   Cuturi M, 2016, SIAM J IMAGING SCI, V9, P320, DOI 10.1137/15M1032600
   Davis JV, 2007, P 24 INT C MACH LEAR, P209, DOI DOI 10.1145/1273496.1273523
   DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9
   Frogner C., 2015, NIPS, P2044
   Gardner J.R., 2014, ICML, P937
   Glorot X, 2011, P 28 INT C MACH LEAR, P513
   Goldberger J., 2005, ADV NEURAL INFORM PR, P513
   Gopalan P. K., 2014, ADV NEURAL INFORM PR, V27, P3176
   Greene  D., 2006, P 23 INT C MACH LEAR, P377, DOI DOI 10.1145/1143844.1143892
   Hinton G.E., 2002, ADV NEURAL INFORM PR, P833
   Kusner M. J., 2015, ICML
   Levina E, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P251, DOI 10.1109/ICCV.2001.937632
   Levy O., 2014, NIPS
   Mikolov T., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951
   Mikolov  Tomas, 2013, WORKSH ICLR
   Mohan Ananth, 2011, JMLR P YAHOO LEARNIN, P77
   Ontrup J., 2001, NIPS
   Pele O, 2009, IEEE I CONF COMP VIS, P460, DOI 10.1109/ICCV.2009.5459199
   Perina A., 2013, P 2013 C NEUR INF PR, P10
   Robertson S. E., 1995, Text REtrieval Conference (TREC-3) (NIST SP 500-225), P109
   Rubner Y, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P59, DOI 10.1109/ICCV.1998.710701
   SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0
   Sanders N. J., 2011, SANDERS TWITTER SENT
   Tang D., 2014, P 52 ANN M ASS COMP, V1, P1555, DOI DOI 10.3115/V1/P14-1146
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang F., 2012, ECCV
   Wang X., 2009, TECHNICAL REPORT
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Yang L., 2006, DISTANCE METRIC LEAR, V2
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700103
DA 2019-06-15
ER

PT S
AU Huang, H
   Paulus, M
AF Huang, He
   Paulus, Martin
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning under uncertainty: a comparison between R-W and Bayesian
   approach
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID WIN-STAY
AB Accurately differentiating between what are truly unpredictably random and systematic changes that occur at random can have profound effect on affect and cognition. To examine the underlying computational principles that guide different learning behavior in an uncertain environment, we compared an R-W model and a Bayesian approach in a visual search task with different volatility levels. Both R-W model and the Bayesian approach reflected an individual's estimation of the environmental volatility, and there is a strong correlation between the learning rate in R-W model and the belief of stationarity in the Bayesian approach in different volatility conditions. In a low volatility condition, R-W model indicates that learning rate positively correlates with lose-shift rate, but not choice optimality (inverted U shape). The Bayesian approach indicates that the belief of environmental stationarity positively correlates with choice optimality, but not lose-shift rate (inverted U shape). In addition, we showed that comparing to Expert learners, individuals with high lose-shift rate (sub-optimal learners) had significantly higher learning rate estimated from R-W model and lower belief of stationarity from the Bayesian model.
C1 [Huang, He; Paulus, Martin] Laureate Inst Brain Res, Tulsa, OK 74133 USA.
RP Huang, H (reprint author), Laureate Inst Brain Res, Tulsa, OK 74133 USA.
EM crane081@gmail.com; mpaulus@laureateinstitute.org
CR Behrens TEJ, 2007, NAT NEUROSCI, V10, P1214, DOI 10.1038/nn1954
   BLAKELY E, 1988, PSYCHOL REC, V38, P111, DOI 10.1007/BF03395009
   Bonawitz E, 2014, COGNITIVE PSYCHOL, V74, P35, DOI 10.1016/j.cogpsych.2014.06.003
   Browning M, 2015, NAT NEUROSCI, V18, P590, DOI 10.1038/nn.3961
   Frederick S., 2002, HEURISTICS BIASES PS, P49, DOI DOI 10.1017/CB09780511808098.004
   Gershman SJ, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004567
   Lee MD, 2011, COGN SYST RES, V12, P164, DOI 10.1016/j.cogsys.2010.07.007
   Mathys CD, 2014, FRONT HUM NEUROSCI, V8, DOI 10.3389/fnhum.2014.00825
   Rescorla R. A, 1972, CLASSICAL CONDITION, V21, P64, DOI [10.1101/gr.110528.110, DOI 10.1037/A0030892]
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Worthy DA, 2014, J MATH PSYCHOL, V59, P41, DOI 10.1016/j.jmp.2013.10.001
   Yu A., 2009, ADV NEURAL INFORM PR, P1873, DOI DOI 10.1371/JOURNAL.PONE.0099909
   Yu A J., 2014, DECISION, V1, P275, DOI [10.1037/dec0000013, DOI 10.1037/DEC0000013]
NR 13
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703045
DA 2019-06-15
ER

PT S
AU Huang, KJ
   Fu, X
   Sidiropoulos, ND
AF Huang, Kejun
   Fu, Xiao
   Sidiropoulos, Nicholas D.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Anchor-Free Correlated Topic Modeling: Identifiability and Algorithm
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB In topic modeling, many algorithms that guarantee identifiability of the topics have been developed under the premise that there exist anchor words - i.e., words that only appear (with positive probability) in one topic. Follow-up work has resorted to three or higher-order statistics of the data corpus to relax the anchor word assumption. Reliable estimates of higher-order statistics are hard to obtain, however, and the identification of topics under those models hinges on uncorrelatedness of the topics, which can be unrealistic. This paper revisits topic modeling based on second-order moments, and proposes an anchor-free topic mining framework. The proposed approach guarantees the identification of the topics under a much milder condition compared to the anchor-word assumption, thereby exhibiting much better robustness in practice. The associated algorithm only involves one eigen-decomposition and a few small linear programs. This makes it easy to implement and scale up to very large problem instances. Experiments using the TDT2 and Reuters-21578 corpus demonstrate that the proposed anchor-free approach exhibits very favorable performance (measured using coherence, similarity count, and clustering accuracy metrics) compared to the prior art.
C1 [Huang, Kejun; Fu, Xiao; Sidiropoulos, Nicholas D.] Univ Minnesota, Dept Elect & Comp Engn, Minneapolis, MN 55455 USA.
RP Huang, KJ (reprint author), Univ Minnesota, Dept Elect & Comp Engn, Minneapolis, MN 55455 USA.
EM huang663@umn.edu; xfu@umn.edu; nikos@ece.umn.edu
FU National Science Foundation (NSF) [NSF-ECCS 1608961, NSF IIS-1247632];
   Digital Technology Initiative (DTI) Seed Grant, University of Minnesota
FX This work is supported in part by the National Science Foundation (NSF)
   under the project numbers NSF-ECCS 1608961 and NSF IIS-1247632 and in
   part by the Digital Technology Initiative (DTI) Seed Grant, University
   of Minnesota.
CR Anandkumar A., 2012, TECHNICAL REPORT
   Anandkumar A., 2013, ADV NEURAL INFORM PR, P1986
   Anandkumar A., 2012, ADV NEURAL INFORM PR, P917
   Arora S., 2012, P 44 ANN ACM S THEOR, P145
   Arora S., 2013, P ICML 13
   Arora S, 2012, ANN IEEE SYMP FOUND, P1, DOI 10.1109/FOCS.2012.49
   Bittorf V., 2012, ADV NEURAL INFORM PR, V12, P1214
   Blei DM, 2012, COMMUN ACM, V55, P77, DOI 10.1145/2133806.2133826
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Cai D, 2011, IEEE T KNOWL DATA EN, V23, P902, DOI 10.1109/TKDE.2010.165
   Donoho D., 2003, P NIPS 2013, V16
   Fu X, 2015, IEEE T SIGNAL PROCES, V63, P2306, DOI 10.1109/TSP.2015.2404577
   Gillis N, 2014, SIAM J IMAGING SCI, V7, P1420, DOI 10.1137/130946782
   Gillis N, 2014, IEEE T PATTERN ANAL, V36, P698, DOI 10.1109/TPAMI.2013.226
   Gillis N, 2013, SIAM J MATRIX ANAL A, V34, P1189, DOI 10.1137/120900629
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   Huang K., 2015, P SIAM C DAT MIN SDM
   Huang KJ, 2014, IEEE T SIGNAL PROCES, V62, P211, DOI 10.1109/TSP.2013.2285514
   Kumar A., 2012, P ICML 12
   Ma W.-K., 2010, CONVEX OPTIMIZATION, P229
   Xu W., 2003, P 26 SIGIR TOR ON CA, P267, DOI DOI 10.1145/860435.860485
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702055
DA 2019-06-15
ER

PT S
AU Huang, RT
   Lattimore, T
   Gyorgy, A
   Szepesvari, C
AF Huang, Ruitong
   Lattimore, Tor
   Gyorgy, Andras
   Szepesvari, Csaba
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Following the Leader and Fast Rates in Linear Prediction: Curved
   Constraint Sets and Other Regularities
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB The follow the leader (FTL) algorithm, perhaps the simplest of all online learning algorithms, is known to perform well when the loss functions it is used on are positively curved. In this paper we ask whether there are other "lucky" settings when FTL achieves sublinear, "small" regret. In particular, we study the fundamental problem of linear prediction over a non-empty convex, compact domain. Amongst other results, we prove that the curvature of the boundary of the domain can act as if the losses were curved: In this case, we prove that as long as the mean of the loss vectors have positive lengths bounded away from zero, FTL enjoys a logarithmic growth rate of regret, while, e.g., for polyhedral domains and stochastic data it enjoys finite expected regret. Building on a previously known meta-algorithm, we also get an algorithm that simultaneously enjoys the worst-case guarantees and the bound available for FTL.
C1 [Huang, Ruitong; Szepesvari, Csaba] Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.
   [Lattimore, Tor] Indiana Univ, Sch Informat & Comp, Bloomington, IN 47405 USA.
   [Gyorgy, Andras] Imperial Coll London, Dept Elect & Elect Engn, London, England.
RP Huang, RT (reprint author), Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.
EM ruitong@ualberta.ca; tor.lattimore@gmail.com; a.gyorgy@imperial.ac.uk;
   szepesva@ualberta.ca
FU Alberta Innovates Technology Futures through the Alberta Ingenuity
   Centre for Machine Learning; NSERC
FX This work was supported in part by the Alberta Innovates Technology
   Futures through the Alberta Ingenuity Centre for Machine Learning and by
   NSERC. During part of this work, T. Lattimore was with the Department of
   Computing Science, University of Alberta.
CR Abbasi-Yadkori Y., 2010, FORCED EXPLORATION B
   Abernethy J., 2008, 21 ANN C LEARN THEOR
   Bertsekas D.P., 1999, NONLINEAR PROGRAMMIN
   Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Foster D. J., 2015, ADV NEURAL INFORM PR, P3357
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Gaivoronski AA, 2000, ANN OPER RES, V100, P165, DOI 10.1023/A:1019271201970
   Garber D., 2015, P 32 INT C MACH LEAR, V37, P541
   Hazan E., 2007, P 21 ANN C ADV NEUR, V20, P65
   Hazan E, 2007, MACH LEARN, V69, P169, DOI 10.1007/s10994-007-5016-8
   Huang R., 2016, FOLLOWING LEADER FAS
   Kotlowski W., 2016, ALGORITHMIC LEARNING
   Levitin ES, 1966, USSR COMP MATH MATH, V6, P1, DOI [10.1016/0041-5553(66)90114-5, DOI 10.1016/0041-5553(66)90114-5]
   McMahan H. B., 2010, FOLLOW THE REGULARIZ
   Merhav N., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P413, DOI 10.1145/130385.130430
   Orabona F., 2014, P 15 INT C ART INT S, P823
   Rakhlin Alexander, 2013, COLT, P993
   Sani  Amir, 2014, ADV NEURAL INFORM PR, V27, P810
   Schneider R, 2014, ENCYCLOP MATH APPL, V151, P1
   Shalev-Shwartz S., 2009, ADV NEURAL INFORM PR, V21, P1457
   Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   van Erven T, 2015, J MACH LEARN RES, V16, P1793
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703091
DA 2019-06-15
ER

PT S
AU Huang, TK
   Li, LH
   Vartanian, A
   Amershi, S
   Zhu, XJ
AF Huang, Tzu-Kuo
   Li, Lihong
   Vartanian, Ara
   Amershi, Saleema
   Zhu, Xiaojin
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Active Learning with Oracle Epiphany
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We present a theoretical analysis of active learning with more realistic interactions with human oracles. Previous empirical studies have shown oracles abstaining on difficult queries until accumulating enough information to make label decisions. We formalize this phenomenon with an "oracle epiphany model" and analyze active learning query complexity under such oracles for both the realizable and the agnostic cases. Our analysis shows that active learning is possible with oracle epiphany, but incurs an additional cost depending on when the epiphany happens. Our results suggest new, principled active learning approaches with realistic oracles.
C1 [Huang, Tzu-Kuo] Uber Adv Technol Grp, Pittsburgh, PA 15201 USA.
   [Li, Lihong; Amershi, Saleema] Microsoft Res, Redmond, WA 98052 USA.
   [Vartanian, Ara; Zhu, Xiaojin] Univ Wisconsin, Madison, WI USA.
RP Huang, TK (reprint author), Uber Adv Technol Grp, Pittsburgh, PA 15201 USA.
FU NSF [IIS-0953219, IIS-1623605, DGE-1545481, CCF-1423237]; University of
   Wisconsin-Madison Graduate School; Wisconsin Alumni Research Foundation
FX This work is supported in part by NSF grants IIS-0953219, IIS-1623605,
   DGE-1545481, CCF-1423237, and by the University of Wisconsin-Madison
   Graduate School with funding from the Wisconsin Alumni Research
   Foundation.
CR Balcan M.-F., 2006, ICML, P65, DOI DOI 10.1145/1143844.1143853]
   Beygelzimer Alina, 2010, P ADV NEUR INF PROC, P199
   COHN D, 1994, MACH LEARN, V15, P201, DOI 10.1023/A:1022673506211
   Dasgupta Sanjoy, 2007, P C ADV NEUR INF PRO, P353
   Donmez P., 2008, P 17 ACM C INF KNOWL, P619, DOI DOI 10.1145/1458082.1458165
   El-Yaniv R, 2010, J MACH LEARN RES, V11, P1605
   Hanneke S, 2014, FOUND TRENDS MACH LE, V7, DOI 10.1561/2200000037
   Hsu Daniel, 2010, THESIS
   Huang Tzu-Kuo, 2015, NIPS, P2737
   Kakade S. M., 2009, ADV NEURAL INFORM PR, V21
   Karampatziakis N., 2011, P C UNC ART INT, P392
   Kulesza T., 2014, P 32 ANN ACM C HUM F, P3075, DOI DOI 10.1145/2556288.2557238
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Newell E, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P3155, DOI 10.1145/2858036.2858490
   Sarkar Advait, 2016, CHI
   Shah N. B., 2015, ADV NEURAL INFORM PR, V1, P1
   Zhang Chicheng, 2014, ADV NEURAL INFORM PR, P442
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701012
DA 2019-06-15
ER

PT S
AU Huggins, JH
   Campbell, T
   Broderick, T
AF Huggins, Jonathan H.
   Campbell, Trevor
   Broderick, Tamara
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Coresets for Scalable Bayesian Logistic Regression
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB The use of Bayesian methods in large-scale data settings is attractive because of the rich hierarchical models, uncertainty quantification, and prior specification they provide. Standard Bayesian inference algorithms are computationally expensive, however, making their direct application to large datasets difficult or infeasible. Recent work on scaling Bayesian inference has focused on modifying the underlying algorithms to, for example, use only a random data subsample at each iteration. We leverage the insight that data is often redundant to instead obtain a weighted subset of the data (called a coreset) that is much smaller than the original dataset. We can then use this small coreset in any number of existing posterior inference algorithms without modification. In this paper, we develop an efficient coreset construction algorithm for Bayesian logistic regression models. We provide theoretical guarantees on the size and approximation quality of the coreset - both for fixed, known datasets, and in expectation for a wide class of data generative models. Crucially, the proposed approach also permits efficient construction of the coreset in both streaming and parallel settings, with minimal additional effort. We demonstrate the efficacy of our approach on a number of synthetic and real-world datasets, and find that, in practice, the size of the coreset is independent of the original dataset size. Furthermore, constructing the coreset takes a negligible amount of time compared to that required to run MCMC on it.
C1 [Huggins, Jonathan H.; Campbell, Trevor; Broderick, Tamara] MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA.
RP Huggins, JH (reprint author), MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA.
EM fjhuggins@mit.edu; tdjc@mit.edu; tbroderick@csail.mit.edu
FU Office of Naval Research under ONR MURI [N000141110688]; National
   Defense Science and Engineering Graduate (NDSEG) Fellowship
FX All authors are supported by the Office of Naval Research under ONR MURI
   grant N000141110688. JHH is supported by a National Defense Science and
   Engineering Graduate (NDSEG) Fellowship.
CR AGARWAL P., 2005, COMBINATORIAL COMPUT, V52, P1
   Arthur D, 2007, P 18 ANN ACM SIAM S, P1027, DOI DOI 10.1145/1283383.1283494
   Bachem O., 2016, AAAI C ART INT
   Bardenet R., 2015, MARKOV CHAIN MONTE C
   Betancourt M. J., 2015, INT C MACH LEARN
   Broderick T., 2013, ADV NEURAL INFORM PR
   Campbell T., 2015, ADV NEURAL INFORM PR
   Entezari R., 2016, LIKELIHOOD INFLATING
   Feldman D., 2011, ADV NEURAL INFORM PR, P2142
   Feldman D., 2011, S THEOR COMP
   Feldman D, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1434
   Gelman A, 2008, ANN APPL STAT, V2, P1360, DOI 10.1214/08-AOAS191
   GEORGE EI, 1993, J AM STAT ASSOC, V88, P881, DOI 10.2307/2290777
   Haario H, 2001, BERNOULLI, V7, P223, DOI 10.2307/3318737
   Han L., 2016, LOCAL UNCERTAINTY SA
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Lucic M., 2016, INT C ART INT STAT
   Maclaurin D., 2014, UNCERTAINTY ARTIFICI
   Rabinovich M., 2015, VARIATIONAL CONSENSU
   Roberts GO, 1996, BERNOULLI, V2, P341, DOI DOI 10.2307/3318418
   Scott S. L., 2013, BAYES, V250
   Srivastava S., 2015, INT C ART INT STAT
   Vollmer SJ, 2016, J MACH LEARN RES, V17, P1
   Welling  M., 2011, INT C MACH LEARN
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704011
DA 2019-06-15
ER

PT S
AU Hyvarinen, A
   Morioka, H
AF Hyvarinen, Aapo
   Morioka, Hiroshi
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Unsupervised Feature Extraction by Time-Contrastive Learning and
   Nonlinear ICA
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID INDEPENDENT COMPONENT ANALYSIS; BLIND SOURCE SEPARATION; SLOW FEATURE
   ANALYSIS; NETWORKS
AB Nonlinear independent component analysis (ICA) provides an appealing framework for unsupervised feature learning, but the models proposed so far are not identifiable. Here, we first propose a new intuitive principle of unsupervised deep learning from time series which uses the nonstationary structure of the data. Our learning principle, time-contrastive learning (TCL), finds a representation which allows optimal discrimination of time segments (windows). Surprisingly, we show how TCL can be related to a nonlinear ICA model, when ICA is redefined to include temporal nonstationarities. In particular, we show that TCL combined with linear ICA estimates the nonlinear ICA model up to point-wise transformations of the sources, and this solution is unique - thus providing the first identifiability result for nonlinear ICA which is rigorous, constructive, as well as very general.
C1 [Hyvarinen, Aapo; Morioka, Hiroshi] Univ Helsinki, Dept Comp Sci, Helsinki, Finland.
   [Hyvarinen, Aapo; Morioka, Hiroshi] Univ Helsinki, HIIT, Helsinki, Finland.
   [Hyvarinen, Aapo] UCL, Gatsby Computat Neurosci Unit, London, England.
RP Hyvarinen, A (reprint author), Univ Helsinki, Dept Comp Sci, Helsinki, Finland.; Hyvarinen, A (reprint author), Univ Helsinki, HIIT, Helsinki, Finland.; Hyvarinen, A (reprint author), UCL, Gatsby Computat Neurosci Unit, London, England.
FU JSPS KAKENHI [16J08502]; Academy of Finland
FX This research was supported in part by JSPS KAKENHI 16J08502 and the
   Academy of Finland.
CR Almeida LB, 2004, J MACH LEARN RES, V4, P1297
   Brookes MJ, 2011, P NATL ACAD SCI USA, V108, P16783, DOI 10.1073/pnas.1112685108
   COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9
   de Pasquale F, 2012, NEURON, V74, P753, DOI 10.1016/j.neuron.2012.03.031
   Dinh L., 2015, ARXIV14108516CSLG
   Foldiak P, 1991, NEURAL COMPUT, V3, P194, DOI 10.1162/neco.1991.3.2.194
   Glorot X., 2010, AISTATS 10
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Goroshin R., 2015, ARXIV150402518
   Gutmann M. U., 2014, ARXIV14074981STATCO
   Gutmann MU, 2012, J MACH LEARN RES, V13, P307
   Harmeling S, 2003, NEURAL COMPUT, V15, P1089, DOI 10.1162/089976603765202677
   Hinton G. E., 1994, ADV NEURAL INF PROCE
   Hinton GE, 2007, TRENDS COGN SCI, V11, P428, DOI 10.1016/j.tics.2007.09.004
   Hyvarinen A, 1999, IEEE T NEURAL NETWOR, V10, P626, DOI 10.1109/72.761722
   Hyvarinen A, 1999, NEURAL NETWORKS, V12, P429, DOI 10.1016/S0893-6080(98)00140-3
   Hyvarinen A, 2001, IEEE T NEURAL NETWOR, V12, P1471, DOI 10.1109/72.963782
   Hyvarinen A, 2009, COMPUT IMAGING VIS, V39, P1
   Ioffe S., 2015, CORR
   Jutten C, 2010, HANDBOOK OF BLIND SOURCE SEPARATION: INDEPENDENT COMPONENT ANALYSIS AND APPLICATIONS, P549, DOI 10.1016/B978-0-12-374726-6.00019-9
   Kingma D. P., 2014, ARXIV13126114STATML
   MATSUOKA K, 1995, NEURAL NETWORKS, V8, P411, DOI 10.1016/0893-6080(94)00083-X
   Mobahi H., 2009, P 26 ANN INT C MACH, P737
   Pham DT, 2001, IEEE T SIGNAL PROCES, V49, P1837, DOI 10.1109/78.942614
   Ramkumar P, 2012, HUM BRAIN MAPP, V33, P1648, DOI 10.1002/hbm.21303
   Sprekeler H, 2014, J MACH LEARN RES, V15, P921
   Springenberg JT, 2012, LECT NOTES COMPUT SC, V7663, P347, DOI 10.1007/978-3-642-34475-6_42
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tan Y, 2001, IEEE T NEURAL NETWOR, V12, P124, DOI 10.1109/72.896801
   Valpola H., 2015, ADV INDEPENDENT COMP, P143
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wiskott L, 2002, NEURAL COMPUT, V14, P715, DOI 10.1162/089976602317318938
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703031
DA 2019-06-15
ER

PT S
AU Ito, S
   Fujimaki, R
AF Ito, Shinji
   Fujimaki, Ryohei
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Large-Scale Price Optimization via Network Flow
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID ENERGY MINIMIZATION
AB This paper deals with price optimization, which is to find the best pricing strategy that maximizes revenue or profit, on the basis of demand forecasting models. Though recent advances in regression technologies have made it possible to reveal price-demand relationship of a large number of products, most existing price optimization methods, such as mixed integer programming formulation, cannot handle tens or hundreds of products because of their high computational costs. To cope with this problem, this paper proposes a novel approach based on network flow algorithms. We reveal a connection between supermodularity of the revenue and cross elasticity of demand. On the basis of this connection, we propose an efficient algorithm that employs network flow algorithms. The proposed algorithm can handle hundreds or thousands of products, and returns an exact optimal solution under an assumption regarding cross elasticity of demand. Even if the assumption does not hold, the proposed algorithm can efficiently find approximate solutions as good as other state-of-the-art methods, as empirical results show.
C1 [Ito, Shinji; Fujimaki, Ryohei] NEC Corp Ltd, Tokyo, Japan.
RP Ito, S (reprint author), NEC Corp Ltd, Tokyo, Japan.
EM s-ito@me.jp.nec.com; rfujimaki@nec-labs.com
CR Bitran G., 2003, Manufacturing & Service Operations Management, V5, P203, DOI 10.1287/msom.5.3.203.16031
   Boros E, 2002, DISCRETE APPL MATH, V123, P155, DOI 10.1016/S0166-218X(01)00336-5
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Caro F, 2012, OPER RES, V60, P1404, DOI 10.2139/ssrn.1731402
   Cormen T H., 2009, INTRO ALGORITHMS
   Ferreira K. J., 2015, MANUFACTURING SERVIC, V18, P69
   Gorelick L, 2014, PROC CVPR IEEE, P1154, DOI 10.1109/CVPR.2014.151
   Ito S., 2016, ARXIV E PRINTS
   Klein R., 2008, REVENUE MANAGEMENT
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177
   Kolmogorov V, 2007, IEEE T PATTERN ANAL, V29, P1274, DOI 10.1109/TPAMI.2007.1031
   Koushik D, 2012, INTERFACES, V42, P45, DOI 10.1287/inte.1110.0620
   Lee SH, 2011, THESIS
   Marshall A., 1920, PRINCIPLES EC
   McGill JI, 1999, TRANSPORT SCI, V33, P233, DOI 10.1287/trsc.33.2.233
   Natter M., 2009, MARK INTELL PLAN, V1, P17
   Phillips R.L., 2005, PRICING REVENUE OPTI
   Rother C., 2007, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/CVPR.2007.383203
   Rusmevichientong P, 2006, OPER RES, V54, P82, DOI 10.1287/opre.1050.0252
   STONE CJ, 1985, ANN STAT, V13, P689, DOI 10.1214/aos/1176349548
   Tang M, 2014, LECT NOTES COMPUT SC, V8693, P691, DOI 10.1007/978-3-319-10602-1_45
   Wang J., 2015, P 21 ACM SIGKDD INT, P1245
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702048
DA 2019-06-15
ER

PT S
AU Iwata, T
   Yamada, M
AF Iwata, Tomoharu
   Yamada, Makoto
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Multi-view Anomaly Detection via Robust Probabilistic Latent Variable
   Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We propose probabilistic latent variable models for multi-view anomaly detection, which is the task of finding instances that have inconsistent views given multi-view data. With the proposed model, all views of a non-anomalous instance are assumed to be generated from a single latent vector. On the other hand, an anomalous instance is assumed to have multiple latent vectors, and its different views are generated from different latent vectors. By inferring the number of latent vectors used for each instance with Dirichlet process priors, we obtain multiview anomaly scores. The proposed model can be seen as a robust extension of probabilistic canonical correlation analysis for noisy multi-view data. We present Bayesian inference procedures for the proposed model based on a stochastic EM algorithm. The effectiveness of the proposed model is demonstrated in terms of performance when detecting multi-view anomalies.
C1 [Iwata, Tomoharu] NTT Commun Sci Labs, Kyoto, Japan.
   [Yamada, Makoto] Kyoto Univ, Kyoto, Japan.
RP Iwata, T (reprint author), NTT Commun Sci Labs, Kyoto, Japan.
EM iwata.tomoharu@lab.ntt.co.jp; makoto.m.yamada@ieee.org
FU KAKENHI [16K16114]
FX MY was supported by KAKENHI 16K16114.
CR Aleskerov E., 1997, Proceedings of the IEEE/IAFE 1997 Computational Intelligence for Financial Engineering (CIFEr) (Cat. No.97TH8304), P220, DOI 10.1109/CIFER.1997.618940
   Alvarez A. M., 2013, P ACM INT C INF KNOW
   Antonie Maria-Luiza, 2001, P 2 INT WORKSH MULT, P94
   Archambeau C., 2006, P 23 INT C MACH LEAR, P33
   Bach F. R., 2005, 688 U CAL DEP STAT
   Chandola V, 2009, ACM COMPUT SURV, V41, DOI 10.1145/1541880.1541882
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Christoudias C. M., 2008, P 24 C UNV ART INT U
   Ek CH, 2008, LECT NOTES COMPUT SC, V5237, P62, DOI 10.1007/978-3-540-85853-9_6
   Fanaee-T H, 2016, KNOWL-BASED SYST, V98, P130, DOI 10.1016/j.knosys.2016.01.027
   Gao J., 2010, P 16 ACM SIGKDD INT, P813, DOI DOI 10.1145/1835804.1835907
   Herlocker JL, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P230, DOI 10.1145/312624.312682
   Kevin Duh, 2013, ACM T SPEECH LANG PR, V10, P1, DOI DOI 10.1145/2442076.2442077
   Lawrence ND, 2004, ADV NEUR IN, V16, P329
   Liu A. Y., 2012, 2012 IEEE CS Security and Privacy Workshops (SPW 2012), P117, DOI 10.1109/SPW.2012.18
   Portnoy  L., 2001, P ACM CSS WORKSH DAT
   Scholkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965
   SETHURAMAN J, 1994, STAT SINICA, V4, P639
   Shekhar S., 2002, Intelligent Data Analysis, V6, P451
   Song XY, 2007, IEEE T KNOWL DATA EN, V19, P631, DOI 10.1109/TKDE.2007.1009
   Sun JM, 2005, Fifth IEEE International Conference on Data Mining, Proceedings, P418
   Tipping ME, 1999, J ROY STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196
   Wang X, 2009, IEEE DATA MINING, P1034, DOI 10.1109/ICDM.2009.95
   Wu G, 2011, IEEE INT C CL COMP, P503, DOI 10.1109/CLUSTER.2011.62
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703092
DA 2019-06-15
ER

PT S
AU Jabbari, S
   Rogers, R
   Roth, A
   Wu, ZS
AF Jabbari, Shahin
   Rogers, Ryan
   Roth, Aaron
   Wu, Zhiwei Steven
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning from Rational Behavior: Predicting Solutions to Unknown Linear
   Programs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We define and study the problem of predicting the solution to a linear program (LP) given only partial information about its objective and constraints. This generalizes the problem of learning to predict the purchasing behavior of a rational agent who has an unknown objective function, that has been studied under the name "Learning from Revealed Preferences". We give mistake bound learning algorithms in two settings: in the first, the objective of the LP is known to the learner but there is an arbitrary, fixed set of constraints which are unknown. Each example is defined by an additional known constraint and the goal of the learner is to predict the optimal solution of the LP given the union of the known and unknown constraints. This models the problem of predicting the behavior of a rational agent whose goals are known, but whose resources are unknown. In the second setting, the objective of the LP is unknown, and changing in a controlled way. The constraints of the LP may also change every day, but are known. An example is given by a set of constraints and partial information about the objective, and the task of the learner is again to predict the optimal solution of the partially known LP.
C1 [Jabbari, Shahin; Rogers, Ryan; Roth, Aaron; Wu, Zhiwei Steven] Univ Penn, Philadelphia, PA 19104 USA.
RP Jabbari, S (reprint author), Univ Penn, Philadelphia, PA 19104 USA.
EM jabbari@cis.upenn.edu; ryrogers@sas.upenn.edu; aaroth@cis.upenn.edu;
   wuzhiwei@cis.upenn.edu
CR AMIN K., 2015, P 29 AAAI C ART INT, P770
   Balcan MF, 2014, LECT NOTES COMPUT SC, V8877, P338, DOI 10.1007/978-3-319-13129-0_28
   Beigman E., 2006, P 7 ACM C EL COMM, P36
   Belloni Alexandre, 2015, P 28 C LEARN THEOR, P240
   Bhaskar U, 2014, ANN IEEE SYMP FOUND, P31, DOI 10.1109/FOCS.2014.12
   BLUM A., 2015, P 16 ACM C EC COMP E, P601
   Collins M, 2002, PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P1
   COLLINS M, 2000, P 17 INT C MACH LEAR, P175
   Daniely A., 2014, COLT, P287
   GROTSCHEL M., 1993, ALGORITHMS COMBINATO, V2
   Lafferty J. D., 2001, P 18 INT C MACH LEAR, V3, P282, DOI DOI 10.1038/NPROT.2006.61
   Littlestone N., 1988, Machine Learning, V2, P285, DOI 10.1023/A:1022869011914
   Roth A, 2016, STOC'16: PROCEEDINGS OF THE 48TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P949, DOI 10.1145/2897518.2897579
   VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972
   Zadimoghaddam M, 2012, LECT NOTES COMPUTER, P114
   2010, PREFERENCE LEARNING, P1
NR 16
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701007
DA 2019-06-15
ER

PT S
AU Jain, L
   Jamieson, K
   Nowak, R
AF Jain, Lalit
   Jamieson, Kevin
   Nowak, Robert
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Finite Sample Prediction and Recovery Bounds for Ordinal Embedding
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB The goal of ordinal embedding is to represent items as points in a low-dimensional Euclidean space given a set of constraints like "item i is closer to item j than item k". Ordinal constraints like this often come from human judgments. The classic approach to solving this problem is known as non-metric multidimensional scaling. To account for errors and variation in judgments, we consider the noisy situation in which the given constraints are independently corrupted by reversing the correct constraint with some probability. The ordinal embedding problem has been studied for decades, but most past work pays little attention to the question of whether accurate embedding is possible, apart from empirical studies. This paper shows that under a generative data model it is possible to learn the correct embedding from noisy distance comparisons. In establishing this fundamental result, the paper makes several new contributions. First, we derive prediction error bounds for embedding from noisy distance comparisons by exploiting the fact that the rank of a distance matrix of points in R-d is at most d + 2. These bounds characterize how well a learned embedding predicts new comparative judgments. Second, we show that the underlying embedding can be recovered by solving a simple convex optimization. This result is highly non-trivial since we show that the linear map corresponding to distance comparisons is non-invertible, but there exists a nonlinear map that is invertible. Third, two new algorithms for ordinal embedding are proposed and evaluated in experiments.
C1 [Jain, Lalit] Univ Michigan, Ann Arbor, MI 48109 USA.
   [Jamieson, Kevin] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Nowak, Robert] Univ Wisconsin, Madison, WI 53706 USA.
RP Jain, L (reprint author), Univ Michigan, Ann Arbor, MI 48109 USA.
EM lalitj@umich.edu; kjamieson@berkeley.edu; rdnowak@wisc.edu
FU NSF [CCF-1218189, IIS-1447449]; NIH [1 U54 AI117924-01]; AFOSR
   [FA9550-13-1-0138]; ONR [N00014-15-1-2620, N00014-13-1-0129]
FX This work was partially supported by the NSF grants CCF-1218189 and
   IIS-1447449, the NIH grant 1 U54 AI117924-01, the AFOSR grant
   FA9550-13-1-0138, and by ONR awards N00014-15-1-2620, and
   N00014-13-1-0129. We would also like to thank Amazon Web Services for
   providing the computational resources used for running our simulations.
CR Agarwal S, 2007, J MACHINE LEARNING R, P11
   Arias-Castro  Ery, 2015, ARXIV150102861
   Boucheron Stephane, 2005, ESAIM-PROBAB STAT, V9, P323, DOI DOI 10.1051/ps:2005018
   Dattorro J., 2011, CONVEX OPTIMIZATION
   Davenport Mark A, 2014, INFORM INFERENCE, V3
   Jamieson Kevin G., 2011, 2011 49th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P1077
   Jamieson Kevin G, 2015, ADV NEURAL INFORM PR, P2638
   Joel A, 2015, INTRO MATRIX CONCENT
   Kleindessner M., 2014, COLT, P40
   KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P115, DOI 10.1007/BF02289694
   Lu Y, 2015, ANN ALLERTON CONF, P1473, DOI 10.1109/ALLERTON.2015.7447183
   McFee B, 2011, J MACH LEARN RES, V12, P491
   Oymak S., 2015, ARXIV150704793
   Park  D., 2015, P INT C MACH LEARN I
   Rao  Nikhil, 2013, NIPS WORKSH GREED AL
   Shen J., 2016, ARXIV160501656
   SHEPARD RN, 1962, PSYCHOMETRIKA, V27, P125, DOI 10.1007/BF02289630
   Tamuz O., 2011, P 28 INT C MACH LEAR, P673
   Tarazaga P, 2009, LINEAR MULTILINEAR A, V57, P651, DOI 10.1080/03081080802079259
   Terada Y., 2014, P 31 INT C MACH LEAR, P847
   van der Maaten L. J. P., 2012, MACH LEARN SIGN PROC, P1
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704078
DA 2019-06-15
ER

PT S
AU Jain, P
   Rao, N
   Dhillon, I
AF Jain, Prateek
   Rao, Nikhil
   Dhillon, Inderjit
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Structured Sparse Regression via Greedy Hard-thresholding
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID ALGORITHMS
AB Several learning applications require solving high-dimensional regression problems where the relevant features belong to a small number of (overlapping) groups. For very large datasets and under standard sparsity constraints, hard thresholding methods have proven to be extremely efficient, but such methods require NP hard projections when dealing with overlapping groups. In this paper, we show that such NP-hard projections can not only be avoided by appealing to submodular optimization, but such methods come with strong theoretical guarantees even in the presence of poorly conditioned data (i.e. say when two features have correlation >= 0.99), which existing analyses cannot handle. These methods exhibit an interesting computation-accuracy trade-off and can be extended to significantly harder problems such as sparse overlapping groups. Experiments on both real and synthetic data validate our claims and demonstrate that the proposed methods are orders of magnitude faster than other greedy and convex relaxation techniques for learning with group-structured sparsity.
C1 [Jain, Prateek] Microsoft Res India, Bengaluru, Karnataka, India.
   [Rao, Nikhil] Technicolor, Issy Les Moulineaux, France.
   [Dhillon, Inderjit] UT Austin, Austin, TX USA.
RP Jain, P (reprint author), Microsoft Res India, Bengaluru, Karnataka, India.
CR Bach Francis, 2010, ARXIV10104207
   Baraniuk RG, 2010, IEEE T INFORM THEORY, V56, P1982, DOI 10.1109/TIT.2010.2040894
   Bhan N, 2013, IEEE INT SYMP INFO, P1037, DOI 10.1109/ISIT.2013.6620384
   Blumensath T, 2010, IEEE J-STSP, V4, P298, DOI 10.1109/JSTSP.2010.2042411
   Blumensath T, 2009, IEEE T INFORM THEORY, V55, P1872, DOI 10.1109/TIT.2009.2013003
   Hegde C, 2015, IEEE T INFORM THEORY, V61, P5129, DOI 10.1109/TIT.2015.2457939
   Huang JZ, 2011, J MACH LEARN RES, V12, P3371
   Jacob L, 2009, P 26 ANN INT C MACH, P433, DOI DOI 10.1145/1553374.1553431
   Jain P., 2011, P ADV NEUR INF PROC, V24, P1215
   JAIN P., 2014, ADV NEURAL INFORM PR, P685
   Jenatton R., 2010, P 27 INT C MACH LEAR, P487
   Kyrillidis A., 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P2216, DOI 10.1109/ISIT.2012.6283847
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Rao N., 2012, P 15 INT C ART INT S, P942
   Rao N., 2013, ADV NEURAL INFORM PR, P2202
   Rao N, 2015, IEEE T SIGNAL PROCES, V63, P5798, DOI 10.1109/TSP.2015.2461515
   Shah Parikshit, 2011, 2011 49th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P760
   Swirszcz G., 2009, ADV NEURAL INFORM PR, V22, P1150
   Tewari A., 2011, ADV NEURAL INFORM PR, V24, P882
   van de Geer SA, 2009, ELECTRON J STAT, V3, P1360, DOI 10.1214/09-EJS506
   Yuan X.T., 2014, P 31 INT C MACH LEAR, P127
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703061
DA 2019-06-15
ER

PT S
AU Jain, S
   White, M
   Radivojac, P
AF Jain, Shantanu
   White, Martha
   Radivojac, Predrag
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Estimating the class prior and posterior from noisy positives and
   unlabeled data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We develop a classification algorithm for estimating posterior distributions from positive-unlabeled data, that is robust to noise in the positive labels and effective for high-dimensional data. In recent years, several algorithms have been proposed to learn from positive-unlabeled data; however, many of these contributions remain theoretical, performing poorly on real high-dimensional data that is typically contaminated with noise. We build on this previous work to develop two practical classification algorithms that explicitly model the noise in the positive labels and utilize univariate transforms built on discriminative classifiers. We prove that these univariate transforms preserve the class prior, enabling estimation in the univariate space and avoiding kernel density estimation for high-dimensional data. The theoretical development and parametric and nonparametric algorithms proposed here constitute an important step towards wide-spread use of robust classification algorithms for positive-unlabeled data.
C1 [Jain, Shantanu; White, Martha; Radivojac, Predrag] Indiana Univ, Dept Comp Sci, Bloomington, IN 47405 USA.
RP Jain, S (reprint author), Indiana Univ, Dept Comp Sci, Bloomington, IN 47405 USA.
EM shajain@indiana.edu; martha@indiana.edu; predrag@indiana.edu
FU NSF [DBI-1458477]; NIH [R01MH105524, R01GM103725]; Indiana University
   Precision Health Initiative
FX We thank Prof. Michael W. Trosset for helpful comments. Grant support:
   NSF DBI-1458477, NIH R01MH105524, NIH R01GM103725, and the Indiana
   University Precision Health Initiative.
CR Bashir S, 2005, J MULTIVARIATE ANAL, V93, P102, DOI 10.1016/j.jmva.2003.12.003
   Blanchard G, 2010, J MACH LEARN RES, V11, P2973
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Bouveyron C, 2009, PATTERN RECOGN, V42, P2649, DOI 10.1016/j.patcog.2009.03.027
   Cortes C, 2008, LECT NOTES ARTIF INT, V5254, P38, DOI 10.1007/978-3-540-87987-9_8
   Denis F, 2005, THEOR COMPUT SCI, V348, P70, DOI 10.1016/j.tcs.2005.09.007
   Du Plessis MC, 2014, IEICE T INF SYST, VE97D, P1358, DOI 10.1587/transinf.E97.D.1358
   Elkan C, 2008, P 14 ACM SIGKDD INT, P213, DOI DOI 10.1145/1401890.1401920
   Hawkins DM, 1997, J AM STAT ASSOC, V92, P136, DOI 10.2307/2291457
   Jain S., 2016, ARXIV160101944
   Katz-Samuels J., 2016, ARXIV160206235
   Lawrence N. D., 2001, P 18 INT C MACH LEAR, P306
   Lichman M., 2013, UCI MACHINE LEARNING
   Liu H., 2007, J MACHINE LEARNING R, P283, DOI DOI 10.1214/009053607000000811.508
   Long PM, 2010, MACH LEARN, V78, P287, DOI 10.1007/s10994-009-5165-z
   Manwani N, 2013, IEEE T CYBERNETICS, V43, P1146, DOI 10.1109/TSMCB.2012.2223460
   Menon A. K., 2015, P 32 INT C MACH LEAR, P125
   Niculescu-Mizil A., 2005, P 21 C UNC ART INT E, P413
   Platt J., 1999, PROBABILISTIC OUTPUT, P61
   Ramaswamy H. G., 2016, ARXIV160302501
   Reid MD, 2010, J MACH LEARN RES, V11, P2387
   Saerens M, 2002, NEURAL COMPUT, V14, P21, DOI 10.1162/089976602753284446
   Sanderson T., 2014, P 17 INT C ART INT S, P850
   Scott C., 2013, C LEARN THEOR W CP J, P489
   Scott D. W., 2008, MULTIVARIATE DENSITY, P195
   Steen H, 2004, NAT REV MOL CELL BIO, V5, P699, DOI 10.1038/nrm1468
   Ward G, 2009, BIOMETRICS, V65, P554, DOI 10.1111/j.1541-0420.2008.01116.x
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701025
DA 2019-06-15
ER

PT S
AU Jaitly, N
   Sussillo, D
   Le, QV
   Vinyals, O
   Sutskever, I
   Bengio, S
AF Jaitly, Navdeep
   Sussillo, David
   Le, Quoc V.
   Vinyals, Oriol
   Sutskever, Ilya
   Bengio, Samy
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI An Online Sequence-to-Sequence Model Using Partial Conditioning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Sequence-to-sequence models have achieved impressive results on various tasks. However, they are unsuitable for tasks that require incremental predictions to be made as more data arrives or tasks that have long input sequences and output sequences. This is because they generate an output sequence conditioned on an entire input sequence. In this paper, we present a Neural Transducer that can make incremental predictions as more input arrives, without redoing the entire computation. Unlike sequence-to-sequence models, the Neural Transducer computes the next-step distribution conditioned on the partially observed input sequence and the partially generated sequence. At each time step, the transducer can decide to emit zero to many output symbols. The data can be processed using an encoder and presented as input to the transducer. The discrete decision to emit a symbol at every time step makes it difficult to learn with conventional backpropagation. It is however possible to train the transducer by using a dynamic programming algorithm to generate target discrete decisions. Our experiments show that the Neural Transducer works well in settings where it is required to produce output predictions as data come in. We also find that the Neural Transducer performs well for long sequences even when attention mechanisms are not used.
C1 [Jaitly, Navdeep; Sussillo, David; Le, Quoc V.; Sutskever, Ilya; Bengio, Samy] Google Brain, Mountain View, CA 94043 USA.
   [Vinyals, Oriol] Google DeepMind, London, England.
   [Sutskever, Ilya] Open AI, San Francisco, CA USA.
RP Jaitly, N (reprint author), Google Brain, Mountain View, CA 94043 USA.
EM ndjaitly@google.com; sussillo@google.com; qvl@google.com;
   vinyals@google.com; ilyasu@openai.com; bengio@google.com
CR Bahdanau D., 2015, INT C LEARN REPR
   Bahdanau  Dzmitry, 2015, END TO END ATTENTION
   Chan W., 2015, ARXIV150801211
   Cho  K., 2014, C EMP METH NAT LANG
   Chorowski J. K., 2015, NEURAL INFORM PROCES
   Chorowski Jan, 2014, NEUR INF PROC SYST W
   Graves A., 2014, ARXIV14105401
   Graves A, 2013, IEEE INT C AC SPEECH
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Graves Alex, 2012, INT C MACH LEARN REP
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Mnih V., 2013, CORR
   Neelakantan Arvind, 2015, ARXIV151104834
   Reed S., 2015, ARXIV151106279
   Sordoni A, 2015, ARXIV150606714
   Sukhbaatar S., 2015, ADV NEURAL INFORM PR, P2431
   Sutskever Ilya, 2014, NEURAL INFORM PROCES
   Vinyals O., 2015, NIPS
   Vinyals O., 2015, IEEE C COMP VIS PATT
   Vinyals Oriol, 2015, ICML DEEP LEARN WORK
   Zaremba Wojciech, 2015, ARXIV150500521
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973705008
DA 2019-06-15
ER

PT S
AU Jalali, A
   Han, QY
   Dumitriu, I
   Fazel, M
AF Jalali, Amin
   Han, Qiyang
   Dumitriu, Ioana
   Fazel, Maryam
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Exploiting Tradeoffs for Exact Recovery in Heterogeneous Stochastic
   Block Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB The Stochastic Block Model (SBM) is a widely used random graph model for networks with communities. Despite the recent burst of interest in community detection under the SBM from statistical and computational points of view, there are still gaps in understanding the fundamental limits of recovery. In this paper, we consider the SBM in its full generality, where there is no restriction on the number and sizes of communities or how they grow with the number of nodes, as well as on the connectivity probabilities inside or across communities. For such stochastic block models, we provide guarantees for exact recovery via a semidefinite program as well as upper and lower bounds on SBM parameters for exact recoverability. Our results exploit the tradeoffs among the various parameters of heterogenous SBM and provide recovery guarantees for many new interesting SBM configurations.
C1 [Jalali, Amin; Fazel, Maryam] Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA.
   [Han, Qiyang] Univ Washington, Dept Stat, Seattle, WA 98195 USA.
   [Dumitriu, Ioana] Univ Washington, Dept Math, Seattle, WA 98195 USA.
RP Jalali, A (reprint author), Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA.
EM amjalali@uw.edu; royhan@uw.edu; dumitriu@uw.edu; mfazel@uw.edu
CR Abbe Emmanuel, 2015, ARXIV150300609
   Ailon N., 2013, P 30 INT C MACH LEAR, P995
   Bandeira A. S., 2014, ARXIV14086185
   Bandeira A. S., 2015, EFFICIENT ALGORITHM
   Boppana R. B., 1987, 28th Annual Symposium on Foundations of Computer Science (Cat. No.87CH2471-1), P280, DOI 10.1109/SFCS.1987.22
   Cai TT, 2015, ANN STAT, V43, P1027, DOI 10.1214/14-AOS1290
   Chatterjee S, 2015, ANN STAT, V43, P177, DOI 10.1214/14-AOS1272
   Chen JC, 2006, BIOINFORMATICS, V22, P2283, DOI 10.1093/bioinformatics/btl370
   Chen YY, 2016, BMC MOL BIOL, V17, DOI 10.1186/s12867-016-0055-y
   Chen YD, 2014, J MACH LEARN RES, V15, P2213
   Chen Yudong, 2012, ADV NEURAL INFORM PR, V25, P2204
   Coja-Oghlan A, 2010, COMB PROBAB COMPUT, V19, P227, DOI 10.1017/S0963548309990514
   Decelle A, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.066106
   Guedon O., 2015, PROBABILITY THEORY R, P1
   HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7
   Jiang DX, 2004, IEEE T KNOWL DATA EN, V16, P1370, DOI 10.1109/TKDE.2004.68
   Lang K. J., 2010, P 19 INT C WORLD WID, P631, DOI DOI 10.1145/1772690.1772755
   Liu YT, 2014, 2014 INTERNATIONAL CONFERENCE ON MECHATRONICS AND CONTROL (ICMC), P21, DOI 10.1109/ICMC.2014.7231508
   McSherry F, 2001, ANN IEEE SYMP FOUND, P529, DOI 10.1109/SFCS.2001.959929
   Meila Marina, 2001, RANDOM WALKS VIEW SP
   Mossel E., 2015, P 47 ANN ACM S THEOR, P69
   Rohe K, 2011, ANN STAT, V39, P1878, DOI 10.1214/11-AOS887
   Sandon C, 2015, ARXIV151209080
   Sen S, 2015, ARXIV150405910
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Tomozei D. C., 2014, STOCHASTIC SYSTEMS, V4, P1, DOI [10.1214/11-SSY036, DOI 10.1214/11-SSY036]
   Vu V., 2014, ARXIV14043918
   Yun S. Y., 2014, COLT, P138
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704098
DA 2019-06-15
ER

PT S
AU Jamieson, K
   Haas, D
   Recht, B
AF Jamieson, Kevin
   Haas, Daniel
   Recht, Ben
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI The Power of Adaptivity in Identifying Statistical Alternatives
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB This paper studies the trade-off between two different kinds of pure exploration: breadth versus depth. We focus on the most biased coin problem, asking how many total coin flips are required to identify a "heavy" coin from an infinite bag containing both "heavy" coins with mean theta(1) is an element of (0, 1), and "light" coins with mean theta(0) is an element of (0, theta(1)), where heavy coins are drawn from the bag with proportion alpha is an element of(0, 1/2). When alpha, theta(0), theta(1) are unknown, the key difficulty of this problem lies in distinguishing whether the two kinds of coins have very similar means, or whether heavy coins are just extremely rare. While existing solutions to this problem require some prior knowledge of the parameters. theta(0), theta(1), alpha, we propose an adaptive algorithm that requires no such knowledge yet still obtains near-optimal sample complexity guarantees. In contrast, we provide a lower bound showing that non-adaptive strategies require at least quadratically more samples. In characterizing this gap between adaptive and nonadaptive strategies, we make connections to anomaly detection and prove lower bounds on the sample complexity of differentiating between a single parametric distribution and a mixture of two such distributions.
C1 [Jamieson, Kevin; Haas, Daniel; Recht, Ben] Univ Calif Berkeley, Berkeley, CA 94720 USA.
RP Jamieson, K (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM kjamieson@eecs.berkeley.edu; dhaas@eecs.berkeley.edu;
   brecht@eecs.berkeley.edu
FU ONR [N00014-15-1-2620, N00014-13-1-0129]; NSF CISE Expeditions Award
   [CCF-1139158]; DOE [SN10040 DE-SC0012463]; DARPA XData Award
   [FA8750-12-2-0331]
FX Kevin Jamieson is generously supported by ONR awards N00014-15-1-2620,
   and N00014-13-1-0129. This research is supported in part by NSF CISE
   Expeditions Award CCF-1139158, DOE Award SN10040 DE-SC0012463, and DARPA
   XData Award FA8750-12-2-0331, and gifts from Amazon Web Services,
   Google, IBM, SAP, The Thomas and Stacey Siebel Foundation, Apple Inc.,
   Arimo, Blue Goji, Bosch, Cisco, Cray, Cloudera, Ericsson, Facebook,
   Fujitsu, Guavus, HP, Huawei, Intel, Microsoft, Pivotal, Samsung,
   Schlumberger, Splunk, State Farm and VMware.
CR Acharya Jayadev, 2015, ADV NEURAL INFORM PR, P3577
   Agarwal D, 2007, KNOWL INF SYST, V11, P29, DOI 10.1007/s10115-006-0036-4
   Arlotto A, 2014, MANAGE SCI, V60, P110, DOI 10.1287/mnsc.2013.1754
   Bernstein Michael S, 2011, UIST
   Berry DA, 1997, ANN STAT, V25, P2103, DOI 10.1214/aos/1069362389
   Bonald Thomas, 2013, ADV NEURAL INFORM PR, V26, P2184
   Carpentier Alexandra, 2015, ARXIV150504627
   Chandrasekaran Karthekeyan, 2012, CORR
   Chandrasekaran Karthekeyan, 2014, P C LEARN THEOR BARC, P394
   Eskin, 2000, 17 INT C MACH LEARN, P255
   Freund Y., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P53, DOI 10.1145/307400.307412
   Haas D, 2015, PROC VLDB ENDOW, V9, P372
   Hardt Moritz, 2014, SHARP BOUNDS LEARNIN, P1404
   Jamieson K, 2014, INF SCI SYST CISS 20, P1
   Malloy M. L., 2012, IEEE ANN C INF SCI S, P1
   Pollard David, 2000, ASYMPTOPIA UNPUB
   Siegmund  D., 2013, SEQUENTIAL ANAL TEST
   SPIRA R, 1971, MATH COMPUT, V25, P317, DOI 10.2307/2004927
   Thatte G, 2011, IEEE ACM T NETWORK, V19, P512, DOI 10.1109/TNET.2010.2070845
   Wang Y, 2009, NIPS, P1729
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700036
DA 2019-06-15
ER

PT S
AU Jie, B
   Jiang, X
   Zu, C
   Zhang, DQ
AF Jie, Biao
   Jiang, Xi
   Zu, Chen
   Zhang, Daoqiang
BE Rish, I
   Langs, G
   Wehbe, L
   Cecchi, G
   Chang, KMK
   Murphy, B
TI The New Graph Kernels on Connectivity Networks for Identification of MCI
SO MACHINE LEARNING AND INTERPRETATION IN NEUROIMAGING, MLINI 2014
SE Lecture Notes in Artificial Intelligence
LA English
DT Proceedings Paper
CT 4th International Workshop on Machine Learning and Interpretation in
   Neuroimaging (MLINI) Held at Conference on Neural Information Processing
   Systems (NIPS)
CY DEC 13, 2014
CL Montreal, CANADA
ID MILD COGNITIVE IMPAIRMENT; BRAIN; CONNECTOME
AB Brain connectivity networks have been applied recently to brain disease diagnosis and classification. Especially for both functional and structural connectivity interaction, graph theoretical analysis provided a new measure for human brain organization in vivo, with one fundamental challenge that is how to define the similarity between a pair of graphs. As one kind of similarity measure for graphs, graph kernels have been widely studied and applied in the literature. However, few works exploit to construct graph kernels for brain connectivity networks, where each node corresponds a unique EEG electrode or regions of interest(ROI). Accordingly, in this paper, we construct a new graph kernel for brain connectivity networks, which takes into account the inherent characteristic of nodes and captures the local topological properties of brain connectivity networks. To validate our method, we have performed extensive evaluation on a real mild cognitive impairment (MCI) dataset with the baseline functional magnetic resonance imaging (fMRI) data from Alzheimers disease Neuroimaging Initiative (ADNI) database. Our experimental results demonstrate the efficacy of the proposed method.
C1 [Jie, Biao; Zu, Chen; Zhang, Daoqiang] Nanjing Univ Aeronaut & Astronaut, Dept Comp Sci & Engn, Nanjing 210016, Jiangsu, Peoples R China.
   [Jie, Biao] Anhui Normal Univ, Dept Comp Sci & Technol, Wuhu 241003, Peoples R China.
   [Jiang, Xi] Univ Georgia, Dept Comp Sci, Boyd Grad Studies Res Ctr 415, Athens, GA 30602 USA.
RP Zhang, DQ (reprint author), Nanjing Univ Aeronaut & Astronaut, Dept Comp Sci & Engn, Nanjing 210016, Jiangsu, Peoples R China.
EM jbiao@nuaa.edu.cn; superjx2318@gmail.com; chenzu@nuaa.edu.cn;
   dqzhang@nuaa.edu.cn
CR Bai F, 2012, J NEUROSCI, V32, P4307, DOI 10.1523/JNEUROSCI.5061-11.2012
   Borgwardt KM, 2005, Fifth IEEE International Conference on Data Mining, Proceedings, P74, DOI 10.1109/ICDM.2005.132
   Camps-Valls G, 2010, IEEE GEOSCI REMOTE S, V7, P741, DOI 10.1109/LGRS.2010.2046618
   Feragen A, 2013, ADV NEURAL INFORM PR, P216
   Gartner T, 2003, LECT NOTES ARTIF INT, V2777, P129, DOI 10.1007/978-3-540-45167-9_11
   Han Y, 2011, NEUROIMAGE, V55, P287, DOI 10.1016/j.neuroimage.2010.11.059
   Johansson F.D., 2014, P 31 INT C MACH LEAR, V23, P1
   Kaiser M, 2011, NEUROIMAGE, V57, P892, DOI 10.1016/j.neuroimage.2011.05.025
   Lenzi D, 2011, NEUROBIOL AGING, V32, P1542, DOI 10.1016/j.neurobiolaging.2009.09.006
   Nobili F, 2008, EUR J NUCL MED MOL I, V35, P2191, DOI 10.1007/s00259-008-0869-z
   Pievani M, 2011, J NEUROL, V258, P170
   Scholkopf B., 2002, LEARNING KERNELS
   Shervashidze N, 2009, ADV NEURAL INFORM PR, V22, P1660
   Shervashidze N, 2011, J MACH LEARN RES, V12, P2539
   Shrivastava A., 2013, FRONTIERS NETWORK AN, P1
   Sporns O, 2005, PLOS COMPUT BIOL, V1, P245, DOI 10.1371/journal.pcbi.0010042
   Tzourio-Mazoyer N, 2002, NEUROIMAGE, V15, P273, DOI 10.1006/nimg.2001.0978
   Van Dijk KRA, 2010, J NEUROPHYSIOL, V103, P297, DOI 10.1152/jn.00783.2009
   Vishwanathan SVN, 2010, J MACH LEARN RES, V11, P1201
   Wang JH, 2013, BIOL PSYCHIAT, V73, P472, DOI 10.1016/j.biopsych.2012.03.026
   Wee CY, 2011, NEUROIMAGE, V54, P1812, DOI 10.1016/j.neuroimage.2010.10.026
   Xie Teng, 2011, Front Psychiatry, V2, P77, DOI 10.3389/fpsyt.2011.00077
   Zhang DQ, 2011, NEUROIMAGE, V55, P856, DOI 10.1016/j.neuroimage.2011.01.008
   Zhang YJ, 2011, J BIOMED INFORM, V44, P1086, DOI 10.1016/j.jbi.2011.08.011
NR 24
TC 0
Z9 0
U1 1
U2 3
PU SPRINGER INT PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
BN 978-3-319-45173-2; 978-3-319-45174-9
J9 LECT NOTES ARTIF INT
PY 2016
VL 9444
BP 12
EP 20
DI 10.1007/978-3-319-45174-9_2
PG 9
WC Computer Science, Artificial Intelligence; Neuroimaging
SC Computer Science; Neurosciences & Neurology
GA BG4FZ
UT WOS:000388723400002
DA 2019-06-15
ER

PT S
AU Jie, ZQ
   Liang, XD
   Feng, JS
   Jin, XJ
   Lu, WF
   Yan, SC
AF Jie, Zequn
   Liang, Xiaodan
   Feng, Jiashi
   Jin, Xiaojie
   Lu, Wen Feng
   Yan, Shuicheng
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Tree-Structured Reinforcement Learning for Sequential Object
   Localization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Existing object proposal algorithms usually search for possible object regions over multiple locations and scales separately, which ignore the interdependency among different objects and deviate from the human perception procedure. To incorporate global interdependency between objects into object localization, we propose an effective Tree-structured Reinforcement Learning (Tree-RL) approach to sequentially search for objects by fully exploiting both the current observation and historical search paths. The Tree-RL approach learns multiple searching policies through maximizing the long-term reward that reflects localization accuracies over all the objects. Starting with taking the entire image as a proposal, the Tree-RL approach allows the agent to sequentially discover multiple objects via a tree-structured traversing scheme. Allowing multiple near-optimal policies, Tree-RL offers more diversity in search paths and is able to find multiple objects with a single feed-forward pass. Therefore, Tree-RL can better cover different objects with various scales which is quite appealing in the context of object proposal. Experiments on PASCAL VOC 2007 and 2012 validate the effectiveness of the Tree-RL, which can achieve comparable recalls with current object proposal algorithms via much fewer candidate windows.
C1 [Jie, Zequn; Feng, Jiashi; Jin, Xiaojie; Lu, Wen Feng; Yan, Shuicheng] Natl Univ Singapore, Singapore, Singapore.
   [Liang, Xiaodan] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
RP Jie, ZQ (reprint author), Natl Univ Singapore, Singapore, Singapore.
FU National University of Singapore startup grant [R-263-000-C08-133];
   Ministry of Education of Singapore AcRF Tier One grant
   [R-263-000-C21-112]
FX The work of Jiashi Feng was partially supported by National University
   of Singapore startup grant R-263-000-C08-133 and Ministry of Education
   of Singapore AcRF Tier One grant R-263-000-C21-112.
CR Alexe B, 2010, CVPR
   Alexe Bogdan, 2012, NIPS
   Ba J., 2014, MULTIPLE OBJECT RECO, V1412, P7755
   Caicedo J., 2015, ICCV
   Cheng M. M., 2014, CVPR
   Collobert R, 2011, NIPS WORKSH
   Deng J., 2009, CVPR
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Girshick R., 2015, ICCV
   Girshick R., 2014, CVPR
   Gonzalez-Garcia Abel, 2015, CVPR
   He  K., 2015, ARXIV151203385
   Hosang J, 2016, IEEE T PATTERN ANAL, V38, P814, DOI 10.1109/TPAMI.2015.2465908
   Krahenbuhl P., 2014, ECCV
   Lampert CH, 2009, IEEE T PATTERN ANAL, V31, P2129, DOI 10.1109/TPAMI.2009.144
   Long  J., 2015, CVPR
   Lu Yongxi, 2015, ARXIV151207711
   Mathe S., 2016, CVPR
   Mathe Stefan, 2014, ARXIV14120100
   Mnih V., 2014, NIPS
   Mnih V., 2013, ARXIV, V1312, P5602, DOI DOI 10.1038/NATURE14236
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Najemnik J, 2005, NATURE, V434, P387, DOI 10.1038/nature03390
   Ren S., 2015, NIPS
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Xu K, 2015, ARXIV150203044
   Zitnick C. L., 2014, ECCV
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704056
DA 2019-06-15
ER

PT S
AU Jin, C
   Kakade, SM
   Netrapalli, P
AF Jin, Chi
   Kakade, Sham M.
   Netrapalli, Praneeth
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Provable Efficient Online Matrix Completion via Non-convex Stochastic
   Gradient Descent
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Matrix completion, where we wish to recover a low rank matrix by observing a few entries from it, is a widely studied problem in both theory and practice with wide applications. Most of the provable algorithms so far on this problem have been restricted to the offline setting where they provide an estimate of the unknown matrix using all observations simultaneously. However, in many applications, the online version, where we observe one entry at a time and dynamically update our estimate, is more appealing. While existing algorithms are efficient for the offline setting, they could be highly inefficient for the online setting.
   In this paper, we propose the first provable, efficient online algorithm for matrix completion. Our algorithm starts from an initial estimate of the matrix and then performs non-convex stochastic gradient descent (SGD). After every observation, it performs a fast update involving only one row of two tall matrices, giving near linear total runtime. Our algorithm can be naturally used in the offline setting as well, where it gives competitive sample complexity and runtime to state of the art algorithms. Our proofs introduce a general framework to show that SGD updates tend to stay away from saddle surfaces and could be of broader interests to other non-convex problems.
C1 [Jin, Chi] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Kakade, Sham M.] Univ Washington, Seattle, WA 98195 USA.
   [Netrapalli, Praneeth] Microsoft Res India, Bengaluru, Karnataka, India.
RP Jin, C (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM chijin@cs.berkeley.edu; sham@cs.washington.edu; praneeth@microsoft.com
CR Arora S., 2015, ARXIV150300778
   Brand M, 2003, SIAM PROC S, P37
   Candes EJ, 2015, SIAM REV, V57, P225, DOI 10.1137/151005099
   Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Davidson J., 2010, P 4 ACM C REC SYST, P293, DOI DOI 10.1145/1864708.1864770
   De Sa C., 2014, ARXIV14111134
   Ge R., 2015, ARXIV150302101
   Hardt M., 2014, ANN C LEARN THEOR, P703
   Hardt M, 2014, ANN IEEE SYMP FOUND, P651, DOI 10.1109/FOCS.2014.75
   Jain P., 2014, ARXIV14111087
   Jain P., 2015, ARXIV150705854
   Jain Prateek, 2016, ARXIV160206929
   Ji H, 2010, PROC CVPR IEEE, P1791, DOI 10.1109/CVPR.2010.5539849
   Keshavan R. H., 2012, THESIS
   Koren Y., 2009, NETFLIX PRIZE DOCUME, P1
   Krishnamurthy A., 2013, ADV NEURAL INFORM PR, P836
   Linden G, 2003, IEEE INTERNET COMPUT, V7, P76, DOI 10.1109/MIC.2003.1167344
   Luo X, 2012, KNOWL-BASED SYST, V27, P271, DOI 10.1016/j.knosys.2011.09.006
   Mairal J, 2010, J MACH LEARN RES, V11, P19
   Panageas  I., 2016, ARXIV160500405
   Recht B, 2013, MATH PROGRAM COMPUT, V5, P201, DOI 10.1007/s12532-013-0053-8
   Recht B, 2011, J MACH LEARN RES, V12, P3413
   Simchowitz Max, 2016, U CALIFORNIA BERKELE, V1050, P16
   Sun RY, 2015, ANN IEEE SYMP FOUND, P270, DOI 10.1109/FOCS.2015.25
   Tropp JA, 2012, FOUND COMPUT MATH, V12, P389, DOI 10.1007/s10208-011-9099-z
   Yun Se- Young, 2015, ARXIV150403156
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701061
DA 2019-06-15
ER

PT S
AU Jin, C
   Zhang, YC
   Balakrishnan, S
   Wainwright, MJ
   Jordan, MI
AF Jin, Chi
   Zhang, Yuchen
   Balakrishnan, Sivaraman
   Wainwright, Martin J.
   Jordan, Michael, I
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Local Maxima in the Likelihood of Gaussian Mixture Models: Structural
   Results and Algorithmic Consequences
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID LEARNING MIXTURES; CONVERGENCE; RATES; IDENTIFIABILITY; EM
AB We provide two fundamental results on the population (infinite-sample) likelihood function of Gaussian mixture models with M >= 3 components. Our first main result shows that the population likelihood function has bad local maxima even in the special case of equally-weighted mixtures of well-separated and spherical Gaussians. We prove that the log-likelihood value of these bad local maxima can be arbitrarily worse than that of any global optimum, thereby resolving an open question of Srebro [2007]. Our second main result shows that the EM algorithm (or a first-order variant of it) with random initialization will converge to bad critical points with probability at least 1 - e(-Omega(M)).We further establish that a first-order variant of EM will not converge to strict saddle points almost surely, indicating that the poor performance of the first-order method can be attributed to the existence of bad local maxima rather than bad saddle points. Overall, our results highlight the necessity of careful initialization when using the EM algorithm in practice, even when applied in highly favorable settings.
C1 [Jin, Chi; Zhang, Yuchen; Wainwright, Martin J.; Jordan, Michael, I] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Balakrishnan, Sivaraman] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
RP Jin, C (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM chijin@cs.berkeley.edu; yuczhang@berkeley.edu; siva@stat.cmu.edu;
   wainwrig@berkeley.edu; jordan@cs.berkeley.edu
FU Office of Naval Research MURI [DOD-002888]; Air Force Office of
   Scientific Research Grant [AFOSR-FA9550-14-1-001]; Mathematical Data
   Science program of the Office of Naval Research [N00014-15-1-2670];
   National Science Foundation [CIF-31712-23800]
FX This work was partially supported by Office of Naval Research MURI grant
   DOD-002888, Air Force Office of Scientific Research Grant
   AFOSR-FA9550-14-1-001, the Mathematical Data Science program of the
   Office of Naval Research under grant number N00014-15-1-2670, and
   National Science Foundation Grant CIF-31712-23800.
CR Allman ES, 2009, ANN STAT, V37, P3099, DOI 10.1214/09-AOS689
   Amendola Carlos, 2015, INT C MATH ASP COMP, P579
   Arora S, 2005, ANN APPL PROBAB, V15, P69, DOI 10.1214/10505160404000000512
   Balakrishnan Sivaraman, 2015, ANN STAT
   Belkin M, 2010, ANN IEEE SYMP FOUND, P103, DOI 10.1109/FOCS.2010.16
   Chaudhuri K., 2008, COLT, P9
   CHEN JH, 1995, ANN STAT, V23, P221, DOI 10.1214/aos/1176324464
   Dasgupta S, 2007, J MACH LEARN RES, V8, P203
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   DONOHO DL, 1988, ANN STAT, V16, P552, DOI 10.1214/aos/1176350820
   Ge R., 2015, P 28 C LEARN THEOR, P797
   Genovese CR, 2000, ANN STAT, V28, P1105
   Ghosal S, 2001, ANN STAT, V29, P1233
   Ho Nhat, 2015, ARXIV150102497
   Hsu Daniel, 2013, P 4 C INN THEOR COMP, P11, DOI DOI 10.1145/2422436.2422439
   Lee Jason D, 2016, P C COMP LEARN THEOR, P1246
   Loh P.-L., 2013, ADV NEURAL INFORM PR, P476
   Moitra A, 2010, ANN IEEE SYMP FOUND, P93, DOI 10.1109/FOCS.2010.15
   Panageas  I., 2016, ARXIV160500405
   Pascanu Razvan, 2014, ARXIV14054604
   Srebro N, 2007, LECT NOTES COMPUT SC, V4539, P628, DOI 10.1007/978-3-540-72927-3_47
   TEICHER H, 1963, ANN MATH STAT, V34, P1265, DOI 10.1214/aoms/1177703862
   TITTERINGTON DM, 1985, STAT ANAL FINITE MIX
   Vempala S, 2002, ANN IEEE SYMP FOUND, P113, DOI 10.1109/SFCS.2002.1181888
   Wang ZR, 2014, ANN STAT, V42, P2164, DOI 10.1214/14-AOS1238
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702071
DA 2019-06-15
ER

PT S
AU Jitkrittum, W
   Szabo, Z
   Chwialkowski, K
   Gretton, A
AF Jitkrittum, Wittawat
   Szabo, Zoltan
   Chwialkowski, Kacper
   Gretton, Arthur
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Interpretable Distribution Features with Maximum Testing Power
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Two semimetrics on probability distributions are proposed, given as the sum of differences of expectations of analytic functions evaluated at spatial or frequency locations (i.e, features). The features are chosen so as to maximize the distinguishability of the distributions, by optimizing a lower bound on test power for a statistical test using these features. The result is a parsimonious and interpretable indication of how and where two distributions differ locally. We show that the empirical estimate of the test power criterion converges with increasing sample size, ensuring the quality of the returned features. In real-world benchmarks on high-dimensional text and image data, linear-time tests using the proposed semimetrics achieve comparable performance to the state-of-the-art quadratic-time maximum mean discrepancy test, while returning human-interpretable features that explain the test results.
C1 [Jitkrittum, Wittawat; Szabo, Zoltan; Chwialkowski, Kacper; Gretton, Arthur] UCL, Gatsby Unit, London, England.
RP Jitkrittum, W (reprint author), UCL, Gatsby Unit, London, England.
EM wittawatj@gmail.com; zoltan.szabo.m@gmail.com;
   kacper.chwialkowski@gmail.com; arthur.gretton@gmail.com
FU Gatsby Charitable Foundation
FX We thank the Gatsby Charitable Foundation for the financial support.
CR Baringhaus L, 2004, J MULTIVARIATE ANAL, V88, P190, DOI 10.1016/S0047-259X(03)00079-4
   Bilodeau M, 2008, THEORY MULTIVARIATE
   Bird S., 2009, NATURAL LANGUAGE PRO
   Bousquet O, 2003, ANN I STAT MATH, V55, P371, DOI 10.1007/BF02530506
   Carota C, 1996, J AM STAT ASSOC, V91, P753, DOI 10.2307/2291670
   Chwialkowski K., 2015, P ADV NEUR INF PROC, P1972
   Eric Moulines, 2008, P NEUR INF PROC SYST, P609
   Fromont M, 2012, 25 ANN C LEARN THEOR, V23, P1
   Gretton  A., 2012, ADV NEURAL INFORM PR, P1205
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Kosorok MR, 2008, SPRINGER SER STAT, P3
   Lloyd J. R., 2014, P 28 AAAI C ART INT, P1242
   Lloyd JR, 2015, ADV NEURAL INFORM PR, P829
   Lundqvist D., 1998, TECHNICAL REPORT
   Manning C. D., 2008, INTRO INFORM RETRIEV
   Mueller J., 2015, NIPS, P1693
   Ramdas A., 2015, P 29 AAAI C ART INT, P3571
   Sejdinovic D, 2013, ANN STAT, V41, P2263, DOI 10.1214/13-AOS1140
   Smola A, 2007, LECT NOTES ARTIF INT, V4754, P13
   Srebro N, 2006, LECT NOTES ARTIF INT, V4005, P169, DOI 10.1007/11776420_15
   Sriperumbudur BK, 2011, J MACH LEARN RES, V12, P2389
   Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517
   Steinwart I, 2008, INFORM SCI STAT, P1
   SZEKELY G., 2004, INTERSTAT
   van der Vaart A., 2000, SPRINGER SERIES STAT
   Zaremba W., 2013, ADV NEURAL INFORM PR, P755
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701005
DA 2019-06-15
ER

PT S
AU Johnson, MJ
   Duvenaud, D
   Wiltschko, AB
   Datta, SR
   Adams, RP
AF Johnson, Matthew James
   Duvenaud, David
   Wiltschko, Alexander B.
   Datta, Sandeep R.
   Adams, Ryan P.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Composing graphical models with neural networks for structured
   representations and fast inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We propose a general modeling and inference framework that combines the complementary strengths of probabilistic graphical models and deep learning methods. Our model family composes latent graphical models with neural network observation likelihoods. For inference, we use recognition networks to produce local evidence potentials, then combine them with the model distribution using efficient message-passing algorithms. All components are trained simultaneously with a single stochastic variational inference objective. We illustrate this framework by automatically segmenting and categorizing mouse behavior from raw depth video, and demonstrate several other example models.
C1 [Johnson, Matthew James; Duvenaud, David] Harvard Univ, Cambridge, MA 02138 USA.
   [Wiltschko, Alexander B.; Adams, Ryan P.] Harvard Univ, Twitter, Cambridge, MA 02138 USA.
   [Datta, Sandeep R.] Harvard Med Sch, Boston, MA 02115 USA.
RP Johnson, MJ (reprint author), Harvard Univ, Cambridge, MA 02138 USA.
EM mattjj@seas.harvard.edu; dduvenaud@seas.harvard.edu;
   awiltsch@fas.harvard.edu; srdatta@hms.harvard.edu; rpa@seas.harvard.edu
CR Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746
   Amari S., 2007, METHODS INFORM GEOME
   Archer E., 2015, ARXIV151107367
   Chung Junyoung, 2015, ADV NEURAL INFORM PR, P2962
   Deng L, 2004, IMA V MATH, V138, P115
   Deng L., 1999, COMPUTATIONAL MODELS, P199
   Fox E, 2011, IEEE T SIGNAL PROCES, V59, P1569, DOI 10.1109/TSP.2010.2102756
   Gregor K., 2015, ARXIV150204623
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hoffman Matthew D., 2013, J MACHINE LEARNING R
   Iwata Tomoharu, 2013, 29 C UNC ART INT, P311
   Johnson Matthew J., 2014, INT C MACH LEARN
   Kingma Diederik, 2014, INT C LEARN REPR
   Koller D., 2009, PROBABILISTIC GRAPHI
   Krishnan Rahul G, 2015, ARXIV151105121
   MacKay DJC, 1999, STATISTICS AND NEURAL NETWORKS, P129
   Martens James, 2015, ARXIV150305671
   Martens James, 2015, ARXIV14121193
   Murphy KP, 2012, MACHINE LEARNING PRO
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   Wainwright Martin J., 2008, FDN TRENDS MACHINE L
   Wiltschko AB, 2015, NEURON, V88, P1121, DOI 10.1016/j.neuron.2015.11.031
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703015
DA 2019-06-15
ER

PT S
AU Johnson, TB
   Guestrin, C
AF Johnson, Tyler B.
   Guestrin, Carlos
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Unified Methods for Exploiting Piecewise Linear Structure in Convex
   Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID RULES
AB We develop methods for rapidly identifying important components of a convex optimization problem for the purpose of achieving fast convergence times. By considering a novel problem formulation-the minimization of a sum of piecewise functions-we describe a principled and general mechanism for exploiting piecewise linear structure in convex optimization. This result leads to a theoretically justified working set algorithm and a novel screening test, which generalize and improve upon many prior results on exploiting structure in convex optimization. In empirical comparisons, we study the scalability of our methods. We find that screening scales surprisingly poorly with the size of the problem, while our working set algorithm convincingly outperforms alternative approaches.
C1 [Johnson, Tyler B.; Guestrin, Carlos] Univ Washington, Seattle, WA 98195 USA.
RP Johnson, TB (reprint author), Univ Washington, Seattle, WA 98195 USA.
EM tbjohns@washington.edu; guestrin@cs.washington.edu
FU PECASE [N00014-13-1-0023]; NSF [IIS-1258741]; TerraSwarm Research Center
   [00008169]
FX We thank Hyunsu Cho, Christopher Aicher, and Tianqi Chen for their
   helpful feedback as well as assistance preparing datasets used in our
   experiments. This work is supported in part by PECASE N00014-13-1-0023,
   NSF IIS-1258741, and the TerraSwarm Research Center 00008169.
CR Bach F, 2012, FOUND TRENDS MACH LE, V4, P1, DOI 10.1561/2200000015
   CAI M, 2014, INT CONF ACOUST SPEE
   El Ghaoui L, 2012, PAC J OPTIM, V8, P667
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Fercoq O., 2015, INT C MACH LEARN
   Johnson T. B., 2015, INT C MACH LEARN
   Nan F, 2014, INT CONF ACOUST SPEE
   Ndiaye E., 2016, ARXIV160206225
   Ndiaye E, 2015, ADV NEUR IN, V28
   Takeuchi I., 2013, INT C MACH LEARN
   Tibshirani R, 2012, J R STAT SOC B, V74, P245, DOI 10.1111/j.1467-9868.2011.01004.x
   Wang J, 2015, J MACH LEARN RES, V16, P1063
   Xiang Z. J., 2012, IEEE INT C AC SPEECH
NR 13
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700008
DA 2019-06-15
ER

PT S
AU Joseph, M
   Kearns, M
   Morgenstern, J
   Roth, A
AF Joseph, Matthew
   Kearns, Michael
   Morgenstern, Jamie
   Roth, Aaron
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Fairness in Learning: Classic and Contextual Bandits
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We introduce the study of fairness in multi-armed bandit problems. Our fairness definition demands that, given a pool of applicants, a worse applicant is never favored over a better one, despite a learning algorithm's uncertainty over the true payoffs. In the classic stochastic bandits problem we provide a provably fair algorithm based on "chained" confidence intervals, and prove a cumulative regret bound with a cubic dependence on the number of arms. We further show that any fair algorithm must have such a dependence, providing a strong separation between fair and unfair learning that extends to the general contextual case. In the general contextual case, we prove a tight connection between fairness and the KWIK (Knows What It Knows) learning model: a KWIK algorithm for a class of functions can be transformed into a provably fair contextual bandit algorithm and vice versa. This tight connection allows us to provide a provably fair algorithm for the linear contextual bandit problem with a polynomial dependence on the dimension, and to show (for a different class of functions) a worst-case exponential gap in regret between fair and non-fair learning algorithms.
C1 [Joseph, Matthew; Kearns, Michael; Morgenstern, Jamie; Roth, Aaron] Univ Penn, Dept Comp & Informat Sci, Philadelphia, PA 19104 USA.
RP Joseph, M (reprint author), Univ Penn, Dept Comp & Informat Sci, Philadelphia, PA 19104 USA.
EM majos@cis.upenn.edu; mkearns@cis.upenn.edu; jamiemor@cis.upenn.edu;
   aaroth@cis.upenn.edu
CR Adler Philip, 2016, CORR
   Agarwal  A., 2014, P 31 INT C MACH LEAR, P1638
   Amin K., 2013, P ICML, P588
   Amin Kareem, 2012, ARXIV12023782
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Barocas S., 2016, CALIFORNIA LAW REV, V104
   Barry-Jester Anna Maria, 2015, THE MARSHALL PROJECT
   Beygelzimer A, 2011, P 14 INT C ART INT S, V15, P19
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Byrnes Nanette, 2016, MIT TECHNOLOGY REV
   Calders T, 2010, DATA MIN KNOWL DISC, V21, P277, DOI 10.1007/s10618-010-0190-x
   Chu W., 2011, P 14 INT C AI STAT A, V15, P208
   Coglianese Cary, 2016, GEORGETOWN LAW J
   Dwork Cynthia, 2012, P 3 INN THEOR COMP S, P214, DOI DOI 10.1145/2090236.2090255
   Feldman Michael, 2015, P 21 ACM SIGKDD INT, V21, P259, DOI DOI 10.1145/2783258.2783311
   Fish Benjamin, 2016, SIAM INT S DAT MIN
   Joseph M., 2016, CORR
   Kamishima T., 2011, 2011 IEEE International Conference on Data Mining Workshops, P643, DOI 10.1109/ICDMW.2011.83
   KATEHAKIS MN, 1995, P NATL ACAD SCI USA, V92, P8584, DOI 10.1073/pnas.92.19.8584
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Li  Lianhui, 2009, THESIS
   Li LH, 2011, MACH LEARN, V82, P399, DOI 10.1007/s10994-010-5225-4
   Luong B. T., 2011, P 17 ACM SIGKDD INT, P502, DOI DOI 10.1145/2020408.2020488
   Miller Claire Cain, 2015, NY TIMES
   Rudin Cynthia, 2013, WIRED MAGAZINE
   Strehl AL, 2008, ADV NEURAL INFORM PR, V20, P1417
   Zemel R., 2013, JMLR P, P325
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702102
DA 2019-06-15
ER

PT S
AU Kadmon, J
   Sompolinsky, H
AF Kadmon, Jonathan
   Sompolinsky, Haim
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Optimal Architectures in a Solvable Model of Deep Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID REPRESENTATIONS
AB Deep neural networks have received a considerable attention due to the success of their training for real world machine learning applications. They are also of great interest to the understanding of sensory processing in cortical sensory hierarchies. The purpose of this work is to advance our theoretical understanding of the computational benefits of these architectures. Using a simple model of clustered noisy inputs and a simple learning rule, we provide analytically derived recursion relations describing the propagation of the signals along the deep network. By analysis of these equations, and defining performance measures, we show that these model networks have optimal depths. We further explore the dependence of the optimal architecture on the system parameters.
C1 [Kadmon, Jonathan; Sompolinsky, Haim] Hebrew Univ Jerusalem, Racah Inst Phys, Jerusalem, Israel.
   [Kadmon, Jonathan; Sompolinsky, Haim] Hebrew Univ Jerusalem, ELSC, Jerusalem, Israel.
   [Sompolinsky, Haim] Harvard Univ, Ctr Brain Sci, Cambridge, MA 02138 USA.
RP Kadmon, J (reprint author), Hebrew Univ Jerusalem, Racah Inst Phys, Jerusalem, Israel.; Kadmon, J (reprint author), Hebrew Univ Jerusalem, ELSC, Jerusalem, Israel.
EM jonathan.kadmon@mail.huji.ac.il
FU IARPA [D16PC00002]; Gatsby Charitable Foundation; Simons Foundation SCGB
   grant
FX This work was partially supported by IARPA (contract #D16PC00002),
   Gatsby Charitable Foundation, and Simons Foundation SCGB grant.
CR Babadi B, 2014, NEURON, V83, P1213, DOI 10.1016/j.neuron.2014.07.035
   BALDI P, 1989, NEURAL NETWORKS, V2, P53, DOI 10.1016/0893-6080(89)90014-2
   Cho YM, 2010, NEURAL COMPUT, V22, P2678, DOI 10.1162/NECO_a_00018
   Cohen William W, 2008, EXTRACTING COMPOSING
   DOMANY E, 1989, J PHYS A-MATH GEN, V22, P2081, DOI 10.1088/0305-4470/22/12/013
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   KANTER I, 1987, PHYS REV A, V35, P380, DOI 10.1103/PhysRevA.35.380
   Lee H., 2008, ADV NEURAL INFORM PR, P873
   PERSONNAZ L, 1985, J PHYS LETT-PARIS, V46, pL359
   Saxe A., 2011, ADV NEURAL INFORM PR, V24, P1971
   Saxe A. M., 2013, EXACT SOLUTIONS NONL
   Smolensky P, 1986, INFORM PROCESSING DY
   Turner GC, 2008, J NEUROPHYSIOL, V99, P734, DOI 10.1152/jn.01283.2007
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
NR 14
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702077
DA 2019-06-15
ER

PT S
AU Kaiser, L
   Bengio, S
AF Kaiser, Lukasz
   Bengio, Samy
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Can Active Memory Replace Attention?
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Several mechanisms to focus attention of a neural network on selected parts of its input or memory have been used successfully in deep learning models in recent years. Attention has improved image classification, image captioning, speech recognition, generative models, and learning algorithmic tasks, but it had probably the largest impact on neural machine translation.
   Recently, similar improvements have been obtained using alternative mechanisms that do not focus on a single part of a memory but operate on all of it in parallel, in a uniform way. Such mechanism, which we call active memory, improved over attention in algorithmic tasks, image processing, and in generative modelling.
   So far, however, active memory has not improved over attention for most natural language processing tasks, in particular for machine translation. We analyze this shortcoming in this paper and propose an extended model of active memory that matches existing attention models on neural machine translation and generalizes better to longer sentences. We investigate this model and explain why previous active memory models did not succeed. Finally, we discuss when active memory brings most benefits and where attention can be a better choice.
C1 [Kaiser, Lukasz; Bengio, Samy] Google Brain, Mountain View, CA 94043 USA.
RP Kaiser, L (reprint author), Google Brain, Mountain View, CA 94043 USA.
EM lukaszkaiser@google.com; bengio@google.com
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Bengio Y., 2014, CORR
   Cho K., 2014, CORR
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Graves A., 2014, ARXIV14105401
   Gregor K., 2016, CORR
   Gregor Karol, 2015, CORR
   He K., 2016, CVPR
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Joulin A., 2015, ADV NEURAL INFORM PR
   Kaiser Lukasz, 2016, INT C LEARN REPR ICL
   Kalchbrenner N., 2013, P 2013 C EMP METH NA, P1700
   Kalchbrenner Nal, 2016, INT C LEARN REPR
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Lavin Andrew, 2015, CORR
   Liao Q., 2016, CORR
   Meng F., 2015, P ANN M ASS COMP LIN, P20
   Rezende Danilo Jimenez, 2016, CORR
   Shi Xingjian, 2015, ADV NEURAL INFORM PR
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Toderici George, 2016, INT C LEARN REPR
   Tu Zhaopeng, 2016, CORR
   Vinyals, 2015, ADV NEURAL INFORM PR
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Xu K, 2015, ICML
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702042
DA 2019-06-15
ER

PT S
AU Kale, S
   Lee, C
   Pal, D
AF Kale, Satyen
   Lee, Chansoo
   Pal, David
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Hardness of Online Sleeping Combinatorial Optimization Problems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We show that several online combinatorial optimization problems that admit efficient no-regret algorithms become computationally hard in the sleeping setting where a subset of actions becomes unavailable in each round. Specifically, we show that the sleeping versions of these problems are at least as hard as PAC learning DNF expressions, a long standing open problem. We show hardness for the sleeping versions of ONLINE SHORTEST PATHS, ONLINE MINIMUM SPANNING TREE, ONLINE k-SUBSETS, ONLINE k-TRUNCATED PERMUTATIONS, ONLINE MINIMUM CUT, and ONLINE BIPARTITE MATCHING. The hardness result for the sleeping version of the Online Shortest Paths problem resolves an open problem presented at COLT 2015 [Koolen et al., 2015].
C1 [Kale, Satyen; Lee, Chansoo; Pal, David] Yahoo Res, New York, NY 10024 USA.
   [Lee, Chansoo] Univ Michigan, Ann Arbor, MI 48109 USA.
   [Kale, Satyen] Google Res, New York, NY 10011 USA.
RP Kale, S (reprint author), Yahoo Res, New York, NY 10024 USA.; Kale, S (reprint author), Google Res, New York, NY 10011 USA.
EM satyen@satyenkale.com; chansool@umich.edu; dpal@yahoo-inc.com
CR Abbasi-Yadkori Y., 2013, ADV NEURAL INFORM PR, V26, P2508
   Audibert JY, 2014, MATH OPER RES, V39, P31, DOI 10.1287/moor.2013.0598
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Freund Y, 1997, P 29 ANN ACM S THEOR, P334, DOI DOI 10.1145/258533.258616
   Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016
   Kalai AT, 2012, J COMPUT SYST SCI, V78, P1481, DOI 10.1016/j.jcss.2011.12.026
   Kanade V., 2009, P 12 INT C ART INT S, P272
   Kanade Varun, 2014, ACM T COMPUTATION TH, V6, P11
   KEARNS MJ, 1994, MACH LEARN, V17, P115
   Kleinberg R, 2010, MACH LEARN, V80, P245, DOI 10.1007/s10994-010-5178-7
   Klivans A. R., 2001, P 33 ANN S THEOR COM, P258
   Koolen W. M., 2010, P 23 ANN C LEARN THE, P93
   Koolen Wouter M., 2015, P 28 C LEARN THEOR C
   LITTLESTONE N, 1994, INFORM COMPUT, V108, P212, DOI 10.1006/inco.1994.1009
   Neu Gergely, 2014, ADV NEURAL INFORM PR, P2780
   Takimoto E, 2004, J MACH LEARN RES, V4, P773, DOI 10.1162/1532443041424328
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703086
DA 2019-06-15
ER

PT S
AU Kanagawa, M
   Sriperumbudur, BK
   Fukumizu, K
AF Kanagawa, Motonobu
   Sriperumbudur, Bharath K.
   Fukumizu, Kenji
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Convergence guarantees for kernel-based quadrature rules in misspecified
   settings
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID ERROR
AB Kernel-based quadrature rules are becoming important in machine learning and statistics, as they achieve super-root n convergence rates in numerical integration, and thus provide alternatives to Monte Carlo integration in challenging settings where integrands are expensive to evaluate or where integrands are high dimensional. These rules are based on the assumption that the integrand has a certain degree of smoothness, which is expressed as that the integrand belongs to a certain reproducing kernel Hilbert space (RKHS). However, this assumption can be violated in practice (e.g., when the integrand is a black box function), and no general theory has been established for the convergence of kernel quadratures in such misspecified settings. Our contribution is in proving that kernel quadratures can be consistent even when the integrand does not belong to the assumed RKHS, i.e., when the integrand is less smooth than assumed. Specifically, we derive convergence rates that depend on the (unknown) lesser smoothness of the integrand, where the degree of smoothness is expressed via powers of RKHSs or via Sobolev spaces.
C1 [Kanagawa, Motonobu; Fukumizu, Kenji] Inst Stat Math, Tokyo 1908562, Japan.
   [Sriperumbudur, Bharath K.] Penn State Univ, Dept Stat, University Pk, PA 16802 USA.
RP Kanagawa, M (reprint author), Inst Stat Math, Tokyo 1908562, Japan.
EM kanagawa@ism.ac.jp; bks18@psu.edu; fukumizu@ism.ac.jp
FU MEXT [25120012]
FX We wish to thank the anonymous reviewers for valuable comments. We also
   thank Chris Oates for fruitful discussions. This work has been supported
   in part by MEXT Grant-in-Aid for Scientific Research on Innovative Areas
   (25120012).
CR Adams R. A., 2003, SOBOLEV SPACES
   Bach F., 2012, P 29 INT C MACH LEAR, P1359
   Bach F., 2015, HAL01118276V2
   Berlinet A, 2004, REPRODUCING KERNEL H
   BRIOL F. -X., 2015, ADV NEURAL INFORM PR, P1162
   Briol F-X., 2016, ARXIV151200933V4STAT
   Chen YH, 2010, ADV INTELL SOFT COMP, V66, P109, DOI 10.1145/1866919.1866935
   Dick J., 2013, ACTA NUMERICA, V22
   Dick J, 2007, SIAM J NUMER ANAL, V45, P2141, DOI 10.1137/060658916
   Dick J, 2011, ANN STAT, V39, P1372, DOI 10.1214/11-AOS880
   Dick J, 2008, SIAM J NUMER ANAL, V46, P1519, DOI 10.1137/060666639
   Gerber M, 2015, J R STAT SOC B, V77, P509, DOI 10.1111/rssb.12104
   Hickernell FJ, 1998, MATH COMPUT, V67, P299, DOI 10.1090/S0025-5718-98-00894-1
   Lacoste-Julien S., 2015, P AISTATS 2015
   Novak E., 1988, DETERMINISTIC STOCHA
   Novak E, 2008, EMS TRACTS MATH, V6, P3
   Oates C. J., 2016, P AISTATS 2016
   Oates Chris J, 2017, J ROYAL STAT SOC B
   OHAGAN A, 1991, J STAT PLAN INFER, V29, P245, DOI 10.1016/0378-3758(91)90002-V
   Rasmussen E., 2006, GAUSSIAN PROCESSES M
   Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517
   Steinwart I., 2012, CONSTRUCTIVE APPROXI, V35
   Yang J., 2014, P ICML 2014
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701031
DA 2019-06-15
ER

PT S
AU Kandasamy, K
   Al-Shedivat, M
   Xing, EP
AF Kandasamy, Kirthevasan
   Al-Shedivat, Maruan
   Xing, Eric P.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning HMMs with Nonparametric Emissions via Spectral Decompositions
   of Continuous Matrices
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Recently, there has been a surge of interest in using spectral methods for estimating latent variable models. However, it is usually assumed that the distribution of the observations conditioned on the latent variables is either discrete or belongs to a parametric family. In this paper, we study the estimation of an m-state hidden Markov model (HMM) with only smoothness assumptions, such as Holderian conditions, on the emission densities. By leveraging some recent advances in continuous linear algebra and numerical analysis, we develop a computationally efficient spectral algorithm for learning nonparametric HMMs. Our technique is based on computing an SVD on nonparametric estimates of density functions by viewing them as continuous matrices. We derive sample complexity bounds via concentration results for nonparametric density estimation and novel perturbation theory results for continuous matrices. We implement our method using Chebyshev polynomial approximations. Our method is competitive with other baselines on synthetic and real problems and is also very computationally efficient.
C1 [Kandasamy, Kirthevasan; Al-Shedivat, Maruan; Xing, Eric P.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
RP Kandasamy, K (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM kandasamy@cs.cmu.edu; alshedivat@cs.cmu.edu; epxing@cs.cmu.edu
FU NIH [R01GM114311]; AFRL/DARPA [FA87501220324]
FX The authors would like to thank Alex Townsend, Arthur Gretton, Ahmed
   Hefny, Yaoliang Yu, and Renato Negrinho for the helpful discussions.
   This work was supported by NIH R01GM114311, AFRL/DARPA FA87501220324.
CR Anandkumar A., 2012, ARXIV12030683
   Anandkumar Animashree, 2014, JMLR
   Benaglia T, 2009, J COMPUT GRAPH STAT, V18, P505, DOI 10.1198/jcgs.2009.07175
   Birge Lucien, 1995, ANN OF STAT
   De Castro Yohann, 2015, ARXIV150104787
   Dempster Arthur P, 1977, J ROYAL STAT SOC B
   Driscoll Tobin A, 2014, PAFNUTY PUBL
   Fox L., 1968, CHEBYSHEV POLYNOMIAL
   Gine Evarist, 2002, ANN IHP PROBABILITES
   Hashemi B., 2016, UNPUB
   Hsu Daniel J., 2009, COLT
   Hubner U, 1989, PHYS REV A
   Jaeger Herbert, 2000, NEURAL COMPUTATION
   Kandasamy Kirthevasan, 2015, NIPS
   Lee Woo Young, 1998, INTEGRAL EQUATIONS O
   Littman M. L., 2001, NIPS, P1555
   Liu H, 2011, J MACH LEARN RES, V12, P907
   Paxson Vern, 1995, IEEE ACM T NETWORKIN
   Rabiner L. R., 1989, P IEEE
   Robins J, 2009, METRIKA, V69, P227, DOI 10.1007/s00184-008-0214-3
   Siddiqi Sajid M., 2010, AISTATS
   Singh Satinder, 2004, UAI
   Song Le, 2014, NONPARAMETRIC ESTIMA, P640
   Song Le, 2010, ICML
   Stewart GW, 1990, MATRIX PERTURBATION
   TOWNSEND  A., 2014, THESIS
   Townsend Alex, 2013, SIAM J SCI COMPUTING
   Townsend Alex, 2015, P R SOC A
   Trefethen L., 2013, APPROXIMATION THEORY
   Tsybakov A. B., 2008, INTRO NONPARAMETRIC
   Wasserman L, 2006, ALL NONPARAMETRIC ST
   Welch Lloyd R, 2003, IEEE INFORM THEORY S
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700050
DA 2019-06-15
ER

PT S
AU Kandasamy, K
   Dasarathy, G
   Schneider, J
   Poczos, B
AF Kandasamy, Kirthevasan
   Dasarathy, Gautam
   Schneider, Jeff
   Poczos, Barnabas
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI The Multi-fidelity Multi-armed Bandit
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We study a variant of the classical stochastic K-armed bandit where observing the outcome of each arm is expensive, but cheap approximations to this outcome are available. For example, in online advertising the performance of an ad can be approximated by displaying it for shorter time periods or to narrower audiences. We formalise this task as a multi fidelity bandit, where, at each time step, the forecaster may choose to play an arm at any one of M fidelities. The highest fidelity (desired outcome) expends cost lambda((M)). The mth fidelity (an approximation) expends lambda((M)) < lambda((M)) and returns a biased estimate of the highest fidelity. We develop MF-UCB, a novel upper confidence bound procedure for this setting and prove that it naturally adapts to the sequence of available approximations and costs thus attaining better regret than naive strategies which ignore the approximations. For instance, in the above online advertising example, MF-UCB would use the lower fidelities to quickly eliminate suboptimal ads and reserve the larger expensive experiments on a small set of promising candidates. We complement this result with a lower bound and show that MF-UCB is nearly optimal under certain conditions.
C1 [Kandasamy, Kirthevasan; Schneider, Jeff; Poczos, Barnabas] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Dasarathy, Gautam] Rice Univ, Houston, TX 77251 USA.
RP Kandasamy, K (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM kandasamy@cs.cmu.edu; gautamd@rice.edu; schneide@cs.cmu.edu;
   bapoczos@cs.cmu.edu
CR Agrawal Rajeev, 1995, ADV APPL PROBABILITY
   Audibert Jean-Yves, 2009, THEOR COMPUT SCI
   Auer Peter, 2003, J MACH LEARN RES
   Baram Y, 2004, J MACH LEARN RES, V5, P255
   Bubeck  S., 2012, FDN TRENDS MACHINE L
   Cutler Mark, 2014, IEEE INT C ROB AUT I
   Huang D., 2006, STRUCTURAL MULTIDISC
   Kandasamy Kirthevasan, 2016, ADV NEURAL INFORM PR
   Lai T. L., 1985, ADV APPL MATH
   Rajnarayan Dev, 2008, AIAA ISSMO MULT AN O
   Robbins  H., 1952, B AM MATH SOC
   Thompson W. R., 1933, BIOMETRIKA
   Tran-Thanh Long, 2014, UAI
   Wasserman L., 2010, ALL STAT CONCISE COU
   Xia Yingce, 2015, IJCAI
   Zhang C., 2015, ADV NEURAL INFORM PR
NR 16
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973705006
DA 2019-06-15
ER

PT S
AU Kandasamy, K
   Dasarathy, G
   Oliva, J
   Schneider, J
   Poczos, B
AF Kandasamy, Kirthevasan
   Dasarathy, Gautam
   Oliva, Junier
   Schneider, Jeff
   Poczos, Barnabas
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Gaussian Process Bandit Optimisation with Multi-fidelity Evaluations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB In many scientific and engineering applications, we are tasked with the optimisation of an expensive to evaluate black box function f. Traditional methods for this problem assume just the availability of this single function. However, in many cases, cheap approximations to f may be obtainable. For example, the expensive real world behaviour of a robot can be approximated by a cheap computer simulation. We can use these approximations to eliminate low function value regions cheaply and use the expensive evaluations of f in a small but promising region and speedily identify the optimum. We formalise this task as a multi-fidelity bandit problem where the target function and its approximations are sampled from a Gaussian process. We develop MF-GP-UCB, a novel method based on upper confidence bound techniques. In our theoretical analysis we demonstrate that it exhibits precisely the above behaviour, and achieves better regret than strategies which ignore multi-fidelity information. MF-GP-UCB outperforms such naive strategies and other multi-fidelity methods on several synthetic and real experiments.
C1 [Kandasamy, Kirthevasan; Oliva, Junier; Schneider, Jeff; Poczos, Barnabas] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Dasarathy, Gautam] Rice Univ, Houston, TX 77251 USA.
RP Kandasamy, K (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM kandasamy@cs.cmu.edu; gautamd@rice.edu; joliva@cs.cmu.edu;
   schneide@cs.cmu.edu; bapoczos@cs.cmu.edu
CR Agarwal Alekh, 2011, COLT
   Auer Peter, 2003, J MACH LEARN RES
   Brochu E, 2010, CORR
   Bubeck  S., 2012, FDN TRENDS MACHINE L
   Cutler Mark, 2014, ICRA
   Dani V., 2008, COLT
   Davis T. M., 2007, ASTROPHYSICAL J
   Djolonga J, 2013, NIPS
   Forrester Alexander I. J., 2007, ANN STAT
   Ghosal Subhashis, 2006, ANN STAT
   Huang D., 2006, STRUCTURAL MULTIDISC
   Jones D. R., 1993, J OPTIM THEORY APPL
   Jones Donald R., 1998, J GLOBAL OPTIMIZATIO
   Kandasamy Kirthevasan, 2015, INT C MACH LEARN
   Kandasamy Kirthevasan, 2016, NIPS
   Kandasamy Kirthevasan, 2016, ICML
   Kawaguchi K., 2015, NIPS
   Kirkpatrick S., 1983, SCIENCE
   Klein A., 2015, BAYESOPT
   Martinez-Cantin R, 2007, P ROB SCI SYST
   Mockus Jonas, 1994, J GLOBAL OPTIMIZATIO
   Munos R., 2011, NIPS
   Parkinson D., 2006, PHYS REV
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Robbins  H., 1952, B AM MATH SOC
   Sabharwal A, 2015, AAAI
   Snoek  Jasper, 2012, NIPS
   Srinivas N., 2010, ICML
   Swersky  Kevin, 2013, NIPS
   Thompson W. R., 1933, BIOMETRIKA
   Viola Paul, 2001, COMPUTER VISION PATT
   Xiong Shifeng, 2013, TECHNOMETRICS
   Zhang C., 2015, NIPS
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700082
DA 2019-06-15
ER

PT S
AU Karnin, Z
AF Karnin, Zohar
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Verification Based Solution for Structured MAB Problems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We consider the problem of finding the best arm in a stochastic Multi-armed Bandit (MAB) game and propose a general framework based on verification that applies to multiple well-motivated generalizations of the classic MAB problem. In these generalizations, additional structure is known in advance, causing the task of verifying the optimality of a candidate to be easier than discovering the best arm. Our results are focused on the scenario where the failure probability ! must be very low; we essentially show that in this high confidence regime, identifying the best arm is as easy as the task of verification. We demonstrate the effectiveness of our framework by applying it, and matching or improving the state-of-the art results in the problems of: Linear bandits, Dueling bandits with the Condorcet assumption, Copeland dueling bandits, Unimodal bandits and Graphical bandits.
C1 [Karnin, Zohar] Yahoo Res, New York, NY 10036 USA.
RP Karnin, Z (reprint author), Yahoo Res, New York, NY 10036 USA.
EM zkarnin@ymail.com
CR Ailon N., 2014, P 31 INT C MACH LEAR, V32, P856
   Auer P., 2003, Journal of Machine Learning Research, V3, P397, DOI 10.1162/153244303321897663
   Balsubramani Akshay, 2016, P 29 C LEARN THEOR C
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Busa-Fekete Robert, 2014, P 28 AAAI C ART INT
   Combes R., 2014, P 31 INT C MACH LEAR, P521
   DiCastro Dotan, 2011, CORR
   Even-Dar E, 2006, J MACH LEARN RES, V7, P1079
   Grunwald Peter, 2015, P 28 C LEARN THEOR C, V40
   Hofmann K, 2013, INFORM RETRIEVAL, V16, P63, DOI 10.1007/s10791-012-9197-9
   Joachims  T., 2002, KDD
   Karnin Z., 2013, P 30 INT C MACH LEAR, V28, P1238
   Komiyama Junpei, 2016, COPELAND DUELING BAN
   Piccolboni A, 2001, LECT NOTES ARTIF INT, V2111, P208
   Soare M., 2014, ADV NEURAL INFORM PR, P828
   Urvoy T., 2013, P 30 INT C MACH LEAR, V28, P91
   Yu J. Y., 2011, P 28 INT C MACH LEAR, P41
   Yu Kai, 2006, P 23 INT C MACH LEAR, P1081
   Yue Y., 2011, ICML
   Yue YS, 2012, J COMPUT SYST SCI, V78, P1538, DOI 10.1016/j.jcss.2011.12.028
   Zoghi M, 2014, P INT C MACH LEARN I, P10
   Zoghi M, 2015, P 8 ACM INT C WEB SE, P17, DOI [10.1145/2684822.2685290, DOI 10.1145/2684822.2685290]
   Zoghi M, 2015, ADV NEURAL INFORM PR, P307
   2010, PREFERENCE LEARNING, P1
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704049
DA 2019-06-15
ER

PT S
AU Kathuria, T
   Deshpande, A
   Kohli, P
AF Kathuria, Tarun
   Deshpande, Amit
   Kohli, Pushmeet
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Batched Gaussian Process Bandit Optimization via Determinantal Point
   Processes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Gaussian Process bandit optimization has emerged as a powerful tool for optimizing noisy black box functions. One example in machine learning is hyper-parameter optimization where each evaluation of the target function may require training a model which may involve days or even weeks of computation. Most methods for this so-called "Bayesian optimization" only allow sequential exploration of the parameter space. However, it is often desirable to propose batches or sets of parameter values to explore simultaneously, especially when there are large parallel processing facilities at our disposal. Batch methods require modeling the interaction between the different evaluations in the batch, which can be expensive in complex scenarios. In this paper, we propose a new approach for parallelizing Bayesian optimization by modeling the diversity of a batch via Determinantal point processes (DPPs) whose kernels are learned automatically. This allows us to generalize a previous result as well as prove better regret bounds based on DPP sampling. Our experiments on a variety of synthetic and real-world robotics and hyper-parameter optimization tasks indicate that our DPP-based methods, especially those based on DPP sampling, outperform state-of-the-art methods.
C1 [Kathuria, Tarun; Deshpande, Amit; Kohli, Pushmeet] Microsoft Res, Redmond, WA 98052 USA.
RP Kathuria, T (reprint author), Microsoft Res, Redmond, WA 98052 USA.
EM t-takat@microsoft.com; amitdesh@microsoft.com; pkohli@microsoft.com
CR Anari N., 2016, COLT
   Anderson B. S., 2000, ICML
   Auer P., 2002, J MACHINE LEARNING R, V3, p397 , DOI DOI 10.1162/153244303321897663
   Azimi J., 2010, BATCH BAYESIAN OPTIM
   Brualdi R., 1983, LINEAR ALGEBRA ITS A
   Civril A, 2013, ALGORITHMICA, V65, P159, DOI 10.1007/s00453-011-9582-6
   Civril A, 2009, THEOR COMPUT SCI, V410, P4801, DOI 10.1016/j.tcs.2009.06.018
   Contal E., 2013, ECML
   Desautels T, 2014, J MACH LEARN RES, V15, P3873
   Deshpande A., 2010, FOCS
   Gonzalez J., 2016, AISTATS
   Hennig P., 2012, JMLR, V13
   Hernandex-Lobato J. M., 2014, NIPS
   Katakis Ioannis, 2008, ECML PKDD DISCOVERY
   Krause  Andreas, 2011, NIPS
   Kulesza A, 2012, FOUND TRENDS MACH LE, V5, P123, DOI 10.1561/2200000044
   Kulesza Alex, 2011, ICML
   Li C., 2016, ICML
   Lizotte D. J., 2008, THESIS
   Lyons R, 2003, PUBL MATH IHES, P167
   Nikolov A., 2015, P 47 ANN ACM S THEOR, P861
   Prabhu  Y., 2014, KDD
   Rasmussen C. E., 2008, GAUSSIAN PROCESSES M
   Robbins  H., 1952, B AM MATH SOC
   Seeger MW, 2008, IEEE T INFORM THEORY, V54, P2376, DOI 10.1109/TIT.2007.915707
   Shah A., 2015, NIPS
   Shirai T, 2003, J FUNCT ANAL, V205, P414, DOI 10.1016/S0022-1236(03)00171-X
   Snoek  Jasper, 2012, NIPS
   Srinivas N, 2012, IEEE T INFORM THEORY, V58, P3250, DOI 10.1109/TIT.2011.2182033
   Thornton C., 2003, KDD
   Tsoumakas G, 2008, ECML PKDD 2008 WORKS
   Wang GG, 2007, J MECH DESIGN, V129, P370, DOI 10.1115/1.2429697
   Wang Z., 2016, AISTATS
   Westervelt E., 2007, CONTROL AUTOMATION S
   Ziemba W., 2006, STOCHASTIC OPTIMIZAT
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703088
DA 2019-06-15
ER

PT S
AU Kawahara, Y
AF Kawahara, Yoshinobu
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Dynamic Mode Decomposition with Reproducing Kernels for Koopman Spectral
   Analysis
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID PROPER ORTHOGONAL DECOMPOSITION; REDUCTION; SYSTEMS
AB A spectral analysis of the Koopman operator, which is an infinite dimensional linear operator on an observable, gives a (modal) description of the global behavior of a nonlinear dynamical system without any explicit prior knowledge of its governing equations. In this paper, we consider a spectral analysis of the Koopman operator in a reproducing kernel Hilbert space (RKHS). We propose a modal decomposition algorithm to perform the analysis using finite-length data sequences generated from a nonlinear system. The algorithm is in essence reduced to the calculation of a set of orthogonal bases for the Krylov matrix in RKHS and the eigendecomposition of the projection of the Koopman operator onto the subspace spanned by the bases. The algorithm returns a decomposition of the dynamics into a finite number of modes, and thus it can be thought of as a feature extraction procedure for a nonlinear dynamical system. Therefore, we further consider applications in machine learning using extracted features with the presented analysis. We illustrate the method on the applications using synthetic and real-world data.
C1 [Kawahara, Yoshinobu] Osaka Univ, Inst Sci & Ind Res, Suita, Osaka, Japan.
   [Kawahara, Yoshinobu] RIKEN, Ctr Adv Integrated Intelligence Res, Tokyo, Japan.
RP Kawahara, Y (reprint author), Osaka Univ, Inst Sci & Ind Res, Suita, Osaka, Japan.; Kawahara, Y (reprint author), RIKEN, Ctr Adv Integrated Intelligence Res, Tokyo, Japan.
EM ykawahara@sanken.osaka-u.ac.jp
FU JSPS KAKENHI [JP16H01548]
FX This work was supported by JSPS KAKENHI Grant Number JP16H01548.
CR ARNOLDI WE, 1951, Q APPL MATH, V9, P17, DOI 10.1090/qam/42792
   Bagheri S, 2013, J FLUID MECH, V726, P596, DOI 10.1017/jfm.2013.249
   Bagheri S, 2009, J FLUID MECH, V624, P33, DOI 10.1017/S0022112009006053
   Berger E, 2014, IEEE ROMAN, P593, DOI 10.1109/ROMAN.2014.6926317
   BONNET JP, 1994, EXP FLUIDS, V17, P307, DOI 10.1007/BF01874409
   Brunton BW, 2016, J NEUROSCI METH, V258, P1, DOI 10.1016/j.jneumeth.2015.10.010
   Canu S, 2006, NEUROCOMPUTING, V69, P714, DOI 10.1016/j.neucom.2005.12.009
   Chen KK, 2012, J NONLINEAR SCI, V22, P887, DOI 10.1007/s00332-012-9130-9
   Csorgo M., 1988, LIMIT THEOREMS CHANG
   Duke D, 2012, J FLUID MECH, V691, P594, DOI 10.1017/jfm.2011.516
   Ghahramani Z., P 1998 C ADV NEUR IN, P431
   Holmes P., 1996, TURBULENCE COHERENT
   Jovanovic MR, 2014, PHYS FLUIDS, V26, DOI 10.1063/1.4863670
   Katayama T., 2005, COMM CONT E
   Kawahara Y., 2007, ADV NEURAL INFORM PR, V19, P665
   Koopman BO, 1931, P NATL ACAD SCI USA, V17, P315, DOI 10.1073/pnas.17.5.315
   Kulesza A., P 29 AAAI C ART INT, P2715
   Kwok JTY, 2004, IEEE T NEURAL NETWOR, V15, P1517, DOI 10.1109/TNN.2004.837781
   Melnyk I., 2015, P 18 INT C ART INT S, P690
   Mezic I, 2005, NONLINEAR DYNAM, V41, P309, DOI 10.1007/s11071-005-2824-x
   Muld TW, 2012, COMPUT FLUIDS, V57, P87, DOI 10.1016/j.compfluid.2011.12.012
   Noack BR, 2003, J FLUID MECH, V497, P335, DOI 10.1017/S0022112003006694
   Rowley CW, 2009, J FLUID MECH, V641, P115, DOI 10.1017/S0022112009992059
   Rowley CW, 2005, INT J BIFURCAT CHAOS, V15, P997, DOI 10.1142/S0218127405012429
   Schmid P. J., 2010, INT J HEAT FLUID FL, V32, P1098
   Schmid PJ, 2010, J FLUID MECH, V656, P5, DOI 10.1017/S0022112010001217
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
   Shawe-Taylor John, 2004, KERNEL METHODS PATTE
   SIROVICH L, 1987, Q APPL MATH, V45, P561, DOI 10.1090/qam/910462
   Song L., P 27 INT C MACH LEAR, P991
   Suzuki Y., 2013, IEEE T POWER SYSTEMS, V29, P899
   Tu JH, 2014, J COMPUT DYN, V1, P391, DOI DOI 10.3934/JCD.2014.1.391
   Van Overschee P, 1996, SUBSPACE IDENTIFICAT
   Wang J.M., 2006, NIPS, V18, P1441
   Williams MO, 2015, J NONLINEAR SCI, V25, P1307, DOI 10.1007/s00332-015-9258-5
   Wolf L, 2004, J MACH LEARN RES, V4, P913, DOI 10.1162/1532443041827934
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704107
DA 2019-06-15
ER

PT S
AU Khalvati, K
   Park, SA
   Dreher, JC
   Rao, RPN
AF Khalvati, Koosha
   Park, Seongmin A.
   Dreher, Jean-Claude
   Rao, Rajesh P. N.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI A Probabilistic Model of Social Decision Making based on Reward
   Maximization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID COOPERATION
AB A fundamental problem in cognitive neuroscience is how humans make decisions, act, and behave in relation to other humans. Here we adopt the hypothesis that when we are in an interactive social setting, our brains perform Bayesian inference of the intentions and cooperativeness of others using probabilistic representations. We employ the framework of partially observable Markov decision processes (POMDPs) to model human decision making in a social context, focusing specifically on the volunteer's dilemma in a version of the classic Public Goods Game. We show that the POMDP model explains both the behavior of subjects as well as neural activity recorded using fMRI during the game. The decisions of subjects can be modeled across all trials using two interpretable parameters. Furthermore, the expected reward predicted by the model for each subject was correlated with the activation of brain areas related to reward expectation in social interactions. Our results suggest a probabilistic basis for human social decision making within the framework of expected reward maximization.
C1 [Khalvati, Koosha; Rao, Rajesh P. N.] Univ Washington, Dept Comp Sci, Seattle, WA 98105 USA.
   [Park, Seongmin A.; Dreher, Jean-Claude] CNRS, Inst Sci Cognit Marc Jeannerod, UMR 5229, Lyon, France.
RP Khalvati, K (reprint author), Univ Washington, Dept Comp Sci, Seattle, WA 98105 USA.
EM koosha@cs.washington.edu; park@isc.cnrs.fr; dreher@isc.cnrs.fr;
   rao@cs.washington.edu
FU LABEX [ANR-11-LABEX-0042, ANR-11-IDEX-0007]; NSF-ANR 'Social_POMDP'; NSF
   [EEC-1028725, 1318733]; ONR [N000141310817]; CRCNS/NIMH
   [1R01MH112166-01]; ANR BrainCHOICE [14-CE13-0006]
FX This work was supported by LABEX ANR-11-LABEX-0042, ANR-11-IDEX-0007,
   NSF-ANR 'Social_POMDP' and ANR BrainCHOICE no14-CE13-0006 to JC. D, NSF
   grants EEC-1028725 and 1318733, ONR grant N000141310817, and CRCNS/NIMH
   grant 1R01MH112166-01.
CR Archetti M, 2011, EVOLUTION, V65, P885, DOI 10.1111/j.1558-5646.2010.01176.x
   Bault N, 2015, SOC COGN AFFECT NEUR, V10, P877, DOI 10.1093/scan/nsu138
   Burton-Chellew MN, 2013, P NATL ACAD SCI USA, V110, P216, DOI 10.1073/pnas.1210960110
   Chung D, 2015, SOC COGN AFFECT NEUR, V10, P1210, DOI 10.1093/scan/nsv006
   Dayan P, 2008, COGN AFFECT BEHAV NE, V8, P429, DOI 10.3758/CABN.8.4.429
   DIEKMANN A, 1993, INT J GAME THEORY, V22, P75, DOI 10.1007/BF01245571
   Fermin A. S. R., 2016, SCI REPORTS, V6
   Fischbacher U, 2001, ECON LETT, V71, P397, DOI 10.1016/S0165-1765(01)00394-9
   Gmytrasiewicz PJ, 2005, J ARTIF INTELL RES, V24, P49, DOI 10.1613/jair.1579
   Hula A, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004254
   Khalvati K., 2015, ADV NEURAL INFORM PR, V28, P2413
   Khalvati K., 2013, P 27 AAAI C ART INT, P187
   Lin A, 2012, SOC COGN AFFECT NEUR, V7, P274, DOI 10.1093/scan/nsr006
   Miller EK, 2001, ANNU REV NEUROSCI, V24, P167, DOI 10.1146/annurev.neuro.24.1.167
   Olson Mancur, 1971, LOGIC COLLECTIVE ACT
   Park SA, 2013, SOC NEUROSCI-UK, V8, P568, DOI 10.1080/17470919.2013.835280
   Rao RPN, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00146
   Ruff CC, 2014, NAT REV NEUROSCI, V15, P549, DOI 10.1038/nrn3776
   SMALLWOOD RD, 1973, OPER RES, V21, P1071, DOI 10.1287/opre.21.5.1071
   Smith T., 2004, P INT C UNC ART INT
   Steinbeis N, 2016, CURR OPIN BEHAV SCI, V10, P28, DOI 10.1016/j.cobeha.2016.04.009
   Thrun S., 2005, PROBABILISTIC ROBOTI
   Tom SM, 2007, SCIENCE, V315, P515, DOI 10.1126/science.1134239
   Wang J, 2012, P NATL ACAD SCI USA, V109, P14363, DOI 10.1073/pnas.1120867109
   Wunder M., 2013, P 14 ACM C EL COMM, P891
NR 25
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704061
DA 2019-06-15
ER

PT S
AU Khetan, A
   Oh, S
AF Khetan, Ashish
   Oh, Sewoong
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Achieving Budget-optimality with Adaptive Schemes in Crowdsourcing
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Adaptive schemes, where tasks are assigned based on the data collected thus far, are widely used in practical crowdsourcing systems to efficiently allocate the budget. However, existing theoretical analyses of crowdsourcing systems suggest that the gain of adaptive task assignments is minimal. To bridge this gap, we investigate this question under a strictly more general probabilistic model, which has been recently introduced to model practical crowdsourcing datasets. Under this generalized Dawid-Skene model, we characterize the fundamental trade-off between budget and accuracy. We introduce a novel adaptive scheme that matches this fundamental limit. A given budget is allocated over multiple rounds. In each round, a subset of tasks with high enough confidence are classified, and increasing budget is allocated on remaining ones that are potentially more difficult. On each round, decisions are made based on the leading eigenvector of (weighted) non-backtracking operator corresponding to the bipartite assignment graph. We further quantify the gain of adaptivity, by comparing the tradeoff with the one for non-adaptive schemes, and confirm that the gain is significant and can be made arbitrarily large depending on the distribution of the difficulty level of the tasks at hand.
C1 [Khetan, Ashish; Oh, Sewoong] Univ Illinois, Dept ISE, Champaign, IL 61820 USA.
RP Khetan, A (reprint author), Univ Illinois, Dept ISE, Champaign, IL 61820 USA.
EM khetan2@illinois.edu; swoh@illinois.edu
FU NSF SaTC award [CNS-1527754]; NSF CISE award [CCF-1553452]
FX This work is supported by NSF SaTC award CNS-1527754, and NSF CISE award
   CCF-1553452.
CR Alon N, 2004, PROBABILISTIC METHOD
   Dalvi N. N., 2013, P 22 INT C WORLD WID, P285, DOI DOI 10.1145/2488388.2488414
   Dawid A. P., 1979, APPL STAT, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]
   Ghosh A., 2011, P 12 ACM C EL COMM, P167, DOI DOI 10.1145/1993574.1993599
   Ho CJ, 2013, P 30 INT C MACH LEAR, V28, P534
   Jin Rong, 2003, ADV NEURAL INFORM PR, P921
   Karger David R., 2013, Performance Evaluation Review, V41, P81
   Karger D. R., 2011, NIPS, V24, P1953
   Karger DR, 2014, OPER RES, V62, P1, DOI 10.1287/opre.2013.1235
   Li H., 2014, ARXIV14114086
   Liu Q., 2012, NIPS, P701
   Maron O., 1993, HOEFFDING RACES ACCE, P263
   Mezard M., 2009, INFORM PHYS COMPUTAT
   Ok J., 2016, INT C MACH LEARN
   Shah Nihar B, 2016, ARXIV160609632
   Sheng V. S., 2008, P 14 ACM SIGKDD INT, P614, DOI DOI 10.1145/1401890.1401965
   Smyth P., 1995, Advances in Neural Information Processing Systems 7, P1085
   Welinder P., 2010, ADV NEURAL INFORM PR, P2424
   Whitehill J., 2009, ADV NEURAL INFORM PR, P2035
   Williams D., 1991, PROBABILITY MARTINGA
   Zhang Y., 2014, ADV NEURAL INFORM PR, P1260
   Zhou D., 2012, NIPS, V25, P2204
   Zhou Dengyong, 2015, ARXIV150307240
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700088
DA 2019-06-15
ER

PT S
AU Khetan, A
   Oh, S
AF Khetan, Ashish
   Oh, Sewoong
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Computational and Statistical Tradeoffs in Learning to Rank
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB For massive and heterogeneous modern data sets, it is of fundamental interest to provide guarantees on the accuracy of estimation when computational resources are limited. In the application of learning to rank, we provide a hierarchy of rank-breaking mechanisms ordered by the complexity in thus generated sketch of the data. This allows the number of data points collected to be gracefully traded off against computational resources available, while guaranteeing the desired level of accuracy. Theoretical guarantees on the proposed generalized rank-breaking implicitly provide such trade-offs, which can be explicitly characterized under certain canonical scenarios on the structure of the data.
C1 [Khetan, Ashish; Oh, Sewoong] Univ Illinois, Dept ISE, Champaign, IL 61820 USA.
RP Khetan, A (reprint author), Univ Illinois, Dept ISE, Champaign, IL 61820 USA.
EM khetan2@illinois.edu; swoh@illinois.edu
FU NSF SaTC award [CNS-1527754]; NSF CISE award [CCF-1553452]
FX This work is supported by NSF SaTC award CNS-1527754, and NSF CISE award
   CCF-1553452.
CR Agarwal A., 2012, ARXIV12080129
   Ali A, 2012, MATH SOC SCI, V64, P28, DOI 10.1016/j.mathsocsci.2011.08.008
   Betzler N, 2014, AUTON AGENT MULTI-AG, V28, P721, DOI 10.1007/s10458-013-9236-y
   Bottou L, 2008, ADV NEURAL INFORM PR, P161
   Chandrasekaran V, 2013, P NATL ACAD SCI USA, V110, pE1181, DOI 10.1073/pnas.1302293110
   Chen Y., 2015, ARXIV150407218
   Deshpande Y., 2015, ARXIV150206590
   Goldberg K, 2001, INFORM RETRIEVAL, V4, P133, DOI 10.1023/A:1011419012209
   HAJEK B., 2014, ADV NEURAL INFORM PR, P1475
   Hayes T. P., 2005, COMBINATORICS PROBAB
   Kamishima T, 2003, P 9 ACM SIGKDD INT C, P583, DOI DOI 10.1145/956750.956823
   Khetan A., 2016, INT C MACH LEARN
   Lucic M., 2015, AISTATS
   Maystre L., 2015, ADV NEURAL INFORM PR
   Meka R., 2015, P 47 ANN ACM S THEOR, P87, DOI DOI 10.1145/2746539.2746600
   Negahban S., 2014, ARXIV12091688
   Prekopa A., 1980, STOCHASTIC PROGRAMMI
   Shah N. B., 2015, ARXIV151005610
   Shah N. B., 2015, ARXIV150501462
   Shalev-Shwartz S., 2008, P 25 INT C MACH LEAR, P928, DOI DOI 10.1145/1390156.1390273
   Soufiani H.A., 2012, NIPS, P126
   Soufiani H. Azari, 2014, P 31 INT C MACH LEAR, P360
   Soufiani H. Azari, 2013, ADV NEURAL INFORM PR, V26, P2706
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703078
DA 2019-06-15
ER

PT S
AU Khim, J
   Jog, V
   Loh, PL
AF Khim, Justin
   Jog, Varun
   Loh, Po-Ling
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Computing and maximizing influence in linear threshold and triggering
   models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We establish upper and lower bounds for the influence of a set of nodes in certain types of contagion models. We derive two sets of bounds, the first designed for linear threshold models, and the second more broadly applicable to a general class of triggering models, which subsumes the popular independent cascade models, as well. We quantify the gap between our upper and lower bounds in the case of the linear threshold model and illustrate the gains of our upper bounds for independent cascade models in relation to existing results. Importantly, our lower bounds are monotonic and submodular, implying that a greedy algorithm for influence maximization is guaranteed to produce a maximizer within a (1 - 1/epsilon)-factor of the truth. Although the problem of exact influence computation is NP-hard in general, our bounds may be evaluated efficiently. This leads to an attractive, highly scalable algorithm for influence maximization with rigorous theoretical guarantees.
C1 [Khim, Justin] Univ Penn, Dept Stat, Wharton Sch, Philadelphia, PA 19104 USA.
   [Jog, Varun; Loh, Po-Ling] Univ Wisconsin, Elect & Comp Engn Dept, Madison, WI 53706 USA.
RP Khim, J (reprint author), Univ Penn, Dept Stat, Wharton Sch, Philadelphia, PA 19104 USA.
EM jkhim@wharton.upenn.edu; vjog@wisc.edu; loh@ece.wisc.edu
CR Adamic LA, 2003, SOC NETWORKS, V25, P211, DOI 10.1016/S0378-8733(03)00009-1
   Borgs C, 2014, P 25 ANN ACM SIAM S, P946, DOI DOI 10.1137/1.9781611973402.70
   Chen W., 2010, P 16 ACM SIGKDD INT, P1029, DOI DOI 10.1145/1835804.1835934
   Chen W, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P199, DOI 10.1145/1557019.1557047
   Domingos P., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P57
   Durrett R., 1988, LECT NOTES PARTICLE
   Eun Jee Lee, 2016, 2016 Annual Conference on Information Science and Systems (CISS), P649, DOI 10.1109/CISS.2016.7460579
   Hecker M, 2009, BIOSYSTEMS, V96, P86, DOI 10.1016/j.biosystems.2008.12.004
   Jiang CR, 2010, INT ASIA CONF INFORM, P88, DOI 10.1109/CAR.2010.5456772
   Kempe D., 2003, P 9 ACM SIGKDD INT C, P137, DOI DOI 10.1145/956750.956769
   Kermack WO, 1927, P R SOC LOND A-CONTA, V115, P700, DOI 10.1098/rspa.1927.0118
   Lemonnier R., 2016, SPECTRAL BOUNDS RAND
   Lemonnier R., 2014, ADV NEURAL INFORM PR, P846
   Leskovec J, 2007, ACM T WEB, V1, DOI 10.1145/1232722.1232727
   Liggett T., 2012, INTERACTING PARTICLE, V276
   Movarraei N., 2015, INT J APPL MATH RES, V4, P30
   Movarraei N., 2014, INT J APPL MATH RES, V3, P178
   Movarraei N., 2015, INT J APPL MATH RES, V4, P267
   Nemhauser G. L., 1978, Mathematics of Operations Research, V3, P177, DOI 10.1287/moor.3.3.177
   Newman MEJ, 2003, SIAM REV, V45, P167, DOI 10.1137/S003614450342480
   Scaman K., 2015, ADV NEURAL INFORM PR, P2017
   Sporns O, 2011, ANN NY ACAD SCI, V1224, P109, DOI 10.1111/j.1749-6632.2010.05888.x
   Zhou C, 2014, WWW'14 COMPANION: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P421, DOI 10.1145/2567948.2577336
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702094
DA 2019-06-15
ER

PT S
AU Kim, JH
   Lee, SW
   Kwak, D
   Heo, MO
   Kim, J
   Ha, JW
   Zhang, BT
AF Kim, Jin-Hwa
   Lee, Sang-Woo
   Kwak, Donghyun
   Heo, Min-Oh
   Kim, Jeonghee
   Ha, Jung-Woo
   Zhang, Byoung-Tak
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Multimodal Residual Learning for Visual QA
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Deep neural networks continue to advance the state-of-the-art of image recognition tasks with various methods. However, applications of these methods to multimodality remain limited. We present Multimodal Residual Networks (MRN) for the multimodal residual learning of visual question-answering, which extends the idea of the deep residual learning. Unlike the deep residual learning, MRN effectively learns the joint representation from vision and language information. The main idea is to use element-wise multiplication for the joint residual mappings exploiting the residual learning of the attentional models in recent studies. Various alternative models introduced by multimodality are explored based on our study. We achieve the state-of-the-art results on the Visual QA dataset for both Open-Ended and Multiple-Choice tasks. Moreover, we introduce a novel method to visualize the attention effect of the joint representations for each learning block using back-propagation algorithm, even though the visual features are collapsed without spatial information.
C1 [Kim, Jin-Hwa; Lee, Sang-Woo; Kwak, Donghyun; Heo, Min-Oh; Zhang, Byoung-Tak] Seoul Natl Univ, Seoul, South Korea.
   [Kim, Jeonghee; Ha, Jung-Woo] Naver Corp, Naver Labs, Seongnam, South Korea.
   [Zhang, Byoung-Tak] Surromind Robot, Seoul, South Korea.
RP Kim, JH (reprint author), Seoul Natl Univ, Seoul, South Korea.
EM jhkim@bi.snu.ac.kr; slee@bi.snu.ac.kr; dhkwak@bi.snu.ac.kr;
   moheo@bi.snu.ac.kr; jeonghee.kim@navercorp.com;
   jungwoo.ha@navercorp.com; btzhang@bi.snu.ac.kr
FU Naver Corp.; Korea government [IITP-R0126-16-1072-SW.StarLab,
   KEIT-10044009-HRI.MESSI, KEIT-10060086-RISF, ADD-UD130070ID-BMRR]
FX The authors would like to thank Patrick Emaase for helpful comments and
   editing. This work was supported by Naver Corp. and partly by the Korea
   government (IITP-R0126-16-1072-SW.StarLab, KEIT-10044009-HRI.MESSI,
   KEIT-10060086-RISF, ADD-UD130070ID-BMRR).
CR Agrawal Aishwarya, 2015, INT C COMP VIS
   Andreas J., 2016, ARXIV160101705
   Bird S., 2009, NATURAL LANGUAGE PRO
   Cho K., 2014, ARXIV14091259
   Gal Y., 2015, ARXIV151205287
   He  K., 2015, ARXIV151203385
   Hinton G. E, 2012, ARXIV12070580
   Ilievski I, 2016, ARXIV160401485
   Ioffe S., 2015, P 32 INT C MACH LEAR
   Jiang Z, 2016, IEEE INT CONF COMMUN
   Karpathy Andrej, 2015, 28 IEEE C COMP VIS P
   Kim Jin-Hwa, 2016, P KIIS SPRING C, V26, P165
   Kiros Ryan, 2015, ARXIV150606726
   Leonard  N., 2015, ARXIV151107889
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740
   Lu J., 2015, DEEPER LSTM NORMALIZ
   Malinowski Mateusz, 2015, ARXIV150501121
   Nair V., 2010, P 27 INT C MACH LEAR
   Ngiam J, 2011, IEEE INT C MACH LEAR, P689, DOI DOI 10.1145/2647868
   Noh H, 2015, ARXIV151105756
   Ren MY, 2015, ADV NEUR IN, V28
   Rocktaschel T., 2016, INT C LEARN REPR, P1
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Simonyan  K., 2015, INT C LEARN REPR
   Srivastava N., 2012, ADV NEURAL INFORM PR, P2222, DOI DOI 10.1109/CVPR.2013.49
   Sukhbaatar S., 2015, ADV NEURAL INFORM PR, V28, P2440
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P5
   Xiong C, 2016, ARXIV160301417
   Yang  Zichao, 2015, ARXIV151102274
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703082
DA 2019-06-15
ER

PT S
AU Kim, J
   Chen, YC
   Balakrishnan, S
   Rinaldo, A
   Wasserman, L
AF Kim, Jisu
   Chen, Yen-Chi
   Balakrishnan, Sivaraman
   Rinaldo, Alessandro
   Wasserman, Larry
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Statistical Inference for Cluster Trees
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB A cluster tree provides a highly-interpretable summary of a density function by representing the hierarchy of its high-density clusters. It is estimated using the empirical tree, which is the cluster tree constructed from a density estimator. This paper addresses the basic question of quantifying our uncertainty by assessing the statistical significance of topological features of an empirical cluster tree. We first study a variety of metrics that can be used to compare different trees, analyze their properties and assess their suitability for inference. We then propose methods to construct and summarize confidence sets for the unknown true cluster tree. We introduce a partial ordering on cluster trees which we use to prune some of the statistically insignificant features of the empirical tree, yielding interpretable and parsimonious cluster trees. Finally, we illustrate the proposed methods on a variety of synthetic examples and furthermore demonstrate their utility in the analysis of a Graft-versus-Host Disease (GvHD) data set.
C1 [Kim, Jisu; Balakrishnan, Sivaraman; Rinaldo, Alessandro; Wasserman, Larry] Carnegie Mellon Univ, Dept Stat, Pittsburgh, PA 15213 USA.
   [Chen, Yen-Chi] Univ Washington, Dept Stat, Seattle, WA 98195 USA.
RP Kim, J (reprint author), Carnegie Mellon Univ, Dept Stat, Pittsburgh, PA 15213 USA.
EM jisuk1@andrew.cmu.edu; yenchic@uw.edu; siva@stat.cmu.edu;
   arinaldo@stat.cmu.edu; larry@stat.cmu.edu
CR Balakrishnan S., 2012, ADV NEURAL INFORM PR
   Bauer U., 2015, LEIBNIZ INT P INFORM, V34, P461
   Brinkman RR, 2007, BIOL BLOOD MARROW TR, V13, P691, DOI 10.1016/j.bbmt.2007.02.002
   CHAUDHURI K., 2010, ADV NEURAL INFORM PR, P343
   Chaudhuri K., 2014, IEEE T INFORM THEORY
   Chazal F., 2014, ARXIV14127197
   Chen Y.-C., 2015, ARXIV150405438
   Chernozhukov V., 2016, ANN PROBABILITY
   DONOHO DL, 1988, ANN STAT, V16, P1390, DOI 10.1214/aos/1176351045
   Efron B., 1996, P NATL ACAD SCI, V93
   Einmahl U, 2005, ANN STAT, V33, P1380, DOI 10.1214/009053605000000129
   ELDRIDGE J., 2015, P 28 C LEARN THEOR, P588
   FELSENSTEIN J, 1985, EVOLUTION, V39, P783, DOI 10.1111/j.1558-5646.1985.tb00420.x
   Genovese CR, 2014, ANN STAT, V42, P1511, DOI 10.1214/14-AOS1218
   Guibas L, 2013, DISCRETE COMPUT GEOM, V49, P22, DOI 10.1007/s00454-012-9465-x
   Hartigan J. A., 1981, J AM STAT ASS
   Klemela J., 2009, SMOOTHING MULTIVARIA, V737
   KPOTUFE S., 2011, P 28 INT C MACH LEAR, P225
   Scott D. W., 2015, MULTIVARIATE DENSITY
   Silverman B. W., 1986, DENSITY ESTIMATION S, V26
   Stuetzle W., 2010, J COMPUTATIONAL GRAP, V19
   Wasserman L., 2010, ALL STAT CONCISE COU
   Wasserman L, 2006, ALL NONPARAMETRIC ST
   Wellner Jon, 2013, WEAK CONVERGENCE EMP
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704033
DA 2019-06-15
ER

PT S
AU Kingma, DP
   Salimans, T
   Jozefowicz, R
   Chen, X
   Sutskever, I
   Welling, M
AF Kingma, Diederik P.
   Salimans, Tim
   Jozefowicz, Rafal
   Chen, Xi
   Sutskever, Ilya
   Welling, Max
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Improved Variational Inference with Inverse Autoregressive Flow
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.
C1 [Welling, Max] Univ Amsterdam, Amsterdam, Netherlands.
   [Welling, Max] Univ Calif Irvine, Irvine, CA 92717 USA.
   [Welling, Max] Canadian Inst Adv Res CIFAR, Toronto, ON, Canada.
RP Welling, M (reprint author), Univ Amsterdam, Amsterdam, Netherlands.; Welling, M (reprint author), Univ Calif Irvine, Irvine, CA 92717 USA.; Welling, M (reprint author), Canadian Inst Adv Res CIFAR, Toronto, ON, Canada.
EM dpkingma@openai.com; tim@openai.com; rafal@openai.com; peter@openai.com;
   ilya@openai.com; M.Welling@uva.nl
CR Bowman S. R., 2015, ARXIV151106349
   Burda Y., 2015, ARXIV150900519
   Clevert D.A., 2015, ARXIV151107289
   Deco G., 1995, Advances in Neural Information Processing Systems 7, P247
   Dinh L., 2016, ARXIV160508803
   Dinh L., 2014, ARXIV14108516
   Germain M, 2015, RXIV150203509
   Gregor K, 2016, ARXIV160408772
   Gregor K., 2013, ARXIV13108499
   He K., 2016, ARXIV160305027
   He  K., 2015, ARXIV151203385
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Jozefowicz R., 2015, P 32 INT C MACH LEAR, V37, P2342, DOI DOI 10.1109/CVPR.2015.72987
   Kaae Sonderby C, 2016, ARXIV160202282
   Kingma D. P., 2013, P 2 INT C LEARN REPR
   Oord  A.v.d., 2016, ARXIV160106759
   Paisley J, 2012, P 29 INT C MACH LEAR, P1367
   Ranganath R, 2015, ARXIV151102386
   Rezende D., 2015, P 32 INT C MACH LEAR, P1530
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   Salimans T, 2016, ARXIV160208734
   Salimans T., 2016, ARXIV160207868
   Salimans T, 2014, ARXIV14106460
   Sohl-Dickstein J, 2015, ARXIV150303585
   Tran Dustin, 2015, ARXIV151106499
   Van Den Oord A., 2016, ARXIV160903499
   van den Oord A, 2016, ARXIV160605328
   van den Oord A., 2014, ADV NEURAL INFORM PR, P3518
   Zagoruyko S, 2016, ARXIV160507146
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
NR 30
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704105
DA 2019-06-15
ER

PT S
AU Kingravi, HA
   Maske, H
   Chowdhary, G
AF Kingravi, Hassan A.
   Maske, Harshal
   Chowdhary, Girish
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Kernel Observers: Systems-Theoretic Modeling and Inference of
   Spatiotemporally Evolving Processes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID NONSTATIONARY COVARIANCE FUNCTIONS
AB We consider the problem of estimating the latent state of a spatiotemporally evolving continuous function using very few sensor measurements. We show that layering a dynamical systems prior over temporal evolution of weights of a kernel model is a valid approach to spatiotemporal modeling, and that it does not require the design of complex nonstationary kernels. Furthermore, we show that such a differentially constrained predictive model can be utilized to determine sensing locations that guarantee that the hidden state of the phenomena can be recovered with very few measurements. We provide sufficient conditions on the number and spatial location of samples required to guarantee state recovery, and provide a lower bound on the minimum number of samples required to robustly infer the hidden states. Our approach outperforms existing methods in numerical experiments.
C1 [Kingravi, Hassan A.] Pindrop, Atlanta, GA 30308 USA.
   [Maske, Harshal; Chowdhary, Girish] Univ Illinois, Urbana, IL 61801 USA.
RP Kingravi, HA (reprint author), Pindrop, Atlanta, GA 30308 USA.
EM hkingravi@pindrop.com; hmaske2@illinois.edu; girishc@illinois.edu
FU AFOSR [FA9550-15-1-0146]
FX This work was supported by AFOSR grant #FA9550-15-1-0146.
CR Brezis  Haim, 2010, FUNCTIONAL ANAL SOBO
   Cressie NAC, 2011, STAT SPATIO TEMPORAL
   Csato L, 2002, NEURAL COMPUT, V14, P641, DOI 10.1162/089976602317250933
   Garg Sahil, 2012, P 26 AAAI C ART INT
   Higdon D, 1998, ENVIRON ECOL STAT, V5, P173, DOI 10.1023/A:1009666805688
   Jayasumana Sadeep, 2015, IEEE T PATTERN ANAL
   Ma CS, 2003, STAT PROBABIL LETT, V61, P411, DOI 10.1016/S0167-7152(02)00401-7
   Mardia KV, 1998, TEST, V7, P217, DOI 10.1007/BF02565111
   Paciorek CJ, 2004, ADV NEUR IN, V16, P273
   Perez-Cruz F, 2013, IEEE SIGNAL PROC MAG, V30, P40, DOI 10.1109/MSP.2013.2250352
   Pfingsten Tobias, 2006, NONSTATIONARY GAUSSI
   Plagemann C, 2008, LECT NOTES ARTIF INT, V5212, P204, DOI 10.1007/978-3-540-87481-2_14
   Rahimi A., 2007, ADV NEURAL INFORM PR, P1177
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Schmidt AM, 2003, J R STAT SOC B, V65, P743, DOI 10.1111/1467-9868.00413
   Singh Amarjeet, 2010, 2010 IEEE International Conference on Robotics and Automation (ICRA 2010), P5490, DOI 10.1109/ROBOT.2010.5509934
   Wikle CK, 2002, STAT MODEL, V2, P299, DOI 10.1191/1471082x02st036oa
   Williams CKI, 2001, ADV NEUR IN, V13, P682
   WONHAM WM, 1974, LINEAR MULTIVARIABLE
   Zhou  Kemin, 1996, ROBUST OPTIMAL CONTR
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701046
DA 2019-06-15
ER

PT S
AU Kirillov, A
   Shekhovtsov, A
   Rother, C
   Savchynskyy, B
AF Kirillov, Alexander
   Shekhovtsov, Alexander
   Rother, Carsten
   Savchynskyy, Bogdan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Joint M-Best-Diverse Labelings as a Parametric Submodular Minimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID ALGORITHM
AB We consider the problem of jointly inferring the M-best diverse labelings for a binary (high-order) submodular energy of a graphical model. Recently, it was shown that this problem can be solved to a global optimum, for many practically interesting diversity measures. It was noted that the labelings are, so-called, nested. This nestedness property also holds for labelings of a class of parametric submodular minimization problems, where different values of the global parameter 7 give rise to different solutions. The popular example of the parametric submodular minimization is the monotonic parametric max-flow problem, which is also widely used for computing multiple labelings. As the main contribution of this work we establish a close relationship between diversity with submodular energies and the parametric submodular minimization. In particular, the joint M-best diverse labelings can be obtained by running a non-parametric submodular minimization (in the special case- max-flow) solver for M different values of 7 in parallel, for certain diversity measures. Importantly, the values for 7 can be computed in a closed form in advance, prior to any optimization. These theoretical results suggest two simple yet efficient algorithms for the joint M-best diverse problem, which outperform competitors in terms of runtime and quality of results. In particular, as we show in the paper, the new methods compute the exact M-best diverse labelings faster than a popular method of Batra et al., which in some sense only obtains approximate solutions.
C1 [Kirillov, Alexander; Rother, Carsten; Savchynskyy, Bogdan] Tech Univ Dresden, Dresden, Germany.
   [Shekhovtsov, Alexander] Graz Univ Technol, Graz, Austria.
RP Kirillov, A (reprint author), Tech Univ Dresden, Dresden, Germany.
EM alexander.kirillov@tu-dresden.de
FU European Research Council (ERC) under the European Union's Horizon 2020
   research and innovation programme [647769]; ERC [640156]
FX This project has received funding from the European Research Council
   (ERC) under the European Union's Horizon 2020 research and innovation
   programme (grant agreement No 647769). A. Shekhovtsov was supported by
   ERC starting grant agreement 640156.
CR Ahmed F., 2015, ICCV
   Bach F, 2013, FOUND TRENDS MACH LE, V6, P145, DOI 10.1561/2200000039
   Batra D., 2012, UAI
   Batra D., 2012, ECCV
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Boykov Y., 2001, ICCV
   Carreira J, 2010, PROC CVPR IEEE, P3241, DOI 10.1109/CVPR.2010.5540063
   Chen C., 2013, AISTATS
   Dey D., 2015, ICCV
   Elidan G., 2011, PROBABILISTIC INFERE
   Everingham M., 2012, PASCAL VISUAL OBJECT
   Fleischer L, 2003, DISCRETE APPL MATH, V131, P311, DOI 10.1016/S0166-218X(02)00458-4
   Fromer M., 2009, NIPS, V22
   GALLO G, 1989, SIAM J COMPUT, V18, P30, DOI 10.1137/0218003
   Guzman-Rivera A., 2014, AISTATS
   Guzman-Rivera A., 2013, AISTATS
   Guzman-Rivera A., 2012, NIPS, V25
   Hochbaum DS, 2008, OPER RES, V56, P992, DOI 10.1287/opre.1080.0524
   Hower D, 2009, IEEE I CONF COMP VIS, P849, DOI 10.1109/ICCV.2009.5459305
   Jug F., 2014, BAMBI
   Kappes J., 2015, INT J COMPUT VISION, V115, P1, DOI DOI 10.1007/S11263-015-0809-X
   Kirillov A., 2015, NIPS
   Kirillov A., 2015, ICCV
   Kohli P., 2007, PAMI
   Kolmogorov V., 2007, IEEE 11 INT C COMP V, P1
   Kolmogorov V., 2004, TPAMI
   Kulesza A., 2010, NIPS, V23
   Lawler E. L., 1972, MANAGEMENT SCI, V18
   Long  J., 2015, CVPR
   Nilsson D, 1998, STAT COMPUT, V8, P159, DOI 10.1023/A:1008990218483
   Prasad A., 2014, NIPS, V27
   Premachandran V., 2014, CVPR
   Ramakrishna V., 2012, NIPS WORKSH PERT OPT
   Schlesinger M. I., 2002, 10 LECT STAT STRUCTU
   Sener O., 2015, ROBOTICS SCI SYSTEMS
   Szeliski R., 2008, TPAMI, V30, P1068, DOI DOI 10.1109/TPAMI.2007.70844
   Tarlow D., 2010, AISTATS
   TOPKIS DM, 1978, OPER RES, V26, P305, DOI 10.1287/opre.26.2.305
   Wang S., 2015, ICCV
   Yadollahpour P., 2013, CVPR
   Yanover C., 2004, NIPS, V17
NR 41
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703032
DA 2019-06-15
ER

PT S
AU Kondor, R
   Pan, H
AF Kondor, Risi
   Pan, Horace
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI The Multiscale Laplacian Graph Kernel
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Many real world graphs, such as the graphs of molecules, exhibit structure at multiple different scales, but most existing kernels between graphs are either purely local or purely global in character. In contrast, by building a hierarchy of nested subgraphs, the Multiscale Laplacian Graph kernels (MLG kernels) that we define in this paper can account for structure at a range of different scales. At the heart of the MLG construction is another new graph kernel, called the Feature Space Laplacian Graph kernel (FLG kernel), which has the property that it can lift a base kernel defined on the vertices of two graphs to a kernel between the graphs. The MLG kernel applies such FLG kernels to subgraphs recursively. To make the MLG kernel computationally feasible, we also introduce a randomized projection procedure, similar to the Nystrom method, but for RKHS operators.
C1 [Kondor, Risi] Univ Chicago, Dept Comp Sci, Dept Stat, Chicago, IL 60637 USA.
   [Pan, Horace] Univ Chicago, Dept Comp Sci, Chicago, IL 60637 USA.
RP Kondor, R (reprint author), Univ Chicago, Dept Comp Sci, Dept Stat, Chicago, IL 60637 USA.
EM risi@cs.uchicago.edu; hopan@uchicago.edu
FU University of Chicago Research Computing Center [DARPA-D16AP00112,
   NSF-1320344]
FX This work was completed in part with computing resources provided by the
   University of Chicago Research Computing Center and with the support of
   DARPA-D16AP00112 and NSF-1320344.
CR Alexa Marc, 2009, PROC EUR S GEOM PROC, V28
   Borgwardt K. M., 2005, P INT SYST MOL BIOL
   Borgwardt K.M., 2005, P IEEE INT C DAT MIN, P74, DOI DOI 10.1109/1CDM.2005.132
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Dai Hanjun, 2016, P INT C MACH LEARN I
   DEBNATH AK, 1991, J MED CHEM, V34, P786, DOI 10.1021/jm00106a046
   Drineas P, 2005, J MACH LEARN RES, V6, P2153
   Feragen Aasa, 2013, ADV NEURAL INFORM PR
   Gartner T., 2002, NIPS 02 WORKSH UNR D
   Inokuchi A, 2003, MACH LEARN, V50, P321, DOI 10.1023/A:1021726221443
   Jebara Tony, 2003, P ANN C COMP LEARN T
   Johansson F.D., 2015, 21 ACM SIGKDD INT C, P467
   Kondor R., 2008, P 25 INT C MACH LEAR, P496
   Kondor Risi, 2003, P INT C MACH LEARN I
   Kubinyi H, 2003, NAT REV DRUG DISCOV, V2, P665, DOI 10.1038/nrd1156
   Mika S, 1999, ADV NEUR IN, V11, P536
   Neumann M., 2016, MACHINE LEARNING
   Scholkopf B., 2002, LEARNING KERNELS
   Shervashidze N., 2009, P 12 INT C ART INT S, V2009, P488
   Shervashidze N, 2011, J MACH LEARN RES, V12, P2539
   Toivonen H, 2003, BIOINFORMATICS, V19, P1183, DOI 10.1093/bioinformatics/btg130
   Vishwanathan S. V. N., 2010, J MACHINE LEARNING R, V11
   Wale N, 2008, KNOWL INF SYST, V14, P347, DOI 10.1007/s10115-007-0103-5
   Williams C., 2001, ADV NEURAL INFORM PR
NR 24
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700099
DA 2019-06-15
ER

PT S
AU Kontorovich, A
   Sabato, S
   Urner, R
AF Kontorovich, Aryeh
   Sabato, Sivan
   Urner, Ruth
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Active Nearest-Neighbor Learning in Metric Spaces
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID CLASSIFICATION; CONVERGENCE; BOUNDS; RATES
AB We propose a pool-based non-parametric active learning algorithm for general metric spaces, called MArgin Regularized Metric Active Nearest Neighbor (MARMANN), which outputs a nearest-neighbor classifier. We give prediction error guarantees that depend on the noisy-margin properties of the input sample, and are competitive with those obtained by previously proposed passive learners. We prove that the label complexity of MARMANN is significantly lower than that of any passive learner with similar error guarantees. Our algorithm is based on a generalized sample compression scheme and a new label-efficient active model-selection procedure.
C1 [Kontorovich, Aryeh; Sabato, Sivan] Ben Gurion Univ Negev, Dept Comp Sci, IL-8499000 Beer Sheva, Israel.
   [Urner, Ruth] Max Planck Inst Intelligent Syst, Dept Empir Inference, D-72076 Tubingen, Germany.
RP Kontorovich, A (reprint author), Ben Gurion Univ Negev, Dept Comp Sci, IL-8499000 Beer Sheva, Israel.
FU Israel Science Foundation [1141/12, 755/15]; Yahoo Faculty award
FX Sivan Sabato was partially supported by the Israel Science Foundation
   (grant No. 555/15). Aryeh Kontorovich was partially supported by the
   Israel Science Foundation (grants No. 1141/12 and 755/15) and a Yahoo
   Faculty award. We thank Lee-Ad Gottlieb and Dana Ron for helpful
   discussions.
CR Awasthi P., 2013, CORR
   Balcan M. - F., 2007, COLT
   Balcan MF, 2010, MACH LEARN, V80, P111, DOI 10.1007/s10994-010-5174-y
   Balcan MF, 2009, J COMPUT SYST SCI, V75, P78, DOI 10.1016/j.jcss.2008.07.003
   Berlind C., 2015, P 32 INT C MACH LEAR, P1870
   Castro RM, 2007, LECT NOTES COMPUT SC, V4539, P5, DOI 10.1007/978-3-540-72927-3_3
   Chaudhuri K., 2014, NIPS
   COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964
   Dasgupta S., 2004, ADV NEURAL INFORM PR, V17, P337
   Dasgupta S., 2008, P 25 INT C MACH LEAR, P208, DOI DOI 10.1145/1390156.1390183
   Dasgupta S., 2012, COLT
   Devroye L, 1985, WILEY SERIES PROBABI
   Devroye L., 1996, APPL MATH, V31
   FIX E, 1989, INT STAT REV, V57, P238, DOI 10.2307/1403797
   Gonen A, 2013, J MACH LEARN RES, V14, P2583
   Gottlieb L., 2016, ARTIFICIAL INTELLIGE
   Gottlieb L., 2014, ADV NEURAL INFORM PR, P370
   Gottlieb L.-A., 2010, COLT, P433
   Gottlieb LA, 2016, THEOR COMPUT SCI, V620, P105, DOI 10.1016/j.tcs.2015.10.040
   Gottlieb LA, 2014, IEEE T INFORM THEORY, V60, P5750, DOI 10.1109/TIT.2014.2339840
   Gottlieb LA, 2010, LECT NOTES COMPUT SC, V6302, P192, DOI 10.1007/978-3-642-15369-3_15
   Graepel T, 2005, MACH LEARN, V59, P55, DOI 10.1007/s10994-005-0462-7
   Hanneke S, 2015, J MACH LEARN RES, V16, P3487
   Hanneke S, 2011, ANN STAT, V39, P333, DOI 10.1214/10-AOS843
   Kontorovich A., 2015, AISTATS
   Kontorovich A., 2016, CORR
   Kpotufe S., 2011, NIPS
   Kpotufe S., 2015, P 28 ANN C LEARN THE, P1176
   Krauthgamer R., 2004, P 15 ANN ACM SIAM S, P791
   KULKARNI SR, 1995, IEEE T INFORM THEORY, V41, P1028, DOI 10.1109/18.391248
   Maurer Andreas, 2009, COLT
   McCallum A. K., 1998, ICML
   STONE CJ, 1977, ANN STAT, V5, P595, DOI 10.1214/aos/1176343886
   Urner R., 2013, 26 ANN C LEARN THEOR, P376
   von Luxburg U, 2004, J MACH LEARN RES, V5, P669
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700064
DA 2019-06-15
ER

PT S
AU Koolen, WM
   Grunwald, P
   van Erven, T
AF Koolen, Wouter M.
   Grunwald, Peter
   van Erven, Tim
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Combining Adversarial Guarantees and Stochastic Fast Rates in Online
   Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID BOUNDS
AB We consider online learning algorithms that guarantee worst-case regret rates in adversarial environments (so they can be deployed safely and will perform robustly), yet adapt optimally to favorable stochastic environments (so they will perform well in a variety of settings of practical importance). We quantify the friendliness of stochastic environments by means of the well-known Bernstein (a.k.a. generalized Tsybakov margin) condition. For two recent algorithms (Squint for the Hedge setting and MetaGrad for online convex optimization) we show that the particular form of their data-dependent individual-sequence regret guarantees implies that they adapt automatically to the Bernstein parameters of the stochastic environment. We prove that these algorithms attain fast rates in their respective settings both in expectation and with high probability.
C1 [Koolen, Wouter M.] Ctr Wiskunde & Informat, Sci Pk 123, NL-1098 XG Amsterdam, Netherlands.
   [Grunwald, Peter] CWI, Nampa, ID USA.
   [Grunwald, Peter; van Erven, Tim] Leiden Univ, Niels Bohrweg 1, NL-2333 CA Leiden, Netherlands.
RP Koolen, WM (reprint author), Ctr Wiskunde & Informat, Sci Pk 123, NL-1098 XG Amsterdam, Netherlands.
EM wmkoolen@cwi.nl; pdg@cwi.nl; tim@timvanerven.nl
FU Netherlands Organization for Scientific Research (NWO) [639.021.439]
FX Koolen acknowledges support by the Netherlands Organization for
   Scientific Research (NWO, Veni grant 639.021.439).
CR Audibert J-Y., 2004, THESIS
   Audibert JY, 2009, ANN STAT, V37, P1591, DOI 10.1214/08-AOS623
   Bartlett PL, 2006, PROBAB THEORY REL, V135, P311, DOI 10.1007/s00440-005-0462-3
   Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Cesa-Bianchi N, 2007, MACH LEARN, V66, P321, DOI 10.1007/s10994-006-5001-7
   Chiang C., 2012, P 25 C LEARN THEOR C
   Crammer K., 2009, NIPS, V22
   de Rooij S, 2014, J MACH LEARN RES, V15, P1281
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Even-Dar E., 2008, MACHINE LEARNING, V72
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Gaillard P., 2014, P 27 COLT
   Gaillard P., 2015, P 28 C LEARN THEOR C
   Grunwald P., 2012, ALT 12
   Hazan E, 2010, MACH LEARN, V80, P165, DOI 10.1007/s10994-010-5175-x
   Koltchinskii V, 2006, ANN STAT, V34, P2593, DOI 10.1214/009053606000001019
   Koolen W., 2015, RELATIVE ENTROPY BOU
   Koolen W. M., 2015, COLT, V40, P1155
   Koolen Wouter M., 2014, P ADV NEUR INF PROC, V27, P2294
   Luo H., 2015, P 28 COLT
   Massart P, 2006, ANN STAT, V34, P2326, DOI 10.1214/009053606000000786
   McMahan H. B, 2010, P 23 C LEARN THEOR, P244
   Mehta N., 2014, NIPS, V27
   Orabona F, 2015, MACH LEARN, V99, P411, DOI 10.1007/s10994-014-5474-8
   Rakhlin A., 2014, P 27 COLT
   Sani A., 2014, NIPS, V27
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Steinhardt Jacob, 2014, P 31 INT C MACH LEAR, P1593
   Tsybakov AB, 2004, ANN STAT, V32, P135
   van Erven T., 2016, ADV NEURAL INFORM PR, V29
   van Erven T, 2015, J MACH LEARN RES, V16, P1793
   Wintenberger O., 2015, ARXIV14041356
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703110
DA 2019-06-15
ER

PT S
AU Krause, O
   Arbones, DR
   Igel, C
AF Krause, Oswin
   Arbones, Didac R.
   Igel, Christian
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI CMA-ES with Optimal Covariance Update and Storage Complexity
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID EVOLUTION
AB The covariance matrix adaptation evolution strategy (CMA-ES) is arguably one of the most powerful real-valued derivative-free optimization algorithms, finding many applications in machine learning. The CMA-ES is a Monte Carlo method, sampling from a sequence of multi-variate Gaussian distributions. Given the function values at the sampled points, updating and storing the covariance matrix dominates the time and space complexity in each iteration of the algorithm. We propose a numerically stable quadratic-time covariance matrix update scheme with minimal memory requirements based on maintaining triangular Cholesky factors. This requires a modification of the cumulative step-size adaption (CSA) mechanism in the CMA-ES, in which we replace the inverse of the square root of the covariance matrix by the inverse of the triangular Cholesky factor. Because the triangular Cholesky factor changes smoothly with the matrix square root, this modification does not change the behavior of the CMA-ES in terms of required objective function evaluations as verified empirically. Thus, the described algorithm can and should replace the standard CMA-ES if updating and storing the covariance matrix matters.
C1 [Krause, Oswin; Arbones, Didac R.; Igel, Christian] Univ Copenhagen, Dept Comp Sci, Copenhagen, Denmark.
RP Krause, O (reprint author), Univ Copenhagen, Dept Comp Sci, Copenhagen, Denmark.
EM oswin.krause@di.ku.dk; didac@di.ku.dk; igel@di.ku.dk
RI Krause, Oswin/L-8814-2016
FU Innovation Fund Denmark through the project "Personalized breast cancer
   screening"; Innovation Fund Denmark through the project "Cyber Fraud
   Detection Using Advanced Machine Learning Techniques"
FX We acknowledge support from the Innovation Fund Denmark through the
   projects "Personalized breast cancer screening" (OK, CI) and "Cyber
   Fraud Detection Using Advanced Machine Learning Techniques" (DRA, CI).
CR Akimoto Y, 2014, GECCO'14: PROCEEDINGS OF THE 2014 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P373, DOI 10.1145/2576768.2598258
   Akimoto Y, 2012, ALGORITHMICA, V64, P698, DOI 10.1007/s00453-011-9564-8
   Auger A., 2015, THESIS
   Beyer H. - G., 2007, SCHOLARPEDIA, V2, P1965
   Beyer HG, 2014, EVOL COMPUT, V22, P679, DOI 10.1162/EVCO_a_00132
   Bringmann K, 2013, ARTIF INTELL, V204, P22, DOI 10.1016/j.artint.2013.08.001
   Eiben AE, 2015, NATURE, V521, P476, DOI 10.1038/nature14544
   Gomez F, 2008, J MACH LEARN RES, V9, P937
   Hansen M, 1996, IEEE C EVOL COMPUTAT, P312, DOI 10.1109/ICEC.1996.542381
   Hansen N, 2001, EVOL COMPUT, V9, P159, DOI 10.1162/106365601750190398
   Hansen N., 2015, TECHNICAL REPORT
   Heidrich-Meisner V., 2009, P 26 ANN INT C MACH, P401, DOI 10.1145/1553374.1553426
   Heidrich-Meisner V, 2009, J ALGORITHMS, V64, P152, DOI 10.1016/j.jalgor.2009.04.002
   Igel C., 2010, ENCY MACHINE LEARNIN
   Igel C, 2008, J MACH LEARN RES, V9, P993
   Krause O., 2015, P 2015 ACM C FDN GEN, P129
   Loshchilov I., 2015, EVOLUTIONARY COMPUTA
   Loshchilov I, 2014, GECCO'14: PROCEEDINGS OF THE 2014 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P397, DOI 10.1145/2576768.2598294
   Omidvar MN, 2010, LECT NOTES ARTIF INT, V6464, P303, DOI 10.1007/978-3-642-17432-2_31
   Poland J., 2001, P GEN EV COMP C, P1050
   Ros R, 2008, LECT NOTES COMPUT SC, V5199, P296, DOI 10.1007/978-3-540-87700-4_30
   Stich S. U., PARALLEL PROBLEM SOL, P448
   SUN Y, 2013, PROCEEDINGS OF THE G, P61
   Suttorp T, 2009, MACH LEARN, V75, P167, DOI 10.1007/s10994-009-5102-1
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703093
DA 2019-06-15
ER

PT S
AU Krichene, W
   Bayen, AM
   Bartlett, PL
AF Krichene, Walid
   Bayen, Alexandre M.
   Bartlett, Peter L.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Adaptive Averaging in Accelerated Descent Dynamics
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We study accelerated descent dynamics for constrained convex optimization. This dynamics can be described naturally as a coupling of a dual variable accumulating gradients at a given rate eta(t), and a primal variable obtained as the weighted average of the mirrored dual trajectory, with weights w(t). Using a Lyapunov argument, we give sufficient conditions on eta and w to achieve a desired convergence rate. As an example, we show that the replicator dynamics (an example of mirror descent on the simplex) can be accelerated using a simple averaging scheme.
   We then propose an adaptive averaging heuristic which adaptively computes the weights to speed up the decrease of the Lyapunov function. We provide guarantees on adaptive averaging in continuous-time, prove that it preserves the quadratic convergence rate of accelerated first-order methods in discrete-time, and give numerical experiments to compare it with existing heuristics, such as adaptive restarting. The experiments indicate that adaptive averaging performs at least as well as adaptive restarting, with significant improvements in some cases.
C1 [Krichene, Walid; Bayen, Alexandre M.; Bartlett, Peter L.] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Bartlett, Peter L.] QUT, Brisbane, Qld, Australia.
   [Krichene, Walid] Google, New York, NY USA.
RP Krichene, W (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.; Krichene, W (reprint author), Google, New York, NY USA.
EM walid@eecs.berkeley.edu; bayen@berkeley.edu; bartlett@cs.berkeley.edu
CR Attouch H., 2015, CORR
   Attouch H., 2016, CORR
   Attouch H, 2016, SIAM J OPTIMIZ, V26, P1824, DOI 10.1137/15M1046095
   Aubin J.-P., 1991, VIABILITY THEORY
   Bloch A., 1994, HAMILTONIAN GRADIENT
   BROWN AA, 1989, J OPTIMIZ THEORY APP, V62, P211, DOI 10.1007/BF00941054
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Flammarion N., 2015, P 28 C LEARN THEOR C, P658
   Helmke U., 1994, COMMUNICATIONS CONTR
   Kingma D.P., 2014, P 3 INT C LEARN REPR
   Krichene W., 2015, NIPS
   Lyapunov A., 1992, CONTROL THEORY APPL
   Nemirovsky A. S., 1983, WILEY INTERSCIENCE S
   Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5
   Nesterov Y., 1983, SOV MATH DOKL, V27, P372
   Nesterov Y, 2008, MATH PROGRAM, V112, P159, DOI 10.1007/s10107-006-0089-x
   O'Donoghue B, 2015, FOUND COMPUT MATH, V15, P715, DOI 10.1007/s10208-013-9150-3
   Rockafellar R T, 1970, CONVEX ANAL
   Sigmund K., 1986, COMPLEXITY LANGUAGE, P88
   Su Weijie, 2014, NIPS
   Teschl G., 2012, ORDINARY DIFFERENTIA, V140
   Weibull J W., 1997, EVOLUTIONARY GAME TH
   Wibisono A., 2016, CORR
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704077
DA 2019-06-15
ER

PT S
AU Kriege, NM
   Giscard, PL
   Wilson, RC
AF Kriege, Nils M.
   Giscard, Pierre-Louis
   Wilson, Richard C.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI On Valid Optimal Assignment Kernels and Applications to Graph
   Classification
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB The success of kernel methods has initiated the design of novel positive semidefinite functions, in particular for structured data. A leading design paradigm for this is the convolution kernel, which decomposes structured objects into their parts and sums over all pairs of parts. Assignment kernels, in contrast, are obtained from an optimal bijection between parts, which can provide a more valid notion of similarity. In general however, optimal assignments yield indefinite functions, which complicates their use in kernel methods. We characterize a class of base kernels used to compare parts that guarantees positive semidefinite optimal assignment kernels. These base kernels give rise to hierarchies from which the optimal assignment kernels are computed in linear time by histogram intersection. We apply these results by developing the Weisfeiler-Lehman optimal assignment kernel for graphs. It provides high classification accuracy on widely-used benchmark data sets improving over the original Weisfeiler-Lehman kernel.
C1 [Kriege, Nils M.] TU Dortmund, Dept Comp Sci, Dortmund, Germany.
   [Giscard, Pierre-Louis; Wilson, Richard C.] Univ York, Dept Comp Sci, York, N Yorkshire, England.
RP Kriege, NM (reprint author), TU Dortmund, Dept Comp Sci, Dortmund, Germany.
EM nils.kriege@tu-dortmund.de; pierre-louis.giscard@york.ac.uk;
   richard.wilson@york.ac.uk
FU German Science Foundation (DFG) within the Collaborative Research Center
   [SFB 876]; Royal Commission for the Exhibition of 1851
FX N. M. Kriege is supported by the German Science Foundation (DFG) within
   the Collaborative Research Center SFB 876 "Providing Information by
   Resource-Constrained Data Analysis", project A6 "Resource-efficient
   Graph Mining". P.-L. Giscard is grateful for the financial support
   provided by the Royal Commission for the Exhibition of 1851.
CR Bai  L., 2015, P ICML, P30
   Barla A, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P513
   Borgwardt K.M., 2005, P IEEE INT C DAT MIN, P74, DOI DOI 10.1109/1CDM.2005.132
   Borgwardt KM, 2005, BIOINFORMATICS, V21, pI47, DOI 10.1093/bioinformatics/bti1007
   Boughorbel S., 2005, INT C IM P ICIP 2005
   Burkard R., 2012, ASSIGNMENT PROBLEMS
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Frohlich H., 2005, P 22 INT C MACH LEAR, P225
   Gori M, 2005, IEEE T PATTERN ANAL, V27, P1100, DOI 10.1109/TPAMI.2005.138
   Grauman K., 2007, P ADV NEUR INF PROC, V19, P505
   Grauman K, 2007, J MACH LEARN RES, V8, P725
   Haussler D., 1999, UCSCCRL9910
   Ismagilov RS, 1997, MATH NOTES+, V62, P186, DOI 10.1007/BF02355907
   Johansson F.D., 2015, 21 ACM SIGKDD INT C, P467
   Loosli G., 2015, IEEE T PATTERN ANAL, P1
   Pachauri D., 2013, ADV NEURAL INFORM PR, P1860
   Riesen K, 2009, IMAGE VISION COMPUT, V27, P950, DOI 10.1016/j.imavis.2008.04.004
   Schiavinato M, 2015, LECT NOTES COMPUT SC, V9370, P146, DOI 10.1007/978-3-319-24261-3_12
   Shervashidze N, 2011, J MACH LEARN RES, V12, P2539
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153
   Vert J.P., 2008, CORR
   Vishwanathan SVN, 2010, J MACH LEARN RES, V11, P1201
   Yanardag P., 2015, P 21 ACM SIGKDD INT, P1365, DOI DOI 10.1145/2783258.2783417
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701023
DA 2019-06-15
ER

PT S
AU Krishnamurthy, A
   Agarwal, A
   Dudik, M
AF Krishnamurthy, Akshay
   Agarwal, Alekh
   Dudik, Miroslav
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Contextual semibandits via supervised learning oracles
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We study an online decision making problem where on each round a learner chooses a list of items based on some side information, receives a scalar feedback value for each individual item, and a reward that is linearly related to this feedback. These problems, known as contextual semibandits, arise in crowdsourcing, recommendation, and many other domains. This paper reduces contextual semibandits to supervised learning, allowing us to leverage powerful supervised learning methods in this partial-feedback setting. Our first reduction applies when the mapping from feedback to reward is known and leads to a computationally efficient algorithm with near-optimal regret. We show that this algorithm outperforms state-of-the-art approaches on real-world learning-to-rank datasets, demonstrating the advantage of oracle-based algorithms. Our second reduction applies to the previously unstudied setting when the linear mapping from feedback to reward is unknown. Our regret guarantees are superior to prior techniques that ignore the feedback.
C1 [Krishnamurthy, Akshay] Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.
   [Agarwal, Alekh; Dudik, Miroslav] Microsoft Res, New York, NY USA.
RP Krishnamurthy, A (reprint author), Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.
EM akshay@cs.umass.edu; alekha@microsoft.com; mdudik@microsoft.com
CR Agarwal A., 2014, ICML
   Audibert J.-Y., 2014, MATH OF OR
   Auer Peter, 2002, SIAM J COMPUTING
   Cesa-Bianchi N., 2012, JCSS
   Chapelle O., 2011, YAHOO LEARNING RANK
   Chen W., 2013, ICML
   Chu W., 2011, AISTATS
   Daume H., 2009, MLJ
   Dudik M., 2011, UAI
   Gyorgy A., 2007, JMLR
   Hsu Daniel, 2010, THESIS
   Kale S., 2010, NIPS
   Kveton B., 2015, AISTATS
   Lafferty J. O., 2001, ICML
   Langford J., 2008, NIPS
   Li L., 2010, WWW
   MSLR, MSLR MICR LEARN RANK
   Neu G., 2015, NIPS
   Qin L., 2014, ICDM
   Rakhlin A., 2016, ICML
   Robins JM, 1989, HLTH SERVICE RES MET
   Swaminathan A., 2016, ARXIV160504812V2
   Syrgkanis V., 2016, ICML
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704038
DA 2019-06-15
ER

PT S
AU Krishnamurthy, A
   Agarwal, A
   Langford, J
AF Krishnamurthy, Akshay
   Agarwal, Alekh
   Langford, John
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI PAC Reinforcement Learning with Rich Observations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We propose and study a new model for reinforcement learning with rich observations, generalizing contextual bandits to sequential decision making. These models require an agent to take actions based on observations ( features) with the goal of achieving long-term performance competitive with a large set of policies. To avoid barriers to sample-efficient learning associated with large observation spaces and general POMDPs, we focus on problems that can be summarized by a small number of hidden states and have long-term rewards that are predictable by a reactive function class. In this setting, we design and analyze a new reinforcement learning algorithm, Least Squares Value Elimination by Exploration. We prove that the algorithm learns near optimal behavior after a number of episodes that is polynomial in all relevant parameters, logarithmic in the number of policies, and independent of the size of the observation space. Our result provides theoretical justification for reinforcement learning with function approximation.
C1 [Krishnamurthy, Akshay] Univ Massachusetts, Amherst, MA 01003 USA.
   [Agarwal, Alekh; Langford, John] Microsoft Res, New York, NY 10011 USA.
RP Krishnamurthy, A (reprint author), Univ Massachusetts, Amherst, MA 01003 USA.
EM akshay@cs.umass.edu; alekha@microsoft.com; jcl@microsoft.com
CR Agarwal A., 2012, AISTATS
   Antos A., 2008, MLJ
   Auer P., 2002, SICOMP
   Azizzadenesheli K., 2016, COLT
   Baird L., 1995, ICML
   Brafman R. I., 2003, JMLR
   Dann C., 2015, NIPS
   Dudik M., 2011, UAI
   Jiang N., 2015, ICML
   Jong N., 2007, ABSTRACTION REFORMUL
   Kakade S., 2003, ICML
   Kakade Sham, 2002, ICML
   Kearns M., 1999, NIPS
   Kearns M. J., 2002, MLJ
   Langford J., 2008, NIPS
   Li L., 2010, ANN MATH AI
   Li L., 2006, ISAIM
   Mansour Y., 1999, COLT
   Meuleau N., 1999, UAI
   Mnih V., 2015, NATURE
   Nguyen P., 2013, AISTATS
   Pazis Jason, 2016, AAAI
   Perkins T. J., 2002, NIPS
   Reveliotis S., 2007, DEDS
   Strehl A. L, 2006, ICML
   Sutton R. S., 1999, NIPS
   Tsitsiklis J. N., 1997, IEEE TAC
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704099
DA 2019-06-15
ER

PT S
AU Krishnasamy, S
   Sen, R
   Johari, R
   Shakkottai, S
AF Krishnasamy, Subhashini
   Sen, Rajat
   Johari, Ramesh
   Shakkottai, Sanjay
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Regret of Queueing Bandits
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID ALLOCATION; POLICIES; RULE
AB We consider a variant of the multiarmed bandit problem where jobs queue for service, and service rates of different servers may be unknown. We study algorithms that minimize queue-regret: the (expected) difference between the queue-lengths obtained by the algorithm, and those obtained by a "genie"-aided matching algorithm that knows exact service rates. A naive view of this problem would suggest that queue-regret should grow logarithmically: since queue-regret cannot be larger than classical regret, results for the standard MAB problem give algorithms that ensure queue-regret increases no more than logarithmically in time. Our paper shows surprisingly more complex behavior. In particular, the naive intuition is correct as long as the bandit algorithm's queues have relatively long regenerative cycles: in this case queue-regret is similar to cumulative regret, and scales (essentially) logarithmically. However, we show that this "early stage" of the queueing bandit eventually gives way to a "late stage", where the optimal queue-regret scaling is O (1/t). We demonstrate an algorithm that (order-wise) achieves this asymptotic queue-regret, and also exhibits close to optimal switching time from the early stage to the late stage.
C1 [Krishnasamy, Subhashini; Sen, Rajat; Shakkottai, Sanjay] Univ Texas Austin, Austin, TX 78712 USA.
   [Johari, Ramesh] Stanford Univ, Stanford, CA 94305 USA.
RP Krishnasamy, S (reprint author), Univ Texas Austin, Austin, TX 78712 USA.
FU NSF [CNS-1161868, CNS-1343383, CNS-1320175]; ARO [W911NF-16-1-0377,
   W911NF-15-1-0227, W911NF-14-1-0387]; US DoT
FX This work is partially supported by NSF Grants CNS-1161868, CNS-1343383,
   CNS-1320175, ARO grants W911NF-16-1-0377, W911NF-15-1-0227,
   W911NF-14-1-0387 and the US DoT supported D-STOP Tier 1 University
   Transportation Center.
CR Agrawal S., 2011, ARXIV11111797
   Audibert J. Y., 2010, COLT 23 C LEARN THEO, P13
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Bubeck S., 2013, ARXIV13021611
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   BUYUKKOC C, 1985, ADV APPL PROBAB, V17, P237, DOI 10.2307/1427064
   Chapelle O, 2011, ADV NEURAL INFORM PR, P2249
   Combes Richard, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P245, DOI 10.1145/2745844.2745847
   Cox D. R., 1961, QUEUES
   Garivier A., 2011, ARXIV11022490
   GITTINS JC, 1979, J ROY STAT SOC B MET, V41, P148
   Jacko P., 2010, MODERN TRENDS CONTRO, P248
   Kaufmann Emilie, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P199, DOI 10.1007/978-3-642-34106-9_18
   Kushner H., 2013, HEAVY TRAFFIC ANAL C, V47
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Lott C, 2000, PROBAB ENG INFORM SC, V14, P259, DOI 10.1017/S0269964800143013
   Mahajan A, 2008, SIGNALS COMMUN TECHN, P121, DOI 10.1007/978-0-387-49819-5_6
   Nino-Mora J, 2007, TOP, V15, P161, DOI 10.1007/s11750-007-0025-0
   Nino-Mora J, 2006, QUEUEING SYST, V54, P281, DOI 10.1007/s11134-006-0302-x
   Perchet V., 2015, ARXIV150500369
   Russo D, 2014, MATH OPER RES, V39, P1221, DOI 10.1287/moor.2014.0650
   Salomon A, 2013, J MACH LEARN RES, V14, P187
   Scott SL, 2010, APPL STOCH MODEL BUS, V26, P639, DOI 10.1002/asmb.874
   Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.2307/2332286
   Tsybakov A. B., 2008, INTRO NONPARAMETRIC
   van Mieghem JA, 1995, ANN APPL PROBAB, V5, P809, DOI 10.1214/aoap/1177004706
   Whitt W, 1974, LECTURE NOTES EC MAT, V98, P307, DOI 10.1007/978-3-642-80838-8_15
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703006
DA 2019-06-15
ER

PT S
AU Krotov, D
   Hopfield, JJ
AF Krotov, Dmitry
   Hopfield, John J.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Dense Associative Memory for Pattern Recognition
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID NEURAL-NETWORKS; CAPACITY
AB A model of associative memory is studied, which stores and reliably retrieves many more patterns than the number of neurons in the network. We propose a simple duality between this dense associative memory and neural networks commonly used in deep learning. On the associative memory side of this duality, a family of models that smoothly interpolates between two limiting cases can be constructed. One limit is referred to as the feature-matching mode of pattern recognition, and the other one as the prototype regime. On the deep learning side of the duality, this family corresponds to feedforward neural networks with one hidden layer and various activation functions, which transmit the activities of the visible neurons to the hidden layer. This family of activation functions includes logistics, rectified linear units, and rectified polynomials of higher degrees. The proposed duality makes it possible to apply energy-based intuition from associative memory to analyze computational properties of neural networks with unusual activation functions-the higher rectified polynomials which until now have not been used in deep learning. The utility of the dense memories is illustrated for two test cases: the logical gate XOR and the recognition of handwritten digits from the MNIST data set.
C1 [Krotov, Dmitry] Inst Adv Study, Simons Ctr Syst Biol, Olden Lane, Princeton, NJ 08540 USA.
   [Hopfield, John J.] Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA.
RP Krotov, D (reprint author), Inst Adv Study, Simons Ctr Syst Biol, Olden Lane, Princeton, NJ 08540 USA.
EM krotov@ias.edu; hopfield@princeton.edu
CR ABBOTT LF, 1987, PHYS REV A, V36, P5091, DOI 10.1103/PhysRevA.36.5091
   AMIT DJ, 1985, PHYS REV LETT, V55, P1530, DOI 10.1103/PhysRevLett.55.1530
   BALDI P, 1987, PHYS REV LETT, V58, P913, DOI 10.1103/PhysRevLett.58.913
   Chen H. H., 1986, AIP Conference Proceedings, P86
   GARDNER E, 1987, J PHYS A-MATH GEN, V20, P3453, DOI 10.1088/0305-4470/20/11/046
   Glorot X., 2011, P 14 INT C ART INT S, P315, DOI DOI 10.1177/1753193410395357
   Goodfellow IJ, 2014, ARXIV14126572
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   HORN D, 1988, J PHYS-PARIS, V49, P389, DOI 10.1051/jphys:01988004903038900
   Kamyshanska H., 2013, ICML, P720
   Kamyshanska H, 2015, IEEE T PATTERN ANAL, V37, P1261, DOI 10.1109/TPAMI.2014.2362140
   KANTER I, 1987, PHYS REV A, V35, P380, DOI 10.1103/PhysRevA.35.380
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   LeCun Yann, 2006, PREDICTING STRUCTURE, V1
   MCELIECE RJ, 1987, IEEE T INFORM THEORY, V33, P461, DOI 10.1109/TIT.1987.1057328
   Minsky M, 1969, PERCEPTRON INTRO COM, V19, P2
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Psaltis D., 1986, AIP Conference Proceedings, P370
   Rasmus A., 2015, ADV NEURAL INFORM PR, P3546
   Simard P., 2003, BEST PRACTICES CONVO, P958
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Wan L., 2013, P 30 INT C MACH LEAR, P1058
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700085
DA 2019-06-15
ER

PT S
AU Krummenacher, G
   McWilliams, B
   Kilcher, Y
   Buhmann, JM
   Meinshausen, N
AF Krummenacher, Gabriel
   McWilliams, Brian
   Kilcher, Yannic
   Buhmann, Joachim M.
   Meinshausen, Nicolai
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Scalable Adaptive Stochastic Optimization Using Random Projections
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Adaptive stochastic gradient methods such as ADAGRAD have gained popularity in particular for training deep neural networks. The most commonly used and studied variant maintains a diagonal matrix approximation to second order information by accumulating past gradients which are used to tune the step size adaptively. In certain situations the full-matrix variant of ADAGRAD is expected to attain better performance, however in high dimensions it is computationally impractical. We present ADA-LR and RADAGRAD two computationally efficient approximations to full-matrix ADAGRAD based on randomized dimensionality reduction. They are able to capture dependencies between features and achieve similar performance to full-matrix ADAGRAD but at a much smaller computational cost. We show that the regret of ADA-LR is close to the regret of full-matrix ADAGRAD which can have an up-to exponentially smaller dependence on the dimension than the diagonal variant. Empirically, we show that ADA-LR and RADAGRAD perform similarly to full-matrix ADAGRAD. On the task of training convolutional neural networks as well as recurrent neural networks, RADAGRAD achieves faster convergence than diagonal ADAGRAD.
C1 [Krummenacher, Gabriel; Buhmann, Joachim M.; Meinshausen, Nicolai] Swiss Fed Inst Technol, Dept Comp Sci, Inst Machine Learning, Zurich, Switzerland.
   [Meinshausen, Nicolai] Swiss Fed Inst Technol, Dept Math, Seminar Stat, Zurich, Switzerland.
   [McWilliams, Brian] Disney Res, Zurich, Switzerland.
RP Krummenacher, G (reprint author), Swiss Fed Inst Technol, Dept Comp Sci, Inst Machine Learning, Zurich, Switzerland.
EM gabriel.krummenacher@inf.ethz.ch; brian@disneyresearch.com;
   yannic.kilcher@inf.ethz.ch; jbuhmann@inf.ethz.ch;
   meinshausen@stat.math.ethz.ch
CR Allen-Zhu Z., 2016, P 33 INT C MACH LEAR
   Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746
   Balduzzi D., 2016, ARXIV160401952
   Balduzzi D., 2016, P 33 INT C MACH LEAR
   Byrd R. H., 2014, ARXIV14017020
   Dauphin Y N., 2015, ARXIV150204390
   Defazio A., 2014, ADV NEURAL INFORM PR
   Desjardins G., 2015, ADV NEURAL INFORM PR, P2062
   Duchi J. C., 2013, ADV NEURAL INFORM PR
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Golub G. H., 2012, MATRIX COMPUTATIONS, V3
   Gonen A., 2015, ARXIV150602649
   Gong YC, 2013, PROC CVPR IEEE, P484, DOI 10.1109/CVPR.2013.69
   Grosse R., 2015, P 32 INT C MACH LEAR, P2304
   Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806
   Heinze C., 2016, P AISTATS
   Heinze C., 2014, ARXIV14063469
   Hofmann T., 2015, ADV NEURAL INFORM PR
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Keskar N. S., 2015, ADAQN ADAPTIVE QUASI
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Lucchi A., 2015, ARXIV150308316
   Luo H., 2016, ARXIV160202202
   Mahoney M. W., 2011, ARXIV11045557V3
   Marcus M.P., 1993, COMPUTATIONAL LINGUI, V19, P313, DOI DOI 10.1080/07494460903404410
   Martens J., 2015, P 32 INT C MACH LEAR
   McWilliams B, 2014, ADV NEUR IN, V27
   Neyshabur Behnam, 2015, ADV NEURAL INFORM PR, P2413
   Pennington J., 2014, EMNLP, P1532, DOI DOI 10.3115/V1/D14-1162
   Zhang L., 2012, ARXIV12113046
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700018
DA 2019-06-15
ER

PT S
AU Kulkarni, TD
   Narasimhan, KR
   Saeedi, A
   Tenenbaum, JB
AF Kulkarni, Tejas D.
   Narasimhan, Karthik R.
   Saeedi, Ardavan
   Tenenbaum, Joshua B.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Hierarchical Deep Reinforcement Learning: Integrating Temporal
   Abstraction and Intrinsic Motivation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. One of the key difficulties is insufficient exploration, resulting in an agent being unable to learn robust policies. Intrinsically motivated agents can explore new behavior for their own sake rather than to directly solve external goals. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical action-value functions, operating at different temporal scales, with goal-driven intrinsically motivated deep reinforcement learning. A top-level q-value function learns a policy over intrinsic goals, while a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse and delayed feedback: (1) a complex discrete stochastic decision process with stochastic transitions, and (2) the classic ATARI game - 'Montezuma's Revenge'.
C1 [Kulkarni, Tejas D.] DeepMind, London, England.
   [Narasimhan, Karthik R.; Saeedi, Ardavan] MIT, CSAIL, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
   [Tenenbaum, Joshua B.] MIT, BCS, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
   [Kulkarni, Tejas D.] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
RP Kulkarni, TD (reprint author), DeepMind, London, England.; Kulkarni, TD (reprint author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
EM tejasdkulkarni@gmail.com; karthikn@mit.edu; ardavans@mit.edu;
   jbt@mit.edu
CR Baranes A, 2013, ROBOT AUTON SYST, V61, P49, DOI 10.1016/j.robot.2012.05.008
   Barto AG, 2003, DISCRETE EVENT DYN S, V13, P343
   Bellemare Marc G, 2012, J ARTIFICIAL INTELLI
   Cobo L. C., 2013, P 2013 INT C AUT AG, P1061
   DAYAN P, 1993, NEURAL COMPUT, V5, P613, DOI 10.1162/neco.1993.5.4.613
   Dietterich TG, 2000, J ARTIF INTELL RES, V13, P227, DOI 10.1613/jair.639
   Diuk C., 2008, P 25 INT C MACH LEAR, P240, DOI DOI 10.1145/1390156.1390187
   Eslami SM, 2016, ARXIV160308575
   Frank M., 2015, INTRINSIC MOTIVATION, P245
   Goel S., 2003, FLAIRS C, P346
   Guestrin C, 2003, J ARTIF INTELL RES, V19, P399, DOI 10.1613/jair.1000
   Guestrin Carlos, 2003, P INT JOINT C ART IN, P1003
   Hernandez-Gardiol N, 2001, ADV NEUR IN, V13, P1047
   Koutnik J, 2014, GECCO'14: PROCEEDINGS OF THE 2014 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P541, DOI 10.1145/2576768.2598358
   Mnih V., 2016, ARXIV160201783
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Mohamed S., 2015, ADV NEURAL INFORM PR, V28, P2116
   Nair A., 2015, ARXIV150704296
   Osband I., 2016, ARXIV160204621
   Oudeyer P.-Y., 2009, FRONT NUEROROBOT, V1, P6
   Schaul T., 2015, P 32 INT C MACH LEAR, P1312
   Schmidhuber J, 2010, IEEE T AUTON MENT DE, V2, P230, DOI 10.1109/TAMD.2010.2056368
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Simsek O., 2005, P 22 INT C MACH LEAR, P816, DOI DOI 10.1145/1102351.1102454
   Singh S., 2004, 18 ANN C NEUR INF PR, P1281, DOI DOI 10.1109/TAMD.2010.2051031
   Singh  S., 2009, P ANN C COGN SCI SOC, V2009, P2601
   Singh S, 2010, IEEE T AUTON MENT DE, V2, P70, DOI 10.1109/TAMD.2010.2051031
   Sorg J., 2010, P 9 INT C AUT AG MUL, V1, P31
   Spelke ES, 2007, DEVELOPMENTAL SCI, V10, P89, DOI 10.1111/j.1467-7687.2007.00569.x
   Stachenfeld K., 2014, ADV NEURAL INFORM PR, V27, P2528
   Stadie BC, 2015, ARXIV150700814
   Sutton R., 1998, INTRO REINFORCEMENT
   Sutton R. S., 2011, 10 INT C AUT AG MULT, V2, P761
   Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1
   Szepesvari C., 2014, ADV NEURAL INFORM PR, P990
NR 35
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701090
DA 2019-06-15
ER

PT S
AU Kumagai, W
AF Kumagai, Wataru
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning Bound for Parameter Transfer Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We consider a transfer-learning problem by using the parameter transfer approach, where a suitable parameter of feature mapping is learned through one task and applied to another objective task. Then, we introduce the notion of the local stability and parameter transfer learnability of parametric feature mapping, and thereby derive a learning bound for parameter transfer algorithms. As an application of parameter transfer learning, we discuss the performance of sparse coding in self-taught learning. Although self-taught learning algorithms with plentiful unlabeled data often show excellent empirical performance, their theoretical analysis has not been studied. In this paper, we also provide the first theoretical learning bound for self-taught learning.
C1 [Kumagai, Wataru] Kanagawa Univ, Fac Engn, Yokohama, Kanagawa, Japan.
RP Kumagai, W (reprint author), Kanagawa Univ, Fac Engn, Yokohama, Kanagawa, Japan.
EM kumagai@kanagawa-u.ac.jp
CR Arora S., 2015, ARXIV150300778
   Baxter Jonathan, 2000, J ARTIFICAL INTELLIG, V12, P3
   Coates A, 2011, P 14 INT C ART INT S, P215
   Dai W, 2008, P 25 INT C MACH LEAR, DOI [10.1145/1390156.1390182, DOI 10.1145/1390156.1390182]
   Le QV, 2013, INT CONF ACOUST SPEE, P8595, DOI 10.1109/ICASSP.2013.6639343
   Lee H, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1113
   Mairal J., 2009, ADV NEURAL INFORM PR, P1033
   Maurer A., 2012, ARXIV12090738
   Maurer A, 2009, MACH LEARN, V75, P327, DOI 10.1007/s10994-009-5109-7
   Mehta N., 2013, P 30 INT C MACH LEAR, P36
   Negahban S., 2009, ADV NEURAL INFORM PR, P1348
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Raina R., 2007, LEARNING, P759, DOI DOI 10.1145/1273496.1273592
   Sridharan K., 2009, ADV NEURAL INFORM PR, V21, P1545
   Trindade L, 2013, INT CONF MACH LEARN, P277, DOI 10.1109/ICMLC.2013.6890481
   Vainsencher D, 2011, J MACH LEARN RES, V12, P3259
   Zhao P, 2006, J MACH LEARN RES, V7, P2541
   Zhu XF, 2013, PATTERN RECOGN, V46, P215, DOI 10.1016/j.patcog.2012.07.018
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700100
DA 2019-06-15
ER

PT S
AU Lagree, P
   Vernade, C
   Cappe, O
AF Lagree, Paul
   Vernade, Claire
   Cappe, Olivier
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Multiple-Play Bandits in the Position-Based Model
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Sequentially learning to place items in multi-position displays or lists is a task that can be cast into the multiple-play semi-bandit setting. However, a major concern in this context is when the system cannot decide whether the user feedback for each item is actually exploitable. Indeed, much of the content may have been simply ignored by the user. The present work proposes to exploit available information regarding the display position bias under the so-called Position-based click model (PBM). We first discuss how this model differs from the Cascade model and its variants considered in several recent works on multiple-play bandits. We then provide a novel regret lower bound for this model as well as computationally efficient algorithms that display good empirical and theoretical performance.
C1 [Lagree, Paul] Univ Paris Saclay, Univ Paris Sud, LRI, St Aubin, France.
   [Vernade, Claire; Cappe, Olivier] Univ Paris Saclay, Telecom ParisTech, CNRS, LTCI, St Aubin, France.
RP Lagree, P (reprint author), Univ Paris Saclay, Univ Paris Sud, LRI, St Aubin, France.
EM paul.lagree@u-psud.fr; vernade@enst.fr
FU French research project ALICIA [ANR-13-CORD-0020]; Machine Learning for
   Big Data Chair at Telecom ParisTech
FX This work was partially supported by the French research project ALICIA
   (grant ANR-13-CORD-0020) and by the Machine Learning for Big Data Chair
   at Telecom ParisTech.
CR ANANTHARAM V, 1987, IEEE T AUTOMAT CONTR, V32, P968, DOI 10.1109/TAC.1987.1104491
   Boucheron S., 2013, CONCENTRATION INEQUA
   Chen W., 2013, P 30 INT C MACH LEAR
   Chuklin A., 2015, SYNTHESIS LECT INFOR, V7, P1, DOI DOI 10.2200/S00654ED1V01Y201507ICR043
   Combes R., 2015, P 2015 ACM SIGMETRIC
   Combes R., 2015, ADV NEURAL INFORM PR
   Craswell N., 2008, P INT C WEB SEARCH D
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Garivier A., 2011, P C LEARN THEOR
   Graves TL, 1997, SIAM J CONTROL OPTIM, V35, P715, DOI 10.1137/S0363012994275440
   Kaufmann E., 2015, J MACHINE LEARNING R
   Komiyama J., 2015, P 32 INT C MACH LEAR
   Kveton B., 2015, P 18 INT C ART INT S
   Kveton B., 2015, P 32 INT C MACH LEAR
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Magureanu S., 2014, P C LEARN THEOR
   Radlinski F., 2008, P 25 INT C MACH LEAR
   Richardson M., 2007, P 16 INT C WORLD WID
   Sumeet K., 2016, P 33 INT C MACH LEAR
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704070
DA 2019-06-15
ER

PT S
AU Lahouti, F
   Hassibi, B
AF Lahouti, Farshad
   Hassibi, Babak
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Fundamental Limits of Budget-Fidelity Trade-off in Label Crowdsourcing
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Digital crowdsourcing (CS) is a modern approach to perform certain large projects using small contributions of a large crowd. In CS, a taskmaster typically breaks down the project into small batches of tasks and assigns them to so-called workers with imperfect skill levels. The crowdsourcer then collects and analyzes the results for inference and serving the purpose of the project. In this work, the CS problem, as a human-in-the-loop computation problem, is modeled and analyzed in an information theoretic rate-distortion framework. The purpose is to identify the ultimate fidelity that one can achieve by any form of query from the crowd and any decoding (inference) algorithm with a given budget. The results are established by a joint source channel (de)coding scheme, which represent the query scheme and inference, over parallel noisy channels, which model workers with imperfect skill levels. We also present and analyze a query scheme dubbed k-ary incidence coding and study optimized query pricing in this setting.
C1 [Lahouti, Farshad; Hassibi, Babak] CALTECH, Elect Engn Dept, Pasadena, CA 91125 USA.
RP Lahouti, F (reprint author), CALTECH, Elect Engn Dept, Pasadena, CA 91125 USA.
EM lahouti@caltech.edu; hassibi@caltech.edu
CR Abraham I., 2013, 26 C LEARN THEOR COL
   Audubon, 2015, HIST CHRISTM BIRD CO
   Caltech, 2016, COMM SEISM NETW PROJ
   Cover T. M., 2006, ELEMENTS INFORM THEO
   Dawid A. P., 1979, APPL STAT, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]
   Dekel Ofer, 2009, P 22 ANN C LEARN THE
   Karger DR, 2014, OPER RES, V62, P1, DOI 10.1287/opre.2013.1235
   Kim Y.-H., 2011, NETWORK INFORM THEOR
   Korlakai Vinayak  R., 2014, ADV NEURAL INFORM PR, P2996
   Raykar VC, 2010, J MACH LEARN RES, V11, P1297
   Smyth P., 1995, Advances in Neural Information Processing Systems 7, P1085
   Whitehill J., 2009, ADV NEURAL INFORM PR, P2035
   Zhang Y., 2014, ADV NEURAL INFORM PR, P1260
NR 13
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702020
DA 2019-06-15
ER

PT S
AU Lakkaraju, H
   Leskovec, J
AF Lakkaraju, Himabindu
   Leskovec, Jure
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Confusions over Time: An Interpretable Bayesian Model to Characterize
   Trends in Decision Making
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID PREDICTION
AB We propose Confusions over Time (CoT), a novel generative framework which facilitates a multi-granular analysis of the decision making process. The CoT not only models the confusions or error properties of individual decision makers and their evolution over time, but also allows us to obtain diagnostic insights into the collective decision making process in an interpretable manner. To this end, the CoT models the confusions of the decision makers and their evolution over time via time-dependent confusion matrices. Interpretable insights are obtained by grouping similar decision makers (and items being judged) into clusters and representing each such cluster with an appropriate prototype and identifying the most important features characterizing the cluster via a subspace feature indicator vector. Experimentation with real world data on bail decisions, asthma treatments, and insurance policy approval decisions demonstrates that CoT can accurately model and explain the confusions of decision makers and their evolution over time.
C1 [Lakkaraju, Himabindu; Leskovec, Jure] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
RP Lakkaraju, H (reprint author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
EM himalv@cs.stanford.edu; jure@cs.stanford.edu
CR Bien J, 2011, ANN APPL STAT, V5, P2403, DOI 10.1214/11-AOAS495
   Dawid A. P., 1979, APPL STAT, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]
   Dekel O., 2009, COLT
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   Joglekar M., 2013, P 19 ACM SIGKDD INT, P686
   Kamar E., 2012, P 11 INT C AUT AG MU, P467
   Kemp C., 2006, AAAI, V3, P5
   Kim Been, 2014, ADV NEURAL INFORM PR, P1952
   Lakkaraju H., 2015, P 2015 SIAM INT C DA, P181
   Lakkaraju Himabindu, 2016, KDD, V2016, P1675
   Letham B, 2015, ANN APPL STAT, V9, P1350, DOI 10.1214/15-AOAS848
   Liu C., 2012, ICML, P225
   Lou Y., 2012, P 18 ACM SIGKDD INT, P150
   Neath AA, 2012, WIRES COMPUT STAT, V4, P199, DOI 10.1002/wics.199
   Nowicki K, 2001, J AM STAT ASSOC, V96, P1077, DOI 10.1198/016214501753208735
   Raykar VC, 2010, J MACH LEARN RES, V11, P1297
   Raykar Vikas C., 2009, P 26 ANN INT C MACH, P889, DOI DOI 10.1145/1553374.1553488
   Snow R., 2008, P C EMP METH NAT LAN, P254, DOI DOI 10.3115/1613715.1613751
   Whitehill J., 2009, ADV NEURAL INFORM PR, P2035
   Xu Kevin S., 2013, Social Computing, Behavioral-Cultural Modeling and Prediction. 6th International Conference, SBP 2013. Proceedings, P201, DOI 10.1007/978-3-642-37210-0_22
   Zhou D., 2012, NIPS, V25, P2204
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701091
DA 2019-06-15
ER

PT S
AU Lam, RR
   Willcox, KE
   Wolpert, DH
AF Lam, Remi R.
   Willcox, Karen E.
   Wolpert, David H.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Bayesian Optimization with a Finite Budget: An Approximate Dynamic
   Programming Approach
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID GLOBAL OPTIMIZATION
AB We consider the problem of optimizing an expensive objective function when a finite budget of total evaluations is prescribed. In that context, the optimal solution strategy for Bayesian optimization can be formulated as a dynamic programming instance. This results in a complex problem with uncountable, dimension-increasing state space and an uncountable control space. We show how to approximate the solution of this dynamic programming problem using rollout, and propose rollout heuristics specifically designed for the Bayesian optimization setting. We present numerical experiments showing that the resulting algorithm for optimization with a finite budget outperforms several popular Bayesian optimization algorithms.
C1 [Lam, Remi R.; Willcox, Karen E.] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
   [Wolpert, David H.] Santa Fe Inst, Santa Fe, NM 87501 USA.
RP Lam, RR (reprint author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
EM rlam@mit.edu; kwillcox@mit.edu; dhw@santafe.edu
FU AFOSR MURI [FA9550-15-1-0038]
FX This work was supported in part by the AFOSR MURI on multi-information
   sources of multi-physics systems under Award Number FA9550-15-1-0038,
   program manager Dr. Jean-Luc Cambier.
CR Bertsekas D. P., 1995, DYNAMIC PROGRAMMING, V1
   Brochu E., 2010, ARXIV10122599
   Cashore J. M., WORKING PAPER
   Ginsbourger D, 2010, CONTRIB STAT, P89, DOI 10.1007/978-3-7908-2410-0_12
   Gonzalez-Hernandez J, 2016, J PEDIATR SURG, V51, P790, DOI 10.1016/j.jpedsurg.2016.02.024
   Hennig P, 2012, J MACH LEARN RES, V13, P1809
   Hernandez-Lobato J M, 2014, ADV NEURAL INFORM PR, P918
   Huang D, 2006, J GLOBAL OPTIM, V34, P441, DOI 10.1007/s10898-005-2454-3
   Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147
   Jones DR, 2001, J GLOBAL OPTIM, V21, P345, DOI 10.1023/A:1012771025575
   Ling C. K., 2016, 30 AAAI C ART INT
   Lizotte D. J., 2008, THESIS
   Marchant R., 2015, SEQUENTIAL BAYESIAN
   OSBORNE M. A., 2009, 3 INT C LEARN INT OP, P1
   Powell W. B., 2011, APPROXIMATE DYNAMIC, V842
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Snoek J., 2012, ADV NEURAL INFORM PR, V2012, P2951
   Srinivas N., 2010, ICML, P1015, DOI DOI 10.1109/TIT.2011.2182033
   Villemonteix J, 2009, J GLOBAL OPTIM, V44, P509, DOI 10.1007/s10898-008-9354-2
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701045
DA 2019-06-15
ER

PT S
AU Lasserre, JB
   Pauwels, E
AF Lasserre, Jean-Bernard
   Pauwels, Edouard
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Sorting out typicality with the inverse moment matrix SOS polynomial
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID SHAPE; ZEROS
AB We study a surprising phenomenon related to the representation of a cloud of data points using polynomials. We start with the previously unnoticed empirical observation that, given a collection (a cloud) of data points, the sublevel sets of a certain distinguished polynomial capture the shape of the cloud very accurately. This distinguished polynomial is a sum-of-squares (SOS) derived in a simple manner from the inverse of the empirical moment matrix. In fact, this SOS polynomial is directly related to orthogonal polynomials and the Christoffel function. This allows to generalize and interpret extremality properties of orthogonal polynomials and to provide a mathematical rationale for the observed phenomenon. Among diverse potential applications, we illustrate the relevance of our results on a network intrusion detection task for which we obtain performances similar to existing dedicated methods reported in the literature.
C1 [Lasserre, Jean-Bernard] Univ Toulouse, CNRS, LAAS, F-31400 Toulouse, France.
   [Lasserre, Jean-Bernard] Univ Toulouse, IMT, F-31400 Toulouse, France.
   [Pauwels, Edouard] Univ Toulouse 3 Paul Sabatier, IRIT, F-31400 Toulouse, France.
   [Pauwels, Edouard] Univ Toulouse 3 Paul Sabatier, IMT, F-31400 Toulouse, France.
RP Lasserre, JB (reprint author), Univ Toulouse, CNRS, LAAS, F-31400 Toulouse, France.; Lasserre, JB (reprint author), Univ Toulouse, IMT, F-31400 Toulouse, France.
EM lasserre@laas.fr; edouard.pauwels@irit.fr
FU project ERC-ADG TAMING [666981]; Air Force Office of Scientific
   Research, Air Force Material Command [FA9550-15-1-0500]; European
   Research Council
FX This work was partly supported by project ERC-ADG TAMING 666981,
   ERC-Advanced Grant of the European Research Council and grant number
   FA9550-15-1-0500 from the Air Force Office of Scientific Research, Air
   Force Material Command.
CR Berman RJ, 2009, INDIANA U MATH J, V58, P1921, DOI 10.1512/iumj.2009.58.3644
   Bos L., 1998, REND CIRC MAT PA 2 S, V52, P277
   Chandola V, 2009, ACM COMPUT SURV, V41, DOI 10.1145/1541880.1541882
   Davis J., 2006, P 23 INT C MACH LEAR, V23, P233, DOI [DOI 10.1145/1143844.1143874, 10.1145/1143844.1143874]
   Dunkl C. F., 2001, ORTOGONAL POLYNOMIAL
   Golub GH, 1999, SIAM J SCI COMPUT, V21, P1222, DOI 10.1137/S1064827597328315
   Gustafsson B, 2009, ADV MATH, V222, P1405, DOI 10.1016/j.aim.2009.06.010
   HADI AS, 1994, J ROY STAT SOC B MET, V56, P393
   Helton JW, 2008, ANN PROBAB, V36, P1453, DOI 10.1214/07-AOP365
   Knorr E. M., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P126
   Lasserre JB, 2015, INT GAME THEORY REV, V17, DOI 10.1142/S0219198915400010
   Lasserre JB, 2015, DISCRETE COMPUT GEOM, V54, P993, DOI 10.1007/s00454-015-9739-1
   Lichman M., 2013, UCI MACHINE LEARNING
   Oliver J. J., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96), P364
   Press W. H., 2007, NUMERICAL RECIPES AR
   Szego G., 1974, C PUBLICATIONS
   Totik V, 2000, J ANAL MATH, V81, P283, DOI 10.1007/BF02788993
   Williams G, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P709, DOI 10.1109/ICDM.2002.1184035
   Yamanishi K, 2004, DATA MIN KNOWL DISC, V8, P275, DOI 10.1023/B:DAMI.0000023676.72185.7c
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704020
DA 2019-06-15
ER

PT S
AU Lattimore, F
   Lattimore, T
   Reid, MD
AF Lattimore, Finnian
   Lattimore, Tor
   Reid, Mark D.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Causal Bandits: Learning Good Interventions via Causal Inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We study the problem of using causal models to improve the rate at which good interventions can be learned online in a stochastic environment. Our formalism combines multi- arm bandits and causal inference to model a novel type of bandit feedback that is not exploited by existing approaches. We propose a new algorithm that exploits the causal feedback and prove a bound on its simple regret that is strictly better (in all quantities) than algorithms that do not use the additional causal information.
C1 [Lattimore, Finnian; Reid, Mark D.] Australian Natl Univ, Canberra, ACT, Australia.
   [Lattimore, Finnian; Reid, Mark D.] NICTA, Data61, Sydney, NSW, Australia.
   [Lattimore, Tor] Indiana Univ, Bloomington, IN USA.
RP Lattimore, F (reprint author), Australian Natl Univ, Canberra, ACT, Australia.; Lattimore, F (reprint author), NICTA, Data61, Sydney, NSW, Australia.
EM finn.lattimore@gmail.com; tor.lattimore@gmail.com; mark.reid@anu.edu.au
CR Agarwal  A., 2014, P 31 INT C MACH LEAR, P1638
   Alon N, 2015, P 28 C LEARN THEOR C, V40, P23
   Audibert J. Y., 2010, COLT 23 C LEARN THEO, P13
   Avner O., 2012, ICML, P409
   Bareinboim Elias, 2015, ADV NEURAL INFORM PR, P1342
   Bartok G, 2014, MATH OPER RES, V39, P967, DOI 10.1287/moor.2014.0663
   Bottou L, 2013, J MACH LEARN RES, V14, P3207
   Bubeck S, 2009, LECT NOTES ARTIF INT, V5809, P23
   CHERNOFF H, 1959, ANN MATH STAT, V30, P755, DOI 10.1214/aoms/1177706205
   Eberhardt F., 2005, UAI
   Eberhardt F., 2010, J MACHINE LEARNING R, P87
   Even-Dar E., 2002, Computational Learning Theory. 15th Annual Conference on Computational Learning Theory, COLT 2002. Proceedings (Lecture Notes in Artificial Intelligence Vol.2375), P255
   Hauser A, 2014, INT J APPROX REASON, V55, P926, DOI 10.1016/j.ijar.2013.11.007
   Hu H, 2014, NIPS, V27, P2339
   Jamieson K. G., 2014, P 27 C LEARN THEOR, V35, P423
   John Langford, 2008, ADV NEURAL INFORM PR, P817
   Kocak T., 2014, ADV NEURAL INFORM PR, P613
   Koller D., 2009, PROBABILISTIC GRAPHI
   Lazaric A., 2012, ADV NEURAL INFORM PR, V25, P3212
   Li L., 2014, ARXIV14093653
   Ortega PA, 2014, COMPLEX ADAPT SYST M, V2, DOI 10.1186/2194-3206-2-2
   Pearl J., 2000, CAUSALITY MODELS REA
   ROBBINS H, 1952, B AM MATH SOC, V58, P527, DOI 10.1090/S0002-9904-1952-09620-8
   Wu Y, 2015, P C ADV NEUR INF PRO, V28, P1360
   Yu J. Y., 2009, P 26 ANN INT C MACH, P1177
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701052
DA 2019-06-15
ER

PT S
AU Lee, CE
   Li, YH
   Shah, D
   Song, D
AF Lee, Christina E.
   Li, Yihua
   Shah, Devavrat
   Song, Dogyoon
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Blind Regression: Nonparametric Regression for Latent Variable Models
   via Collaborative Filtering
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We introduce the framework of blind regression motivated by matrix completion for recommendation systems: given m users, n movies, and a subset of user-movie ratings, the goal is to predict the unobserved user-movie ratings given the data, i.e., to complete the partially observed matrix. Following the framework of non-parametric statistics, we posit that user u and movie i have features x(1) (u) and x(2) (i) respectively, and their corresponding rating y (u; i) is a noisy measurement of f (x(1) (u); x(2) (i)) for some unknown function f. In contrast with classical regression, the features x = (x(1) (u); x(2) (i)) are not observed, making it challenging to apply standard regression methods to predict the unobserved ratings.
   Inspired by the classical Taylor's expansion for differentiable functions, we provide a prediction algorithm that is consistent for all Lipschitz functions. In fact, the analysis through our framework naturally leads to a variant of collaborative filtering, shedding insight into the widespread success of collaborative filtering in practice. Assuming each entry is sampled independently with probability at least max(m(-1+delta), n(-1/2+delta)) with delta > 0, we prove that the expected fraction of our estimates with error greater than epsilon is less than gamma(2)/epsilon(2) plus a polynomially decaying term, where gamma(2) is the variance of the additive entry-wise noise term.
   Experiments with the MovieLens and Netflix datasets suggest that our algorithm provides principled improvements over basic collaborative filtering and is competitive with matrix factorization methods.
C1 [Lee, Christina E.; Li, Yihua; Shah, Devavrat; Song, Dogyoon] MIT, Dept Elect Engn & Comp Sci, Lab Informat & Decis Syst, Cambridge, MA 02139 USA.
RP Lee, CE (reprint author), MIT, Dept Elect Engn & Comp Sci, Lab Informat & Decis Syst, Cambridge, MA 02139 USA.
EM celee@mit.edu; liyihua@mit.edu; devavrat@mit.edu; dgsong@mit.edu
FU ARO under MURI [133668-5079809]; NSF [CMMI-1462158, CMMI-1634259];
   Samsung Scholarship; Siebel Scholarship; NSF; Claude E. Shannon Research
   Assistantship
FX This work is supported in parts by ARO under MURI award 133668-5079809,
   by NSF under grants CMMI-1462158 and CMMI-1634259, and additionally by a
   Samsung Scholarship, Siebel Scholarship, NSF Graduate Fellowship, and
   Claude E. Shannon Research Assistantship.
CR Aditya ST, 2011, IEEE T INFORM THEORY, V57, P2327, DOI 10.1109/TIT.2011.2111190
   Bresler G, 2014, ADV NEURAL INFORM PR, P3347
   Bresler G., 2015, ARXIV150705371
   Cai D, 2008, IEEE DATA MINING, P63, DOI 10.1109/ICDM.2008.57
   Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Chatterjee S, 2015, ANN STAT, V43, P177, DOI 10.1214/14-AOS1272
   Fazel M, 2003, P AMER CONTR CONF, P2156
   Ganti R. S., 2015, ADV NEURAL INF PROC, P1864
   Goldberg D., 1992, COMMUN ACM
   Jain P., 2013, P 45 ANN ACM S THEOR, P665, DOI DOI 10.1145/2488608.2488693
   Keshavan R., 2009, IEEE T INF THEORY, V56
   Kleinberg J., 2004, P 36 ANN ACM S THEOR, P569
   Kleinberg J.M., 2003, P 4 ACM C EL COMM EC, P1
   Koren Y, 2011, RECOMMENDER SYSTEMS HANDBOOK, P145, DOI 10.1007/978-0-387-85820-3_5
   Lin Z., 2009, CAMSAP, V61
   Linden G, 2003, IEEE INTERNET COMPUT, V7, P76, DOI 10.1109/MIC.2003.1167344
   Liu Z, 2009, SIAM J MATRIX ANAL A, V31, P1235, DOI 10.1137/090755436
   MACK YP, 1982, VERW GEBIETE, V61, P405
   Maurer A., 2009, ARXIV E PRINTS
   Mazumder R, 2010, J MACH LEARN RES, V11, P2287
   Negahban S, 2012, J MACH LEARN RES, V13, P1665
   Ning X., 2015, RECOMMENDER SYSTEMS, P37, DOI DOI 10.1007/978-1-4899-7637-6_2
   Rohde A, 2011, ANN STAT, V39, P887, DOI 10.1214/10-AOS860
   Shen BH, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P757
   Srebro N., 2004, ADV NEURAL INFORM PR, P1321
   Wand M. P., 1994, KERNEL SMOOTHING
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700072
DA 2019-06-15
ER

PT S
AU Lee, J
   James, LF
   Choi, S
AF Lee, Juho
   James, Lancelot F.
   Choi, Seungjin
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Finite-Dimensional BFRY Priors and Variational Bayesian Inference for
   Power Law Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Bayesian nonparametric methods based on the Dirichlet Process (DP), gamma process and beta process, have proven effective in capturing aspects of various datasets arising in machine learning. However, it is now recognized that such processes have their limitations in terms of the ability to capture power law behavior. As such there is now considerable interest in models based on the Stable Processs (SP), Generalized Gamma process (GGP) and Stable-Beta Process (SBP). These models present new challenges in terms of practical statistical implementation. In analogy to tractable processes such as the finite-dimensional Dirichlet process, we describe a class of random processes, we call iid finite-dimensional BFRY processes, that enables one to begin to develop efficient posterior inference algorithms such as variational Bayes that readily scale to massive datasets. For illustrative purposes, we describe a simple variational Bayes algorithm for normalized SP mixture models, and demonstrate its usefulness with experiments on synthetic and real-world datasets.
C1 [Lee, Juho; Choi, Seungjin] POSTECH, Pohang, South Korea.
   [James, Lancelot F.] HKUST, Hong Kong, Peoples R China.
RP Lee, J (reprint author), POSTECH, Pohang, South Korea.
EM stonecold@postech.ac.kr; lancelot@ust.hk; seungjin@postech.ac.kr
FU IITP [B0101-16-0307]; National Research Foundation (NRF) of Korea
   [NRF-2013R1A2A2A01067464]; HKSAR [RGC-HKUST 601712]
FX This work was supported by IITP (No. B0101-16-0307, Basic Software
   Research in Human-Level Lifelong Machine Learning (Machine Learning
   Center)) and by National Research Foundation (NRF) of Korea
   (NRF-2013R1A2A2A01067464), and supported in part by the grant RGC-HKUST
   601712 of the HKSAR.
CR Bertoin J, 2006, PROBAB MATH STAT-POL, V26, P315
   Blei D. M., 2016, ARXIV160100670
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Caron F., 2012, NIPS
   Caron F., 2014, ARXIV14011137
   Chen C., 2012, ICML
   Cinlar E., 2010, PROBABILITY STOCHAST
   Devroye L, 2014, STAT METHOD APPL-GER, V23, P307, DOI 10.1007/s10260-014-0260-0
   DUANE S, 1987, PHYS LETT B, V195, P216, DOI 10.1016/0370-2693(87)91197-X
   Favaro S, 2013, STAT SCI, V28, P335, DOI 10.1214/13-STS422
   FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360
   Griffiths T. L., 2006, NIPS
   Ishwaran H, 2004, J AM STAT ASSOC, V99, P175, DOI 10.1198/016214504000000179
   Ishwaran H, 2001, J AM STAT ASSOC, V96, P161, DOI 10.1198/016214501750332758
   Ishwaran H, 2002, STAT SINICA, V12, P941
   Ishwaran H, 2001, J AM STAT ASSOC, V96, P1316, DOI 10.1198/016214501753382255
   James L. F., 2015, ARXIV151007309
   James LF, 2005, ANN STAT, V33, P1771, DOI 10.1214/0090536050000000336
   KINGMAN JFC, 1975, J ROY STAT SOC B MET, V37, P1
   KINGMAN JFC, 1967, PAC J MATH, V21, P59, DOI 10.2140/pjm.1967.21.59
   Kurihara K., 2007, IJCAI
   Pitman J, 1997, ANN PROBAB, V25, P855
   Pitman J, 2015, BERNOULLI, V21, P2484, DOI 10.3150/14-BEJ652
   Teh Y. W., 2009, NIPS
   Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302
   Thibaux R., 2007, AISTATS
   Veitch V., 2015, ARXIV151203099
   Welling M., 2011, ICML
   Winkel M, 2005, J APPL PROBAB, V42, P138, DOI 10.1239/jap/1110381376
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702095
DA 2019-06-15
ER

PT S
AU Lee, M
   Jin, SH
   Mimno, D
AF Lee, Moontae
   Jin, Seok Hyun
   Mimno, David
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Beyond Exchangeability: The Chinese Voting Process
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Many online communities present user-contributed responses such as reviews of products and answers to questions. User-provided helpfulness votes can highlight the most useful responses, but voting is a social process that can gain momentum based on the popularity of responses and the polarity of existing votes. We propose the Chinese Voting Process (CVP) which models the evolution of helpfulness votes as a self-reinforcing process dependent on position and presentation biases. We evaluate this model on Amazon product reviews and more than 80 StackExchange forums, measuring the intrinsic quality of individual responses and behavioral coefficients of different communities.
C1 [Lee, Moontae; Jin, Seok Hyun; Mimno, David] Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.
RP Lee, M (reprint author), Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.
EM moontae@cs.cornell.edu; sj372@cornell.edu; mimno@cornell.edu
CR Aldous David J., 1985, LECT NOTES MATH, V1117, P1, DOI [10.1007/BFb0099421, DOI 10.1007/BFB0099421]
   Blei D., 2003, ADV NEURAL INFORM PR
   Blei DM, 2011, J MACH LEARN RES, V12, P2461
   Danescu-Niculescu-Mizil C., 2009, P 18 INT C WORLD WID, P141, DOI DOI 10.1145/1526709.1526729
   Ghose A., 2007, P 9 INT C EL COMM, P303, DOI DOI 10.1145/1282100.1282158
   Joachims T, 2007, ACM T INFORM SYST, V25, DOI 10.1145/1229179.1229181
   Kim S. -M., 2006, P 2006 C EMP METH NA
   Liu J., 2007, EMNLP CONLL, P334
   Mamykina L., 2011, P SIGCHI C HUM FACT
   Martin L., 2014, P 28 AAAI C ART INT, P1551
   Otterbacher J, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P955
   Salganik MJ, 2008, SOC PSYCHOL QUART, V71, P338, DOI 10.1177/019027250807100404
   Salganik MJ, 2006, SCIENCE, V311, P854, DOI 10.1126/science.1121066
   Shandwick W., 2012, KRC RES
   Siersdorfer S, 2014, ACM T WEB, V8, DOI 10.1145/2628441
   Sipos R, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P337, DOI 10.1145/2566486.2567998
   Socher Richard, 2013, P C EMP METH NAT LAN, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791
   Tausczik Y. R., 2014, COMPUTER SUPPORTED C
   Yue Y., 2010, P 19 INT C WORLD WID
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702109
DA 2019-06-15
ER

PT S
AU Lee, S
   Purushwalkam, S
   Cogswell, M
   Ranjan, V
   Crandall, D
   Batra, D
AF Lee, Stefan
   Purushwalkam, Senthil
   Cogswell, Michael
   Ranjan, Viresh
   Crandall, David
   Batra, Dhruv
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Stochastic Multiple Choice Learning for Training Diverse Deep Ensembles
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Many practical perception systems exist within larger processes that include interactions with users or additional components capable of evaluating the quality of predicted solutions. In these contexts, it is beneficial to provide these oracle mechanisms with multiple highly likely hypotheses rather than a single prediction. In this work, we pose the task of producing multiple outputs as a learning problem over an ensemble of deep networks - introducing a novel stochastic gradient descent based approach to minimize the loss with respect to an oracle. Our method is simple to implement, agnostic to both architecture and loss function, and parameter-free. Our approach achieves lower oracle error compared to existing methods on a wide range of tasks and deep architectures. We also show qualitatively that the diverse solutions produced often provide interpretable representations of task ambiguity.
C1 [Lee, Stefan; Cogswell, Michael; Ranjan, Viresh; Batra, Dhruv] Virginia Tech, Blacksburg, VA 24061 USA.
   [Purushwalkam, Senthil] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Crandall, David] Indiana Univ, Bloomington, IN 47405 USA.
RP Lee, S (reprint author), Virginia Tech, Blacksburg, VA 24061 USA.
EM steflee@vt.edu; spurushw@andrew.cmu.edu; cogswell@vt.edu;
   rviresh@vt.edu; djcran@indiana.edu; dbatra@vt.edu
FU National Science Foundation CAREER award; Army Research Office YIP
   award; ICTAS Junior Faculty award; Office of Naval Research grant
   [N00014-14-1-0679]; Google Faculty Research award; AWS in Education
   Research grant; NSF CAREER award [IIS-1253549]; Intelligence Advanced
   Research Projects Activity (IARPA) via Air Force Research Laboratory
   [FA8650-12-C-7212]; NSF [ACI-0910812, CNS-0521433]; Lily Endowment,
   Inc.; Indiana METACyt Initiative
FX This work was supported in part by a National Science Foundation CAREER
   award, an Army Research Office YIP award, ICTAS Junior Faculty award,
   Office of Naval Research grant N00014-14-1-0679, Google Faculty Research
   award, AWS in Education Research grant, and NVIDIA GPU donation, all
   awarded to DB, and by an NSF CAREER award (IIS-1253549), the
   Intelligence Advanced Research Projects Activity (IARPA) via Air Force
   Research Laboratory contract FA8650-12-C-7212, a Google Faculty Research
   award, and an NVIDIA GPU donation, all awarded to DC. Computing
   resources used by this work are supported in part by NSF (ACI-0910812
   and CNS-0521433), the Lily Endowment, Inc., and the Indiana METACyt
   Initiative. The U.S. Government is authorized to reproduce and
   distribute reprints for Governmental purposes notwithstanding any
   copyright annotation thereon. Disclaimer: The views and conclusions
   contained herein are those of the authors and should not be interpreted
   as necessarily representing the official policies or endorsements,
   either expressed or implied, of IARPA, AFRL, NSF, or the U.S.
   Government.
CR Ahmed K., 2016, ARXIV160406119
   Batra D., 2012, EUR C COMP VIS ECCV
   Batra  Dhruv, 2012, ADV NEURAL INFORM PR
   Chatfield Ken, 2014, ARXIV14053531
   Dey D., 2015, P IEEE INT C COMP VI
   Everingham M., 2011, PASCAL VISUAL OBJECT
   Geiger  A., 2013, INT J ROBOTICS RES I
   Guzman-Rivera A., 2014, P INT C ART INT STAT
   Hariharan B, 2011, P IEEE INT C COMP VI
   He  K., 2015, ARXIV151203385
   Hinton G. E., 2014, ADV NEUR INF PROC SY
   Jia Y., 2013, CAFFE OPEN SOURCE CO
   Karpathy A., 2015, P IEEE C COMP VIS PA
   Kirillov A., 2015, ADV NEURAL INFORM PR
   Kirillov A., 2015, P IEEE INT C COMP VI
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Lin Tsung-Yi, 2014, MICROSOFT COCO COMMO
   Liu Y, 1999, NEURAL NETWORKS, V12, P1399, DOI 10.1016/S0893-6080(99)00073-8
   Long J., 2015, P IEEE C COMP VIS PA
   Melville P., 2005, Information Fusion, V6, P99, DOI 10.1016/j.inffus.2004.04.001
   Microsoft, 2016, DEC COMP VIS RES ON
   Park D, 2011, IEEE I CONF COMP VIS, P2627, DOI 10.1109/ICCV.2011.6126552
   Prasad A., 2014, ADV NEURAL INFORM PR
   Russakovsky  O., 2012, IMAGENET LARGE SCALE
   Strehl A., 2003, Journal of Machine Learning Research, V3, P583, DOI 10.1162/153244303321897735
   Tumer K., 1996, Connection Science, V8, P385, DOI 10.1080/095400996116839
   Vedantam R., 2015, P IEEE C COMP VIS PA
   Vondrick C, 2016, PROC CVPR IEEE, P98, DOI 10.1109/CVPR.2016.18
   WIRED, 2015, FAC AI CAN CAPT PHOT
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702017
DA 2019-06-15
ER

PT S
AU Lei, Q
   Zhong, K
   Dhillon, IS
AF Lei, Qi
   Zhong, Kai
   Dhillon, Inderjit S.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Coordinate-wise Power Method
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB In this paper, we propose a coordinate-wise version of the power method from an optimization viewpoint. The vanilla power method simultaneously updates all the coordinates of the iterate, which is essential for its convergence analysis. However, different coordinates converge to the optimal value at different speeds. Our proposed algorithm, which we call coordinate-wise power method, is able to select and update the most important k coordinates in O(kn) time at each iteration, where n is the dimension of the matrix and k <= n is the size of the active set. Inspired by the "greedy" nature of our method, we further propose a greedy coordinate descent algorithm applied on a non-convex objective function specialized for symmetric matrices. We provide convergence analyses for both methods. Experimental results on both synthetic and real data show that our methods achieve up to 23 times speedup over the basic power method. Meanwhile, due to their coordinate-wise nature, our methods are very suitable for the important case when data cannot fit into memory. Finally, we introduce how the coordinate-wise mechanism could be applied to other iterative methods that are used in machine learning.
C1 [Lei, Qi; Zhong, Kai; Dhillon, Inderjit S.] Univ Texas Austin, Inst Computat Engn & Sci, Austin, TX 78712 USA.
   [Dhillon, Inderjit S.] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.
RP Lei, Q (reprint author), Univ Texas Austin, Inst Computat Engn & Sci, Austin, TX 78712 USA.
EM leiqi@ices.utexas.edu; zhongkai@ices.utexas.edu; inderjit@cs.utexas.edu
FU NSF [CCF-1320746, IIS-1546452, CCF-1564000]
FX This research was supported by NSF grants CCF-1320746, IIS-1546452 and
   CCF-1564000.
CR Dhillon I. S., 2011, ADV NEURAL INFORM PR, P2160
   Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806
   Hardt M., 2014, ADV NEURAL INFORM PR, P2861, DOI DOI 10.1080/01621459.1963
   Hardt Moritz, 2013, P 45 ANN ACM S THEOR, P331, DOI [DOI 10.1145/2488608.2488650, 10.1145/2488608.2488650]
   Hoare C. A. R., 1961, COMMUN ACM, V4, P321, DOI DOI 10.1145/366622.366644
   Hsieh C.-J., 2011, P 17 ACM SIGKDD INT, P1064
   Ipsen Ilse, 2005, 7 IMACS INT S IT MET, V5
   Jain P., 2013, P 45 ANN ACM S THEOR, P665, DOI DOI 10.1145/2488608.2488693
   Journee M, 2010, J MACH LEARN RES, V11, P517
   Kwak Haewoon, 2010, WWW, P591, DOI DOI 10.1145/1772690.1772751
   Needell D., 2014, P ADV NEUR INF PROC, P1017
   Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001
   Nutini J., 2015, P 32 INT C MACH LEAR, P1632
   Parlett Beresford N, 1998, SYMMETRIC EIGENVALUE, V20
   SAAD Y., 2003, ITERATIVE METHODS SP
   Shamir O., 2015, P 32 INT C MACH LEAR, P144
   Si Si, 2014, ADV NEURAL INFORM PR, P2798
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Yuan XT, 2013, J MACH LEARN RES, V14, P899
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700067
DA 2019-06-15
ER

PT S
AU Lepora, NF
AF Lepora, Nathan F.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Threshold Learning for Optimal Decision Making
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID BASAL GANGLIA; REWARD RATE; REINFORCEMENT; DISCRIMINATION
AB Decision making under uncertainty is commonly modelled as a process of competitive stochastic evidence accumulation to threshold (the drift-diffusion model). However, it is unknown how animals learn these decision thresholds. We examine threshold learning by constructing a reward function that averages over many trials to Wald's cost function that defines decision optimality. These rewards are highly stochastic and hence challenging to optimize, which we address in two ways: first, a simple two-factor reward-modulated learning rule derived from Williams' REINFORCE method for neural networks; and second, Bayesian optimization of the reward function with a Gaussian process. Bayesian optimization converges in fewer trials than REINFORCE but is slower computationally with greater variance. The REINFORCE method is also a better model of acquisition behaviour in animals and a similar learning rule has been proposed for modelling basal ganglia function.
C1 [Lepora, Nathan F.] Univ Bristol, Dept Engn Math, Bristol, Avon, England.
RP Lepora, NF (reprint author), Univ Bristol, Dept Engn Math, Bristol, Avon, England.
EM n.lepora@bristol.ac.uk
CR Balci F, 2011, ATTEN PERCEPT PSYCHO, V73, P640, DOI 10.3758/s13414-010-0049-7
   Bogacz R, 2007, NEURAL COMPUT, V19, P442, DOI 10.1162/neco.2007.19.2.442
   Bogacz R, 2006, PSYCHOL REV, V113, P700, DOI 10.1037/0033-295X.113.4.700
   Bogacz R, 2010, Q J EXP PSYCHOL, V63, P863, DOI 10.1080/17470210903091643
   Brochu E., 2010, ARXIV10122599
   Drugowitsch J, 2012, J NEUROSCI, V32, P3612, DOI 10.1523/JNEUROSCI.4010-11.2012
   Frank MJ, 2006, PSYCHOL REV, V113, P300, DOI 10.1037/0033-295X.113.2.300
   Gold JI, 2002, NEURON, V36, P299, DOI 10.1016/S0896-6273(02)00971-6
   Gold JI, 2007, ANNU REV NEUROSCI, V30, P535, DOI 10.1146/annurev.neuro.29.051605.113038
   Lepora NF, 2012, NEURAL COMPUT, V24, P2924, DOI 10.1162/NECO_a_00360
   Mayrhofer JM, 2013, J NEUROPHYSIOL, V109, P273, DOI 10.1152/jn.00488.2012
   Pelikan M., 2005, HIERARCHICAL BAYESIA, P31, DOI 10.1007/978-3-540-32373-0_3
   Peters J, 2008, NEURAL NETWORKS, V21, P682, DOI 10.1016/j.neunet.2008.02.003
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   RATCLIFF R, 1978, PSYCHOL REV, V85, P59, DOI 10.1037//0033-295X.85.2.59
   Ratcliff R, 2008, NEURAL COMPUT, V20, P873, DOI 10.1162/neco.2008.12-06-420
   Simen P, 2006, NEURAL NETWORKS, V19, P1013, DOI 10.1016/j.neunet.2006.05.038
   Simen P, 2009, J EXP PSYCHOL HUMAN, V35, P1865, DOI 10.1037/a0016926
   Snoek J., 2012, ADV NEURAL INFORM PR, V2012, P2951
   Stich KP, 2006, J EXP BIOL, V209, P4802, DOI 10.1242/jeb.02574
   Sutton RS, 2000, ADV NEUR IN, V12, P1057
   WALD A, 1948, ANN MATH STAT, V19, P326, DOI 10.1214/aoms/1177730197
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704019
DA 2019-06-15
ER

PT S
AU Li, B
   Wang, YN
   Singh, A
   Vorobeychik, Y
AF Li, Bo
   Wang, Yining
   Singh, Aarti
   Vorobeychik, Yevgeniy
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Data Poisoning Attacks on Factorization-Based Collaborative Filtering
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Recommendation and collaborative filtering systems are important in modern information and e-commerce applications. As these systems are becoming increasingly popular in the industry, their outputs could affect business decision making, introducing incentives for an adversarial party to compromise the availability or integrity of such systems. We introduce a data poisoning attack on collaborative filtering systems. We demonstrate how a powerful attacker with full knowledge of the learner can generate malicious data so as to maximize his/her malicious objectives, while at the same time mimicking normal user behavior to avoid being detected. While the complete knowledge assumption seems extreme, it enables a robust assessment of the vulnerability of collaborative filtering schemes to highly motivated attacks. We present efficient solutions for two popular factorization-based collaborative filtering algorithms: the alternative minimization formulation and the nuclear norm minimization method. Finally, we test the effectiveness of our proposed algorithms on real-world data and discuss potential defensive strategies.
C1 [Li, Bo; Vorobeychik, Yevgeniy] Vanderbilt Univ, 221 Kirkland Hall, Nashville, TN 37235 USA.
   [Wang, Yining; Singh, Aarti] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
RP Li, B (reprint author), Vanderbilt Univ, 221 Kirkland Hall, Nashville, TN 37235 USA.
EM bo.li.2@vanderbilt.edu; ynwang.yining@gmail.com; aarti@cs.cmu.edu;
   yevgeniy.vorobeychik@vanderbilt.edu
FU NSF [CNS-1238959, IIS-1526860]; ONR [N00014-15-1-2621]; ARO
   [W911NF-16-1-0069]; AFRL [FA8750-14-2-0180]; Sandia National
   Laboratories; Symantec Labs Graduate Research Fellowship
FX This research was partially supported by the NSF (CNS-1238959,
   IIS-1526860), ONR (N00014-15-1-2621), ARO (W911NF-16-1-0069), AFRL
   (FA8750-14-2-0180), Sandia National Laboratories, and Symantec Labs
   Graduate Research Fellowship.
CR Alfeld Scott, 2016, AAAI
   Barreno Marco, 2006, P 2006 ACM S INF COM, P16, DOI DOI 10.1145/1128817.1128824
   Biggio B., 2012, ICML
   Bo Li, 2014, ADV NEURAL INFORM PR, P2087
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Candes Emmanuel, 2007, FDN COMPUTATIONAL MA, V9, P717
   Chen YD, 2013, IEEE T INFORM THEORY, V59, P4324, DOI 10.1109/TIT.2013.2249572
   Chen Yudong, 2011, ICML
   Dalvi Nilesh, 2004, P 10 ACM SIGKDD INT, P99, DOI DOI 10.1145/1014052.1014066
   Jain Prateek, 2013, STOC
   Klopp Olga, 2014, ARXIV14128132
   Li Bo, 2015, P 18 INT C ART INT S, P599
   Lowd Daniel, 2005, P 11 ACM SIGKDD INT, P641, DOI DOI 10.1145/1081870.1081950
   Mei Shike, 2015, AAAI
   Mei Shike, 2015, AISTATS
   Mobasher B, 2005, P 2005 WEBKDD WORKSH
   Nie Feiping, 2012, ICDM
   O'Mahony M. P., 2002, Database and Expert Systems Applications. 13th International Conference, DEXA 2002. Proceedings (Lecture Notes in Computer Science Vol.2453), P494
   Wang Jun, 2006, SIGIR
   Wang Yu-Xiang, 2012, ICML
   Welling M., 2011, P 28 INT C MACH LEAR, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3
   Xiao Huang, 2015, ICML
NR 22
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700106
DA 2019-06-15
ER

PT S
AU Li, CT
   Jegelka, S
   Sra, S
AF Li, Chengtao
   Jegelka, Stefanie
   Sra, Suvrit
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Fast Mixing Markov Chains for Strongly Rayleigh Measures, DPPs, and
   Constrained Sampling
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We study probability measures induced by set functions with constraints. Such measures arise in a variety of real-world settings, where prior knowledge, resource limitations, or other pragmatic considerations impose constraints. We consider the task of rapidly sampling from such constrained measures, and develop fast Markov chain samplers for them. Our first main result is for MCMC sampling from Strongly Rayleigh (SR) measures, for which we present sharp polynomial bounds on the mixing time. As a corollary, this result yields a fast mixing sampler for Determinantal Point Processes (DPPs), yielding (to our knowledge) the first provably fast MCMC sampler for DPPs since their inception over four decades ago. Beyond SR measures, we develop MCMC samplers for probabilistic models with hard constraints and identify sufficient conditions under which their chains mix rapidly. We illustrate our claims by empirically verifying the dependence of mixing times on the key factors governing our theoretical bounds.
C1 [Li, Chengtao; Jegelka, Stefanie; Sra, Suvrit] MIT, Cambridge, MA 02139 USA.
RP Li, CT (reprint author), MIT, Cambridge, MA 02139 USA.
EM ctli@mit.edu; stefje@csail.mit.edu; suvrit@mit.edu
FU NSF CAREER [1553284]; Google Research Award
FX This research was partially supported by NSF CAREER 1553284 and a Google
   Research Award.
CR ALDOUS DJ, 1982, J LOND MATH SOC, V25, P564, DOI 10.1112/jlms/s2-25.3.564
   Anari N., 2015, FOCS
   Anari N., 2016, COLT
   Borcea J, 2009, J AM MATH SOC, V22, P521
   Bouchard- Cote A., 2010, NIPS
   BRODER A, 1989, ANN IEEE SYMP FOUND, P442, DOI 10.1109/SFCS.1989.63516
   Brooks SP, 1998, J COMPUT GRAPH STAT, V7, P434, DOI 10.2307/1390675
   Bubley R, 1997, ANN IEEE SYMP FOUND, P223, DOI 10.1109/SFCS.1997.646111
   Cesa- Bianchi N., 2009, COLT
   Diaconis P., 1991, ANN APPL PROBAB, V1, P36
   Djolonga Josip, 2014, ADV NEURAL INFORM PR, P244
   Dyer M, 1998, RANDOM STRUCT ALGOR, V13, P285
   Dyer M., 1999, FOCS
   Ermon S., 2013, ADV NEURAL INFORM PR, P2085
   Feder T., 1992, Proceedings of the Twenty-Fourth Annual ACM Symposium on the Theory of Computing, P26, DOI 10.1145/129712.129716
   Frieze A, 2014, SIAM J COMPUT, V43, P497, DOI 10.1137/120890971
   Gartrell Mike, 2016, ARXIV160205436
   Gelman A, 1992, STAT SCI, V7, P457, DOI DOI 10.1214/SS/1177011136
   Gotovos A., 2015, NIPS
   Greig D. M., 1989, J ROYAL STAT SOC
   Iyer R., 2015, AISTATS
   Jerrum M., 1993, SIAM J COMPUTING
   Jerrum M., 2004, JACM
   Kang B., 2013, P C NEUR INF PROC SY, V26, P2319
   Kathuria T., 2016, SAMPLING CONSTRAINED
   Kojima M., 2014, ARXIV14062100
   Kulesza A., 2011, P INT C MACH LEARN, P1193
   Kulesza A., 2012, ARXIV12076083
   Li C., 2016, ICML
   Maddison C. J., 2014, NIPS
   Mariet Z., 2016, ICLR
   Morris B, 2004, SIAM J COMPUT, V34, P195, DOI 10.1137/S0097539702411915
   Rebeschini P., 2015, COLT
   Sinclair A., 1992, COMBINATORICS PROBAB, V1, P351, DOI [10.1017/S0963548300000390, DOI 10.1017/S0963548300000390]
   Smith D., 2008, EMNLP
   Spielman D., 2008, STOC
   Zhang J, 2015, IEEE I CONF COMP VIS, P1859, DOI 10.1109/ICCV.2015.216
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701039
DA 2019-06-15
ER

PT S
AU Li, CJ
   Wang, ZR
   Liu, H
AF Li, Chris Junchi
   Wang, Zhaoran
   Liu, Han
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Online ICA: Understanding Global Dynamics of Nonconvex Optimization via
   Diffusion Processes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Solving statistical learning problems often involves nonconvex optimization. Despite the empirical success of nonconvex statistical optimization methods, their global dynamics, especially convergence to the desirable local minima, remain less well understood in theory. In this paper, we propose a new analytic paradigm based on diffusion processes to characterize the global dynamics of nonconvex statistical optimization. As a concrete example, we study stochastic gradient descent (SGD) for the tensor decomposition formulation of independent component analysis. In particular, we cast different phases of SGD into diffusion processes, i.e., solutions to stochastic differential equations. Initialized from an unstable equilibrium, the global dynamics of SGD transit over three consecutive phases: (i) an unstable Ornstein-Uhlenbeck process slowly departing from the initialization, (ii) the solution to an ordinary differential equation, which quickly evolves towards the desirable local minimum, and (iii) a stable Ornstein-Uhlenbeck process oscillating around the desirable local minimum. Our proof techniques are based upon Stroock and Varadhan's weak convergence of Markov chains to diffusion processes, which are of independent interest.
C1 [Li, Chris Junchi; Wang, Zhaoran; Liu, Han] Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA.
RP Li, CJ (reprint author), Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA.
EM junchil@princeton.edu; zhaoran@princeton.edu; hanliu@princeton.edu
CR Agarwal A., 2013, ARXIV13107991
   Aldous D, 1989, APPL MATH SCI, V77
   Anandkumar A., 2016, ARXIV160205908
   Anandkumar A., 2014, ARXIV14111488
   Anandkumar A, 2014, J MACH LEARN RES, V15, P2773
   Arora S., 2015, ARXIV150300778
   Balakrishnan S., 2014, ARXIV14082156
   Bhojanapalli Srinadh, 2015, ARXIV150903917
   Bronshtein I. N., 1998, HDB MATH
   Cai T. T., 2015, ARXIV150603382
   Candes E., 2014, ARXIV14071065
   Chen Y., 2015, ADV NEURAL INFORM PR
   Chen Y., 2015, ARXIV150903025
   Darken C., 1991, ADV NEURAL INFORM PR
   De Sa C., 2014, ARXIV14111134
   Durrett R., 2010, PROBABILITY THEORY E
   Ethier S. N., 1985, MARKOV PROCESSES CHA, V282
   Ge R., 2015, ARXIV150302101
   Golub G. H., 2012, MATRIX COMPUTATIONS
   Gu Q., 2014, ADV NEURAL INFORM PR
   Gu Q., 2016, INT C ART INT STAT
   Hardt M., 2014, FDN COMPUTER SCI
   Hirsch MW, 2013, DIFFERENTIAL EQUATIONS, DYNAMICAL SYSTEMS, AND AN INTRODUCTION TO CHAOS, 3RD EDITION, P1
   Jain P., 2013, S THEOR COMP
   Jain P., 2014, ARXIV14111087
   Jain P., 2015, ARXIV150705854
   Lee J. D., 2016, ARXIV160204915
   Levin D. A., 2009, MARKOV CHAINS MIXING
   Li C. J., 2016, ARXIV160305305
   Li Q., 2015, ARXIV151106251
   LIU H., 2014, ARXIV14085352
   Loh PL, 2015, J MACH LEARN RES, V16, P559
   Mandt Stephan, 2016, ARXIV160202666
   Mobahi H., 2016, ARXIV160104114
   Nesterov Y., 2004, INTRO LECT CONVEX OP, V87
   Netrapalli P., 2014, ADV NEURAL INFORM PR
   Netrapalli P., 2013, ADV NEURAL INFORM PR
   Oksendal B., 2003, STOCHASTIC DIFFERENT
   Panageas  I., 2016, ARXIV160500405
   Qu Q., 2014, ADV NEURAL INFORM PR
   Stroock D. W., 1979, MULTIDIMENSIONAL DIF, V233
   Su W., 2014, ADV NEURAL INFORM PR
   Sun J., 2015, ARXIV151006096
   Sun  J., 2015, ARXIV151104777
   Sun J., 2016, ARXIV160206664
   Sun J., 2015, ARXIV151103607
   Sun R., 2015, FDN COMPUTER SCI
   Sun W., 2015, ARXIV150201425
   Sun W, 2015, ADV NEUR IN, V28
   Tan K. M., 2016, ARXIV160408697
   Tu S., 2015, ARXIV150703566
   Wang Z., 2015, ADV NEURAL INFORM PR
   Wang ZR, 2014, ANN STAT, V42, P2164, DOI 10.1214/14-AOS1238
   White C. D., 2015, ARXIV150607868
   Yang Zhuoran, 2015, ARXIV151104514
   Zhang Y., 2014, ADV NEURAL INFORM PR
   ZHAO T, 2015, ADV NEURAL INFORM PR
   Zheng Q., 2015, ARXIV150606081
NR 58
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702053
DA 2019-06-15
ER

PT S
AU Li, DN
   Yang, K
   Wong, WH
AF Li, Dangna
   Yang, Kun
   Wong, Wing Hung
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Density Estimation via Discrepancy Based Adaptive Sequential Partition
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID K-MEANS; ALGORITHM; SEARCH
AB Given iid observations from an unknown absolute continuous distribution defined on some domain Omega, we propose a nonparametric method to learn a piecewise constant function to approximate the underlying probability density function. Our density estimate is a piecewise constant function defined on a binary partition of Omega. The key ingredient of the algorithm is to use discrepancy, a concept originates from Quasi Monte Carlo analysis, to control the partition process. The resulting algorithm is simple, efficient, and has a provable convergence rate. We empirically demonstrate its efficiency as a density estimation method. We also show how it can be utilized to find good initializations for k-means.
C1 [Li, Dangna] Stanford Univ, ICME, Stanford, CA 94305 USA.
   [Yang, Kun] Google, Mountain View, CA 94043 USA.
   [Wong, Wing Hung] Stanford Univ, Dept Stat, Stanford, CA 94305 USA.
RP Li, DN (reprint author), Stanford Univ, ICME, Stanford, CA 94305 USA.
EM dangna@stanford.edu; kunyang@stanford.edu; whwong@stanford.edu
FU  [NIH-R01GM109836];  [NSF-DMS1330132];  [NSF-DMS1407557]
FX This work was supported by NIH-R01GM109836, NSF-DMS1330132 and
   NSF-DMS1407557. The second author's work was done when the author was a
   graduate student at Stanford University.
CR Aghaeepour N, 2013, NAT METHODS, V10, P228, DOI [10.1038/NMETH.2365, 10.1038/nmeth.2365]
   Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   Doerr Carola, 2013, PREPRINT
   Fraley C, 1998, SIAM J SCI COMPUT, V20, P270, DOI 10.1137/S1064827596311451
   Gnewuch M., 2012, MONTE CARLO QUASIMON, V23, P43
   Gnewuch M, 2012, SIAM J NUMER ANAL, V50, P781, DOI 10.1137/110833865
   Gray RM, 1998, COMPRESSION AND COMPLEXITY OF SEQUENCES 1997 - PROCEEDINGS, P172, DOI 10.1109/SEQUEN.1997.666914
   Heinrich Stefan, 2000, ACTA ARITHMETICA WAR, V96, P279
   Katsavounidis I, 1994, IEEE SIGNAL PROC LET, V1, P144, DOI 10.1109/97.329844
   Kaul M, 2013, 2013 IEEE 14TH INTERNATIONAL CONFERENCE ON MOBILE DATA MANAGEMENT (MDM 2013), VOL 1, P137, DOI 10.1109/MDM.2013.24
   Kuipers L., 2012, UNIFORM DISTRIBUTION
   Liang JJ, 2001, MATH COMPUT, V70, P337
   Liu H, 2011, J MACH LEARN RES, V12, P907
   Lu L, 2013, J AM STAT ASSOC, V108, P1402, DOI 10.1080/01621459.2013.813389
   Owen A. B., 2005, SER BIOSTAT, P49
   PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472
   Ram Parikshit, 2011, P 17 ACM SIGKDD INT, P627
   Redmond SJ, 2007, PATTERN RECOGN LETT, V28, P965, DOI 10.1016/j.patrec.2007.01.001
   Spitzer MH, 2015, SCIENCE, V349, DOI 10.1126/science.1259425
   Su T, 2007, INTELL DATA ANAL, V11, P319, DOI 10.3233/IDA-2007-11402
   Wong WH, 2010, ANN STAT, V38, P1433, DOI 10.1214/09-AOS755
   Xu Q, 2015, PATTERN RECOGN LETT, V54, P50, DOI 10.1016/j.patrec.2014.11.017
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701074
DA 2019-06-15
ER

PT S
AU Li, P
   Mitzenmacher, M
   Slawski, M
AF Li, Ping
   Mitzenmacher, Michael
   Slawski, Martin
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Quantized Random Projections and Non-Linear Estimation of Cosine
   Similarity
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID JOHNSON
AB Random projections constitute a simple, yet effective technique for dimensionality reduction with applications in learning and search problems. In the present paper, we consider the problem of estimating cosine similarities when the projected data undergo scalar quantization to b bits. We here argue that the maximum likelihood estimator (MLE) is a principled approach to deal with the non-linearity resulting from quantization, and subsequently study its computational and statistical properties. A specific focus is on the on the trade-off between bit depth and the number of projections given a fixed budget of bits for storage or transmission. Along the way, we also touch upon the existence of a qualitative counterpart to the Johnson-Lindenstrauss lemma in the presence of quantization.
C1 [Li, Ping; Slawski, Martin] Rutgers State Univ, New Brunswick, NJ 07102 USA.
   [Mitzenmacher, Michael] Harvard Univ, Cambridge, MA 02138 USA.
RP Li, P (reprint author), Rutgers State Univ, New Brunswick, NJ 07102 USA.
EM pingli@stat.rutgers.edu; michaelm@eecs.harvard.edu;
   martin.slawski@rutgers.edu
FU NSF [CCF-1535795, CCF-1320231];  [NSF-Bigdata-1419210]; 
   [NSF-III-1360971]
FX The work of Ping Li and Martin Slawski is supported by
   NSF-Bigdata-1419210 and NSF-III-1360971. The work of Michael
   Mitzenmacher is supported by NSF CCF-1535795 and NSF CCF-1320231.
CR Bingham E., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P245
   Boutsidis C., 2010, ADV NEURAL INFORM PR, V23, P298
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Charikar M. S., 2002, P 34 ANN ACM S THEOR, P380, DOI DOI 10.1145/509907.509965
   Dasgupta S, 2003, RANDOM STRUCT ALGOR, V22, P60, DOI 10.1002/rsa.10073
   Dasgupta S., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P634, DOI 10.1109/SFFCS.1999.814639
   Genz A., BVN FUNCTION COMPUTI
   Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876
   Jacques L., 2013, ARXIV13051786
   Jacques L, 2015, IEEE T INFORM THEORY, V61, P5012, DOI 10.1109/TIT.2015.2453355
   Johnson W. B., 1984, CONT MATH, V26, P189, DOI DOI 10.1090/CONM/026/737400
   KIEFFER JC, 1983, IEEE T INFORM THEORY, V29, P42, DOI 10.1109/TIT.1983.1056622
   Laska JN, 2012, IEEE T SIGNAL PROCES, V60, P3496, DOI 10.1109/TSP.2012.2194710
   Li M, 2012, IEEE INT WORKSH MULT, P1, DOI 10.1109/MMSP.2012.6343406
   Li P., 2014, P INT C MACH LEARN I
   Li P, 2006, LECT NOTES ARTIF INT, V4005, P635, DOI 10.1007/11776420_46
   Lopes M., 2011, ADV NEURAL INFORM PR, P1206
   Madigan D., 2003, P 9 ACM SIGKDD INT C, P517
   Maillard O., 2009, ADV NEURAL INFORM PR, V22, P1213
   MAX J, 1960, IRE T INFORM THEOR, V6, P7, DOI 10.1109/TIT.1960.1057548
   SHENTON LR, 1963, J ROY STAT SOC B, V25, P305
   Srivastava R, 2016, J COMPUT GRAPH STAT, V25, P954, DOI 10.1080/10618600.2015.1062771
   Vempala S.S., 2005, RANDOM PROJECTION ME
   Wang F., 2010, P 10 SIAM INT C DAT, P281
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704017
DA 2019-06-15
ER

PT S
AU Li, RY
   Jia, JY
AF Li, Ruiyu
   Jia, Jiaya
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Visual Question Answering with Question Representation Update (QRU)
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Our method aims at reasoning over natural language questions and visual images. Given a natural language question about an image, our model updates the question representation iteratively by selecting image regions relevant to the query and learns to give the correct answer. Our model contains several reasoning layers, exploiting complex visual relations in the visual question answering (VQA) task. The proposed network is end-to-end trainable through back-propagation, where its weights are initialized using pre-trained convolutional neural network (CNN) and gated recurrent unit (GRU). Our method is evaluated on challenging datasets of COCO-QA [19] and VQA [2] and yields state-of-the-art performance.
C1 [Li, Ruiyu; Jia, Jiaya] Chinese Univ Hong Kong, Hong Kong, Peoples R China.
RP Li, RY (reprint author), Chinese Univ Hong Kong, Hong Kong, Peoples R China.
EM ryli@cse.cuhk.edu.hk; leojia@cse.cuhk.edu.hk
FU Research Grants Council of the Hong Kong SAR [2150760]; National Science
   Foundation China [61133009]
FX This work is supported by a grant from the Research Grants Council of
   the Hong Kong SAR (project No. 2150760) and by the National Science
   Foundation China, under Grant 61133009. We thank NVIDIA for providing
   Ruiyu Li a Tesla K40 GPU accelerator for this work.
CR Andreas J., 2016, ARXIV160101705
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Chen K, 2015, ARXIV151105960
   DENG J, 2009, PROC CVPR IEEE, P248, DOI DOI 10.1109/CVPR.2009.5206848
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Hu B., 2014, ADV NEURAL INFORM PR, P2042
   Hu R., 2015, ARXIV151104164
   Ilija I., 2016, ARXIV160401485
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kiros R, 2015, ADV NEURAL INFORM PR, P3276
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740
   MA L, 2015, ARXIV150600333
   Malinowski M, 2014, ADV NEURAL INFORM PR, P1682
   Malinowski M, 2015, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2015.9
   Noh H, 2015, ARXIV151105756
   Peng B., 2015, ARXIV150805508
   Ren M., 2015, ADV NEURAL INFORM PR, P2935
   Shih K. J., 2015, ARXIV151107394
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Sukhbaatar  Sainbayar, 2015, ARXIV150308895
   Sutskever I., 2014, ARXIV14093215
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   WU ZB, 1994, 32ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P133
   Xiong C, 2016, ARXIV160301417
   Xu H., 2015, ARXIV151105234
   Xu K, 2015, ARXIV150203044
   Yang  Zichao, 2015, ARXIV151102274
   Zhou B., 2015, ARXIV151202167
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702008
DA 2019-06-15
ER

PT S
AU Li, SCX
   Marlin, B
AF Li, Steven Cheng-Xian
   Marlin, Benjamin
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI A scalable end-to-end Gaussian process adapter for irregularly sampled
   time series classification
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID SPECTRAL-ANALYSIS
AB We present a general framework for classification of sparse and irregularly-sampled time series. The properties of such time series can result in substantial uncertainty about the values of the underlying temporal processes, while making the data difficult to deal with using standard classification methods that assume fixed-dimensional feature spaces. To address these challenges, we propose an uncertainty-aware classification framework based on a special computational layer we refer to as the Gaussian process adapter that can connect irregularly sampled time series data to any black-box classifier learnable using gradient descent. We show how to scale up the required computations based on combining the structured kernel interpolation framework and the Lanczos approximation method, and how to discriminatively train the Gaussian process adapter in combination with a number of classifiers end-to-end using backpropagation.
C1 [Li, Steven Cheng-Xian; Marlin, Benjamin] Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.
RP Li, SCX (reprint author), Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.
EM cxl@cs.umass.edu; marlin@cs.umass.edu
FU National Science Foundation [1350522]
FX This work was supported by the National Science Foundation under Grant
   No. 1350522.
CR BARTELS RH, 1972, COMMUN ACM, V15, P820, DOI 10.1145/361573.361582
   BJORCK A, 1983, LINEAR ALGEBRA APPL, V52-3, P127, DOI 10.1016/0024-3795(83)90010-1
   Chow E, 2014, SIAM J SCI COMPUT, V36, pA588, DOI 10.1137/130920587
   Clark JS, 2004, ECOLOGY, V85, P3140, DOI 10.1890/03-0520
   Dubrulle A.A., 2001, ELECTRON T NUMER ANA, V12, P216
   Feng YT, 1995, COMPUTER METHODS APP
   Golub G. H., 2012, MATRIX COMPUTATIONS, V3
   Golub G. H., 1977, MATH SOFTWARE, V3, P361
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   Kingma Diederik P, 2014, P 21 INT C LEARN REP
   LeCun Yann, 2004, P COMP VIS PATT REC
   Li Steven Cheng-Xian, 2015, 31 C UNC ART INT
   Liu J., 2009, PERVASIVE MOBILE COM
   Marlin B.M., 2012, P 2 ACM SIGHIT INT H, P389, DOI DOI 10.1145/2110363.2110408
   Parlett Beresford N, 1980, SYMMETRIC EIGENVALUE, V7
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Ruf T, 1999, BIOL RHYTHM RES, V30, P178, DOI 10.1076/brhm.30.2.178.1422
   SAAD Y, 1980, SIAM J NUMER ANAL, V17, P687, DOI 10.1137/0717059
   SAAD Y., 2003, ITERATIVE METHODS SP
   SCARGLE JD, 1982, ASTROPHYS J, V263, P835, DOI 10.1086/160554
   Schulz M, 1997, COMPUT GEOSCI, V23, P929, DOI 10.1016/S0098-3004(97)00087-3
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Turner MIlicIan W, 2009, IMA J NUMER ANAL
   Wilson Andrew Gordon, 2013, P 30 INT C MACH LEAR
   Wilson Andrew Gordon, 2015, P 32 INT C MACH LEAR
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703111
DA 2019-06-15
ER

PT S
AU Li, X
   Qin, T
   Yang, J
   Liu, TY
AF Li, Xiang
   Qin, Tao
   Yang, Jian
   Liu, Tie-Yan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI LightRNN: Memory and Computation-Efficient Recurrent Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Recurrent neural networks (RNNs) have achieved state-of-the-art performances in many natural language processing tasks, such as language modeling and machine translation. However, when the vocabulary is large, the RNN model will become very big (e.g., possibly beyond the memory capacity of a GPU device) and its training will become very inefficient. In this work, we propose a novel technique to tackle this challenge. The key idea is to use 2-Component (2C) shared embedding for word representations. We allocate every word in the vocabulary into a table, each row of which is associated with a vector, and each column associated with another vector. Depending on its position in the table, a word is jointly represented by two components: a row vector and a column vector. Since the words in the same row share the row vector and the words in the same column share the column vector, we only need 2 root vertical bar V vertical bar vectors to represent a vocabulary of vertical bar V vertical bar unique words, which are far less than the vertical bar V vertical bar vectors required by existing approaches. Based on the 2-Component shared embedding, we design a new RNN algorithm and evaluate it using the language modeling task on several benchmark datasets. The results show that our algorithm significantly reduces the model size and speeds up the training process, without sacrifice of accuracy (it achieves similar, if not better, perplexity as compared to state-of-the-art language models). Remarkably, on the One-Billion-Word benchmark Dataset, our algorithm achieves comparable perplexity to previous language models, whilst reducing the model size by a factor of 40-100, and speeding up the training process by a factor of 2. We name our proposed algorithm LightRNN to reflect its very small model size and very high training speed.
C1 [Li, Xiang; Yang, Jian] Nanjing Univ Sci & Technol, Nanjing, Jiangsu, Peoples R China.
   [Qin, Tao; Liu, Tie-Yan] Microsoft Res Asia, Beijing, Peoples R China.
RP Li, X (reprint author), Nanjing Univ Sci & Technol, Nanjing, Jiangsu, Peoples R China.
EM implusdream@gmail.com; taoqin@microsoft.com; csjyang@njust.edu.cn;
   tie-yan.liu@microsoft.com
FU National Science Fund of China [91420201, 61472187, 61502235, 61233011,
   61373063]; Key Project of Chinese Ministry of Education [313030]; 973
   Program [2014CB349303]; Program for Changjiang Scholars and Innovative
   Research Team in University
FX The authors would like to thank the anonymous reviewers for their
   critical and constructive comments and suggestions. This work was
   partially supported by the National Science Fund of China under Grant
   Nos. 91420201, 61472187, 61502235, 61233011 and 61373063, the Key
   Project of Chinese Ministry of Education under Grant No. 313030, the 973
   Program No. 2014CB349303, and Program for Changjiang Scholars and
   Innovative Research Team in University. We also would like to thank
   Professor Xiaolin Hu from Department of Computer Science and Technology,
   Tsinghua National Laboratory for Information Science and Technology
   (TNList) for giving a lot of wonderful advices.
CR Ahuja Ravindra K, 1988, TECHNICAL REPORT
   Appleyard Jeremy, 2016, ARXIV160401946
   BENGIO Y, 2003, AISTATS
   Botha Jan A, 2014, ARXIV14054273
   Chelba C., 2013, ARXIV13123005
   Chen Welin, 2015, ARXIV151204906
   Chung J., 2014, CORR
   Gers FA, 2000, NEURAL COMPUT, V12, P2451, DOI 10.1162/089976600300015015
   Goodman J, 2001, INT CONF ACOUST SPEE, P561, DOI 10.1109/ICASSP.2001.940893
   Graves A, 2013, ARXIV13080850
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Ji Shihao, 2015, ARXIV151106909
   Kim Y., 2015, ARXIV150806615
   Mikolov T, 2010, NTFRSPEECH, V2, P3
   Mikolov T, 2011, INT CONF ACOUST SPEE, P5528
   Mnih A., 2009, ADV NEURAL INFORM PR, P1081
   Morin F., 2005, P AISTATS, P246
   Papadimitriou C. H., 1982, COMBINATORIAL OPTIMI
   Pomikalek J, 2012, LREC 2012 - EIGHTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P502
   Preis R, 1999, LECT NOTES COMPUT SC, V1563, P259
   Sak Haim, 2014, ARXIV14021128
   Sundermeyer M, 2012, 13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3, P194
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Tang D., 2015, P 2015 C EMP METH NA, P1422, DOI DOI 10.18653/V1/D15-1167
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
   Weston J., 2014, ARXIV14103916
   Yu  D., 2014, TECHNICAL REPORT
   Zaremba W, 2014, ARXIV14092329
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704037
DA 2019-06-15
ER

PT S
AU Li, YY
   Pirk, S
   Su, H
   Qi, CR
   Guibas, LJ
AF Li, Yangyan
   Pirk, Soren
   Su, Hao
   Qi, Charles R.
   Guibas, Leonidas J.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI FPNN: Field Probing Neural Networks for 3D Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID DESCRIPTORS
AB Building discriminative representations for 3D data has been an important task in computer graphics and computer vision research. Convolutional Neural Networks (CNNs) have shown to operate on 2D images with great success for a variety of tasks. Lifting convolution operators to 3D (3DCNNs) seems like a plausible and promising next step. Unfortunately, the computational complexity of 3D CNNs grows cubically with respect to voxel resolution. Moreover, since most 3D geometry representations are boundary based, occupied regions do not increase proportionately with the size of the discretization, resulting in wasted computation. In this work, we represent 3D spaces as volumetric fields, and propose a novel design that employs field probing filters to efficiently extract features from them. Each field probing filter is a set of probing points - sensors that perceive the space. Our learning algorithm optimizes not only the weights associated with the probing points, but also their locations, which deforms the shape of the probing filters and adaptively distributes them in 3D space. The optimized probing points sense the 3D space "intelligently", rather than operating blindly over the entire domain. We show that field probing is significantly more efficient than 3DCNNs, while providing state-of-the-art performance, on classification tasks for 3D object recognition benchmark datasets.
C1 [Li, Yangyan; Pirk, Soren; Su, Hao; Qi, Charles R.; Guibas, Leonidas J.] Stanford Univ, Stanford, CA 94305 USA.
   [Li, Yangyan] Shandong Univ, Jinan, Shandong, Peoples R China.
RP Li, YY (reprint author), Stanford Univ, Stanford, CA 94305 USA.; Li, YY (reprint author), Shandong Univ, Jinan, Shandong, Peoples R China.
FU NSF [DMS-1546206, IIS-1528025]; UCB MURI grant [N00014-13-1-0341];
   Chinese National 973 Program [2015CB352501]; Stanford AI Lab-Toyota
   Center for Artificial Intelligence Research; Max Planck Center for
   Visual Computing and Communication; Google Focused Research award
FX We would first like to thank all the reviewers for their valuable
   comments and suggestions. Yangyan thanks Daniel Cohen-Or and Zhenhua
   Wang for their insightful proofreading. The work was supported in part
   by NSF grants DMS-1546206 and IIS-1528025, UCB MURI grant
   N00014-13-1-0341, Chinese National 973 Program (2015CB352501), the
   Stanford AI Lab-Toyota Center for Artificial Intelligence Research, the
   Max Planck Center for Visual Computing and Communication, and a Google
   Focused Research award.
CR Boscaini D, 2016, COMPUT GRAPH FORUM, V35, P431, DOI 10.1111/cgf.12844
   Boscaini D, 2015, COMPUT GRAPH FORUM, V34, P13, DOI 10.1111/cgf.12693
   BRONSTEIN A.M., 2011, TOG, V30, P1, DOI DOI 10.1145/1899404.1899405
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303
   DENG J, 2009, PROC CVPR IEEE, P248, DOI DOI 10.1109/CVPR.2009.5206848
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Eitel A, 2015, IEEE INT C INT ROBOT, P681, DOI 10.1109/IROS.2015.7353446
   Glorot X., 2010, JLMR P TRACK, P249, DOI DOI 10.1.1/207.2059
   Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23
   Ioffe S, 2015, INT C MACH LEARN, V32, P448, DOI DOI 10.1007/S13398-014-0173-7.2
   Jia Y., 2014, P 22 ACM INT C MULT, P675, DOI DOI 10.1145/2647868.2654889
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Kazhdan M., 2003, Symposium on Geometry Processing, P156
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Litman R, 2014, IEEE T PATTERN ANAL, V36, P171, DOI 10.1109/TPAMI.2013.148
   Masci Jonathan, 2015, ICCV WORKSH 3D REPR
   Maturana  Daniel, 2015, IEEE RSJ INT C INT R
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Nieaner M., 2013, T GRAPHICS, V32
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   Qi C. R., 2016, CVPR
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shi BG, 2015, IEEE SIGNAL PROC LET, V22, P2339, DOI 10.1109/LSP.2015.2480802
   Socher R., 2012, ADV NEURAL INFORM PR, V25, P665, DOI DOI 10.1002/2014GB005021
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Su Hao, 2015, ICCV
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   WU ZR, 2015, PROC CVPR IEEE, P1912, DOI DOI 10.1109/CVPR.2015.7298801
NR 31
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703052
DA 2019-06-15
ER

PT S
AU Li, YZ
   Turner, RE
AF Li, Yingzhen
   Turner, Richard E.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Renyi Divergence Variational Inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB This paper introduces the variational Renyi bound (VR) that extends traditional variational inference to Renyi's alpha-divergences. This new family of variational methods unifies a number of existing approaches, and enables a smooth interpolation from the evidence lower-bound to the log (marginal) likelihood that is controlled by the value of alpha that parametrises the divergence. The reparameterization trick, Monte Carlo approximation and stochastic optimisation methods are deployed to obtain a tractable and unified framework for optimisation. We further consider negative alpha values and propose a novel variational inference method as a new special case in the proposed framework. Experiments on Bayesian neural networks and variational auto-encoders demonstrate the wide applicability of the VR bound.
C1 [Li, Yingzhen; Turner, Richard E.] Univ Cambridge, Cambridge CB2 1PZ, England.
RP Li, YZ (reprint author), Univ Cambridge, Cambridge CB2 1PZ, England.
EM yl494@cam.ac.uk; ret26@cam.ac.uk
FU Schlumberger Foundation FFTF fellowship; EPSRC [EP/M026957/1,
   EP/L000776/1]
FX We thank the Cambridge MLG members and the reviewers for comments. YL
   thanks the Schlumberger Foundation FFTF fellowship. RET thanks EPSRC
   grants #EP/M026957/1 and EP/L000776/1.
CR Amari S., 1985, DIFFERENTIAL GEOMETR
   Beal M.J., 2003, THESIS
   Broderick T., 2013, ADV NEURAL INFORM PR
   Bui T. D, 2016, P 33 INT C MACH LEAR
   Burda Yuri, 2016, INT C LEARN REPR ICL
   Dehaene G., 2015, ARXIV150308060
   Depeweg S., 2016, ARXIV160507127
   Gelman A, 2014, ARXIV14124869
   Grunwald P. D., 2007, MINIMUM DESCRIPTION
   Hernandez-Lobato Jose Miguel, 2016, INT C MACH LEARN ICM
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Kingma D, 2014, INT C LEARN REPR ICL
   Kingma  D.P., 2015, INT C LEARN REPR ICL
   Kucukelbir A., 2015, ADV NEURAL INFORM PR
   Li Y., 2015, ADV NEURAL INFORM PR
   Minka T., 2004, MSRT2004149
   Minka T., 2005, TECH REP
   Minka T. P., 2001, UNCERTAINTY ARTIFICI
   Opper M, 2005, J MACH LEARN RES, V6, P2177
   Paisley J., 2012, INT C MACH LEARN ICM
   Ranganath Rajesh, 2014, ARTIFICIAL INTELLIGE
   Renyi A., 1961, BERK S MATH STAT PRO, V1
   Rezende D. J., 2014, P 30 INT C MACH LEAR
   Salimans T, 2013, BAYESIAN ANAL, V8, P837, DOI 10.1214/13-BA858
   TSALLIS C, 1988, J STAT PHYS, V52, P479, DOI 10.1007/BF01016429
   Turner Richard E., 2011, BAYESIAN TIME SERIES, P109
   van Erven T, 2014, IEEE T INFORM THEORY, V60, P3797, DOI 10.1109/TIT.2014.2320500
   Xu M., 2014, ADV NEURAL INFORM PR
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701065
DA 2019-06-15
ER

PT S
AU Li, YB
   Dong, WS
   Xie, XM
   Shi, GM
   Li, X
   Xu, DL
AF Li, Yongbo
   Dong, Weisheng
   Xie, Xuemei
   Shi, Guangming
   Li, Xin
   Xu, Donglai
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning Parametric Sparse Models for Image Super-Resolution
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Learning accurate prior knowledge of natural images is of great importance for single image super-resolution (SR). Existing SR methods either learn the prior from the low/high-resolution patch pairs or estimate the prior models from the input low-resolution (LR) image. Specifically, high-frequency details are learned in the former methods. Though effective, they are heuristic and have limitations in dealing with blurred LR images; while the latter suffers from the limitations of frequency aliasing. In this paper, we propose to combine those two lines of ideas for image super-resolution. More specifically, the parametric sparse prior of the desirable high-resolution (HR) image patches are learned from both the input low-resolution (LR) image and a training image dataset. With the learned sparse priors, the sparse codes and thus the HR image patches can be accurately recovered by solving a sparse coding problem. Experimental results show that the proposed SR method outperforms existing state-of-the-art methods in terms of both subjective and objective image qualities.
C1 [Li, Yongbo; Dong, Weisheng; Xie, Xuemei] Xidian Univ, Sch Elect Engn, State Key Lab ISN, Xian, Shaanxi, Peoples R China.
   [Shi, Guangming] Xidian Univ, Chinese Minist Educ, Key Lab IPIU, Xian, Shaanxi, Peoples R China.
   [Li, Xin] West Virginia Univ, Lane Dept CSEE, Morgantown, WV 26506 USA.
   [Xu, Donglai] Teesside Univ, Sch Sci & Engn, Middlesbrough, Cleveland, England.
RP Dong, WS (reprint author), Xidian Univ, Sch Elect Engn, State Key Lab ISN, Xian, Shaanxi, Peoples R China.
EM yongboli@stu.xidian.edu.cn; wsdong@mail.xidian.edu.cn;
   xmxie@mail.xidian.edu.cn; gmshi@xidian.edu.cn; Xin.Li@mail.wvu.edu
FU Natural Science Foundation (NSF) of China [61622210, 61471281, 61632019,
   61472301, 61390512]; Specialized Research Fund for the Doctoral Program
   of Higher Education [20130203130001]
FX This work was supported in part by the Natural Science Foundation (NSF)
   of China under Grants(No. No. 61622210, 61471281, 61632019, 61472301,
   and 61390512), in part by the Specialized Research Fund for the Doctoral
   Program of Higher Education (No. 20130203130001).
CR BEVILACQUA M., 2012, LOW COMPLEXITY SINGL
   Dai D, 2015, COMPUT GRAPH FORUM, V34, P95, DOI 10.1111/cgf.12544
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong WS, 2015, INT J COMPUT VISION, V114, P217, DOI 10.1007/s11263-015-0808-y
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847
   Dong WS, 2011, IEEE I CONF COMP VIS, P1259, DOI 10.1109/ICCV.2011.6126377
   Dong WS, 2011, IEEE T IMAGE PROCESS, V20, P1838, DOI 10.1109/TIP.2011.2108306
   Egiazarian K, 2015, EUR SIGNAL PR CONF, P2849, DOI 10.1109/EUSIPCO.2015.7362905
   Irani M., 1993, Journal of Visual Communication and Image Representation, V4, P324, DOI 10.1006/jvci.1993.1030
   Kim KI, 2010, IEEE T PATTERN ANAL, V32, P1127, DOI 10.1109/TPAMI.2010.25
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   LI YB, 2015, P IEEE INT C COMP VI, P450, DOI DOI 10.1109/ICCV.2015.59
   Marquina A, 2008, J SCI COMPUT, V37, P367, DOI 10.1007/s10915-008-9214-8
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yu GS, 2012, IEEE T IMAGE PROCESS, V21, P2481, DOI 10.1109/TIP.2011.2176743
   Zeyde R, 2010, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703014
DA 2019-06-15
ER

PT S
AU Li, YZ
   Liang, YY
   Risteski, A
AF Li, Yuanzhi
   Liang, Yingyu
   Risteski, Andrej
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Recovery Guarantee of Non-negative Matrix Factorization via Alternating
   Updates
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Non-negative matrix factorization is a popular tool for decomposing data into feature and weight matrices under non-negativity constraints. It enjoys practical success but is poorly understood theoretically. This paper proposes an algorithm that alternates between decoding the weights and updating the features, and shows that assuming a generative model of the data, it provably recovers the ground-truth under fairly mild conditions. In particular, its only essential requirement on features is linear independence. Furthermore, the algorithm uses ReLU to exploit the non-negativity for decoding the weights, and thus can tolerate adversarial noise that can potentially be as large as the signal, and can tolerate unbiased noise much larger than the signal. The analysis relies on a carefully designed coupling between two potential functions, which we believe is of independent interest.
C1 [Li, Yuanzhi; Liang, Yingyu; Risteski, Andrej] Princeton Univ, Dept Comp Sci, 35 Olden St, Princeton, NJ 08540 USA.
RP Li, YZ (reprint author), Princeton Univ, Dept Comp Sci, 35 Olden St, Princeton, NJ 08540 USA.
EM yuanzhil@cs.princeton.edu; yingyul@cs.princeton.edu;
   risteski@cs.princeton.edu
FU NSF [CCF-1527371, DMS-1317308]; Simons Investigator Award; Simons
   Collaboration Grant;  [ONR-N00014-16-1-2329]
FX This work was supported in part by NSF grants CCF-1527371, DMS-1317308,
   Simons Investigator Award, Simons Collaboration Grant, and
   ONR-N00014-16-1-2329.
CR Anandkumar A., 2012, TECHNICAL REPORT
   Arora S., 2012, FOCS
   Arora S., 2013, ICML
   Arora S., 2012, ADV NEURAL INFORM PR, P2375
   Arora S., 2015, COLT
   Arora S., 2012, P 44 ANN ACM S THEOR, P145
   Awasthi Pranjal, 2015, NIPS, P2089
   Bhattacharyya Chiranjib, 2016, P 33 INT C MACH LEAR
   Blei David M, 2012, COMMUNICATIONS ACM
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Lee DD, 1997, ADV NEUR IN, V9, P515
   Lee DD, 2001, ADV NEUR IN, V13, P556
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Pehlevan C, 2014, CONF REC ASILOMAR C, P769, DOI 10.1109/ACSSC.2014.7094553
   Pehlevan Cengiz, 2015, ADV NEURAL INFORM PR, P2260
   Zhu J, 2012, ARXIV12023778
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703053
DA 2019-06-15
ER

PT S
AU Li, YZ
   Risteski, A
AF Li, Yuanzhi
   Risteski, Andrej
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Approximate maximum entropy principles via Goemans-Williamson with
   applications to provable variational methods
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID ALGORITHMS; CUT
AB The well known maximum- entropy principle due to Jaynes, which states that given mean parameters, the maximum entropy distribution matching them is in an exponential family has been very popular in machine learning due to its "Occam's razor" interpretation. Unfortunately, calculating the potentials in the maximum-entropy distribution is intractable [BGS14]. We provide computationally efficient versions of this principle when the mean parameters are pairwise moments: we design distributions that approximately match given pairwise moments, while having entropy which is comparable to the maximum entropy distribution matching those moments.
   We additionally provide surprising applications of the approximate maximum entropy principle to designing provable variational methods for partition function calculations for Ising models without any assumptions on the potentials of the model. More precisely, we show that we can get approximation guarantees for the log-partition function comparable to those in the low-temperature limit, which is the setting of optimization of quadratic forms over the hypercube. ([AN06])
C1 [Li, Yuanzhi; Risteski, Andrej] Princeton Univ, Dept Comp Sci, Princeton, NJ 08450 USA.
RP Li, YZ (reprint author), Princeton Univ, Dept Comp Sci, Princeton, NJ 08450 USA.
EM yuanzhil@cs.princeton.edu; risteski@cs.princeton.edu
CR Alon N, 2006, INVENT MATH, V163, P499, DOI 10.1007/s00222-005-0465-9
   Alon N, 2006, SIAM J COMPUT, V35, P787, DOI 10.1137/S0097539704441629
   Barak B., 2014, STOC, P31, DOI DOI 10.1145/2591796.2591886
   Bethge Matthias, 2007, NEAR MAXIMUM ENTROPY
   Bresler G., 2014, P NIPS, V27, P1062
   Charikar M, 2004, ANN IEEE SYMP FOUND, P54, DOI 10.1109/FOCS.2004.39
   Ellis R. S., 2012, ENTROPY LARGE DEVIAT, V271
   ELLIS RS, 1978, J STAT PHYS, V19, P149, DOI 10.1007/BF01012508
   Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684
   GRIFFITH.RB, 1967, J MATH PHYS, V8, P478, DOI 10.1063/1.1705219
   JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620
   JERRUM M, 1993, SIAM J COMPUT, V22, P1087, DOI 10.1137/0222066
   Risteski Andrej, 2016, P C LEARN THEOR COLT
   Singh Mohit, 2014, P SYMP THEOR COMP ST, P50
   Sly A, 2012, ANN IEEE SYMP FOUND, P361, DOI 10.1109/FOCS.2012.56
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Wainwright Martin J, 2003, TREE REWEIGHTED BELI
   Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P2313, DOI 10.1109/TIT.2005.850091
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701026
DA 2019-06-15
ER

PT S
AU Li, YZ
   Risteski, A
AF Li, Yuanzhi
   Risteski, Andrej
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Algorithms and matching lower bounds for approximately-convex
   optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB In recent years, a rapidly increasing number of applications in practice requires optimizing non-convex objectives, like training neural networks, learning graphical models, maximum likelihood estimation. Though simple heuristics such as gradient descent with very few modifications tend to work well, theoretical understanding is very weak.
   We consider possibly the most natural class of non-convex functions where one could hope to obtain provable guarantees: functions that are "approximately convex", i.e. functions (f) over tilde : R-d -> R for which there exists a convex function f such that for all x, vertical bar(f) over tilde (x) - f(x)vertical bar <= Delta for a fixed value Delta. We then want to minimize (f) over tilde, i.e. output a point (x) over tilde such that (f) over tilde ((x) over tilde) <= min(x) (f) over tilde (x) + epsilon.
   It is quite natural to conjecture that for fixed epsilon, the problem gets harder for larger Delta, however, the exact dependency of epsilon and Delta is not known. In this paper, we significantly improve the known lower bound on Delta as a function of epsilon and an algorithm matching this lower bound for a natural class of convex bodies. More precisely, we identify a function T : R+ -> R+ such that when Delta = O (T(epsilon)), we can give an algorithm that outputs a point (x) over tilde such that (f) over tilde ((x) over tilde) <= min(x) (f) over tilde (x) + epsilon within time poly (d, 1/epsilon). On the other hand, when Delta = Omega (T(epsilon)), we also prove an information theoretic lower bound that any algorithm that outputs such a (x) over tilde must use super polynomial number of evaluations of (f) over tilde.
C1 [Li, Yuanzhi; Risteski, Andrej] Princeton Univ, Dept Comp Sci, Princeton, NJ 08450 USA.
RP Li, YZ (reprint author), Princeton Univ, Dept Comp Sci, Princeton, NJ 08450 USA.
EM yuanzhil@cs.princeton.edu; risteski@cs.princeton.edu
CR Agarwal A, 2011, ADV NEURAL INFORM PR, V24, P1035
   Agarwal A, 2010, P 23 ANN C LEARN THE, P28
   Belloni Alexandre, 2015, P 28 C LEARN THEOR, P240
   Duchi JC, 2015, IEEE T INFORM THEORY, V61, P2788, DOI 10.1109/TIT.2015.2409256
   Dyer M, 2014, MATH PROGRAM, V147, P207, DOI 10.1007/s10107-013-0718-0
   Flaxman AD, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P385
   Nemirovskii Arkadii, 1983, WILEY INTERSCIENCE S
   Nesterov Yurii, FDN COMPUTATIONAL MA, P1
   Shamir Ohad, 2012, ARXIV12092388
   Singer Yaron, 2015, NIPS, P3186
NR 10
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704100
DA 2019-06-15
ER

PT S
AU Li, Z
   Gong, BQ
   Yang, TB
AF Li, Zhe
   Gong, Boqing
   Yang, Tianbao
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Improved Dropout for Shallow and Deep Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Dropout has been witnessed with great success in training deep neural networks by independently zeroing out the outputs of neurons at random. It has also received a surge of interest for shallow learning, e.g., logistic regression. However, the independent sampling for dropout could be suboptimal for the sake of convergence. In this paper, we propose to use multinomial sampling for dropout, i.e., sampling features or neurons according to a multinomial distribution with different probabilities for different features/neurons. To exhibit the optimal dropout probabilities, we analyze the shallow learning with multinomial dropout and establish the risk bound for stochastic optimization. By minimizing a sampling dependent factor in the risk bound, we obtain a distribution-dependent dropout with sampling probabilities dependent on the second order statistics of the data distribution. To tackle the issue of evolving distribution of neurons in deep learning, we propose an efficient adaptive dropout (named evolutional dropout) that computes the sampling probabilities on-the-fly from a mini-batch of examples. Empirical studies on several benchmark datasets demonstrate that the proposed dropouts achieve not only much faster convergence and but also a smaller testing error than the standard dropout. For example, on the CIFAR-100 data, the evolutional dropout achieves relative improvements over 10% on the prediction performance and over 50% on the convergence speed compared to the standard dropout.
C1 [Li, Zhe; Yang, Tianbao] Univ Iowa, Iowa City, IA 52245 USA.
   [Gong, Boqing] Univ Cent Florida, Orlando, FL 32816 USA.
RP Li, Z (reprint author), Univ Iowa, Iowa City, IA 52245 USA.
EM zhe-li-1@uiowa.edu; bgong@crcv.ucf.edu; tianbao-yang@uiowa.edu
FU National Science Foundation [IIS-1463988, IIS-1545995]; NSF
   [IIS-1566511]
FX We thank anonymous reviewers for their comments. Z. Li and T. Yang are
   partially supported by National Science Foundation (IIS-1463988,
   IIS-1545995). B. Gong is supported in part by NSF (IIS-1566511) and a
   gift from Adobe.
CR Ba  J., 2013, ADV NEURAL INFORM PR, P3084
   Baldi P, 2013, ADV NEURAL INFORM PR, P2814
   Graham Benjamin, 2015, CORR
   Hinton Geoffrey E, 2012, ARXIV PREPRINT ARXIV
   Ioffe S., 2015, ARXIV150203167
   Kingma D.P., 2015, CORR
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Netzer Y., 2011, NIPS WORKSH DEEP LEA, V2011, P4
   Neyshabur Behnam, 2015, ADV NEURAL INFORM PR, P2413
   Ranzato MA, 2010, INT C ART INT STAT, V9, P621
   Shalev-Shwartz Shai, 2009, 22 C LEARN THEOR COL
   Srebro N., 2010, ADV NEURAL INFORM PR, P2199
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sutskever I., 2013, P 30 INT C MACH LEAR, V28, P1139
   Wager Stefan, 2013, ADV NEURAL INFORM PR, P351
   Wan L., 2013, P 30 INT C MACH LEAR, P1058
   Wang S., 2013, P 30 INT C MACH LEAR, p[118, 126]
   Wang Sida I, 2013, P C EMP METH NAT LAN, P1170
   Zhang Sixin, 2014, ARXIV14126651
   Zhuo JW, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P4126
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704085
DA 2019-06-15
ER

PT S
AU Lian, XR
   Zhang, H
   Hsieh, CJ
   Huang, YJ
   Liu, J
AF Lian, Xiangru
   Zhang, Huan
   Hsieh, Cho-Jui
   Huang, Yijun
   Liu, Ji
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI A Comprehensive Linear Speedup Analysis for Asynchronous Stochastic
   Parallel Optimization from Zeroth-Order to First-Order
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Asynchronous parallel optimization received substantial successes and extensive attention recently. One of core theoretical questions is how much speedup (or benefit) the asynchronous parallelization can bring to us. This paper provides a comprehensive and generic analysis to study the speedup property for a broad range of asynchronous parallel stochastic algorithms from the zeroth order to the first order methods. Our result recovers or improves existing analysis on special cases, provides more insights for understanding the asynchronous parallel behaviors, and suggests a novel asynchronous parallel zeroth order method for the first time. Our experiments provide novel applications of the proposed asynchronous parallel zeroth order method on hyper parameter tuning and model blending problems.
C1 [Lian, Xiangru; Huang, Yijun; Liu, Ji] Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA.
   [Zhang, Huan] Univ Calif Davis, Dept Elect & Comp Engn, Davis, CA 95616 USA.
   [Hsieh, Cho-Jui] Univ Calif Davis, Dept Comp Sci, Davis, CA 95616 USA.
RP Lian, XR (reprint author), Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA.
EM xiangru@yandex.com; victzhang@gmail.com; chohsieh@ucdavis.edu;
   huangyj0@gmail.com; ji.liu.uwisc@gmail.com
FU NSF [CNS-1548078]
FX This project is in part supported by the NSF grant CNS-1548078. We
   especially thank Chen-Tse Tsai for providing the code and data for the
   Yahoo Music Competition.
CR Agarwal A, 2011, ADV NEURAL INFORM PR, V24, P1035
   Agarwal A., 2011, NIPS
   Avron H, 2015, J ACM, V62, DOI 10.1145/2814566
   Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223
   Chaturapruek S., 2015, ADV NEURAL INFORM PR, P1531
   Chen P.-L., 2012, KDDCUP
   De C. M. Sa, 2015, ADV NEURAL INFORM PR, P2674
   Dean J., 2012, NIPS
   Dror Gideon, 2012, KDD CUP, P8
   Duchi JC, 2012, SIAM J OPTIMIZ, V22, P674, DOI 10.1137/110831659
   Fellus J, 2015, NEUROCOMPUTING, V169, P262, DOI 10.1016/j.neucom.2014.11.076
   Feyzmahdavian H. R., 2015, ASYNCHRONOUS MINI BA
   Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811
   Gimpel K., 2010, P 14 C COMP NAT LANG, P213
   Hong M., 2014, ARXIV14126058
   Hsieh C.-J., 2015, P 32 INT C MACH LEAR, V15, P2370
   Jamieson K. G., 2012, NIPS
   Li M., 2014, OSDI
   Lian X., 2015, ADV NEURAL INFORM PR, P2719
   Liu J., 2014, ARXIV14033862
   Liu J., 2014, ICML
   Liu J, 2014, ASYNCHRONOUS PARALLE
   Mania Horia, 2015, ARXIV150706970
   Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277
   Nesterov Y., 2011, FDN COMPUTATIONAL MA, P1
   Niu F., 2011, NIPS
   Paine T., 2013, NIPS
   Peng Z., 2015, AROCK ALGORITHMIC FR
   Petroni F., 2014, ACM C REC SYST
   Reddi S. J., 2015, NIPS, P2629
   Richtarik P, 2014, MATH PROGRAM, V144, P1, DOI 10.1007/s10107-012-0614-z
   Sridhar S., 2013, NIPS
   Wei J., 2013, ARXIV13127869
   Yun H., 2013, ARXIV13120193
   Zhang R., 2014, ICML
   Zhang  S., 2014, DEEP LEARNING ELASTI
   Zhao Shen-Yi, 2016, AAAI, P2379
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704075
DA 2019-06-15
ER

PT S
AU Liang, JW
   Fadili, JM
   Peyre, G
AF Liang, Jingwei
   Fadili, Jalal M.
   Peyre, Gabriel
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI A Multi-step Inertial Forward-Backward Splitting Method for Non-convex
   Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID PROXIMAL METHOD; ALGORITHM; CONVERGENCE
AB We propose a multi-step inertial Forward-Backward splitting algorithm for minimizing the sum of two non-necessarily convex functions, one of which is proper lower semi-continuous while the other is differentiable with a Lipschitz continuous gradient. We first prove global convergence of the algorithm with the help of the Kurdyka-Lojasiewicz property. Then, when the non-smooth part is also partly smooth relative to a smooth submanifold, we establish finite identification of the latter and provide sharp local linear convergence analysis. The proposed method is illustrated on several problems arising from statistics and machine learning.
C1 [Liang, Jingwei; Fadili, Jalal M.] Normandie Univ, ENSICAEN, CNRS, GREYC, Caen, France.
   [Peyre, Gabriel] CNRS, DMA, ENS Paris, Paris, France.
RP Liang, JW (reprint author), Normandie Univ, ENSICAEN, CNRS, GREYC, Caen, France.
EM Jingwei.Liang@greyc.ensicaen.fr; Jalal.Fadili@greyc.ensicaen.fr;
   Gabriel.Peyre@ens.fr
FU European Research Council (ERC project SIGMA-Vision)
FX This work was partly supported by the European Research Council (ERC
   project SIGMA-Vision).
CR Alvarez F, 2000, SIAM J CONTROL OPTIM, V38, P1102, DOI 10.1137/S0363012998335802
   Alvarez F, 2001, SET-VALUED ANAL, V9, P3, DOI 10.1023/A:1011253113155
   Attouch H, 2014, SIAM J OPTIMIZ, V24, P232, DOI 10.1137/130910294
   Attouch H, 2013, MATH PROGRAM, V137, P91, DOI 10.1007/s10107-011-0484-9
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Bolte J, 2007, SIAM J OPTIMIZ, V17, P1205, DOI 10.1137/050644641
   Bolte J, 2010, T AM MATH SOC, V362, P3319, DOI 10.1090/S0002-9947-09-05048-X
   Bot R. I., 2014, EURO J COMPUTATIONAL, P1
   Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Chambolle A., 2015, J OPTIMIZATION THEOR, V166, P1
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P6, DOI 10.1109/TIT.2005.860430
   Drusvyatskiy D, 2013, MATH PROGRAM, V140, P5, DOI 10.1007/s10107-012-0624-x
   Frankel P, 2015, J OPTIMIZ THEORY APP, V165, P874, DOI 10.1007/s10957-014-0642-3
   Le HY, 2013, OPTIM LETT, V7, P731, DOI 10.1007/s11590-012-0456-x
   Lewis AS, 2013, SIAM J OPTIMIZ, V23, P74, DOI 10.1137/110852103
   Lewis AS, 2008, MATH OPER RES, V33, P216, DOI 10.1287/moor.1070.0291
   Lewis AS, 2003, SIAM J OPTIMIZ, V13, P702, DOI 10.1137/S1052623401387623
   Lewis AS, 2001, SIAM J MATRIX ANAL A, V23, P368, DOI 10.1137/S089547980036838X
   LIANG J., 2014, ADV NEURAL INFORM PR, V27, P1970
   Liang J., 2015, ARXIV150303703
   Lorenz DA, 2015, J MATH IMAGING VIS, V51, P311, DOI 10.1007/s10851-014-0523-2
   Moudafi A, 2003, J COMPUT APPL MATH, V155, P447, DOI 10.1016/S0377-0427(02)00906-8
   Ochs P, 2014, SIAM J IMAGING SCI, V7, P1388, DOI 10.1137/130942954
   Polyak B.T., 1964, USSR COMP MATH MATH, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]
   ROCKAFELLAR RT, 1998, GRUND MATH WISS, V317, P1
   vandenDries L., 1998, MATH SOC LECT NOTES, V248
NR 26
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702032
DA 2019-06-15
ER

PT S
AU Liao, RJ
   Schwing, A
   Zemel, RS
   Urtasun, R
AF Liao, Renjie
   Schwing, Alexander
   Zemel, Richard S.
   Urtasun, Raquel
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning Deep Parsimonious Representations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB In this paper we aim at facilitating generalization for deep networks while supporting interpretability of the learned representations. Towards this goal, we propose a clustering based regularization that encourages parsimonious representations. Our k-means style objective is easy to optimize and flexible, supporting various forms of clustering, such as sample clustering, spatial clustering, as well as co-clustering. We demonstrate the effectiveness of our approach on the tasks of unsupervised learning, classification, fine grained categorization, and zero-shot learning.
C1 [Liao, Renjie; Zemel, Richard S.; Urtasun, Raquel] Univ Toronto, Toronto, ON, Canada.
   [Schwing, Alexander] Univ Illinois, Urbana, IL USA.
   [Zemel, Richard S.] Canadian Inst Adv Res, Toronto, ON, Canada.
RP Liao, RJ (reprint author), Univ Toronto, Toronto, ON, Canada.
EM rjliao@cs.toronto.edu; aschwing@illinois.edu; zemel@cs.toronto.edu;
   urtasun@cs.toronto.edu
FU Intelligence Advanced Research Projects Activity (IARPA) via Department
   of Interior/Interior Business Center (DoI/IBC) [D16PC00003]; NVIDIA; 
   [ONR-N00014-14-1-0232]
FX This work was partially supported by ONR-N00014-14-1-0232, NVIDIA and
   the Intelligence Advanced Research Projects Activity (IARPA) via
   Department of Interior/Interior Business Center (DoI/IBC) contract
   number D16PC00003. The U.S. Government is authorized to reproduce and
   distribute reprints for Governmental purposes notwithstanding any
   copyright annotation thereon. Disclaimer: The views and conclusions
   contained herein are those of the authors and should not be interpreted
   as necessarily representing the official policies or endorsements,
   either expressed or implied, of IARPA, DoI/IBC, or the U.S. Government.
CR Akata Z., 2013, P CVPR
   Akata Z., 2015, P CVPR
   [Anonymous], 2015, TENSORFLOW LARGE SCA
   Arthur D., 2007, P SODA
   Chen G., 2015, ARXIV150103084
   Chen W., 2015, ARXIV150404788
   Cogswell M., 2016, P ICLR
   De Lathauwer Lieven, 2000, SIAM J MATRIX ANAL A
   Donahue J., 2013, ARXIV13101531
   Goodfellow I. J., 2013, P ICLR
   Han S., 2016, P ICLR
   Hanson S. J., 1989, P NIPS
   Hariharan B., 2014, P ECCV
   Hassibi B., 1993, P NIPS
   Hershey J. R., 2015, ARXIV150804306
   Hinton  G., 2012, IEEE SIGNAL PROCESSI
   Hinton Geoffrey, 2006, SCIENCE
   Ioffe S., 2015, ICML 2015
   Jia Y., 2014, ARXIV14085093
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A., 2012, P NIPS
   Lecun Y., 2015, NATURE
   Lecun Y., 1998, P IEEE
   LeCun Y., 1989, P NIPS
   Manning C. D., 2008, INTRO INFORM RETRIEV
   Srivastava N., 2014, JMLR
   Sutskever I., 2014, P NIPS
   Tian F., 2014, AAAI
   Tibshirani R., 1996, J ROYAL STAT SOC
   Tikhonov A. N., 1943, STABILITY INVERSE PR
   Trigeorgis G., 2014, P ICML
   Wan L., 2013, P ICML
   Wang Z., 2015, ARXIV150900151
   Welinder P, 2010, CNSTR2010001 CALTECH
   Yang J., 2016, ARXIV160403628
   Zhang N., 2014, P ECCV
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702010
DA 2019-06-15
ER

PT S
AU Lin, JH
   Rosasco, L
AF Lin, Junhong
   Rosasco, Lorenzo
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Optimal Learning for Multi-pass Stochastic Gradient Methods
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID APPROXIMATION
AB We analyze the learning properties of the stochastic gradient method when multiple passes over the data and mini-batches are allowed. In particular, we consider the square loss and show that for a universal step-size choice, the number of passes acts as a regularization parameter, and optimal finite sample bounds can be achieved by early-stopping. Moreover, we show that larger step-sizes are allowed when considering mini-batches. Our analysis is based on a unifying approach, encompassing both batch and stochastic gradient methods as special cases.
C1 [Lin, Junhong; Rosasco, Lorenzo] MIT, LCSL, IIT, Cambridge, MA 02139 USA.
   [Rosasco, Lorenzo] Univ Genoa, DIBRIS, Genoa, Italy.
RP Lin, JH (reprint author), MIT, LCSL, IIT, Cambridge, MA 02139 USA.
EM junhong.lin@iit.it; lrosasco@mit.edu
FU Center for Brains, Minds and Machines (CBMM) - NSF STC [CCF-1231216];
   Italian Ministry of Education, University and Research [RBFR12M3AC]
FX This material is based upon work supported by the Center for Brains,
   Minds and Machines (CBMM), funded by NSF STC award CCF-1231216. L. R.
   acknowledges the financial support of the Italian Ministry of Education,
   University and Research FIRB project RBFR12M3AC.
CR Bauer F, 2007, J COMPLEXITY, V23, P52, DOI 10.1016/j.jco.2006.07.001
   Bottou L, 2008, ADV NEURAL INFORM PR, P161
   Boyd S., 2007, EE364B STANDF U
   Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]
   Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339
   Cotter A., 2011, ADV NEURAL INFORM PR, P1647
   Cucker F, 2007, C MO AP C M, P1, DOI 10.1017/CBO9780511618796
   Dekel O, 2012, J MACH LEARN RES, V13, P165
   Dieuleveut A, 2016, ANN STAT, V44, P1363, DOI 10.1214/15-AOS1391
   Hardt Moritz, 2016, INT C MACH LEARN
   Lin J., 2016, J MACHINE LEARNING R, V17, P1
   Lin J, 2016, INT CONF EUR ENERG
   Minsker  S., 2011, ARXIV11125448
   Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277
   Ng A., 2016, MACHINE LEARNING
   Orabona Francesco, 2014, ADV NEURAL INFORM PR, P1116
   PINELIS IF, 1986, THEOR PROBAB APPL+, V30, P143, DOI 10.1137/1130013
   Poljak B. T., 1987, INTRO OPTIMIZATION
   Rosasco L., 2015, ADV NEURAL INFORM PR, P1621
   Rudi A., 2015, ADV NEURAL INFORM PR, V28, P1648
   Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4
   Shamir O., 2013, INT C MACH LEARN, V28, P71
   Smale S, 2007, CONSTR APPROX, V26, P153, DOI 10.1007/s00365-006-0659-y
   Sra S, 2012, OPTIMIZATION FOR MACHINE LEARNING, P1
   Steinwart I, 2008, INFORM SCI STAT, P1
   Tarres P, 2014, IEEE T INFORM THEORY, V60, P5716, DOI 10.1109/TIT.2014.2332531
   Tropp Joel A, 2012, TECHNICAL REPORT
   Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2
   Ying YM, 2008, FOUND COMPUT MATH, V8, P561, DOI 10.1007/s10208-006-0237-y
   Zhang T, 2005, NEURAL COMPUT, V17, P2077, DOI 10.1162/0899766054323008
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701070
DA 2019-06-15
ER

PT S
AU Lin, M
   Ye, JP
AF Lin, Ming
   Ye, Jieping
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI A Non-convex One-Pass Framework for Generalized Factorization Machine
   and Rank-One Matrix Sensing
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We develop an efficient alternating framework for learning a generalized version of Factorization Machine (gFM) on steaming data with provable guarantees. When the instances are sampled from d dimensional random Gaussian vectors and the target second order coefficient matrix in gFM is of rank k, our algorithm converges linearly, achieves O(epsilon) recovery error after retrieving O(k(3)d log(1/epsilon)) training instances, consumes O(kd) memory in one-pass of dataset and only requires matrix-vector product operations in each iteration. The key ingredient of our framework is a construction of an estimation sequence endowed with a so-called Conditionally Independent RIP condition (CI-RIP). As special cases of gFM, our framework can be applied to symmetric or asymmetric rank-one matrix sensing problems, such as inductive matrix completion and phase retrieval.
C1 [Lin, Ming; Ye, Jieping] Univ Michigan, Ann Arbor, MI 48109 USA.
RP Lin, M (reprint author), Univ Michigan, Ann Arbor, MI 48109 USA.
EM linmin@umich.edu; jpye@umich.edu
FU NIH [RF1AG051710]; NSF [III-1421057, III-1421100]
FX This work was supported in part by research grants from NIH
   (RF1AG051710) and NSF (III-1421057 and III-1421100).
CR Blondel Mathieu, 2016, POLYNOMIAL NETWORKS, P850
   Cai TT, 2015, ANN STAT, V43, P102, DOI 10.1214/14-AOS1267
   Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Candes Emmanuel J., 2011, ARXIV11090573
   Chen YX, 2015, IEEE T INFORM THEORY, V61, P4034, DOI 10.1109/TIT.2015.2429594
   Davenport Mark A., 2016, ARXIV160106422
   Hardt Moritz, 2013, ARXIV13120925
   Hardt Moritz, 2013, ARXIV13112495
   Hardt Moritz, 2014, ARXIV14074070
   Hong L., 2013, P 6 ACM INT C WEB SE, P557, DOI DOI 10.1145/2433396.2433467
   Jain P., 2013, P 45 ANN ACM S THEOR, P665, DOI DOI 10.1145/2488608.2488693
   JAIN P, 2013, ARXIV13060626
   Jain Prateek, 2012, ARXIV12120467
   Koltchinskii V, 2011, LECT NOTES MATH, V2033, P1, DOI 10.1007/978-3-642-22147-7
   Kueng Richard, 2014, ARXIV14106913
   Lee Kiryung, 2013, ARXIV13120525
   Netrapalli Praneeth, 2013, ARXIV13060160
   Rendle Steffen, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P995, DOI 10.1109/ICDM.2010.127
   Rendle S., 2011, P 34 INT ACM SIGIR C, P635, DOI DOI 10.1145/2009916.2010002
   Stewart GW, 1990, MATRIX PERTURBATION
   Tropp Joel A., 2014, ARXIV14051102
   Tropp Joel A., 2015, ARXIV150101571
   Zhao  T., 2015, NONCONVEX LOW RANK M
   Zhao Tuo, 2015, Adv Neural Inf Process Syst, V28, P559
   Zhong K, 2015, LECT NOTES ARTIF INT, V9355, P3, DOI 10.1007/978-3-319-24486-0_1
   Zhu Peizhen, 2013, J NUMERICAL MATH, V21
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703046
DA 2019-06-15
ER

PT S
AU Lin, P
   Zhang, B
   Guo, T
   Wang, Y
   Chen, F
AF Lin, Peng
   Zhang, Bang
   Guo, Ting
   Wang, Yang
   Chen, Fang
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Infinite Hidden Semi-Markov Modulated Interaction Point Process
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB The correlation between events is ubiquitous and important for temporal events modelling. In many cases, the correlation exists between not only events' emitted observations, but also their arrival times. State space models (e.g., hidden Markov model) and stochastic interaction point process models (e.g., Hawkes process) have been studied extensively yet separately for the two types of correlations in the past. In this paper, we propose a Bayesian nonparametric approach that considers both types of correlations via unifying and generalizing the hidden semi-Markov model and interaction point process model. The proposed approach can simultaneously model both the observations and arrival times of temporal events, and automatically determine the number of latent states from data. A Metropolis-within-particle-Gibbs sampler with ancestor resampling is developed for efficient posterior inference. The approach is tested on both synthetic and real-world data with promising outcomes.
C1 [Lin, Peng; Zhang, Bang; Guo, Ting; Wang, Yang; Chen, Fang] Data61 CSIRO, Australian Technol Pk,13 Garden St, Eveleigh, NSW 2015, Australia.
   [Lin, Peng] Univ New South Wales, Sch Comp Sci & Engn, Sydney, NSW, Australia.
RP Lin, P (reprint author), Data61 CSIRO, Australian Technol Pk,13 Garden St, Eveleigh, NSW 2015, Australia.; Lin, P (reprint author), Univ New South Wales, Sch Comp Sci & Engn, Sydney, NSW, Australia.
EM peng.lin@data61.csiro.au; bang.zhang@data61.csiro.au;
   ting.guo@data61.csiro.au; yang.wang@data61.csiro.au;
   fang.chen@data61.csiro.au
CR Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x
   [Anonymous], 2011, P SUSTKDD WORKSH DAT
   Beal M., 2001, ADV NEURAL INFORM PR, P577
   Daley DJ, 2007, INTRO THEORY POINT P, VII
   DIGGLE PJ, 1994, INT STAT REV, V62, P99, DOI 10.2307/1403548
   Fox E. B., 2008, P 25 INT C MACH LEAR, P312, DOI DOI 10.1145/1390156.1390196
   HAWKES AG, 1971, BIOMETRIKA, V58, P83, DOI 10.1093/biomet/58.1.83
   HAWKES AG, 1974, J APPL PROBAB, V11, P493, DOI 10.2307/3212693
   Johnson M. J., 2012, ARXIV12033485
   KINGMAN JFC, 1964, P CAMB PHILOS SOC, V60, P923
   Li L., 2015, P AAAI, P672
   Lin P., 2016, 30 AAAI C ART INT AA
   Lindsten F, 2014, J MACH LEARN RES, V15, P2145
   Murphy K. P., 2002, HIDDEN SEMIMAR UNPUB
   OGATA Y, 1981, IEEE T INFORM THEORY, V27, P23, DOI 10.1109/TIT.1981.1056305
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Rasmussen JG, 2013, METHODOL COMPUT APPL, V15, P623, DOI 10.1007/s11009-011-9272-5
   Tripuraneni N., 2015, ADV NEURAL INFORM PR, P2386
   Van Gael J., 2008, P 25 INT C MACH LEAR, V25, P1088, DOI DOI 10.1145/1390156.1390293
   Whye Teh Yee, 2006, J AM STAT ASSOC, V101, P476, DOI DOI 10.1198/016214506000000302
   Yu SZ, 2010, ARTIF INTELL, V174, P215, DOI 10.1016/j.artint.2009.11.011
   Zhou Ke, 2013, P 30 INT C MACH LEAR, V28, P1301
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701100
DA 2019-06-15
ER

PT S
AU Linderman, SW
   Adams, RP
   Pillow, JW
AF Linderman, Scott W.
   Adams, Ryan P.
   Pillow, Jonathan W.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Bayesian latent structure discovery from multi-neuron recordings
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID MODELS
AB Neural circuits contain heterogeneous groups of neurons that differ in type, location, connectivity, and basic response properties. However, traditional methods for dimensionality reduction and clustering are ill-suited to recovering the structure underlying the organization of neural circuits. In particular, they do not take advantage of the rich temporal dependencies in multi-neuron recordings and fail to account for the noise in neural spike trains. Here we describe new tools for inferring latent structure from simultaneously recorded spike train data using a hierarchical extension of a multi-neuron point process model commonly known as the generalized linear model (GLM). Our approach combines the GLM with flexible graph-theoretic priors governing the relationship between latent features and neural connectivity patterns. Fully Bayesian inference via Polya-gamma augmentation of the resulting model allows us to classify neurons and infer latent dimensions of circuit organization from correlated spike trains. We demonstrate the effectiveness of our method with applications to synthetic data and multi-neuron recordings in primate retina, revealing latent patterns of neural types and locations from spike trains alone.
C1 [Linderman, Scott W.] Columbia Univ, New York, NY 10027 USA.
   [Adams, Ryan P.] Harvard Univ, Cambridge, MA 02138 USA.
   [Adams, Ryan P.] Twitter, San Francisco, CA USA.
   [Pillow, Jonathan W.] Princeton Univ, Princeton, NJ 08544 USA.
RP Linderman, SW (reprint author), Columbia Univ, New York, NY 10027 USA.
EM swl2133@columbia.edu; rpa@seas.harvard.edu; pillow@princeton.edu
FU Simons Foundation [SCGB-418011]; NSF [IIS-1421780]; Alfred P. Sloan
   Foundation; McKnight Foundation; Simons Collaboration on the Global
   Brain [SCGB AWD1004351]; NSF CAREER Award [IIS-1150186]; NIMH [MH099611]
FX We thank E. J. Chichilnisky, A. M. Litke, A. Sher and J. Shlens for
   retinal data. SWL is supported by the Simons Foundation SCGB-418011. RPA
   is supported by NSF IIS-1421780 and the Alfred P. Sloan Foundation. JWP
   was supported by grants from the McKnight Foundation, Simons
   Collaboration on the Global Brain (SCGB AWD1004351), NSF CAREER Award
   (IIS-1150186), and NIMH grant MH099611.
CR Ahrens MB, 2013, NAT METHODS, V10, P413, DOI [10.1038/nmeth.2434, 10.1038/NMETH.2434]
   BRILLINGER DR, 1976, BIOL CYBERN, V22, P213, DOI 10.1007/BF00365087
   Gerhard F, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003138
   Goris RLT, 2014, NAT NEUROSCI, V17, P858, DOI 10.1038/nn.3711
   Grewe BF, 2010, NAT METHODS, V7, P399, DOI 10.1038/nmeth.1453
   Hoff P. D., 2008, ADV NEURAL INFORM PR, V20, P1
   Macke J. H., 2011, ADV NEURAL INF PROCE, V24, P1350
   MITCHELL TJ, 1988, J AM STAT ASSOC, V83, P1023, DOI 10.2307/2290129
   Nowicki K, 2001, J AM STAT ASSOC, V96, P1077, DOI 10.1198/016214501753208735
   Orbanz P, 2015, IEEE T PATTERN ANAL, V37, P437, DOI 10.1109/TPAMI.2014.2334607
   Paninski L, 2004, NETWORK-COMP NEURAL, V15, P243, DOI 10.1088/0954-898X/15/4/002
   Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140
   Polson NG, 2013, J AM STAT ASSOC, V108, P1339, DOI 10.1080/01621459.2013.829001
   Prevedel R, 2014, NAT METHODS, V11, P727, DOI [10.1038/NMETH.2964, 10.1038/nmeth.2964]
   Sanes JR, 2015, ANNU REV NEUROSCI, V38, P221, DOI 10.1146/annurev-neuro-071714-034120
   Scott J., 2012, ADV NEURAL INFORM PR, P1898
   Soudry D., 2013, ARXIV13093724
   Stevenson IH, 2009, IEEE T NEUR SYS REH, V17, P203, DOI 10.1109/TNSRE.2008.2010471
   Truccolo W, 2005, J NEUROPHYSIOL, V93, P1074, DOI 10.1152/jn.00697.2004
   Vidne M, 2012, J COMPUT NEUROSCI, V33, P97, DOI 10.1007/s10827-011-0376-2
   Windle J, 2014, ARXIV14050506
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701042
DA 2019-06-15
ER

PT S
AU Lindgren, EM
   Wu, SS
   Dimakis, AG
AF Lindgren, Erik M.
   Wu, Shanshan
   Dimakis, Alexandros G.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Leveraging Sparsity for Efficient Submodular Data Summarization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB The facility location problem is widely used for summarizing large datasets and has additional applications in sensor placement, image retrieval, and clustering. One difficulty of this problem is that submodular optimization algorithms require the calculation of pairwise benefits for all items in the dataset. This is infeasible for large problems, so recent work proposed to only calculate nearest neighbor benefits. One limitation is that several strong assumptions were invoked to obtain provable approximation guarantees. In this paper we establish that these extra assumptions are not necessary-solving the sparsified problem will be almost optimal under the standard assumptions of the problem. We then analyze a different method of sparsification that is a better model for methods such as Locality Sensitive Hashing to accelerate the nearest neighbor computations and extend the use of the problem to a broader family of similarities. We validate our approach by demonstrating that it rapidly generates interpretable summaries.
C1 [Lindgren, Erik M.; Wu, Shanshan; Dimakis, Alexandros G.] Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA.
RP Lindgren, EM (reprint author), Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA.
EM erikml@utexas.edu; shanshan@utexas.edu; dimakis@austin.utexas.edu
RI Dimakis, Alexandros G/P-6034-2019
OI Dimakis, Alexandros G/0000-0002-4244-7033
FU National Science Foundation Graduate Research Fellowship [DGE-1110007];
   NSF [CCF 1344179, 1344364, 1407278, 1422549]; ARO [YIP W911NF-14-1-0258]
FX This material is based upon work supported by the National Science
   Foundation Graduate Research Fellowship under Grant No. DGE-1110007 as
   well as NSF Grants CCF 1344179, 1344364, 1407278, 1422549 and ARO YIP
   W911NF-14-1-0258.
CR Alon N., 2008, PROBABILISTIC METHOD
   Andoni A., 2012, EXACT ALGORITHMS A 2
   Andoni A., 2015, NIPS
   Avrachenkov K., 2007, SIAM J NUMERICAL ANA
   Badanidiyuru A., 2014, KDD
   Barbosa R., 2015, ICML
   Bentley J. L., 1975, COMMUNICATIONS ACM
   Beygelzimer A., 2006, ICML
   Charikar Moses S, 2002, STOC
   Chen J., 2009, JMLR
   Datar M., 2004, S COMP GEOM
   DVORETZKY A, 1956, ANN MATH STAT, V27, P642, DOI 10.1214/aoms/1177728174
   Feige U., 1998, J ACM
   Garcia V., 2010, INT C IM PROC
   Gionis  A., 1999, VLDB
   Gupta P., 2013, WWW
   Hoeffding  Wassily, 1963, J AM STAT ASS
   IMDb, 2016, ALT INT
   Indyk  P., 1998, STOC
   Jain Prateek, 2013, STOC
   Koren Y., 2009, COMPUTER
   Krause A., 2010, ICML
   Krause A., 2008, J WATER RESOURCES PL
   Kumar R., 2015, ACM T PARALLEL COMPU
   Leskovec  J., 2007, KDD
   Lin H., 2012, UAI
   MASSART P, 1990, ANN PROBAB, V18, P1269, DOI 10.1214/aop/1176990746
   Meng X., 2016, JMLR
   Minoux M., 1978, OPTIMIZATION TECHNIQ
   Mirrokni V., 2015, STOC
   Mirzasoleiman B., 2015, AAAI
   Mirzasoleiman Baharan, 2016, ICML
   Mirzasoleiman Baharan, 2013, NIPS
   Nemhauser G., 1978, MATH PROGRAMMING
   Neyshabur B., 2015, ICML
   Page L, 1999, PAGERANK CITATION RA
   Shrivastava A., 2014, NIPS
   Tschiatschek S., 2014, NIPS
   Wei K., 2014, ICML
   Wei Kai, 2015, ICML
NR 40
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703018
DA 2019-06-15
ER

PT S
AU Liu, C
   Zhu, J
   Song, Y
AF Liu, Chang
   Zhu, Jun
   Song, Yang
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Stochastic Gradient Geodesic MCMC Methods
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We propose two stochastic gradient MCMC methods for sampling from Bayesian posterior distributions defined on Riemann manifolds with a known geodesic flow, e.g. hyperspheres. Our methods are the first scalable sampling methods on these manifolds, with the aid of stochastic gradients. Novel dynamics are conceived and 2nd-order integrators are developed. By adopting embedding techniques and the geodesic integrator, the methods do not require a global coordinate system of the manifold and do not involve inner iterations. Synthetic experiments show the validity of the method, and its application to the challenging inference for spherical topic models indicate practical usability and efficiency.
C1 [Liu, Chang; Zhu, Jun] Tsinghua Univ, State Key Lab Intell Tech & Syst, Ctr Bioinspired Comp Res, Dept Comp Sci & Tech,TNList Lab, Beijing, Peoples R China.
   [Song, Yang] Tsinghua Univ, Dept Phys, Beijing, Peoples R China.
   [Song, Yang] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
RP Song, Y (reprint author), Tsinghua Univ, Dept Phys, Beijing, Peoples R China.; Song, Y (reprint author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
EM chang-li14@mails.tsinghua.edu.cn; dcszj@tsinghua.edu.cn;
   songyang@stanford.edu
FU National Basic Research Program (973 Program) of China [2013CB329403];
   National NSF of China [61620106010, 61322308, 61332007]; Tsinghua
   Initiative Scientific Research Program [20141080934]; Youth Top-notch
   Talent Support Program
FX The work was supported by the National Basic Research Program (973
   Program) of China (No. 2013CB329403), National NSF of China Projects
   (Nos. 61620106010, 61322308, 61332007), the Youth Top-notch Talent
   Support Program, and Tsinghua Initiative Scientific Research Program
   (No. 20141080934).
CR Abraham R, 1978, FDN MECH
   Ahn Sungjin, 2012, ARXIV12066380
   Anh N.K., 2013, 4 INT S INF COMM TEC, P131
   Banerjee A, 2005, J MACH LEARN RES, V6, P1345
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Brubaker Marcus, 2012, P 15 INT C ART INT S, P161
   Byrne S, 2013, SCAND J STAT, V40, P825, DOI 10.1111/sjos.12036
   Chen C., 2015, NIPS, P2269
   Chen T., 2014, P 31 INT C MACH LEAR, P1683
   Ding N., 2014, ADV NEURAL INFORM PR, P3203
   Du Chao, 2015, ARXIV150604557
   Ghosh K, 2003, J APPL STAT, V30, P145, DOI 10.1080/0266476022000023712
   Girolami M, 2011, J R STAT SOC B, V73, P123, DOI 10.1111/j.1467-9868.2010.00765.x
   Gopal Siddharth, 2014, P 31 INT C MACH LEAR
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   James I. M., 1976, TOPOLOGY STIEFEL MAN, V24
   Lan Shiwei, 2014, JMLR Workshop Conf Proc, V32, P629
   Li Chunyuan, 2015, ARXIV151207662
   Ma Y. A., 2015, ADV NEURAL INFORM PR, P2899
   Mardia Kanti V., 2000, DIRECTIONAL STAT, P159
   NASH J, 1956, ANN MATH, V63, P20, DOI 10.2307/1969989
   Neal R., 2011, HDB MARKOV CHAIN MON, P2, DOI DOI 10.1201/B10905-6
   Reisinger J., 2010, P 27 INT C MACH LEAR, P903
   Song Y, 2016, BMC HEALTH SERV RES, V16, DOI 10.1186/s12913-016-1283-z
   STIEFEL E, 1935, COMMENT MATH HELV, V8, P305, DOI [10.1007/BF01199559, DOI 10.1007/BF01199559]
   Straub J., 2015, P 18 INT C ART INT S, P930
   Taghia J, 2014, IEEE T PATTERN ANAL, V36, P1701, DOI 10.1109/TPAMI.2014.2306426
   Teh Y. W., 2013, ADV NEURAL INFORM PR, V26, P3102
   Welling M., 2011, P 28 INT C MACH LEAR, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702029
DA 2019-06-15
ER

PT S
AU Liu, CY
   Belkin, M
AF Liu, Chaoyue
   Belkin, Mikhail
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Clustering with Bregman Divergences: an Asymptotic Analysis
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID QUANTIZATION; APPROXIMATION
AB Clustering, in particular k-means clustering, is a central topic in data analysis. Clustering with Bregman divergences is a recently proposed generalization of k-means clustering which has already been widely used in applications. In this paper we analyze theoretical properties of Bregman clustering when the number of the clusters k is large. We establish quantization rates and describe the limiting distribution of the centers as k -> infinity, extending well-known results for k-means clustering.
C1 [Liu, Chaoyue; Belkin, Mikhail] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
RP Liu, CY (reprint author), Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
EM liu.2656@osu.edu; mbelkin@cse.ohio-state.edu
FU National Science Foundation
FX We thank the National Science Foundation for financial support and to
   Brian Kulis for discussions.
CR Aloise D, 2009, MACH LEARN, V75, P245, DOI 10.1007/s10994-009-5103-0
   Alsabti K., 1998, IPPS SPDP WORKSH HIG
   Banerjee A, 2005, J MACH LEARN RES, V6, P1705
   Bregman L. M., 1967, USSR COMP MATH MATH, V7, P200, DOI DOI 10.1016/0041-5553(67)90040-7
   BUCKLEW JA, 1982, IEEE T INFORM THEORY, V28, P239, DOI 10.1109/TIT.1982.1056486
   Coates Adam, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P561, DOI 10.1007/978-3-642-35289-8_30
   Fischer A, 2010, J MULTIVARIATE ANAL, V101, P2207, DOI 10.1016/j.jmva.2010.05.008
   Gersho A., 2012, VECTOR QUANTIZATION, V159
   Graf S., 2000, FDN QUANTIZATION PRO
   Jain AK, 1999, ACM COMPUT SURV, V31, P264, DOI 10.1145/331499.331504
   Jiang K., 2012, ADV NEURAL INFORM PR, P3158
   Kaufman L, 2009, FINDING GROUPS DATA, V344
   Krishna K, 1999, IEEE T SYST MAN CY B, V29, P433, DOI 10.1109/3477.764879
   LINDER T, 1991, PROBL CONTROL INFORM, V20, P475
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   MacQueen J., 1965, P 5 BERK S MATH STAT, P281, DOI DOI 10.1234/12345678
   Mahajan M, 2009, LECT NOTES COMPUT SC, V5431, P274
   MCCLURE DE, 1975, Q APPL MATH, V33, P1
   Nock R, 2008, LECT NOTES ARTIF INT, V5212, P154, DOI 10.1007/978-3-540-87481-2_11
   PANTER PF, 1951, P IRE, V39, P44, DOI 10.1109/JRPROC.1951.230419
   Que Q., 2016, AISTATS, P1375
   Telgarsky M., 2012, ARXIV12066446
   Wagstaff K., 2001, P 18 INT C MACH LEAR, V18, P577, DOI 10.1109/TPAMI.2002.1017616
   Zhang JW, 2011, NEUROCOMPUTING, V74, P1720, DOI 10.1016/j.neucom.2011.02.004
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704074
DA 2019-06-15
ER

PT S
AU Liu, MY
   Tuzel, O
AF Liu, Ming-Yu
   Tuzel, Oncel
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Coupled Generative Adversarial Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We propose coupled generative adversarial network (CoGAN) for learning a joint distribution of multi-domain images. In contrast to the existing approaches, which require tuples of corresponding images in different domains in the training set, CoGAN can learn a joint distribution without any tuple of corresponding images. It can learn a joint distribution with just samples drawn from the marginal distributions. This is achieved by enforcing a weight-sharing constraint that limits the network capacity and favors a joint distribution solution over a product of marginal distributions one. We apply CoGAN to several joint distribution learning tasks, including learning a joint distribution of color and depth images, and learning a joint distribution of face images with different attributes. For each task it successfully learns the joint distribution without any tuple of corresponding images. We also demonstrate its applications to domain adaptation and image transformation.
C1 [Liu, Ming-Yu; Tuzel, Oncel] MERL, Cambridge, MA 02139 USA.
RP Liu, MY (reprint author), MERL, Cambridge, MA 02139 USA.
EM mliu@merl.com; oncel@merl.com
CR Darrell T., 2014, ARXIV14123474
   Denton E. L., 2015, NIPS
   Dosovitskiy Alexey, 2015, CVPR
   Fernando B, 2015, PATTERN RECOGN LETT, V65, P60, DOI 10.1016/j.patrec.2015.07.009
   Ganin Y., 2016, JMLR
   Goodfellow I., 2014, NIPS
   Gregor K., 2015, ICML
   He K., 2015, ICCV
   Ioffe S., 2015, ARXIV150203167
   Kingma D. P., 2015, ICLR
   Kingma D. P., 2014, NIPS
   Kingma Diederik P, 2014, ICLR
   Kiros  R., 2014, ARXIV14112539
   Krizhevsky A., 2012, NIPS
   Lai K., 2011, ICRA
   Lecun Y., 1998, P IEEE
   Li  Yujia, 2016, ICML
   Liu  Z., 2015, ICCV
   Long M., 2013, ICCV
   Mirza M., 2014, ARXIV14111784, DOI DOI 10.1029/2009WR008312
   Ngiam  Jiquan, 2011, ICML
   Radford A., 2016, ICLR
   Reed S. E., 2015, NIPS
   Rezende D. J., 2014, ICML
   Rozantsev A., 2016, ARXIV160306432
   Silberman  N., 2012, ECCV
   Sohl-Dickstein  Jascha, 2015, ICML
   Srivastava  N., 2012, NIPS
   Theis  Lucas, 2016, ICLR
   Wang S, 2012, CVPR
   Yan  Xinchen, 2015, ARXIV151200570
   Yang J., 2010, IEEE TIP
   Yim J., 2015, CVPR
NR 33
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704068
DA 2019-06-15
ER

PT S
AU Liu, Q
   Wang, DL
AF Liu, Qiang
   Wang, Dilin
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Stein Variational Gradient Descent: A General Purpose Bayesian Inference
   Algorithm
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein's identity and a recently proposed kernelized Stein discrepancy, which is of independent interest.
C1 [Liu, Qiang; Wang, Dilin] Dartmouth Coll, Dept Comp Sci, Hanover, NH 03755 USA.
RP Liu, Q (reprint author), Dartmouth Coll, Dept Comp Sci, Hanover, NH 03755 USA.
EM qiang.liu@dartmouth.edu; dilin.wang.gr@dartmouth.edu
FU NSF CRII [1565796]
FX This work is supported in part by NSF CRII 1565796.
CR Challis E., 2012, NIPS
   Chwialkowski K., 2016, ARXIV160202964
   Dai B., 2016, AISTATS
   DelMoral P, 2013, MONOGR STAT APPL PRO, V126, P1
   Gal Y, 2015, ARXIV150602142
   Gershman S., 2012, ICML
   Gorham J., 2015, ADV NEURAL INFORM PR, P226
   Han S., 2016, AISTATS
   Hernandez-Lobato J. M., 2015, ICML
   Hoffman M. D., 2013, JMLR
   Hoffman MD, 2014, J MACH LEARN RES, V15, P1593
   Jaakkola Tommi S., 1999, LEARNING GRAPHICAL M, P163
   Kac M., 1959, PROBABILITY RELATED, V1
   Kucukelbir Alp, 2015, NIPS
   Kulkarni T. D., 2014, ARXIV14025715
   Lawrence C. M. B. N., 1998, NIPS
   Lawrence N. D., 2001, THESIS
   Li Y., 2015, NIPS
   Li Y., 2016, ARXIV160202311
   Liu Q., 2016, ARXIV160203253
   Maclaurin D., 2014, UAI
   Marzouk Y., 2016, ARXIV160205023
   Oates Chris J, 2017, J ROYAL STAT SOC B
   Rahimi A., 2007, ADV NEURAL INFORM PR, P1177
   Ranganath Rajesh, 2014, AISTATS
   Rezende D. J., 2015, ICML
   Robert C, 2013, MONTE CARLO STAT MET
   Smith A., 2013, SEQUENTIAL MONTE CAR
   STEIN C., 2004, IMS LECT NOTES MONOG, V46, P1
   Titsias M., 2014, P 31 INT C MACH LEAR, P1971
   Tran D., 2016, ICLR
   Tran D., 2015, NIPS
   Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5
   Welling M., 2011, ICML
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702085
DA 2019-06-15
ER

PT S
AU Liu, Y
   Chen, YL
AF Liu, Yang
   Chen, Yiling
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI A Bandit Framework for Strategic Regression
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID PREDICTION
AB We consider a learner's problem of acquiring data dynamically for training a regression model, where the training data are collected from strategic data sources. A fundamental challenge is to incentivize data holders to exert effort to improve the quality of their reported data, despite that the quality is not directly verifiable by the learner. In this work, we study a dynamic data acquisition process where data holders can contribute multiple times. Using a bandit framework, we leverage the long-term incentive of future job opportunities to incentivize high-quality contributions. We propose a Strategic Regression-Upper Confidence Bound (SRUCB) framework, a UCB-style index combined with a simple payment rule, where the index of a worker approximates the quality of his past contributions and is used by the learner to determine whether the worker receives future work. For linear regression and a certain family of non-linear regression problems, we show that SR-UCB enables an O (root logT/T)-Bayesian Nash Equilibrium (BNE) where each worker exerts a target effort level that the learner has chosen, with T being the number of data acquisition stages. The SR-UCB framework also has some other desirable properties: (1) The indexes can be updated in an online fashion (hence computation is light). (2) A slight variant, namely Private SR-UCB (PSR-UCB), is able to preserve (O (log(-1) T); O (log(-1) T))-differential privacy for workers' data, with only a small compromise on incentives (each worker exerting a target effort level is an O (log(6)T/root T)-BNE).
C1 [Liu, Yang; Chen, Yiling] Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA.
RP Liu, Y (reprint author), Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA.
EM yangl@seas.harvard.edu; yiling@seas.harvard.edu
FU NSF [CCF-1301976]
FX We acknowledge the support of NSF grant CCF-1301976.
CR Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Cai Yang, 2014, ARXIV14082539
   Chan THH, 2011, ACM T INFORM SYST SE, V14, DOI 10.1145/2043621.2043626
   Cummings Rachel, 2015, P 28 C LEARN THEOR C, P448
   Dwork C., ALGORITHMIC FDN DIFF
   Dwork Cynthia, 2006, AUTOMATA LANGUAGES P
   Ghosh Arpita, 2013, P 4 C INN THEOR COMP, P233
   Gneiting T, 2007, J AM STAT ASSOC, V102, P359, DOI 10.1198/016214506000001437
   Ho C.-J., 2014, P 15 ACM C EC COMP, P359, DOI DOI 10.1145/2600057.2602880
   Ioannidis Stratis, 2013, Web and Internet Economics. 9th International Conference, WINE 2013. Proceedings: LNCS 8289, P277, DOI 10.1007/978-3-642-45046-4_23
   Jurca R, 2007, EC'07: PROCEEDINGS OF THE EIGHTH ANNUAL CONFERENCE ON ELECTRONIC COMMERCE, P200
   Kairouz Peter, 2013, ARXIV13110776
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Lebanon Guy, M ESTIMATORS Z ESTIM
   Mansour Y, 2015, P 16 ACM C EC COMP A, P565
   Miller N, 2005, MANAGE SCI, V51, P1359, DOI 10.1287/ninsc.1050.0379
   Rakhlin A., 2011, ARXIV11095647
   Rao BLS Prakasa, 1984, J MULTIVARIATE ANAL
   Sheng V. S., 2008, P 14 ACM SIGKDD INT
   Toulis Panos, 2015, P 16 ACM EC 15, P285
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701047
DA 2019-06-15
ER

PT S
AU Lokhov, AY
AF Lokhov, Andrey Y.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Reconstructing Parameters of Spreading Models from Partial Observations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Spreading processes are often modelled as a stochastic dynamics occurring on top of a given network with edge weights corresponding to the transmission probabilities. Knowledge of veracious transmission probabilities is essential for prediction, optimization, and control of diffusion dynamics. Unfortunately, in most cases the transmission rates are unknown and need to be reconstructed from the spreading data. Moreover, in realistic settings it is impossible to monitor the state of each node at every time, and thus the data is highly incomplete. We introduce an efficient dynamic message-passing algorithm, which is able to reconstruct parameters of the spreading model given only partial information on the activation times of nodes in the network. The method is generalizable to a large class of dynamic models, as well to the case of temporal graphs.
C1 [Lokhov, Andrey Y.] Los Alamos Natl Lab, Ctr Nonlinear Studies, Los Alamos, NM 87545 USA.
   [Lokhov, Andrey Y.] Los Alamos Natl Lab, Theoret Div T4, Los Alamos, NM 87545 USA.
RP Lokhov, AY (reprint author), Los Alamos Natl Lab, Ctr Nonlinear Studies, Los Alamos, NM 87545 USA.; Lokhov, AY (reprint author), Los Alamos Natl Lab, Theoret Div T4, Los Alamos, NM 87545 USA.
EM lokhov@lanl.gov
FU LDRD Program at Los Alamos National Laboratory by the National Nuclear
   Security Administration of the U.S. Department of Energy
   [DE-AC52-06NA25396]
FX The author is grateful to M. Chertkov and T. Misiakiewicz for
   discussions and comments, and acknowledges support from the LDRD Program
   at Los Alamos National Laboratory by the National Nuclear Security
   Administration of the U.S. Department of Energy under Contract No.
   DE-AC52-06NA25396.
CR Abrahao B. D., 2013, P 19 ACM SIGKDD INT, P491
   Altarelli F, 2014, PHYS REV LETT, V112, DOI 10.1103/PhysRevLett.112.118701
   Amin Kareem, 2014, P 31 INT C MACH LEAR, V32, P1845
   Boccaletti S, 2006, PHYS REP, V424, P175, DOI 10.1016/j.physrep.2005.10.009
   Daneshmand Hadi, 2014, Proc Int Conf Mach Learn, V2014, P793
   Dobson I, 2007, CHAOS, V17, DOI 10.1063/1.2737822
   Du N., 2012, ADV NEURAL INFORM PR, P2780
   Farajtabar M., 2015, P 18 INT C ART INT S, P232
   Gomez-Rodriguez M., 2011, P 28 INT C MACH LEAR, P561
   Gripon V, 2013, IEEE INT SYMP INFO, P2488, DOI 10.1109/ISIT.2013.6620674
   Karrer B, 2010, PHYS REV E, V82, DOI 10.1103/PhysRevE.82.016101
   Kempe D., 2003, P 9 ACM SIGKDD INT C, P137, DOI DOI 10.1145/956750.956769
   Lokhov A. Y., 2016, ARXIV160808278
   Lokhov A. Y., 2016, ARXIV150906893
   Lokhov AY, 2015, PHYS REV E, V91, DOI 10.1103/PhysRevE.91.012811
   Myers S.A., 2010, ADV NEURAL INFORM PR, P1741
   Netrapalli Praneeth, 2012, Performance Evaluation Review, V40, P211, DOI 10.1145/2318857.2254783
   Nowzari C, 2016, IEEE CONTR SYST MAG, V36, P26, DOI 10.1109/MCS.2015.2495000
   O'Dea R, 2013, J R SOC INTERFACE, V10, DOI 10.1098/rsif.2013.0016
   Pastor-Satorras R, 2015, REV MOD PHYS, V87, P925, DOI 10.1103/RevModPhys.87.925
   Pouget-Abadie J., 2015, P 32 INT C MACH LEAR, P977
   Sampson Samuel F., 1969, THESIS
   Sefer E., 2015, DAT ENG ICDE 2015 IE
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700093
DA 2019-06-15
ER

PT S
AU Long, MS
   Zhu, H
   Wang, JM
   Jordan, MI
AF Long, Mingsheng
   Zhu, Han
   Wang, Jianmin
   Jordan, Michael I.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Unsupervised Domain Adaptation with Residual Transfer Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB The recent success of deep neural networks relies on massive amounts of labeled data. For a target task where labeled data is unavailable, domain adaptation can transfer a learner from a different source domain. In this paper, we propose a new approach to domain adaptation in deep networks that can jointly learn adaptive classifiers and transferable features from labeled data in the source domain and unlabeled data in the target domain. We relax a shared-classifier assumption made by previous methods and assume that the source classifier and target classifier differ by a residual function. We enable classifier adaptation by plugging several layers into deep network to explicitly learn the residual function with reference to the target classifier. We fuse features of multiple layers with tensor product and embed them into reproducing kernel Hilbert spaces to match distributions for feature adaptation. The adaptation can be achieved in most feed-forward models by extending them with new residual layers and loss functions, which can be trained efficiently via back-propagation. Empirical evidence shows that the new approach outperforms state of the art methods on standard domain adaptation benchmarks.
C1 [Long, Mingsheng; Zhu, Han; Wang, Jianmin] Tsinghua Univ, MOE, KLiss, Beijing, Peoples R China.
   [Long, Mingsheng; Zhu, Han; Wang, Jianmin] Tsinghua Univ, TNList, Beijing, Peoples R China.
   [Long, Mingsheng; Zhu, Han; Wang, Jianmin] Tsinghua Univ, Sch Software, Beijing, Peoples R China.
   [Jordan, Michael I.] Univ Calif Berkeley, Berkeley, CA 94720 USA.
RP Long, MS (reprint author), Tsinghua Univ, MOE, KLiss, Beijing, Peoples R China.; Long, MS (reprint author), Tsinghua Univ, TNList, Beijing, Peoples R China.; Long, MS (reprint author), Tsinghua Univ, Sch Software, Beijing, Peoples R China.
EM mingsheng@tsinghua.edu.cn; zhuhan10@gmail.com; jimwang@tsinghua.edu.cn;
   jordan@berkeley.edu
FU National Natural Science Foundation of China [61502265, 61325008];
   National Key R&D Program of China [2016YFB1000701, 2015BAF32B01]; TNList
   Key Project
FX This work was supported by the National Natural Science Foundation of
   China (61502265, 61325008), National Key R&D Program of China
   (2016YFB1000701, 2015BAF32B01), and TNList Key Project.
CR Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Donahue J., 2014, ICML
   Duan L., 2009, P 26 ANN INT C MACH, P289, DOI DOI 10.1145/1553374.1553411
   Duan LX, 2012, IEEE T PATTERN ANAL, V34, P465, DOI 10.1109/TPAMI.2011.114
   Ganin  Yaroslav, 2015, ICML
   Glorot X., 2011, ICML
   Gong B., 2012, CVPR
   Grandvalet Y., 2004, NIPS
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   He K., 2016, CVPR
   Hoffman J., 2014, NIPS
   Krizhevsky A., 2012, NIPS
   LIN TY, 2015, CVPR, P1449, DOI DOI 10.1109/ICCV.2015.170
   Long M., 2015, ICML
   Mansour  Yishay, 2009, COLT
   Ngiam  Jiquan, 2011, ICML
   Oquab M., 2013, CVPR
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Saenko K., 2010, ECCV
   Sun B., 2016, AAAI
   Torralba A., 2011, CVPR
   Tzeng  E., 2014, DEEP DOMAIN CONFUSIO
   Tzeng E., 2015, ICCV
   Wang X., 2014, NIPS
   Yang J., 2007, P 15 INT C MULT, P188, DOI DOI 10.1145/1291233.1291276
   Yosinski J., 2014, NIPS
   Zhang  K., 2013, ICML
NR 30
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700074
DA 2019-06-15
ER

PT S
AU Lou, XH
   Kansky, K
   Lehrach, W
   Laan, CC
   Marthi, B
   Phoenix, DS
   George, D
AF Lou, Xinghua
   Kansky, Ken
   Lehrach, Wolfgang
   Laan, C. C.
   Marthi, Bhaskara
   Phoenix, D. Scott
   George, Dileep
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Generative Shape Models: Joint Text Recognition and Segmentation with
   Very Little Training Data
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We demonstrate that a generative model for object shapes can achieve state of the art results on challenging scene text recognition tasks, and with orders of magnitude fewer training images than required for competing discriminative methods. In addition to transcribing text from challenging images, our method performs fine-grained instance segmentation of characters. We show that our model is more robust to both affine transformations and non-affine deformations compared to previous approaches.
C1 [Lou, Xinghua; Kansky, Ken; Lehrach, Wolfgang; Laan, C. C.; Marthi, Bhaskara; Phoenix, D. Scott; George, Dileep] Vicarious FPC Inc, San Francisco, CA 94587 USA.
RP Lou, XH (reprint author), Vicarious FPC Inc, San Francisco, CA 94587 USA.
EM xinghua@vicarious.com; ken@vicarious.com; wolfgang@vicarious.com;
   cc@vicarious.com; bhaskara@vicarious.com; scott@vicarious.com;
   dileep@vicarious.com
CR Bissacco A, 2013, IEEE I CONF COMP VIS, P785, DOI 10.1109/ICCV.2013.102
   Coates A, 2011, PROC INT CONF DOC, P440, DOI 10.1109/ICDAR.2011.95
   COUGHLAN JM, 2002, EUR C COMP VIS, V2352, P453
   Felzenszwalb P. F., 2008, IEEE C COMP VIS PATT, V1, P1, DOI DOI 10.1109/CVPR.2008.4587597
   Jaderberg M, 2014, ARXIV14062227
   Jaderberg M., 2014, ARXIV14125903
   Karatzas D, 2013, PROC INT CONF DOC, P1484, DOI 10.1109/ICDAR.2013.221
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee CY, 2014, PROC CVPR IEEE, P4050, DOI 10.1109/CVPR.2014.516
   Mishra A, 2012, PROC CVPR IEEE, P2687, DOI 10.1109/CVPR.2012.6247990
   Neumann L, 2013, IEEE I CONF COMP VIS, P97, DOI 10.1109/ICCV.2013.19
   Novikova T, 2012, LECT NOTES COMPUT SC, V7577, P752, DOI 10.1007/978-3-642-33783-3_54
   Shi CZ, 2013, PROC CVPR IEEE, P2961, DOI 10.1109/CVPR.2013.381
   Tsochantaridis I., 2004, P 21 INT C MACH LEAR, P104, DOI DOI 10.1145/1015330.1015341
   Wang K, 2010, LECT NOTES COMPUT SC, V6311, P591, DOI 10.1007/978-3-642-15549-9_43
   Weinman JJ, 2009, IEEE T PATTERN ANAL, V31, P1733, DOI 10.1109/TPAMI.2009.38
   Yao C, 2014, PROC CVPR IEEE, P4042, DOI 10.1109/CVPR.2014.515
   Ye QX, 2015, IEEE T PATTERN ANAL, V37, P1480, DOI 10.1109/TPAMI.2014.2366765
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700035
DA 2019-06-15
ER

PT S
AU Lu, JS
   Yang, JW
   Batra, D
   Parikh, D
AF Lu, Jiasen
   Yang, Jianwei
   Batra, Dhruv
   Parikh, Devi
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Hierarchical Question-Image Co-Attention for Visual Question Answering
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB A number of recent works have proposed attention models for Visual Question Answering (VQA) that generate spatial maps highlighting image regions relevant to answering the question. In this paper, we argue that in addition to modeling "where to look" or visual attention, it is equally important to model "what words to listen to" or question attention. We present a novel co-attention model for VQA that jointly reasons about image and question attention. In addition, our model reasons about the question (and consequently the image via the co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional convolution neural networks (CNN). Our model improves the state-of-the-art on the VQA dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the COCO-QA dataset. By using ResNet, the performance is further improved to 62.1% for VQA and 65.4% for COCO-QA.
C1 [Lu, Jiasen; Yang, Jianwei; Batra, Dhruv; Parikh, Devi] Virginia Tech, Blacksburg, VA 24061 USA.
   [Batra, Dhruv; Parikh, Devi] Georgia Inst Technol, Atlanta, GA 30332 USA.
RP Lu, JS (reprint author), Virginia Tech, Blacksburg, VA 24061 USA.
EM jiasenlu@vt.edu; jw2yang@vt.edu; dbatra@vt.edu; parikh@vt.edu
FU NSF CAREER; ONR; ONR [N00014-14-1-0679]; Sloan Fellowship; ARO; Paul G.
   Allen Family Foundation; ICTAS Junior Faculty awards; Google; AWS in
   Education Research grant
FX This work was funded in part by NSF CAREER awards to DP and DB, an ONR
   YIP award to DP, ONR Grant N00014-14-1-0679 to DB, a Sloan Fellowship to
   DP, ARO YIP awards to DB and DP, a Allen Distinguished Investigator
   award to DP from the Paul G. Allen Family Foundation, ICTAS Junior
   Faculty awards to DB and DP, Google Faculty Research Awards to DP and
   DB, AWS in Education Research grant to DB, and NVIDIA GPU donations to
   DB. The views and conclusions contained herein are those of the authors
   and should not be interpreted as necessarily representing the official
   policies or endorsements, either expressed or implied, of the U.S.
   Government or any sponsor.
CR Andreas J., 2016, CVPR
   Antol S., 2015, ICCV
   Bandanau D., 2015, ICLR
   Collobert R, 2011, NIPS WORKSH
   Das Abhishek, 2016, ARXIV160603556
   dos Santos Cicero, 2016, ARXIV160203609
   Gao H., 2015, NIPS
   He K., 2016, CVPR
   Hermann K, 2015, NIPS
   Hu B., 2014, NIPS
   Ilievski I, 2016, ARXIV160401485
   Krishna R., 2016, ARXIV160207332
   Lin T.-Y., 2014, ECCV
   Ma L., 2016, AAAI
   Malinowski M., 2015, ICCV
   Ren M., 2015, NIPS
   Rocktaschel Tim, 2016, ICLR
   Shih K. J, 2016, CVPR
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Szegedy C., 2015, CVPR
   Xiong C, 2016, ICML
   Xu H., 2015, ARXIV151105234
   Yang Z, 2016, CVPR
   Yin Wenpeng, 2016, ACL
   Zhang P., 2015, ARXIV151105099
   Zhu Y., 2016, CVPR
   Zitnick C Lawrence, 2016, AI MAGAZINE, V37
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701059
DA 2019-06-15
ER

PT S
AU Lu, J
   Liang, GN
   Sun, JW
   Bi, JB
AF Lu, Jin
   Liang, Guannan
   Sun, Jiangwen
   Bi, Jinbo
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI A Sparse Interactive Model for Matrix Completion with Side Information
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID LOW-RANK MATRIX; THRESHOLDING ALGORITHM
AB Matrix completion methods can benefit from side information besides the partially observed matrix. The use of side features that describe the row and column entities of a matrix has been shown to reduce the sample complexity for completing the matrix. We propose a novel sparse formulation that explicitly models the interaction between the row and column side features to approximate the matrix entries. Unlike early methods, this model does not require the low rank condition on the model parameter matrix. We prove that when the side features span the latent feature space of the matrix to be recovered, the number of observed entries needed for an exact recovery is O(log N) where N is the size of the matrix. If the side features are corrupted latent features of the matrix with a small perturbation, our method can achieve an epsilon-recovery with O(log N) sample complexity. If side information is useless, our method maintains a O(N-3/2) sampling rate similar to classic methods. An efficient linearized Lagrangian algorithm is developed with a convergence guarantee. Empirical results show that our approach outperforms three state-of-the-art methods both in simulations and on real world datasets.
C1 [Lu, Jin; Liang, Guannan; Sun, Jiangwen; Bi, Jinbo] Univ Connecticut, Storrs, CT 06269 USA.
RP Lu, J (reprint author), Univ Connecticut, Storrs, CT 06269 USA.
EM jin.lu@uconn.edu; guannan.liang@uconn.edu; jiangwen.sun@uconn.edu;
   jinbo.bi@uconn.edu
FU NSF [IIS-1320586, DBI-1356655, CCF-1514357]; NIH [R01DA037349]
FX Jinbo Bi and her students Jin Lu, Guannan Liang and Jiangwen Sun were
   supported by NSF grants IIS-1320586, DBI-1356655, and CCF-1514357 and
   NIH R01DA037349.
CR Abernethy J, 2009, J MACH LEARN RES, V10, P803
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061
   Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Chen P, 2004, IEEE T PATTERN ANAL, V26, P1051, DOI 10.1109/TPAMI.2004.52
   Chen TQ, 2012, J MACH LEARN RES, V13, P3619
   Chiang K.-Y., 2015, NIPS, P3429
   Chiang Kai-Yang, 2016, P 33 INT C MACH LEAR, P2291
   Daemen A, 2013, GENOME BIOL, V14, DOI 10.1186/gb-2013-14-10-r110
   Fang EX, 2015, MATH PROGRAM COMPUT, V7, P149, DOI 10.1007/s12532-015-0078-2
   Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872
   JAIN P, 2013, ARXIV13060626
   Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205
   Lin Z., 2010, MATH PROGRAMMING
   Liu GC, 2016, IEEE T SIGNAL PROCES, V64, P5623, DOI 10.1109/TSP.2016.2586753
   Menon A. K., 2011, P 17 ACM SIGKDD INT, P141, DOI DOI 10.1145/2020408.2020436
   Natarajan N, 2014, BIOINFORMATICS, V30, P60, DOI 10.1093/bioinformatics/btu269
   Ning Xia, 2012, P 6 ACM C REC SYST, P155, DOI DOI 10.1145/2365952.2365983
   Recht B, 2011, J MACH LEARN RES, V12, P3413
   Rennie J. D. M., 2005, P 22 INT C MACH LEAR, P713, DOI [10.1145/1102351.1102441, DOI 10.1145/1102351.1102441]
   Shamir O, 2014, J MACH LEARN RES, V15, P3401
   Shi W, 2015, IEEE T SIGNAL PROCES, V63, P6013, DOI 10.1109/TSP.2015.2461520
   Sindhwani Vikas, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P1055, DOI 10.1109/ICDM.2010.164
   Srebro N., 2005, RANK TRACE NORM MAX, P545
   Tseng P, 2001, J OPTIMIZ THEORY APP, V109, P475, DOI 10.1023/A:1017501703105
   Weng ZY, 2012, INT CONF ACOUST SPEE, P2697, DOI 10.1109/ICASSP.2012.6288473
   Xu M., 2013, ADV NEURAL INFORM PR, P2301
   Yang J., 2013, MATH COMPUT, V82
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702012
DA 2019-06-15
ER

PT S
AU Luo, HP
   Agarwal, A
   Cesa-Bianchi, N
   Langford, J
AF Luo, Haipeng
   Agarwal, Alekh
   Cesa-Bianchi, Nicolo
   Langford, John
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Efficient Second Order Online Learning by Sketching
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We propose Sketched Online Newton (SON), an online second order learning algorithm that enjoys substantially improved regret guarantees for ill-conditioned data. SON is an enhanced version of the Online Newton Step, which, via sketching techniques enjoys a running time linear in the dimension and sketch size. We further develop sparse forms of the sketching methods (such as Oja's rule), making the computation linear in the sparsity of features. Together, the algorithm eliminates all computational obstacles in previous second order online learning approaches.
C1 [Luo, Haipeng] Princeton Univ, Princeton, NJ 08544 USA.
   [Agarwal, Alekh; Langford, John] Microsoft Res, New York, NY USA.
   [Cesa-Bianchi, Nicolo] Univ Milan, Milan, Italy.
RP Luo, HP (reprint author), Princeton Univ, Princeton, NJ 08544 USA.
EM haipengl@cs.princeton.edu; alekha@microsoft.com;
   nicolo.cesa-bianchi@unimi.it; jcl@microsoft.com
CR Achlioptas D, 2003, J COMPUT SYST SCI, V66, P671, DOI 10.1016/S0022-0000(03)00025-4
   Balsubramani A., 2013, NIPS
   Byrd RH, 2016, SIAM J OPTIMIZ, V26, P1008, DOI 10.1137/140954362
   Cesa-Bianchi N, 2005, SIAM J COMPUT, V34, P640, DOI 10.1137/S0097539703432542
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Erdogdu M. A., 2015, NIPS
   Frank M., 1956, NAVAL RES LOGIST QUA, V3, P95, DOI [10.1002/nav.3800030109, DOI 10.1002/NAV.3800030109]
   Gao W., 2013, ICML
   Garber D., 2015, ICML
   Garber D, 2016, SIAM J OPTIMIZ, V26, P1493, DOI 10.1137/140985366
   Ghashami M, 2016, SIAM J COMPUT, V45, P1762, DOI 10.1137/15M1009718
   Ghashami Mina, 2016, KDD
   Gonen A., 2015, ARXIV150602649
   Gonen A., 2016, ICML
   Hardt  M., 2014, NIPS
   Hazan E., 2012, ICML
   Hazan E, 2007, MACH LEARN, V69, P169, DOI 10.1007/s10994-007-5016-8
   Indyk  P., 1998, STOC
   Jaggi  M., 2013, ICML
   Kane DM, 2014, J ACM, V61, DOI 10.1145/2559902
   Li C. - L., 2015, ARXIV150601490
   Liberty Edo, 2013, KDD
   LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116
   McMahan H. B., 2010, COLT
   Mokhtari A, 2015, J MACH LEARN RES, V16, P3151
   Moritz P., 2016, AISTATS
   OJA E, 1985, J MATH ANAL APPL, V106, P69, DOI 10.1016/0022-247X(85)90131-3
   OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687
   Orabona F., 2015, ALT
   Orabona F, 2015, MACH LEARN, V99, P411, DOI 10.1007/s10994-014-5474-8
   Pilanci M., 2015, ARXIV150502250
   Ross S., 2013, UAI
   Schraudolph N. N., 2007, AISTATS
   Sohl- Dickstein J., 2014, ICML
   Woodruff D. P., 2014, THEORETICAL COMPUTER, V10, P1
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701064
DA 2019-06-15
ER

PT S
AU Luo, WJ
   Li, YJ
   Urtasun, R
   Zemel, R
AF Luo, Wenjie
   Li, Yujia
   Urtasun, Raquel
   Zemel, Richard
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Understanding the Effective Receptive Field in Deep Convolutional Neural
   Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We study characteristics of receptive fields of units in deep convolutional networks. The receptive field size is a crucial issue in many visual tasks, as the output must respond to large enough areas in the image to capture information about large objects. We introduce the notion of an effective receptive field, and show that it both has a Gaussian distribution and only occupies a fraction of the full theoretical receptive field. We analyze the effective receptive field in several architecture designs, and the effect of nonlinear activations, dropout, sub-sampling and skip connections on it. This leads to suggestions for ways to address its tendency to be too small.
C1 [Luo, Wenjie; Li, Yujia; Urtasun, Raquel; Zemel, Richard] Univ Toronto, Dept Comp Sci, Toronto, ON, Canada.
RP Luo, WJ (reprint author), Univ Toronto, Dept Comp Sci, Toronto, ON, Canada.
EM wenjie@cs.toronto.edu; yujiali@cs.toronto.edu; urtasun@cs.toronto.edu;
   zemel@cs.toronto.edu
CR Badrinarayanan V., 2015, ARXIV150507293
   Chen X., 2015, NIPS
   Eger S, 2013, J INTEGER SEQ, V16
   Erhan D, 2009, VISUALIZING HIGHER L, P1341
   Glorot X., 2010, JLMR P TRACK, P249, DOI DOI 10.1.1/207.2059
   He K., 2016, ARXIV160305027
   He  K., 2015, ARXIV151203385
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Kanwisher N, 1997, J NEUROSCI, V17, P4302
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Larochelle H., 2010, P ADV NEUR INF PROC, P1243
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lovsz L, 2003, DISCRETE MATH ELEMEN
   Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155
   Mordvintsev Alexander, INCEPTIONISM GOING D
   Neuschel Thorsten, 2014, J INTEGER SEQUENCES, V17, P3
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Ungerleider Leslie G., 1994, Current Opinion in Neurobiology, V4, P157, DOI 10.1016/0959-4388(94)90066-3
   Xu K, 2015, ARXIV150203044
   Yu F., 2015, ARXIV151107122
   Zeiler M. D., 2014, EUR C COMP VIS, P818, DOI DOI 10.1007/978-3-319-10590-1_53
   Zhou B., 2014, CORR, V1412, P6856
NR 23
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701060
DA 2019-06-15
ER

PT S
AU Lynn, CW
   Lee, DD
AF Lynn, Christopher W.
   Lee, Daniel D.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Maximizing Influence in an Ising Network: A Mean-Field Optimal Solution
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID STATISTICAL-MECHANICS
AB Influence maximization in social networks has typically been studied in the context of contagion models and irreversible processes. In this paper, we consider an alternate model that treats individual opinions as spins in an Ising system at dynamic equilibrium. We formalize the Ising influence maximization problem, which has a natural physical interpretation as maximizing the magnetization given a budget of external magnetic field. Under the mean-field (MF) approximation, we present a gradient ascent algorithm that uses the susceptibility to efficiently calculate local maxima of the magnetization, and we develop a number of sufficient conditions for when the MF magnetization is concave and our algorithm converges to a global optimum. We apply our algorithm on random and real-world networks, demonstrating, remarkably, that the MF optimal external fields (i.e., the external fields which maximize the MF magnetization) shift from focusing on high-degree individuals at high temperatures to focusing on low-degree individuals at low temperatures. We also establish a number of novel results about the structure of steady-states in the ferromagnetic MF Ising model on general graph topologies, which are of independent interest.
C1 [Lynn, Christopher W.] Univ Penn, Dept Phys & Astron, Philadelphia, PA 19104 USA.
   [Lee, Daniel D.] Univ Penn, Dept Elect & Syst Engn, Philadelphia, PA 19104 USA.
RP Lynn, CW (reprint author), Univ Penn, Dept Phys & Astron, Philadelphia, PA 19104 USA.
EM chlynn@sas.upenn.edu; ddlee@seas.upenn.edu
FU U.S. National Science Foundation; Air Force Office of Scientific
   Research; Department of Transportation
FX We thank Michael Kearns and Eric Horsley for enlightening discussions,
   and we acknowledge support from the U.S. National Science Foundation,
   the Air Force Office of Scientific Research, and the Department of
   Transportation.
CR BLUME LE, 1993, GAME ECON BEHAV, V5, P387, DOI 10.1006/game.1993.1023
   Castellano C, 2009, REV MOD PHYS, V81, P591, DOI 10.1103/RevModPhys.81.591
   De A., 2015, ARXIV150605474
   Domingos P., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P57
   GALAM S, 1991, EUR J SOC PSYCHOL, V21, P49, DOI 10.1002/ejsp.2420210105
   Galam S, 2008, INT J MOD PHYS C, V19, P409, DOI 10.1142/S0129183108012297
   Goyal S., 2014, GEB
   ISENBERG DJ, 1986, J PERS SOC PSYCHOL, V50, P1141, DOI 10.1037/0022-3514.50.6.1141
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Kempe D., 2003, P 9 ACM SIGKDD INT C, P137, DOI DOI 10.1145/956750.956769
   Kindermann R., 1980, MARKOV RANDOM FIELDS
   Leskovec J., 2014, SNAP DATASETS STANFO
   Mas M, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000959
   MCKELVEY RD, 1995, GAME ECON BEHAV, V10, P6, DOI 10.1006/game.1995.1023
   Montanari A., 2010, PNAS, V107
   Mossel E, 2007, ACM S THEORY COMPUT, P128
   Moussaid M, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0084592
   Newman M., 2001, PNAS, V98
   Nishimori H, 1999, PHYS REV E, V60, P132, DOI 10.1103/PhysRevE.60.132
   Opper  M., 2001, ADV MEAN FIELD METHO
   Richardson M., 2002, P 8 ACM SIGKDD INT C, V2, P61, DOI DOI 10.1145/775047.775057
   Rodriguez M. G., 2012, ICML
   Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251
   Sznajd-Weron K., 2000, INT J MODERN PHYS C, V11
   Tanaka T, 1998, PHYS REV E, V58, P2302, DOI 10.1103/PhysRevE.58.2302
   Teboulle M., 2010, 1 ORDER ALGORITHMS C
   Yedidia JS, 2001, NEU INF PRO, P21
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700083
DA 2019-06-15
ER

PT S
AU Magdon-Ismail, M
   Boutsidis, C
AF Magdon-Ismail, Malik
   Boutsidis, Christos
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Optimal Sparse Linear Encoders and Sparse PCA
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID PRINCIPAL COMPONENT ANALYSIS; POWER METHOD; FIT
AB Principal components analysis (PCA) is the optimal linear encoder of data. Sparse linear encoders (e.g., sparse PCA) produce more interpretable features that can promote better generalization. (i) Given a level of sparsity, what is the best approximation to PCA? (ii) Are there efficient algorithms which can achieve this optimal combinatorial tradeoff? We answer both questions by providing the first polynomial-time algorithms to construct optimal sparse linear auto-encoders; additionally, we demonstrate the performance of our algorithms on real data.
C1 [Magdon-Ismail, Malik] Rensselaer Polytech Inst, Troy, NY 12211 USA.
RP Magdon-Ismail, M (reprint author), Rensselaer Polytech Inst, Troy, NY 12211 USA.
EM magdon@cs.rpi.edu; christos.boutsidis@gmail.com
FU NSF:IIS [1124827]; Army Research Laboratory [W911NF-09-2-0053]
FX Magdon-Ismail was partially supported by NSF:IIS 1124827 and by the Army
   Research Laboratory under Cooperative Agreement W911NF-09-2-0053 (the
   ARL-NSCTA). The views and conclusions contained in this document are
   those of the authors and should not be interpreted as representing the
   official policies, either expressed or implied, of the Army Research
   Laboratory or the U.S. Government. The U.S. Government is authorized to
   reproduce and distribute reprints for Government purposes
   notwithstanding any copyright notation here on.
CR Asteris M., 2011, P ISIT
   Asteris M., 2014, P ICML
   BALDI P, 1989, NEURAL NETWORKS, V2, P53, DOI 10.1016/0893-6080(89)90014-2
   BOURLARD H, 1988, BIOL CYBERN, V59, P291, DOI 10.1007/BF00332918
   Boutsidis C., 2014, SIAM J COMPUTING, V43
   CADIMA J, 1995, J APPL STAT, V22, P203, DOI 10.1080/757584614
   Cottrell G., 1988, P SPIE, V1001
   d'Aspremont A, 2008, J MACH LEARN RES, V9, P1269
   d'Aspremont A, 2007, SIAM REV, V49, P434, DOI 10.1137/050645506
   Jolliffe IT, 2003, J COMPUT GRAPH STAT, V12, P531, DOI 10.1198/1061860032148
   Journee M, 2010, J MACH LEARN RES, V11, P517
   KAISER HF, 1958, PSYCHOMETRIKA, V23, P187, DOI 10.1007/BF02289233
   KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P1, DOI 10.1007/BF02289565
   Mackey L., 2009, ADV NEURAL INFORM PR, V21, P1017
   Magdon-Ismail M., 2015, ARXIV150205675
   Makhzani A., 2014, ICLR
   Moghaddam B., 2006, ICML
   OJA E, 1991, ARTIFICIAL NEURAL NETWORKS, VOLS 1 AND 2, P737
   OJA E, 1992, NEURAL NETWORKS, V5, P927, DOI 10.1016/S0893-6080(05)80089-9
   Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720
   SAMMON JW, 1969, IEEE T COMPUT, VC 18, P401, DOI 10.1109/T-C.1969.222678
   Shen HP, 2008, J MULTIVARIATE ANAL, V99, P1015, DOI 10.1016/j.jmva.2007.06.007
   Yuan XT, 2013, J MACH LEARN RES, V14, P899
   Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701109
DA 2019-06-15
ER

PT S
AU Magliacane, S
   Claassen, T
   Mooij, JM
AF Magliacane, Sara
   Claassen, Tom
   Mooij, Joris M.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Ancestral Causal Inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID DIRECTED ACYCLIC GRAPHS; DISCOVERY; LATENT
AB Constraint-based causal discovery from limited data is a notoriously difficult challenge due to the many borderline independence test decisions. Several approaches to improve the reliability of the predictions by exploiting redundancy in the independence information have been proposed recently. Though promising, existing approaches can still be greatly improved in terms of accuracy and scalability. We present a novel method that reduces the combinatorial explosion of the search space by using a more coarse-grained representation of causal information, drastically reducing computation time. Additionally, we propose a method to score causal predictions based on their confidence. Crucially, our implementation also allows one to easily combine observational and interventional data and to incorporate various types of available background knowledge. We prove soundness and asymptotic consistency of our method and demonstrate that it can outperform the state-ofthe-art on synthetic data, achieving a speedup of several orders of magnitude. We illustrate its practical feasibility by applying it to a challenging protein data set.
C1 [Magliacane, Sara] Vrije Univ Amsterdam, Amsterdam, Netherlands.
   [Magliacane, Sara; Mooij, Joris M.] Univ Amsterdam, Amsterdam, Netherlands.
   [Claassen, Tom] Radboud Univ Nijmegen, Nijmegen, Netherlands.
RP Magliacane, S (reprint author), Vrije Univ Amsterdam, Amsterdam, Netherlands.; Magliacane, S (reprint author), Univ Amsterdam, Amsterdam, Netherlands.
EM sara.magliacane@gmail.com; tomc@cs.ru.nl; j.m.mooij@uva.nl
FU NWO, the Netherlands Organization for Scientific Research (VIDI grant)
   [639.072.410]; Dutch programme COMMIT/under the Data2Semantics project;
   NWO [612.001.202]; EU-FP7 grant [603016]
FX SM and JMM were supported by NWO, the Netherlands Organization for
   Scientific Research (VIDI grant 639.072.410). SM was also supported by
   the Dutch programme COMMIT/under the Data2Semantics project. TC was
   supported by NWO grant 612.001.202 (MoCoCaDi), and EU-FP7 grant
   agreement n. 603016 (MATRICS). We also thank Sofia Triantafillou for her
   feedback, especially for pointing out the correct way to read ancestral
   relations from a PAG.
CR Borboudakis G., 2012, P 29 INT C MACH LEAR, P1799
   Claassen T., 2012, P UAI C, P207
   Claassen T., 2011, UAI, P135
   Colombo D, 2012, ANN STAT, V40, P294, DOI 10.1214/11-AOS940
   Eaton D., 2007, INT C ART INT STAT, P107
   Gebser  M., 2014, TECHNICAL REPORT
   Gelfond M, 2008, FOUND ARTIF INTELL, P285, DOI 10.1016/S1574-6526(07)03007-6
   Hyttinen A., 2014, P 30 C UNC ART INT U, P340
   Kalisch M, 2007, J MACH LEARN RES, V8, P613
   Kalisch M, 2012, J STAT SOFTW, V47, P1
   KLEITMAN DJ, 1975, T AM MATH SOC, V205, P205, DOI 10.2307/1997200
   Lifschitz V., 2008, P 23 AAAI C ART INT, V3, P1594
   Margaritis D, 2009, COMPUT INTELL-US, V25, P367, DOI 10.1111/j.1467-8640.2009.00347.x
   Markowetz F, 2005, AISTATS, P214
   Mooij JM, 2013, P 29 ANN C UNC ART I, P431
   Pearl J, 2009, CAUSALITY MODELS REA
   Peters J., 2015, J ROYAL STAT SOC B, V8, P947
   Ramsey J., 2006, P 22 C UNC ART INT, P401
   Rothenhausler D, 2015, ADV NEURAL INFORM PR, P1513
   Roumpelaki A., 2016, CAUS FDN APPL WORKSH
   Sachs K, 2005, SCIENCE, V308, P523, DOI 10.1126/science.1105809
   Spirtes P., 2001, AISTATS, P121
   Spirtes P., 2000, CAUSATION PREDICTION
   Tian J, 2001, P 17 C UNC ART INT, P512
   Triantafillou S, 2015, J MACH LEARN RES, V16, P2147
   Zhang JJ, 2008, ARTIF INTELL, V172, P1873, DOI 10.1016/j.artint.2008.08.001
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702013
DA 2019-06-15
ER

PT S
AU Mairal, J
AF Mairal, Julien
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI End-to-End Kernel Learning with Supervised Convolutional Kernel Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB In this paper, we introduce a new image representation based on a multilayer kernel machine. Unlike traditional kernel methods where data representation is decoupled from the prediction task, we learn how to shape the kernel with supervision. We proceed by first proposing improvements of the recently-introduced convolutional kernel networks (CKNs) in the context of unsupervised learning; then, we derive backpropagation rules to take advantage of labeled training data. The resulting model is a new type of convolutional neural network, where optimizing the filters at each layer is equivalent to learning a linear subspace in a reproducing kernel Hilbert space (RKHS). We show that our method achieves reasonably competitive performance for image classification on some standard "deep learning" datasets such as CIFAR-10 and SVHN, and also for image super-resolution, demonstrating the applicability of our approach to a large variety of image-related tasks.
C1 [Mairal, Julien] INRIA, Rennes, France.
RP Mairal, J (reprint author), INRIA, Rennes, France.
EM julien.mairal@inria.fr
FU ANR (MACARON project) [ANR-14-CE23-0003-01]
FX This work was supported by ANR (MACARON project ANR-14-CE23-0003-01).
CR Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1
   Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bo L., 2011, CVPR
   Bottou L, 1998, LECT NOTES COMPUTER, V1524
   Broomhead D. S., 1988, TECHNICAL REPORT
   Cho Y., 2009, ADV NIPS
   Damianou A., 2013, P AISTATS
   Dong C., 2014, P ECCV
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Goodfellow I., 2013, P ICML
   He K., 2016, P CVPR
   Ioffe Sergey, 2015, P ICML
   Krizhevsky A., 2012, ADV NIPS
   Lee C. - Y., 2015, P AISTATS
   Lee C. - Y., 2016, P AISTATS
   Lin H., 2015, ADV NIPS
   Lin M., 2013, P ICLR
   Mairal J., 2014, ADV NIPS
   Netzer Y., 2011, NIPS WORKSH DEEP LEA
   Paulin M., 2015, P ICCV
   Rahimi A., 2007, ADV NIPS
   Scholkopf B, 2002, LEARNING KERNELS SUP
   Simonyan K., 2015, P ICLR
   Sonnenburg S, 2006, J MACH LEARN RES, V7, P1531
   Sydorov V., 2014, P CVPR
   Timofte R., 2013, P ICCV
   Wang Z., 2015, P ICCV
   Williams C., 2001, ADV NIPS
   Zeiler M. D., 2013, P ICLR
   Zeyde R, 2010, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang K., 2008, P ICML
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701041
DA 2019-06-15
ER

PT S
AU Maitin-Shepard, J
   Jain, V
   Januszewski, M
   Li, P
   Abbeel, P
AF Maitin-Shepard, Jeremy
   Jain, Viren
   Januszewski, Michal
   Li, Peter
   Abbeel, Pieter
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Combinatorial Energy Learning for Image Segmentation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID SCANNING-ELECTRON-MICROSCOPY; RECONSTRUCTION
AB We introduce a new machine learning approach for image segmentation that uses a neural network to model the conditional energy of a segmentation given an image. Our approach, combinatorial energy learning for image segmentation (CELIS) places a particular emphasis on modeling the inherent combinatorial nature of dense image segmentation problems. We propose efficient algorithms for learning deep neural networks to model the energy function, and for local optimization of this energy in the space of supervoxel agglomerations. We extensively evaluate our method on a publicly available 3-D microscopy dataset with 25 billion voxels of ground truth data. On an 11 billion voxel test set, we find that our method improves volumetric reconstruction accuracy by more than 20% as compared to two state-of-the-art baseline methods: graph-based segmentation of the output of a 3-D convolutional neural network trained to predict boundaries, as well as a random forest classifier trained to agglomerate supervoxels that were generated by a 3-D convolutional neural network.
C1 [Maitin-Shepard, Jeremy; Abbeel, Pieter] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Maitin-Shepard, Jeremy; Jain, Viren; Januszewski, Michal; Li, Peter] Google, Mountain View, CA 94043 USA.
RP Maitin-Shepard, J (reprint author), Univ Calif Berkeley, Berkeley, CA 94720 USA.; Maitin-Shepard, J (reprint author), Google, Mountain View, CA 94043 USA.
EM jbms@google.com; viren@google.com; mjanusz@google.com; phli@google.com;
   pabbeel@cs.berkeley.edu
FU National Science Foundation [1118055]
FX This material is based upon work supported by the National Science
   Foundation under Grant No. 1118055.
CR Andres B, 2008, LECT NOTES COMPUT SC, V5096, P142, DOI 10.1007/978-3-540-69321-5_15
   Andres B, 2012, LECT NOTES COMPUT SC, V7574, P778, DOI 10.1007/978-3-642-33712-3_56
   Bogovic John A, 2013, ARXIV13126159
   Briggman K., 2009, ADV NEURAL INFORM PR, P1865
   Briggman KL, 2006, CURR OPIN NEUROBIOL, V16, P562, DOI 10.1016/j.conb.2006.08.010
   Calonder M., 2010, COMPUT VIS ECCV, V2010, P778, DOI DOI 10.1007/978-3-642-15561-1_56
   Chen L., 2014, CORR
   Chen Liang Chieh, 2015, P ICML
   Dan C. C., 2012, NIPS, P2852
   Dean J., 2012, ADV NEURAL INFORM PR, P1223
   Denk W, 2004, PLOS BIOL, V2, P1900, DOI 10.1371/journal.pbio.0020329
   Denk W, 2012, NAT REV NEUROSCI, V13, P351, DOI 10.1038/nrn3169
   Duffield N, 2007, J ACM, V54, DOI 10.1145/1314690.1314696
   Funke J, 2012, PROC CVPR IEEE, P1004, DOI 10.1109/CVPR.2012.6247777
   Hayworth KJ, 2006, MICROSC MICROANAL, V12, P86, DOI DOI 10.1017/S1431927606066268
   Helmstaedter M, 2013, NATURE, V500, P168, DOI 10.1038/nature12346
   Helmstaedter M, 2011, NAT NEUROSCI, V14, P1081, DOI 10.1038/nn.2868
   Helmstaedter M, 2008, CURR OPIN NEUROBIOL, V18, P633, DOI 10.1016/j.conb.2009.03.005
   Jain Viren, 2011, ADV NEURAL INFORM PR, V2
   Knott G, 2008, J NEUROSCI, V28, P2959, DOI 10.1523/JNEUROSCI.3189-07.2008
   Meila M, 2007, J MULTIVARIATE ANAL, V98, P873, DOI 10.1016/j.jmva.2006.11.013
   Nunez-Iglesias J, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0071715
   RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239
   Schwing A. G., 2015, ARXIV150302351
   Sommer C, 2011, I S BIOMED IMAGING, P230, DOI 10.1109/ISBI.2011.5872394
   Takemura SY, 2015, P NATL ACAD SCI USA, V112, P13711, DOI 10.1073/pnas.1509820112
   Takemura S, 2013, NATURE, V500, P175, DOI 10.1038/nature12450
   Turaga SC, 2010, NEURAL COMPUT, V22, P511, DOI 10.1162/neco.2009.10-08-881
   Vazquez-Reina A, 2011, IEEE I CONF COMP VIS, P177, DOI 10.1109/ICCV.2011.6126240
   White J. G., 1986, PHILOS T R SOC B, V1165, P1
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
   Zlateski Aleksandar, 2011, THESIS
   Zlateski Aleksandar, 2015, CORR
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973705009
DA 2019-06-15
ER

PT S
AU Malkomes, G
   Schaff, C
   Garnett, R
AF Malkomes, Gustavo
   Schaff, Chip
   Garnett, Roman
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Bayesian optimization for automated model selection
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Despite the success of kernel-based nonparametric methods, kernel selection still requires considerable expertise, and is often described as a "black art." We present a sophisticated method for automatically searching for an appropriate kernel from an infinite space of potential choices. Previous efforts in this direction have focused on traversing a kernel grammar, only examining the data via computation of marginal likelihood. Our proposed search method is based on Bayesian optimization in model space, where we reason about model evidence as a function to be maximized. We explicitly reason about the data distribution and how it induces similarity between potential model choices in terms of the explanations they can offer for observed data. In this light, we construct a novel kernel between models to explain a given dataset. Our method is capable of finding a model that explains a given dataset well without any human assistance, often with fewer computations of model evidence than previous approaches, a claim we demonstrate empirically.
C1 [Malkomes, Gustavo; Schaff, Chip; Garnett, Roman] Washington Univ, Dept Comp Sci & Engn, St Louis, MO 63130 USA.
RP Malkomes, G (reprint author), Washington Univ, Dept Comp Sci & Engn, St Louis, MO 63130 USA.
EM luizgustavo@wustl.edu; cbschaff@wustl.edu; garnett@wustl.edu
FU National Science Foundation (NSF) [IIA-1355406]; Brazilian Federal
   Agency for Support and Evaluation of Graduate Education (CAPES)
FX This material is based upon work supported by the National Science
   Foundation (NSF) under award number IIA-1355406. Additionally, GM
   acknowledges support from the Brazilian Federal Agency for Support and
   Evaluation of Graduate Education (CAPES).
CR Bach F. R., 2008, C NEUR INF PROC SYST
   Bergstra James S., 2011, C NEUR INF PROC SYST
   Brochu E., 2010, ARXIV10122599
   Damianou A. C., 2013, INT C ART INT STAT A
   Duvenaud David, 2013, INT C MACH LEARN ICM
   Gardner Jacob R., 2015, C NEUR INF PROC SYST
   Gonen M, 2011, J MACH LEARN RES, V12, P2211
   Grosse Roger, 2012, C UNC ART INT UAI
   Hinton G. E., 2008, C NEUR INF PROC SYST
   Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147
   Kuha J, 2004, SOCIOL METHOD RES, V33, P188, DOI 10.1177/0049124103262065
   Lazaro-Gredilla M, 2010, J MACH LEARN RES, V11, P1865
   MacKay D. J. C., 1998, Neural Networks and Machine Learning. Proceedings, P133
   Murphy KP, 2012, MACHINE LEARNING PRO
   Raftery AE, 1996, BIOMETRIKA, V83, P251, DOI 10.1093/biomet/83.2.251
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136
   Snoek Jasper, 2012, C NEUR INF PROC SYST
   Wilson A., 2014, C NEUR INF PROC SYST
   Wilson A. G., 2013, INT C MACH LEARN ICM
   Wilson A. G., 2012, INT C MACH LEARN ICM
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703102
DA 2019-06-15
ER

PT S
AU Mankowitz, DJ
   Mann, TA
   Mannor, S
AF Mankowitz, Daniel J.
   Mann, Timothy A.
   Mannor, Shie
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Adaptive Skills Adaptive Partitions (ASAP)
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We introduce the Adaptive Skills, Adaptive Partitions (ASAP) framework that (1) learns skills (i.e., temporally extended actions or options) as well as (2) where to apply them. We believe that both (1) and (2) are necessary for a truly general skill learning framework, which is a key building block needed to scale up to lifelong learning agents. The ASAP framework can also solve related new tasks simply by adapting where it applies its existing learned skills. We prove that ASAP converges to a local optimum under natural conditions. Finally, our experimental results, which include a RoboCup domain, demonstrate the ability of ASAP to learn where to reuse skills as well as solve multiple tasks with considerably less experience than solving each task from scratch.
C1 [Mankowitz, Daniel J.; Mann, Timothy A.; Mannor, Shie] Technion Israel Inst Technol, Haifa, Israel.
   [Mann, Timothy A.] Google Deepmind, London, England.
RP Mankowitz, DJ (reprint author), Technion Israel Inst Technol, Haifa, Israel.
EM danielm@tx.technion.ac.il; mann.timothy@acm.org; shie@ee.technion.ac.il
FU European Research Council under European Union / ERC [306638]
FX The research leading to these results has received funding from the
   European Research Council under the European Union's Seventh Framework
   Program (FP/2007-2013) / ERC Grant Agreement n. 306638.
CR Akiyama H, 2014, LECT NOTES ARTIF INT, V8371, P528, DOI 10.1007/978-3-662-44468-9_46
   Ammar Haitham Bou, 2015, ARXIV150505798
   Bacon Pierre- Luc, 2015, NIPS DEEP REINF LEAR
   Bai Aijun, 2012, AAMAS
   da Silva B. C., 2012, ICML
   Eaton E, 2013, J MACHINE LEARNING R, P507
   Fu J., 2015, ARXIV150906841
   HAUSKNECHT M., 2015, ARXIV151104143
   Hauskrecht M., 1998, Uncertainty in Artificial Intelligence. Proceedings of the Fourteenth Conference (1998), P220
   Konidaris George, 2009, NIPS
   Mankowitz Daniel J, 2014, INT C MACH LEARN
   Mann Timothy A, 2014, P 31 INT C MACH LEAR
   Mann Timothy Arthur, 2015, AAAI WORKSH
   Masson Warwick, 2015, ARXIV150901644
   Peters J, 2008, NEURAL NETWORKS, V21, P682, DOI 10.1016/j.neunet.2008.02.003
   Peters J, 2006, 2006 IEEE/RSJ International Conference on Intelligent Robots and Systems, Vols 1-12, P2219, DOI 10.1109/IROS.2006.282564
   Precup D., 1998, Machine Learning: ECML-98. 10th European Conference on Machine Learning. Proceedings, P382
   Precup Doina, 1997, ADV NEUR INF PROC SY
   Silver D., 2012, P 29 INT C MACH LEAR
   Sutton R. S., 1999, ARTIFICIAL INTELLIGE
   SUTTON RS, 2000, NIPS, V12, P1057
   Thrun  S., 1995, LIFELONG ROBOT LEARN
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702097
DA 2019-06-15
ER

PT S
AU Mao, JH
   Xu, JJ
   Jing, YS
   Yuille, A
AF Mao, Junhua
   Xu, Jiajing
   Jing, Yushi
   Yuille, Alan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Training and Evaluating Multimodal Word Embeddings with Large-scale Web
   Annotated Images
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB In this paper, we focus on training and evaluating effective word embeddings with both text and visual information. More specifically, we introduce a large-scale dataset with 300 million sentences describing over 40 million images crawled and downloaded from publicly available Pins (i.e. an image with sentence descriptions uploaded by users) on Pinterest [2]. This dataset is more than 200 times larger than MS COCO [22], the standard large-scale image dataset with sentence descriptions. In addition, we construct an evaluation dataset to directly assess the effectiveness of word embeddings in terms of finding semantically similar or related words and phrases. The word/phrase pairs in this evaluation dataset are collected from the click data with millions of users in an image search system, thus contain rich semantic relationships. Based on these datasets, we propose and compare several Recurrent Neural Networks (RNNs) based multimodal (text and image) models. Experiments show that our model benefits from incorporating the visual information into the word embeddings, and a weight sharing strategy is crucial for learning such multimodal embeddings. The project page is: http://www.stat.ucla.edu/(similar to)junhua.mao/multimodal_embedding.html(1).
C1 [Mao, Junhua; Yuille, Alan] Univ Calif Los Angeles, Los Angeles, CA 90024 USA.
   [Xu, Jiajing; Jing, Yushi] Pinterest Inc, San Francisco, CA USA.
   [Yuille, Alan] Johns Hopkins Univ, Baltimore, MD 21218 USA.
RP Mao, JH (reprint author), Univ Calif Los Angeles, Los Angeles, CA 90024 USA.
EM mjhustc@ucla.edu; jiajing@pinterest.com; jing@pinterest.com;
   alan.l.yuille@gmail.com
FU Center for Brains, Minds and Machines NSF STC [CCF-1231216]; Army
   Research Office [ARO 62250-CS]
FX We are grateful to James Rubinstein for setting up the crowdsourcing
   experiments for dataset cleanup. We thank Veronica Mapes, Pawel
   Garbacki, and Leon Wong for discussions and support. We appreciate the
   comments and suggestions from anonymous reviewers of NIPS 2016. This
   work is partly supported by the Center for Brains, Minds and Machines
   NSF STC award CCF-1231216 and the Army Research Office ARO 62250-CS.
CR Agirre E, 2009, P HUM LANG TECHN 200, P19, DOI DOI 10.3115/1620754.1620758
   Bengio Y, 2006, STUD FUZZ SOFT COMP, V194, P137
   Bruni E., 2014, JAIR, V49
   Chen X., 2014, ARXIV14115654
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Cho S. J. K., 2015, ACL
   Donahue J., 2015, CVPR
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1
   Finkelstein L., 2001, P 10 INT C WORLD WID, P406, DOI [10.1145/371920.372094, DOI 10.1145/371920.372094]
   Grubinger M., 2006, INT WORKSH ONTOIMAGE, P13
   Hill F., 2015, COMPUTATIONAL LINGUI
   Hill Felix, 2014, P 2014 C EMP METH NA, P255
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kiela D., 2014, EMNLP, P36
   Kiros  R., 2014, ARXIV14112539
   Kottur S., 2015, ARXIV151107067
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lazaridou A., 2015, ARXIV150102598
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740
   Mao J, 2015, ICLR
   Mao JH, 2015, IEEE I CONF COMP VIS, P2533, DOI 10.1109/ICCV.2015.291
   Mikolov T., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Nelson DL, 2004, BEHAV RES METH INS C, V36, P402, DOI 10.3758/BF03195588
   Ordonez V., 2011, ADV NEURAL INFORM PR, P1143
   Pennington J., 2014, EMNLP, P1532, DOI DOI 10.3115/V1/D14-1162
   Schnabel T., 2015, EMNLP, P298
   Silberer Carina, 2014, P 52 ANN M ASS COMP, P721
   Simonyan Karen, 2015, ICLR
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Young P., 2014, ACL, P479
NR 35
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973705004
DA 2019-06-15
ER

PT S
AU Mao, XJ
   Shen, CH
   Yang, YB
AF Mao, Xiao-Jiao
   Shen, Chunhua
   Yang, Yu-Bin
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks
   with Symmetric Skip Connections
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB In this paper, we propose a very deep fully convolutional encoding-decoding framework for image restoration such as denoising and super-resolution. The network is composed of multiple layers of convolution and deconvolution operators, learning end-to-end mappings from corrupted images to the original ones. The convolutional layers act as the feature extractor, which capture the abstraction of image contents while eliminating noises/corruptions. Deconvolutional layers are then used to recover the image details. We propose to symmetrically link convolutional and deconvolutional layers with skip-layer connections, with which the training converges much faster and attains a higher-quality local optimum. First, the skip connections allow the signal to be back-propagated to bottom layers directly, and thus tackles the problem of gradient vanishing, making training deep networks easier and achieving restoration performance gains consequently. Second, these skip connections pass image details from convolutional layers to deconvolutional layers, which is beneficial in recovering the original image. Significantly, with the large capacity, we can handle different levels of noises using a single model. Experimental results show that our network achieves better performance than recent state-of-the-art methods.
C1 [Mao, Xiao-Jiao; Yang, Yu-Bin] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Jiangsu, Peoples R China.
   [Shen, Chunhua] Univ Adelaide, Sch Comp Sci, Adelaide, SA, Australia.
RP Mao, XJ (reprint author), Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Jiangsu, Peoples R China.
FU Natural Science Foundation of China [61673204, 61273257, 61321491];
   Program for Distinguished Talents of Jiangsu Province, China
   [2013-XXRJ-018]; Fundamental Research Funds for the Central Universities
   [020214380026]; Australian Research Council Future Fellowship
   [FT120100969]
FX This work was in part supported by Natural Science Foundation of China
   (Grants 61673204, 61273257, 61321491), Program for Distinguished Talents
   of Jiangsu Province, China (Grant 2013-XXRJ-018), Fundamental Research
   Funds for the Central Universities (Grant 020214380026), and Australian
   Research Council Future Fellowship (FT120100969). X.-J. Mao's
   contribution was made when visiting University of Adelaide. His visit
   was supported by the joint PhD program of China Scholarship Council.
CR Burger HC, 2012, PROC CVPR IEEE, P2392, DOI 10.1109/CVPR.2012.6247952
   Chatterjee P, 2009, IEEE T IMAGE PROCESS, V18, P1438, DOI 10.1109/TIP.2009.2018575
   Chen F, 2015, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2015.76
   Cui Z, 2014, LECT NOTES COMPUT SC, V8693, P49, DOI 10.1007/978-3-319-10602-1_4
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847
   Gu SH, 2015, IEEE I CONF COMP VIS, P1823, DOI 10.1109/ICCV.2015.212
   Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366
   He K., 2016, P IEEE C COMP VIS PA
   Hong S., 2015, P ADV NEUR INF PROC
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Huang Y., 2015, P ADV NEUR INF PROC, V28, P235
   Jain V., 2008, P ADV NEUR INF PROC, V8, P769
   Jia Y., 2014, ARXIV14085093
   Kingma D. P., 2015, P INT C LEARN REPR
   Liu HF, 2015, PROC CVPR IEEE, P484, DOI 10.1109/CVPR.2015.7298646
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Milanfar P, 2013, IEEE SIGNAL PROC MAG, V30, P106, DOI 10.1109/MSP.2011.2179329
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Osher S, 2005, MULTISCALE MODEL SIM, V4, P460, DOI 10.1137/040605412
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Salvador J, 2015, IEEE I CONF COMP VIS, P325, DOI 10.1109/ICCV.2015.45
   Schulter S, 2015, PROC CVPR IEEE, P3791, DOI 10.1109/CVPR.2015.7299003
   Simonyan K., 2015, P INT C LEARN REPR
   Srivastava R.K., 2015, ADV NEURAL INFORM PR, P2377
   Vincent P., 2008, P 25 INT C MACH LEAR, V2008, P1096, DOI DOI 10.1145/1390156.1390294
   Wang ZY, 2015, IEEE T IMAGE PROCESS, V24, P4359, DOI 10.1109/TIP.2015.2462113
   Wang ZW, 2015, IEEE I CONF COMP VIS, P370, DOI 10.1109/ICCV.2015.50
   Xie J, 2012, P ADV NEUR INF PROC, P350
   Xu J, 2015, IEEE I CONF COMP VIS, P244, DOI 10.1109/ICCV.2015.36
   Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278
NR 34
TC 0
Z9 0
U1 2
U2 2
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701029
DA 2019-06-15
ER

PT S
AU Mariet, Z
   Sra, S
AF Mariet, Zelda
   Sra, Suvrit
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Kronecker Determinantal Point Processes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Determinantal Point Processes (DPPs) are probabilistic models over all subsets a ground set of N items. They have recently gained prominence in several applications that rely on "diverse" subsets. However, their applicability to large problems is still limited due to O(N-3) complexity of core tasks such as sampling and learning. We enable efficient sampling and learning for DPPs by introducing KRONDPP, a DPP model whose kernel matrix decomposes as a tensor product of multiple smaller kernel matrices. This decomposition immediately enables fast exact sampling. But contrary to what one may expect, leveraging the Kronecker product structure for speeding up DPP learning turns out to be more difficult. We overcome this challenge, and derive batch and stochastic optimization algorithms for efficiently learning the parameters of a KRONDPP.
C1 [Mariet, Zelda; Sra, Suvrit] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
RP Mariet, Z (reprint author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
EM zelda@csail.mit.edu; suvrit@mit.edu
FU NSF [IIS-1409802]
FX SS acknowledges partial support from NSF grant IIS-1409802.
CR Affandi R., 2014, ICML
   Affandi R., 2013, ARTIFICIAL INTELLIGE
   Batmanghelich N. K., 2014, ARXIV14116307
   Bhatia R, 2007, PRINC SER APPL MATH, P1
   Borodin A., 2009, ARXIV09111153
   Chao W., 2015, UNCERTAINTY ARTIFICI
   Decreusefond L., 2015, DETERMINANTAL POINT
   Flaxman Seth R, 2015, P 32 INT C MACH LEAR, V37, P607
   Gartrell Mike, 2016, ARXIV160205436
   Gillenwater J., 2014, NIPS
   GOLDSCHMIDT O, 1994, NAV RES LOG, V41, P833, DOI 10.1002/1520-6750(199410)41:6<833::AID-NAV3220410611>3.0.CO;2-Q
   Hough J. B., 2006, PROBABILITY SURVEYS, V3, P9
   Kang B., 2013, P C NEUR INF PROC SY, V26, P2319
   Krause A, 2008, J MACH LEARN RES, V9, P235
   Kulesza A., 2013, THESIS
   Kulesza A., 2012, DETERMINANTAL POINT, V5
   Kulesza Alex, 2011, ICML
   Lavancier F, 2015, J R STAT SOC B, V77, P853, DOI 10.1111/rssb.12096
   Li C., 2016, ARXIV160306052
   Li C., 2015, ARXIV150901618
   Lin H., 2012, UNCERTAINTY ARTIFICI
   Lyons R, 2003, PUBL MATH IHES, P167
   Macchi O., 1975, ADV APPL PROB, V7
   Mariet Z., 2015, ICML
   Mariet Z., 2016, INT C LEARN REPR ICL
   Martens James, 2015, ICML
   VanLoan C. F., 1993, LINEAR ALGEBRA LARGE, P293
   Wu G, 2005, SIAM PROC S, P611
   Yuille AL, 2002, ADV NEUR IN, V14, P1033
   Zhang X., 2015, ICCV
   Zhou T, 2010, P NATL ACAD SCI USA, V107, P4511, DOI 10.1073/pnas.1000488107
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702043
DA 2019-06-15
ER

PT S
AU Mathieu, M
   Zhao, JB
   Sprechmann, P
   Ramesh, A
   Lecun, Y
AF Mathieu, Michael
   Zhao, Junbo
   Sprechmann, Pablo
   Ramesh, Aditya
   Lecun, Yann
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Disentangling factors of variation in deep representations using
   adversarial training
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We introduce a conditional generative model for learning to disentangle the hidden factors of variation within a set of labeled observations, and separate them into complementary codes. One code summarizes the specified factors of variation associated with the labels. The other summarizes the remaining unspecified variability. During training, the only available source of supervision comes from our ability to distinguish among different observations belonging to the same class. Examples of such observations include images of a set of labeled objects captured at different viewpoints, or recordings of set of speakers dictating multiple phrases. In both instances, the intra-class diversity is the source of the unspecified factors of variation: each object is observed at multiple viewpoints, and each speaker dictates multiple phrases. Learning to disentangle the specified factors from the unspecified ones becomes easier when strong supervision is possible. Suppose that during training, we have access to pairs of images, where each pair shows two different objects captured from the same viewpoint. This source of alignment allows us to solve our task using existing methods. However, labels for the unspecified factors are usually unavailable in realistic scenarios where data acquisition is not strictly controlled. We address the problem of disentaglement in this more general setting by combining deep convolutional autoencoders with a form of adversarial training. Both factors of variation are implicitly captured in the organization of the learned embedding space, and can be used for solving single-image analogies. Experimental results on synthetic and real datasets show that the proposed method is capable of generalizing to unseen classes and intra-class variabilities.
C1 [Mathieu, Michael; Zhao, Junbo; Sprechmann, Pablo; Ramesh, Aditya; Lecun, Yann] 719 Broadway,12th Floor, New York, NY 10003 USA.
RP Mathieu, M (reprint author), 719 Broadway,12th Floor, New York, NY 10003 USA.
EM mathieu@cs.nyu.edu; junbo.zhao@cs.nyu.edu; pablo@cs.nyu.edu;
   ar2922@cs.nyu.edu; yann@cs.nyu.edu
CR Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Cheung Brian, 2014, CORR
   Collobert R, 2011, BIGLEARN NIPS WORKSH
   Denton E. L., 2015, NIPS
   Dosovitskiy Alexey, 2014, CORR
   Dumoulin V., 2016, ARXIV160600704
   Edwards Harrison, 2015, ARXIV151105897
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   GEORGHIADES AS, 2001, SYSTEMS MAN CYBERN A, V23, P643
   Goodfellow I., 2014, NIPS
   Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6
   Kingma D. P., 2014, ADV NEURAL INFORM PR, V4, P3581
   Kingma D.P., 2013, ARXIV13126114
   Kulkarni T. D., 2015, ADV NEURAL INFORM PR, P2530
   Larsen A. B. L., 2015, ARXIV151209300
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2004, CVPR
   Louizos Christos, 2016, ICLR
   Makhzani Alireza, 2015, CORR
   Mathieu Michael, 2015, ICLR
   Mirza M., 2014, ARXIV14111784, DOI DOI 10.1029/2009WR008312
   Radford A., 2015, ARXIV151106434
   Ranzato M, 2007, P COMP VIS PATT REC
   Reed S.E., 2015, ADV NEURAL INFORM PR, V28, P1252
   Reed Scott, 2014, P 31 INT C MACH LEAR, V32, P1431
   Rezende D. J, 2014, ARXIV14014082
   Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349
   Theis L., 2015, ARXIV151101844
   Zhao Junbo, 2016, ICLR WORKSH SUBM
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700016
DA 2019-06-15
ER

PT S
AU McIntosh, LT
   Maheswaranathan, N
   Nayebi, A
   Ganguli, S
   Baccus, SA
AF McIntosh, Lane T.
   Maheswaranathan, Niru
   Nayebi, Aran
   Ganguli, Surya
   Baccus, Stephen A.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Deep Learning Models of the Retinal Response to Natural Scenes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID ADAPTATION; PREDICTION; SPIKING
AB A central challenge in sensory neuroscience is to understand neural computations and circuit mechanisms that underlie the encoding of ethologically relevant, natural stimuli. In multilayered neural circuits, nonlinear processes such as synaptic transmission and spiking dynamics present a significant obstacle to the creation of accurate computational models of responses to natural stimuli. Here we demonstrate that deep convolutional neural networks (CNNs) capture retinal responses to natural scenes nearly to within the variability of a cell's response, and are markedly more accurate than linear-nonlinear (LN) models and Generalized Linear Models (GLMs). Moreover, we find two additional surprising properties of CNNs: they are less susceptible to overfitting than their LN counterparts when trained on small amounts of data, and generalize better when tested on stimuli drawn from a different distribution (e.g. between natural scenes and white noise). An examination of the learned CNNs reveals several properties. First, a richer set of feature maps is necessary for predicting the responses to natural scenes compared to white noise. Second, temporally precise responses to slowly varying inputs originate from feedforward inhibition, similar to known retinal mechanisms. Third, the injection of latent noise sources in intermediate layers enables our model to capture the sub-Poisson spiking variability observed in retinal ganglion cells. Fourth, augmenting our CNNs with recurrent lateral connections enables them to capture contrast adaptation as an emergent property of accurately describing retinal responses to natural scenes. These methods can be readily generalized to other sensory modalities and stimulus ensembles. Overall, this work demonstrates that CNNs not only accurately capture sensory circuit responses to natural scenes, but also can yield information about the circuit's internal structure and function.
C1 [McIntosh, Lane T.; Maheswaranathan, Niru; Nayebi, Aran] Stanford Univ, Neurosci PhD Program, Stanford, CA 94305 USA.
   [Ganguli, Surya] Stanford Univ, Dept Appl Phys, Stanford, CA 94305 USA.
   [Ganguli, Surya; Baccus, Stephen A.] Stanford Univ, Neurobiol Dept, Stanford, CA 94305 USA.
RP McIntosh, LT (reprint author), Stanford Univ, Neurosci PhD Program, Stanford, CA 94305 USA.
EM lmcintosh@stanford.edu; nirum@stanford.edu; anayebi@stanford.edu;
   sganguli@stanford.edu; baccus@stanford.edu
FU NSF; NVIDIA Titan X Award; NEI; Burroughs Wellcome; James S. McDonnell
   Foundations; ONR; Sloan; McKnight; Simons
FX The authors would like to thank Ben Poole and EJ Chichilnisky for
   helpful discussions related to this work. Thanks also goes to the
   following institutions for providing funding and hardware grants, LM:
   NSF, NVIDIA Titan X Award, NM: NSF, AN and SB: NEI grants, SG: Burroughs
   Wellcome, Sloan, McKnight, Simons, James S. McDonnell Foundations and
   the ONR.
CR Atick JJ, 1990, NEURAL COMPUT, V2, P308, DOI 10.1162/neco.1990.2.3.308
   Baccus SA, 2002, NEURON, V36, P909, DOI 10.1016/S0896-6273(02)01050-4
   Bastien F, 2012, ARXIV12115590
   Berry MJ, 1997, P NATL ACAD SCI USA, V94, P5411, DOI 10.1073/pnas.94.10.5411
   Calvert PD, 2002, J GEN PHYSIOL, V119, P129, DOI 10.1085/jgp.119.2.129
   Chichilnisky EJ, 2001, NETWORK-COMP NEURAL, V12, P199, DOI 10.1088/0954-898X/12/2/306
   Cho K., 2014, P 8 WORKSH SYNT SEM, P103, DOI DOI 10.3115/V1/W14-4012
   Fairhall AL, 2006, J NEUROPHYSIOL, V96, P2724, DOI 10.1152/jn.00995.2005
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Gollisch T, 2013, J PHYSIOL-PARIS, V107, P338, DOI 10.1016/j.jphysparis.2012.12.001
   Gollisch T, 2010, NEURON, V65, P150, DOI 10.1016/j.neuron.2009.12.009
   Heitman A., 2016, BIORXIV, DOI DOI 10.1101/045336
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   HOCHSTEIN S, 1976, J PHYSIOL-LONDON, V262, P265, DOI 10.1113/jphysiol.1976.sp011595
   Hosoya T, 2005, NATURE, V436, P71, DOI 10.1038/nature03689
   Hyvarinen A, 2009, COMPUT IMAGING VIS, V39, P1
   Jozefowicz R., 2015, P 32 INT C MACH LEAR, V37, P2342, DOI DOI 10.1109/CVPR.2015.72987
   Kastner DB, 2011, NAT NEUROSCI, V14, P1317, DOI 10.1038/nn.2906
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Olveczky BP, 2003, NATURE, V423, P401, DOI 10.1038/nature01652
   Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140
   Pillow JW, 2005, J NEUROSCI, V25, P11003, DOI 10.1523/JNEUROSCI.3305-05.2005
   Pitkow X, 2012, NAT NEUROSCI, V15, P628, DOI 10.1038/nn.3064
   Poole  B., 2014, ARXIV14061831
   Roska B, 2001, NATURE, V410, P583, DOI 10.1038/35069068
   Roska B, 2003, NAT NEUROSCI, V6, P600, DOI 10.1038/nn1061
   Rust NC, 2005, NAT NEUROSCI, V8, P1647, DOI 10.1038/nn1606
   Rust NC, 2005, NEURON, V46, P945, DOI 10.1016/j.neuron.2005.05.021
   Schwartz G, 2007, NAT NEUROSCI, V10, P552, DOI 10.1038/nn1887
   vanSteveninck RRD, 1997, SCIENCE, V275, P1805, DOI 10.1126/science.275.5307.1805
   Yamins D. L., 2013, ADV NEURAL INFORM PR, P3093
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703024
DA 2019-06-15
ER

PT S
AU McQueen, J
   Meila, M
   Perrault-Joncas, D
AF McQueen, James
   Meila, Marina
   Perrault-Joncas, Dominique
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Nearly Isometric Embedding by Relaxation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID DIMENSIONALITY REDUCTION; EIGENMAPS; MANIFOLDS
AB Many manifold learning algorithms aim to create embeddings with low or no distortion (isometric). If the data has intrinsic dimension d, it is often impossible to obtain an isometric embedding in d dimensions, but possible in s > d dimensions. Yet, most geometry preserving algorithms cannot do the latter. This paper proposes an embedding algorithm to overcome this. The algorithm accepts as input, besides the dimension d, an embedding dimension s >= d. For any data embedding Y, we compute a Loss(Y), based on the push-forward Riemannian metric associated with Y, which measures deviation of Y from from isometry. Riemannian Relaxation iteratively updates Y in order to decrease Loss(Y). The experiments confirm the superiority of our algorithm in obtaining low distortion embeddings.
C1 [McQueen, James; Meila, Marina] Univ Washington, Dept Stat, Seattle, WA 98195 USA.
   [Perrault-Joncas, Dominique] Google, Seattle, WA 98103 USA.
RP McQueen, J (reprint author), Univ Washington, Dept Stat, Seattle, WA 98195 USA.
EM jmcq@u.washington.edu; mmp@stat.washington.edu; dcpjoncas@gmail.com
CR Abazajian KN, 2009, ASTROPHYS J SUPPL S, V182, P543, DOI 10.1088/0067-0049/182/2/543
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Bernstein M., 2000, SCIENCE, V290
   Coifman R. R., 2006, APPL COMPUT HARMON A, V21, p[6, 1]
   Delchambre L, 2015, MON NOT R ASTRON SOC, V446, P3545, DOI 10.1093/mnras/stu2219
   Donoho DL, 2003, P NATL ACAD SCI USA, V100, P5591, DOI 10.1073/pnas.1031596100
   Genovese CR, 2012, J MACH LEARN RES, V13, P1263
   Hein M., 2005, P 22 INT C MACH LEAR, P289
   Hein M, 2007, J MACH LEARN RES, V8, P1325
   Lee J. M., 2003, INTRO SMOOTH MANIFOL
   Lee JM, 1997, RIEMANNIAN MANIFOLDS
   Nadler B, 2006, APPL COMPUT HARMON A, V21, P113, DOI 10.1016/j.acha.2005.07.004
   NASH J, 1956, ANN MATH, V63, P20, DOI 10.2307/1969989
   Ozertem U, 2011, J MACH LEARN RES, V12, P1249
   Perrault-Joncas Dominique, 2013, ARXIV13057255V1
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Ting D., 2010, P 27 INT C MACH LEAR, P1079
   Vanderplas J, 2009, ASTRON J, V138, P1365, DOI 10.1088/0004-6256/138/5/1365
   Verma N, 2013, J MACH LEARN RES, V14, P2415
   Weinberger KQ, 2006, INT J COMPUT VISION, V70, P77, DOI 10.1007/s11263-005-4939-z
   Zhang ZY, 2004, SIAM J SCI COMPUT, V26, P313, DOI 10.1137/S1064827502419154
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704059
DA 2019-06-15
ER

PT S
AU Meng, Q
   Ke, GL
   Wang, TF
   Chen, W
   Ye, QW
   Ma, ZM
   Liu, TY
AF Meng, Qi
   Ke, Guolin
   Wang, Taifeng
   Chen, Wei
   Ye, Qiwei
   Ma, Zhi-Ming
   Liu, Tie-Yan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI A Communication-Efficient Parallel Algorithm for Decision Tree
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Decision tree (and its extensions such as Gradient Boosting Decision Trees and Random Forest) is a widely used machine learning algorithm, due to its practical effectiveness and model interpretability. With the emergence of big data, there is an increasing need to parallelize the training process of decision tree. However, most existing attempts along this line suffer from high communication costs. In this paper, we propose a new algorithm, called Parallel Voting Decision Tree (PV-Tree), to tackle this challenge. After partitioning the training data onto a number of (e.g., M) machines, this algorithm performs both local voting and global voting in each iteration. For local voting, the top-k attributes are selected from each machine according to its local data. Then, globally top-2k attributes are determined by a majority voting among these local candidates. Finally, the full-grained histograms of the globally top-2k attributes are collected from local machines in order to identify the best (most informative) attribute and its split point. PV-Tree can achieve a very low communication cost (independent of the total number of attributes) and thus can scale out very well. Furthermore, theoretical analysis shows that this algorithm can learn a near optimal decision tree, since it can find the best attribute with a large probability. Our experiments on real-world datasets show that PV-Tree significantly outperforms the existing parallel decision tree algorithms in the trade-off between accuracy and efficiency.
C1 [Meng, Qi] Peking Univ, Beijing, Peoples R China.
   [Ke, Guolin; Wang, Taifeng; Chen, Wei; Ye, Qiwei; Liu, Tie-Yan] Microsoft Res, Redmond, WA USA.
   [Ma, Zhi-Ming] Chinese Acad Math & Syst Sci, Beijing, Peoples R China.
RP Meng, Q (reprint author), Peking Univ, Beijing, Peoples R China.
EM qimeng13@pku.edu.cn; Guolin.Ke@microsoft.com; taifengw@microsoft.com;
   wche@microsoft.com; qiwye@microsoft.com; mazm@amt.ac.cn;
   tie-yan.liu@microsoft.com
CR Agrawal Rakesh, 2001, US Patent, Patent No. [6,230,151, 6230151]
   Alsabti K., 1998, Proceedings Fourth International Conference on Knowledge Discovery and Data Mining, P2
   Ben-Haim Y, 2010, J MACH LEARN RES, V11, P849
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Breiman L., 1984, CLASSIFICATION REGRE
   Burges CJ, 2010, LEARNING, V11, P23
   Friedman J., 2001, ELEMENTS STAT LEARNI, V1
   Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451
   Gehrke J, 1999, SIGMOD RECORD, VOL 28, NO 2 - JUNE 1999, P169
   Jahrer Michael, 2012, KDDCUP WORKSH
   Jin RM, 2003, SIAM PROC S, P119
   Joshi MV, 1998, FIRST MERGED INTERNATIONAL PARALLEL PROCESSING SYMPOSIUM & SYMPOSIUM ON PARALLEL AND DISTRIBUTED PROCESSING, P573, DOI 10.1109/IPPS.1998.669983
   Kufrin R., 1997, MACH INTELL PATT REC, V20, P279
   Mehta M., 1996, Advances in Database Technology - EDBT '96. 5th International Conference on Extending Database Technology. Proceedings, P18
   Panda B, 2009, PROC VLDB ENDOW, V2, P1426, DOI 10.14778/1687553.1687569
   Pearson Robert Allan, 1993, COARSE GRAINED PARAL
   Quinlan J. R., 1986, Machine Learning, V1, P81, DOI 10.1007/BF00116251
   SAFAVIAN SR, 1991, IEEE T SYST MAN CYB, V21, P660, DOI 10.1109/21.97458
   Shafer J, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P544
   Svore K. M., 2011, SCALING MACHINE LEAR, V2
   Tyree Stephen, 2011, P 20 INT C WORLD WID, P387
   Yu C, 2001, TECH REP
   Zhou Z. - H., 2012, ENSEMBLE METHODS FDN
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703017
DA 2019-06-15
ER

PT S
AU Mercado, P
   Tudisco, F
   Hein, M
AF Mercado, Pedro
   Tudisco, Francesco
   Hein, Matthias
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Clustering Signed Networks with the Geometric Mean of Laplacians
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID STRUCTURAL BALANCE
AB Signed networks allow to model positive and negative relationships. We analyze existing extensions of spectral clustering to signed networks. It turns out that existing approaches do not recover the ground truth clustering in several situations where either the positive or the negative network structures contain no noise. Our analysis shows that these problems arise as existing approaches take some form of arithmetic mean of the Laplacians of the positive and negative part. As a solution we propose to use the geometric mean of the Laplacians of positive and negative part and show that it outperforms the existing approaches. While the geometric mean of matrices is computationally expensive, we show that eigenvectors of the geometric mean can be computed efficiently, leading to a numerical scheme for sparse matrices which is of independent interest.
C1 [Mercado, Pedro; Hein, Matthias] Saarland Univ, Saarbrucken, Germany.
   [Tudisco, Francesco] Univ Padua, Padua, Italy.
RP Mercado, P (reprint author), Saarland Univ, Saarbrucken, Germany.
FU ERC starting grant NOLEPRO
FX The authors acknowledge support by the ERC starting grant NOLEPRO
CR Bhatia R., 2009, POSITIVE DEFINITE MA
   Bini D., 2015, MATRIX MEANS TOOLBOX
   CARTWRIGHT D, 1956, PSYCHOL REV, V63, P277, DOI 10.1037/h0046049
   Chiang K.Y., 2012, P 21 ACM INT C INF K, P615
   DAVIS JA, 1967, HUM RELAT, V20, P181, DOI 10.1177/001872676702000206
   DESAI M, 1994, J GRAPH THEOR, V18, P181, DOI 10.1002/jgt.3190180210
   Doreian P, 2009, SOC NETWORKS, V31, P1, DOI 10.1016/j.socnet.2008.08.001
   Druskin V, 1998, SIAM J MATRIX ANAL A, V19, P755, DOI 10.1137/S0895479895292400
   Fasi M., 201629 MIMS
   Harary F., 1953, MICH MATH J, V2, P143, DOI DOI 10.1307/MMJ/1028989917
   Higham NJ, 2005, SIAM J MATRIX ANAL A, V26, P849, DOI 10.1137/S089547980442218
   Iannazzo B., 2015, NUMER LINEAR ALGEBRA
   Iannazzo B., 2015, OPTIMIZATION ONLINE
   Knizhnerman L, 2010, NUMER LINEAR ALGEBR, V17, P615, DOI 10.1002/nla.652
   Kropivnik S., 1996, DEV STAT METHODOLOGY, P209
   Kunegis J, 2010, P SIAM INT C DAT MIN, P559, DOI DOI 10.1137/1.9781611972801.49
   Leskovec J., 2014, SNAP DATASETS STANFO
   Liu SP, 2015, ADV MATH, V268, P306, DOI 10.1016/j.aim.2014.09.023
   Raissouli M, 2003, LINEAR ALGEBRA APPL, V359, P37, DOI 10.1016/S0024-3795(02)00427-5
   Read KE, 1954, SOUTHWEST J ANTHROP, V10, P1
   Rohe K, 2011, ANN STAT, V39, P1878, DOI 10.1214/11-AOS887
   Tang J., 2015, ARXIV151107569
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701021
DA 2019-06-15
ER

PT S
AU Milan, K
   Veness, J
   Kirkpatrick, J
   Hassabis, D
   Koop, A
   Bowling, M
AF Milan, Kieran
   Veness, Joel
   Kirkpatrick, James
   Hassabis, Demis
   Koop, Anna
   Bowling, Michael
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI The Forget-me-not Process
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We introduce the Forget-me-not Process, an efficient, non-parametric meta-algorithm for online probabilistic sequence prediction for piecewise stationary, repeating sources. Our method works by taking a Bayesian approach to partitioning a stream of data into postulated task-specific segments, while simultaneously building a model for each task. We provide regret guarantees with respect to piecewise stationary data sources under the logarithmic loss, and validate the method empirically across a range of sequence prediction and task identification problems.
C1 [Milan, Kieran; Veness, Joel; Kirkpatrick, James; Hassabis, Demis] Google DeepMind, London, England.
   [Koop, Anna; Bowling, Michael] Univ Alberta, Edmonton, AB, Canada.
RP Milan, K (reprint author), Google DeepMind, London, England.
EM kmilan@google.com; aixi@google.com; kirkpatrick@google.com;
   demishassabis@google.com; anna@cs.ualberta.ca; bowling@cs.ualberta.ca
CR Adams R. P., 2007, BAYESIAN ONLINE CHAN
   Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Collins A., 2012, PLOSBIOLOGY, V10, P1
   Collins AGE, 2013, PSYCHOL REV, V120, P190, DOI 10.1037/a0030852
   Donoso M, 2014, SCIENCE, V344, P1481, DOI 10.1126/science.1252254
   Doya K, 2002, NEURAL COMPUT, V14, P1347, DOI 10.1162/089976602753712972
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Germain M., 2015, ICML, V37, P881
   Gyorgy A, 2012, IEEE T INFORM THEORY, V58, P6709, DOI 10.1109/TIT.2012.2209627
   Hazan Elad, 2009, P 26 ANN INT C MACH, V382, P393
   Hutter M, 2007, THEOR COMPUT SCI, V384, P33, DOI 10.1016/j.tcs.2007.05.016
   KRICHEVSKY RE, 1981, IEEE T INFORM THEORY, V27, P199, DOI 10.1109/TIT.1981.1056331
   Lattimore Tor, 2013, P 24 INT C ALG LEARN, P324
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Veness J., 2015, P 29 AAAI C ART INT, P3016
   Veness J, 2013, IEEE DATA COMPR CONF, P321, DOI 10.1109/DCC.2013.40
   VITTER JS, 1985, ACM T MATH SOFTWARE, V11, P37, DOI 10.1145/3147.3165
   Willems F., 1997, Proceeding. 1997 IEEE International Symposium on Information Theory (Cat. No.97CH36074), DOI 10.1109/ISIT.1997.612983
   Willems FMJ, 1996, IEEE T INFORM THEORY, V42, P2210, DOI 10.1109/18.556608
   WILLEMS FMJ, 1995, IEEE T INFORM THEORY, V41, P653, DOI 10.1109/18.382012
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700019
DA 2019-06-15
ER

PT S
AU Minami, K
   Arai, H
   Sato, I
   Nakagawa, H
AF Minami, Kentaro
   Arai, Hiromi
   Sato, Issei
   Nakagawa, Hiroshi
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Differential Privacy without Sensitivity
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB The exponential mechanism is a general method to construct a randomized estimator that satisfies (epsilon, 0)-differential privacy. Recently, Wang et al. showed that the Gibbs posterior, which is a data-dependent probability distribution that contains the Bayesian posterior, is essentially equivalent to the exponential mechanism under certain boundedness conditions on the loss function. While the exponential mechanism provides a way to build an (epsilon, 0)-differential private algorithm, it requires boundedness of the loss function, which is quite stringent for some learning problems. In this paper, we focus on (epsilon, delta)-differential privacy of Gibbs posteriors with convex and Lipschitz loss functions. Our result extends the classical exponential mechanism, allowing the loss functions to have an unbounded sensitivity.
C1 [Minami, Kentaro; Arai, Hiromi; Sato, Issei; Nakagawa, Hiroshi] Univ Tokyo, Tokyo, Japan.
RP Minami, K (reprint author), Univ Tokyo, Tokyo, Japan.
EM kentaro_minami@mist.i.u-tokyo.ac.jp; arai@dl.itc.u-tokyo.ac.jp;
   sato@k.u-tokyo.ac.jp; nakagawa@dl.itc.u-tokyo.ac.jp
FU JSPS KAKENHI [JP15H02700]
FX This work was supported by JSPS KAKENHI Grant Number JP15H02700.
CR Alquier P., 2015, PROPERTIES VARIATION
   Bassily R., 2014, FOCS
   Boucheron S., 2013, CONCENTRATION INEQUA
   Bubeck S., 2015, NIPS
   Catoni O., 2007, PAC BAYESIAN SUPERVI
   Chaudhuri K., 2014, NIPS
   Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069
   Dalalyan A., 2014, THEORETICAL GUARANTE
   Dimitrakakis C., 2014, ALGORITHMIC LEARNING
   Durmus A., 2015, NONASYMPTOTIC CONVER
   Dwork C, 2006, LECT NOTES COMPUT SC, V4052, P1
   GENTIL I, 2014, ANAL GEOMETRY MARKOV
   Hall R, 2013, J MACH LEARN RES, V14, P703
   Kifer D., 2012, COLT
   Ledoux M, 1999, LECT NOTES MATH, V1709, P120
   McSherry F, 2007, FOCS
   Milman E, 2012, PROBAB THEORY REL, V152, P475, DOI 10.1007/s00440-010-0328-1
   Mir D. J., 2013, THESIS
   Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5
   Wang Y., 2015, ICML
   Zhang T, 2006, ANN STAT, V34, P2180, DOI 10.1214/009053606000000704
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700015
DA 2019-06-15
ER

PT S
AU Mirzasoleiman, B
   Zadimoghaddam, M
   Karbasi, A
AF Mirzasoleiman, Baharan
   Zadimoghaddam, Morteza
   Karbasi, Amin
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Fast Distributed Submodular Cover: Public-Private Data Summarization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB In this paper, we introduce the public-private framework of data summarization motivated by privacy concerns in personalized recommender systems and online social services. Such systems have usually access to massive data generated by a large pool of users. A major fraction of the data is public and is visible to (and can be used for) all users. However, each user can also contribute some private data that should not be shared with other users to ensure her privacy. The goal is to provide a succinct summary of massive dataset, ideally as small as possible, from which customized summaries can be built for each user, i.e. it can contain elements from the public data (for diversity) and users' private data (for personalization). To formalize the above challenge, we assume that the scoring function according to which a user evaluates the utility of her summary satisfies submodularity, a widely used notion in data summarization applications. Thus, we model the data summarization targeted to each user as an instance of a submodular cover problem. However, when the data is massive it is infeasible to use the centralized greedy algorithm to find a customized summary even for a single user. Moreover, for a large pool of users, it is too time consuming to find such summaries separately. Instead, we develop a fast distributed algorithm for submodular cover, FASTCOVER, that provides a succinct summary in one shot and for all users. We show that the solution provided by FASTCOVER is competitive with that of the centralized algorithm with the number of rounds that is exponentially smaller than state of the art results. Moreover, we have implemented FASTCOVER with Spark to demonstrate its practical performance on a number of concrete applications, including personalized location recommendation, personalized movie recommendation, and dominating set on tens of millions of data points and varying number of users.
C1 [Mirzasoleiman, Baharan] Swiss Fed Inst Technol, Zurich, Switzerland.
   [Zadimoghaddam, Morteza] Google Res, New York, NY USA.
   [Karbasi, Amin] Yale Univ, New Haven, CT 06520 USA.
RP Mirzasoleiman, B (reprint author), Swiss Fed Inst Technol, Zurich, Switzerland.
FU Google Faculty Research Award; DARPA Young Faculty Award [D16AP00046]
FX This research was supported by Google Faculty Research Award and DARPA
   Young Faculty Award (D16AP00046).
CR Berger Bonnie, 1994, J COMPUTER SYSTEM SC
   Blelloch Guy E., 2011, SPAA
   Candes  E., 2009, FDN COMPUTATIONAL MA
   Chierichetti Flavio, 2015, KDD
   Dean Jeffrey, 2004, OSDI
   Demaine Erik D, 2014, DISTRIBUTED COMPUTIN
   Dueck Delbert, 2007, ICCV
   El-Arini Khalid, 2011, KDD
   Feige U., 1998, J ACM
   Gomes R., 2010, ICML
   Hui Lin, 2011, N AM CHAPTER ASS COM
   Iyer R., 2013, NIPS
   Krause Andreas, 2008, JMLR
   Kumar Ravi, 2015, TOPC
   Lindgren Erik M, 2015, NIPS
   Mirzasoleiman Baharan, 2016, ICML
   Mirzasoleiman Baharan, 2013, NIPS
   Mirzasoleiman Baharan, 2015, NIPS
   Simon I., 2007, ICCV, P6
   Sipos Ruben, 2012, CIKM
   Stergiou Stergios, 2015, SIGKDD
   Tschiatschek S., 2014, NIPS
   Wolsey Laurence A., 1982, COMBINATORICA
   Yang Jaewon, 2015, KNOWLEDGE INFORM SYS
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704064
DA 2019-06-15
ER

PT S
AU Mohri, M
   Yang, S
AF Mohri, Mehryar
   Yang, Scott
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Optimistic Bandit Convex Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We introduce the general and powerful scheme of predicting information re-use in optimization algorithms. This allows us to devise a computationally efficient algorithm for bandit convex optimization with new state-of-the-art guarantees for both Lipschitz loss functions and loss functions with Lipschitz gradients. This is the first algorithm admitting both a polynomial time complexity and a regret that is polynomial in the dimension of the action space that improves upon the original regret bound for Lipschitz loss functions, achieving a regret of (O) over tilde (T(11/16)d(3/8)). Our algorithm further improves upon the best existing polynomial-in-dimension bound (both computationally and in terms of regret) for loss functions with Lipschitz gradients, achieving a regret of (O) over tilde (T(8/13)d(5/3)).
C1 [Mohri, Mehryar; Yang, Scott] Courant Inst, 251 Mercer St, New York, NY 10012 USA.
   [Mohri, Mehryar] Google, 251 Mercer St, New York, NY 10012 USA.
RP Mohri, M (reprint author), Courant Inst, 251 Mercer St, New York, NY 10012 USA.; Mohri, M (reprint author), Google, 251 Mercer St, New York, NY 10012 USA.
EM mohri@cims.nyu.edu; yangs@cims.nyu.edu
FU NSF [CCF-1535987, IIS-1618662, GRFP DGE-1342536]
FX This work was partly funded by NSF CCF-1535987 and IIS-1618662 and NSF
   GRFP DGE-1342536.
CR Abernethy Jacob, 2008, P 21 ANN C LEARN THE, P263
   Abernethy Jacob, 2009, COLT
   Agarwal A, 2010, P 23 ANN C LEARN THE, P28
   Bubeck S., 2015, ABS150706580 CORR
   Bubeck S., 2015, ABS150206398 CORR
   Bubeck S., 2016, ABS160703084 CORR
   Dani V., STOCHASTIC LINEAR OP
   Defazio A., 2014, ADV NEURAL INFORM PR, P1646
   Dekel O., 2015, NIPS, P2908
   Flaxman AD, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P385
   Hazan E., 2016, ABS160304350 CORR
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Kleinberg R. D., 2004, ADV NEURAL INFORM PR, P697
   Nesterov Y., 2004, INTRO LECT CONVEX OP
   Nesterov  Y., 1994, STUDIES APPL MATH
   Rakhlin Alexander, 2013, COLT, P993
   Saha A., 2011, INT C ART INT STAT A, P636
   Schmidt M. W., 2013, CORR
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703065
DA 2019-06-15
ER

PT S
AU Mokhtari, A
   Daneshmand, H
   Lucchi, A
   Hofmann, T
   Ribeiro, A
AF Mokhtari, Aryan
   Daneshmand, Hadi
   Lucchi, Aurelien
   Hofmann, Thomas
   Ribeiro, Alejandro
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Adaptive Newton Method for Empirical Risk Minimization to Statistical
   Accuracy
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We consider empirical risk minimization for large-scale datasets. We introduce Ada Newton as an adaptive algorithm that uses Newton's method with adaptive sample sizes. The main idea of Ada Newton is to increase the size of the training set by a factor larger than one in a way that the minimization variable for the current training set is in the local neighborhood of the optimal argument of the next training set. This allows to exploit the quadratic convergence property of Newton's method and reach the statistical accuracy of each training set with only one iteration of Newton's method. We show theoretically that we can iteratively increase the sample size while applying single Newton iterations without line search and staying within the statistical accuracy of the regularized empirical risk. In particular, we can double the size of the training set in each iteration when the number of samples is sufficiently large. Numerical experiments on various datasets confirm the possibility of increasing the sample size by factor 2 at each iteration which implies that Ada Newton achieves the statistical accuracy of the full training set with about two passes over the dataset.(1)
C1 [Mokhtari, Aryan; Ribeiro, Alejandro] Univ Penn, Philadelphia, PA 19104 USA.
   [Daneshmand, Hadi; Lucchi, Aurelien; Hofmann, Thomas] Swiss Fed Inst Technol, Zurich, Switzerland.
RP Mokhtari, A (reprint author), Univ Penn, Philadelphia, PA 19104 USA.
EM aryanm@seas.upenn.edu; hadi.daneshmand@inf.ethz.ch;
   aurelien.lucchi@inf.ethz.ch; thomas.hofmann@inf.ethz.ch;
   aribeiro@seas.upenn.edu
CR BACH F., 2012, ADV NEURAL INF PROCE, V25, P2672
   Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Bottou L., 2007, ADV NEURAL INFORM PR, V20, P161
   Boyd S., 2004, CONVEX OPTIMIZATION
   Daneshmand H., 2016, P 33 INT C MACH LEAR, P1463
   Defazio A., 2014, ADV NEURAL INFORM PR, P1646
   Erdogdu Murat A, 2015, ADV NEURAL INFORM PR, V2, P3052
   Frostig R., 2015, P C LEARN THEOR PAR, P728
   Gurbuzbalaban M, 2015, MATH PROGRAM, V151, P283, DOI 10.1007/s10107-015-0897-y
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Lucchi Aurelien, 2015, VARIANCE REDUCED STO
   Mokhtari A, 2015, J MACH LEARN RES, V16, P3151
   Mokhtari A, 2014, IEEE T SIGNAL PROCES, V62, P6089, DOI 10.1109/TSP.2014.2357775
   Moritz P., 2016, ARTIF INTELL, P249
   Nesterov  Y., 1998, INTRO LECT CONVEX PR, Vi
   Nesterov  Y., 2007, GRADIENT METHODS MIN
   POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046
   Qu Zheng, 2016, P 33 INT C MACH LEAR, V48, P1823
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Schraudolph N. N., 2007, P 11 INT C ART INT S, P436
   Shalev-Shwartz S, 2016, MATH PROGRAM, V155, P105, DOI 10.1007/s10107-014-0839-0
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Shalev-Shwartz S, 2010, J MACH LEARN RES, V11, P2635
   Vapnik V., 2013, NATURE STAT LEARNING
   Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702009
DA 2019-06-15
ER

PT S
AU Monk, T
   Savin, C
   Lucke, J
AF Monk, Travis
   Savin, Cristina
   Luecke, Joerg
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Neurons Equipped with Intrinsic Plasticity Learn Stimulus Intensity
   Statistics
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID MODEL; EXCITABILITY
AB Experience constantly shapes neural circuits through a variety of plasticity mechanisms. While the functional roles of some plasticity mechanisms are well-understood, it remains unclear how changes in neural excitability contribute to learning. Here, we develop a normative interpretation of intrinsic plasticity (IP) as a key component of unsupervised learning. We introduce a novel generative mixture model that accounts for the class-specific statistics of stimulus intensities, and we derive a neural circuit that learns the input classes and their intensities. We will analytically show that inference and learning for our generative model can be achieved by a neural circuit with intensity-sensitive neurons equipped with a specific form of IP. Numerical experiments verify our analytical derivations and show robust behavior for artificial and natural stimuli. Our results link IP to non-trivial input statistics, in particular the statistics of stimulus intensities for classes to which a neuron is sensitive. More generally, our work paves the way toward new classification algorithms that are robust to intensity variations.
C1 [Monk, Travis; Luecke, Joerg] Carl von Ossietzky Univ Oldenburg, Cluster Excellence Hearing4all, D-26129 Oldenburg, Germany.
   [Savin, Cristina] IST Austria, A-3400 Klosterneuburg, Austria.
RP Monk, T (reprint author), Carl von Ossietzky Univ Oldenburg, Cluster Excellence Hearing4all, D-26129 Oldenburg, Germany.
EM travis.monk@uol.de; csavin@ist.ac.at; joerg.luecke@uol.de
FU DFG within the Cluster of Excellence EXC 1077/1 (Hearing4all); People
   Programme (Marie Curie Actions) of the European Union's Seventh
   Framework Programme (FP7/2007-2013) under REA [291734];  [LU 1196/5-1]
FX We acknowledge funding by the DFG within the Cluster of Excellence EXC
   1077/1 (Hearing4all) and by grant LU 1196/5-1 (JL and TM) and the People
   Programme (Marie Curie Actions) of the European Union's Seventh
   Framework Programme (FP7/2007-2013) under REA grant agreement no. 291734
   (CS).
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Cudmore RH, 2004, J NEUROPHYSIOL, V92, P341, DOI 10.1152/jn.01059.2003
   Daoudal G, 2003, LEARN MEMORY, V10, P456, DOI 10.1101/lm.64103
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Deneve S, 2008, NEURAL COMPUT, V20, P91, DOI 10.1162/neco.2008.20.1.91
   Douglas RJ, 2004, ANNU REV NEUROSCI, V27, P419, DOI 10.1146/annurev.neuro.27.070203.144152
   Habenschuss S., 2012, ADV NEURAL INFORM PR, P773
   Keck C, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002432
   Lucke J, 2004, NEURAL COMPUT, V16, P501, DOI 10.1162/089976604772744893
   Lucke J, 2008, J MACH LEARN RES, V9, P1227
   Lucke J, 2009, NEURAL COMPUT, V21, P2805, DOI 10.1162/neco.2009.07-07-584
   Neal R. M., 1998, LEARNING GRAPHICAL M
   Neftci E, 2013, P NATL ACAD SCI USA, V110, pE3468, DOI 10.1073/pnas.1212083110
   Nessler B., 2009, ADV NEURAL INFORM PR, V22, P1357
   Nessler B, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003037
   OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687
   Rezende D. J., 2011, ADV NEURAL INF PROCE, P136
   Rozell CJ, 2008, NEURAL COMPUT, V20, P2526, DOI 10.1162/neco.2008.03-07-486
   Savin C, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003489
   Savin C, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000757
   Schmuker M, 2014, P NATL ACAD SCI USA, V111, P2081, DOI 10.1073/pnas.1303053111
   Simoncelli E.P., 2000, ADV NEURAL INF PROCE, V13, P166
   Stemmler M, 1999, NAT NEUROSCI, V2, P521, DOI 10.1038/9173
   Wainwright MJ, 2001, APPL COMPUT HARMON A, V11, P89, DOI 10.1006/acha.2000.0350
   Zylberberg J, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002250
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704106
DA 2019-06-15
ER

PT S
AU Montavon, G
   Muller, KR
   Cuturi, M
AF Montavon, Gregoire
   Mueller, Klaus-Robert
   Cuturi, Marco
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Wasserstein Training of Restricted Boltzmann Machines
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID GRADIENT
AB Boltzmann machines are able to learn highly complex, multimodal, structured and multiscale real-world data distributions. Parameters of the model are usually learned by minimizing the Kullback-Leibler (KL) divergence from training samples to the learned model. We propose in this work a novel approach for Boltzmann machine training which assumes that a meaningful metric between observations is known. This metric between observations can then be used to define the Wasserstein distance between the distribution induced by the Boltzmann machine on the one hand, and that given by the training sample on the other hand. We derive a gradient of that distance with respect to the model parameters. Minimization of this new objective leads to generative models with different statistical properties. We demonstrate their practical potential on data completion and denoising, for which the metric between observations plays a crucial role.
C1 [Montavon, Gregoire; Mueller, Klaus-Robert] Tech Univ Berlin, Berlin, Germany.
   [Cuturi, Marco] Univ Paris Saclay, ENSAE, CREST, St Aubin, France.
   [Mueller, Klaus-Robert] Korea Univ, Dept Brain & Cognit Engn, Seoul, South Korea.
RP Montavon, G (reprint author), Tech Univ Berlin, Berlin, Germany.
EM gregoire.montavon@tu-berlin.de; klaus-robert.mueller@tu-berlin.de;
   marco.cuturi@ensae.fr
FU Brain Korea 21 Plus Program through the National Research Foundation of
   Korea - Ministry of Education; DFG [MU 987/17-1]; JSPS [26700002]
FX This work was supported by the Brain Korea 21 Plus Program through the
   National Research Foundation of Korea funded by the Ministry of
   Education. This work was also supported by the grant DFG (MU 987/17-1).
   M. Cuturi gratefully acknowledges the support of JSPS young researcher A
   grant 26700002. Correspondence to GM, KRM and MC.
CR ACKLEY DH, 1985, COGNITIVE SCI, V9, P147
   Bassetti F, 2006, STAT PROBABIL LETT, V76, P1298, DOI 10.1016/j.spl.2006.02.001
   Cho K, 2013, NEURAL COMPUT, V25, P805, DOI 10.1162/NECO_a_00397
   Courty N., 2016, PATTERN ANAL MACHINE
   Cuturi M., 2014, INT C MACH LEARN, P685
   CUTURI M., 2013, ADV NEURAL INFORM PR, V26, P2292
   Dahl G., 2010, ADV NEURAL INFORM PR, P469
   Frogner C., 2015, NIPS, P2044
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Huber P. J., 2011, ROBUST STAT
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Marlin B.M., 2010, P 13 INT C ART INT S, P509
   Montavon Gregoire, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P621, DOI 10.1007/978-3-642-35289-8_33
   Rubner Y., 1997, P ARPA IM UND WORKSH, P661
   Salakhutdinov R., 2009, ARTIF INTELL, V5, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Srivastava N, 2014, J MACH LEARN RES, V15, P2949
   Tang YC, 2012, PROC CVPR IEEE, P2264, DOI 10.1109/CVPR.2012.6247936
   Tieleman T., 2008, P 25 INT C MACH LEAR, P1064, DOI DOI 10.1145/1390156.1390290
   USDA NRCS, 2012, PLANTS DAT
   Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701105
DA 2019-06-15
ER

PT S
AU Monteiro, JM
   Rao, A
   Ashburner, J
   Shawe-Taylor, J
   Mourao-Miranda, J
AF Monteiro, Joao M.
   Rao, Anil
   Ashburner, John
   Shawe-Taylor, John
   Mourao-Miranda, Janaina
BE Rish, I
   Langs, G
   Wehbe, L
   Cecchi, G
   Chang, KMK
   Murphy, B
TI Leveraging Clinical Data to Enhance Localization of Brain Atrophy
SO MACHINE LEARNING AND INTERPRETATION IN NEUROIMAGING, MLINI 2014
SE Lecture Notes in Artificial Intelligence
LA English
DT Proceedings Paper
CT 4th International Workshop on Machine Learning and Interpretation in
   Neuroimaging (MLINI) Held at Conference on Neural Information Processing
   Systems (NIPS)
CY DEC 13, 2014
CL Montreal, CANADA
DE Sparse Canonical Correlation Analysis; Matrix deflation; Neuroimaging;
   Dementia
ID SUPPORT VECTOR MACHINE; ALZHEIMERS-DISEASE; CLASSIFICATION
AB Sparse Canonical Correlation Analysis (SCCA) has been proposed to find pairs of sparse weight vectors that maximize correlations between sets of paired variables. This is done by computing one weight vector pair, deflating the correlation matrix between the views, and then repeating the process to compute the next pair. However, the deflation step used does not guarantee the orthogonality of the vector pairs. This is a very important requirement if one wishes to study the space spanned by these vectors, which should have very promising neuroscience applications. In the present work, we propose a new method for performing the deflation step in SCCA models. The ability of these vector pairs to generalize to new data was tested using an open-access dementia dataset which included T1-weighted MRI images and clinical information. The proposed method provided weight vector pairs that were both orthogonal and able to generalize to new data.
C1 [Monteiro, Joao M.; Rao, Anil; Shawe-Taylor, John; Mourao-Miranda, Janaina] UCL, Dept Comp Sci, London, England.
   [Ashburner, John] UCL, Wellcome Trust Ctr Neuroimaging, Inst Neurol, London, England.
RP Monteiro, JM (reprint author), UCL, Dept Comp Sci, London, England.
EM joao.monteiro@ucl.ac.uk; a.rao@ucl.ac.uk; j.ashburner@ucl.ac.uk;
   j.shawe-taylor@ucl.ac.uk; j.mourao-miranda@ucl.ac.uk
OI Shawe-Taylor, John/0000-0002-2030-0073
CR Ashburner J, 2007, NEUROIMAGE, V38, P95, DOI 10.1016/j.neuroimage.2007.07.007
   Boutte D., 2010, 2010 IEEE INT C BIOI, P422, DOI DOI 10.1109/BIBM.2010.5706603
   Chi EC, 2013, I S BIOMED IMAGING, P740
   Ecker C, 2010, J NEUROSCI, V30, P10612, DOI 10.1523/JNEUROSCI.5413-09.2010
   Gretton A, 2005, LECT NOTES ARTIF INT, V3734, P63
   Kloppel S, 2008, BRAIN, V131, P681, DOI 10.1093/brain/awm319
   Marcus DS, 2010, J COGNITIVE NEUROSCI, V22, P2677, DOI 10.1162/jocn.2009.21407
   Mourao-Miranda J, 2005, NEUROIMAGE, V28, P980, DOI 10.1016/j.neuroimage.2005.06.070
   Nouretdinov I, 2011, NEUROIMAGE, V56, P809, DOI 10.1016/j.neuroimage.2010.05.023
   Orru G, 2012, NEUROSCI BIOBEHAV R, V36, P1140, DOI 10.1016/j.neubiorev.2012.01.004
   Parkhomenko Elena, 2009, Stat Appl Genet Mol Biol, V8, P1, DOI 10.2202/1544-6115.1406
   Rao A, 2011, IEEE ENG MED BIO, P4499, DOI 10.1109/IEMBS.2011.6091115
   Shawe-Taylor John, 2004, KERNEL METHODS PATTE
   Wan J, 2011, LECT NOTES COMPUT SC, V6892, P376, DOI 10.1007/978-3-642-23629-7_46
   Witten DM, 2009, STAT APPL GENET MOL, V8, DOI 10.2202/1544-6115.1470
   Witten DM, 2009, BIOSTATISTICS
NR 16
TC 0
Z9 0
U1 0
U2 2
PU SPRINGER INT PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
BN 978-3-319-45173-2; 978-3-319-45174-9
J9 LECT NOTES ARTIF INT
PY 2016
VL 9444
BP 60
EP 68
DI 10.1007/978-3-319-45174-9_7
PG 9
WC Computer Science, Artificial Intelligence; Neuroimaging
SC Computer Science; Neurosciences & Neurology
GA BG4FZ
UT WOS:000388723400007
DA 2019-06-15
ER

PT S
AU Montgomery, W
   Levine, S
AF Montgomery, William
   Levine, Sergey
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Guided Policy Search via Approximate Mirror Descent
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Guided policy search algorithms can be used to optimize complex nonlinear policies, such as deep neural networks, without directly computing policy gradients in the high-dimensional parameter space. Instead, these methods use supervised learning to train the policy to mimic a "teacher" algorithm, such as a trajectory optimizer or a trajectory-centric reinforcement learning method. Guided policy search methods provide asymptotic local convergence guarantees by construction, but it is not clear how much the policy improves within a small, finite number of iterations. We show that guided policy search algorithms can be interpreted as an approximate variant of mirror descent, where the projection onto the constraint manifold is not exact. We derive a new guided policy search algorithm that is simpler and provides appealing improvement and convergence guarantees in simplified convex and linear settings, and show that in the more general nonlinear setting, the error in the projection step can be bounded. We provide empirical results on several simulated robotic navigation and manipulation tasks that show that our method is stable and achieves similar or better performance when compared to prior guided policy search methods, with a simpler formulation and fewer hyperparameters.
C1 [Montgomery, William; Levine, Sergey] Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA.
RP Montgomery, W (reprint author), Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA.
EM wmonty@cs.washington.edu; svlevine@cs.washington.edu
FU ONR Young Investigator Program award
FX We thank the anonymous reviewers for their helpful and constructive
   feedback. This research was supported in part by an ONR Young
   Investigator Program award.
CR Bagnell J. A., 2003, INT JOINT C ART INT
   Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6
   Deisenroth M. P., 2013, FDN TRENDS ROBOTICS, V2, P1, DOI DOI 10.1561/2300000021
   Guo  X., 2014, ADV NEURAL INFORM PR
   Levine S., 2014, ADV NEURAL INFORM PR
   Levine S., 2015, INT C ROB AUT ICRA
   Levine S., 2013, ADV NEURAL INFORM PR
   Levine S, 2016, J MACH LEARN RES, V17
   Lillicrap T. P., 2016, INT C LEARN REPR ICL
   MORDATCH I., 2014, ROBOTICS SCI SYSTEMS
   Mordatch Igor, 2015, ADV NEURAL INFORM PR
   Peters  J., 2010, AAAI C ART INT
   Peters J, 2008, NEURAL NETWORKS, V21, P682, DOI 10.1016/j.neunet.2008.02.003
   Ross S., 2011, J MACHINE LEARNING R, P627
   Ross S., 2013, INT C ROB AUT ICRA
   Schulman J., 2015, INT C MACH LEARN ICM
   Song J, 2004, I C CONT AUTOMAT ROB, P2223
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Zhang T., 2016, INT C ROB AUT ICRA
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700069
DA 2019-06-15
ER

PT S
AU Moon, T
   Min, S
   Lee, B
   Yoon, S
AF Moon, Taesup
   Min, Seonwoo
   Lee, Byunghan
   Yoon, Sungroh
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Neural Universal Discrete Denoiser
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID ERRORS
AB We present a new framework of applying deep neural networks (DNN) to devise a universal discrete denoiser. Unlike other approaches that utilize supervised learning for denoising, we do not require any additional training data. In such setting, while the ground-truth label, i.e., the clean data, is not available, we devise "pseudolabels" and a novel objective function such that DNN can be trained in a same way as supervised learning to become a discrete denoiser. We experimentally show that our resulting algorithm, dubbed as Neural DUDE, significantly outperforms the previous state-of-the-art in several applications with a systematic rule of choosing the hyperparameter, which is an attractive feature in practice.
C1 [Moon, Taesup] DGIST, Daegu 42988, South Korea.
   [Min, Seonwoo; Lee, Byunghan; Yoon, Sungroh] Seoul Natl Univ, Seoul 08826, South Korea.
RP Moon, T (reprint author), DGIST, Daegu 42988, South Korea.
EM tsmoon@dgist.ac.kr; mswzeus@snu.ac.kr; styxkr@snu.ac.kr;
   sryoon@snu.ac.kr
FU DGIST Faculty Start-up Fund [2016010060]; Basic Science Research Program
   through the National Research Foundation of Korea [2016R1C1B2012170];
   Ministry of Science, ICT and Future Planning; Brain Korea 21 Plus
   Project (SNU ECE)
FX T. Moon was supported by DGIST Faculty Start-up Fund (2016010060) and
   Basic Science Research Program through the National Research Foundation
   of Korea (2016R1C1B2012170), both funded by Ministry of Science, ICT and
   Future Planning. S. Min, B. Lee, and S. Yoon were supported in part by
   Brain Korea 21 Plus Project (SNU ECE) in 2016.
CR Bastien Frederic, 2012, NIPS WORKSH DEEP LEA
   Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223
   Benitez-Paez A., 2015, BIORXIV021758
   Burger H. C., 2012, CVPR
   Goodwin S., 2015, GENOME RES
   Jain M, 2015, NAT METHODS, V12, P351, DOI [10.1038/nmeth.3290, 10.1038/NMETH.3290]
   Jain V., 2008, NIPS
   Kingma D. P., 2015, ICLR
   Laehnemann D, 2016, BRIEF BIOINFORM, V17, P154, DOI 10.1093/bib/bbv029
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee B., 2016, ARXIV151104836
   Moon T., 2009, IEEE T INFORM THEORY
   Motta G, 2011, IEEE T IMAGE PROCESS, V20, P1, DOI 10.1109/TIP.2010.2053939
   Nesterov Y., 1983, SOV MATH DOKL, V27, P372
   Ordentlich E., 2003, IEEE ICIP
   Ordentlich E, 2008, IEEE T INFORM THEORY, V54, P2243, DOI 10.1109/TIT.2008.920187
   Salmela L, 2011, BIOINFORMATICS, V27, P1455, DOI 10.1093/bioinformatics/btr170
   STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632
   Tieleman, 2012, 65 U TOR
   Weissman T, 2005, IEEE T INFORM THEORY, V51, P5, DOI 10.1109/TIT.2004.839518
   Weissman T, 2007, IEEE T INFORM THEORY, V53, P1253, DOI 10.1109/TIT.2007.892782
   Xie J, 2012, NIPS
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704022
DA 2019-06-15
ER

PT S
AU Munos, R
   Stepleton, T
   Harutyunyan, A
   Bellemare, MG
AF Munos, Remi
   Stepleton, Thomas
   Harutyunyan, Anna
   Bellemare, Marc G.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Safe and efficient off-policy reinforcement learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID CONVERGENCE
AB In this work, we take a fresh look at some old and new algorithms for off-policy, return-based reinforcement learning. Expressing these in a common form, we derive a novel algorithm, Retrace(lambda), with three desired properties: (1) it has low variance; (2) it safely uses samples collected from any behaviour policy, whatever its degree of "off-policyness"; and (3) it is efficient as it makes the best use of samples collected from near on-policy behaviour policies. We analyze the contractive nature of the related operator under both off-policy policy evaluation and control settings and derive online sample-based algorithms. We believe this is the first return-based off-policy control algorithm converging a.s. to Q* without the GLIE assumption (Greedy in the Limit with Infinite Exploration). As a corollary, we prove the convergence of Watkins' Q(lambda), which was an open problem since 1989. We illustrate the benefits of Retrace(lambda) on a standard suite of Atari 2600 games.
C1 [Munos, Remi; Stepleton, Thomas; Bellemare, Marc G.] Google DeepMind, London, England.
   [Harutyunyan, Anna] Vrije Univ Brussel, Brussels, Belgium.
RP Munos, R (reprint author), Google DeepMind, London, England.
EM munos@google.com; stepleton@google.com; anna.harutyunyan@vub.ac.be;
   bellemare@google.com
CR Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912
   Bertsekas D.P., 1996, NEURODYNAMIC PROGRAM
   Geist M, 2014, J MACH LEARN RES, V15, P289
   Hallak A., 2015, ARXIV150905172
   Harutyunyan A., 2016, Q OFF POLICY CORRECT
   Kearns Michael J, 2000, P 13 ANN C COMP LEAR, P142
   Li L., 2015, P 18 INT C ART INT S
   Lin L., 1993, P 10 INT C MACH LEAR, P182
   Mahmood A. R., 2015, C UNC ART INT
   Mahmood A. R., 2015, ARXIV150701569
   Mnih V., 2016, P INT C MACH LEARN
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Precup Doina, 2001, P 18 INT C MACH LEAR, P417
   Precup Doina, 2000, P 17 INT C MACH LEAR
   Puterman M. L., 1994, MARKOV DECISION PROC
   Schaul Tom, 2016, INT C LEARN REPR
   Singh S, 2000, MACH LEARN, V38, P287, DOI 10.1023/A:1007678930559
   Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479
   Sutton R. S., 1996, ADV NEURAL INFORM PR, V8
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Tsitsiklis JN, 2003, J MACH LEARN RES, V3, P59, DOI 10.1162/153244303768966102
   Watkins C. J. C. H., 1989, THESIS
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704062
DA 2019-06-15
ER

PT S
AU Murugesan, K
   Liu, HX
   Carbonell, J
   Yang, YM
AF Murugesan, Keerthiram
   Liu, Hanxiao
   Carbonell, Jaime
   Yang, Yiming
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Adaptive Smoothed Online Multi-Task Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB This paper addresses the challenge of jointly learning both the per-task model parameters and the inter-task relationships in a multi-task online learning setting. The proposed algorithm features probabilistic interpretation, efficient updating rules and flexible modulation on whether learners focus on their specific task or on jointly address all tasks. The paper also proves a sub-linear regret bound as compared to the best linear predictor in hindsight. Experiments over three multi-task learning benchmark datasets show advantageous performance of the proposed approach over several state-of-the-art online multi-task learning baselines.
C1 [Murugesan, Keerthiram; Liu, Hanxiao; Carbonell, Jaime; Yang, Yiming] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
RP Murugesan, K (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM kmuruges@cs.cmu.edu; hanxiaol@cs.cmu.edu; jgc@cs.cmu.edu;
   yiming@cs.cmu.edu
FU NSF [IIS-1216282, IIS-1546329]
FX This work is supported in part by NSF under grants IIS-1216282 and
   IIS-1546329.
CR Abernethy J, 2007, LECT NOTES COMPUT SC, V4539, P484, DOI 10.1007/978-3-540-72927-3_35
   Agarwal Alekh, 2008, UCBEECS2008138
   Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8
   Blitzer J., 2007, ANN M ASS COMP LING, V7, P440, DOI DOI 10.1109/IRPS.2011.5784441
   Cavallanti G, 2010, J MACH LEARN RES, V11, P2901
   Crammer K, 2006, J MACH LEARN RES, V7, P551
   Crammer K., 2012, ADV NEURAL INFORM PR, P1475
   Dekel O, 2007, J MACH LEARN RES, V8, P2233
   Jiang L., 2014, P ADV NEUR INF PROC, P2078
   Kshirsagar M, 2013, BIOINFORMATICS, V29, P217, DOI 10.1093/bioinformatics/btt245
   Kshirsagar Meghana, 2013, NIPS WORKSH MACH LEA
   Kumar M. P., 2010, ADV NEURAL INFORM PR, P1189
   Lugosi Gabor, 2009, ARXIV09023526
   Nemirovsky A-S, 1982, PROBLEM COMPLEXITY M
   Saha Avishek, 2011, INT C ART INT STAT, P643
   Shalev-Shwartz S., 2007, THESIS
   Weinberger K., 2009, P 26 ANN INT C MACH, P1113, DOI DOI 10.1145/1553374.1553516
   Xue Y, 2007, J MACH LEARN RES, V8, P35
   Zhang Y, 2014, ACM T KNOWL DISCOV D, V8, DOI 10.1145/2538028
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703070
DA 2019-06-15
ER

PT S
AU Namkoong, H
   Duchi, JC
AF Namkoong, Hongseok
   Duchi, John C.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Stochastic Gradient Methods for Distributionally Robust Optimization
   with f-divergences
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We develop efficient solution methods for a robust empirical risk minimization problem designed to give calibrated confidence intervals on performance and provide optimal tradeoffs between bias and variance. Our methods apply to distributionally robust optimization problems proposed by Ben-Tal et al., which put more weight on observations inducing high loss via a worst-case approach over a non-parametric uncertainty set on the underlying data distribution. Our algorithm solves the resulting minimax problems with nearly the same computational cost of stochastic gradient descent through the use of several carefully designed data structures. For a sample of size n, the per-iteration cost of our method scales as O(log n), which allows us to give optimality certificates that distributionally robust optimization provides at little extra cost compared to empirical risk minimization and stochastic gradient methods.
C1 [Namkoong, Hongseok; Duchi, John C.] Stanford Univ, Stanford, CA 94305 USA.
RP Namkoong, H (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM hnamk@stanford.edu; jduchi@stanford.edu
FU SAIL-Toyota Center for AI Research; National Science Foundation
   [NSF-CAREER-1553086]; Samsung
FX JCD and HN were partially supported by the SAIL-Toyota Center for AI
   Research and the National Science Foundation award NSF-CAREER-1553086.
   HN was also partially supported Samsung Fellowship.
CR Audibert J-Y, 2010, J MACHINE LEARNING R, V11, P2635
   Ben-Tal A, 2015, OPER RES, V63, P628, DOI 10.1287/opre.2015.1374
   Ben-Tal A, 2013, MANAGE SCI, V59, P341, DOI 10.1287/mnsc.1120.1641
   BenTal A, 2009, PRINC SER APPL MATH, P1
   Borwein J, 2009, P AM MATH SOC, V137, P1081
   Boucheron Stephane, 2005, ESAIM-PROBAB STAT, V9, P323, DOI DOI 10.1051/ps:2005018
   Boyd S., 2004, CONVEX OPTIMIZATION
   Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Clarkson K., 2012, J ASS COMPUTING MACH, V59
   Cormen T. H., 2001, INTRO ALGORITHMS
   CRESSIE N, 1984, J ROY STAT SOC B MET, V46, P440
   Duchi J. C., 2016, ARXIV161003425STATML
   Duchi J. C., 2016, ARXIV161002581STATML
   Duchi J. C., 2008, P 25 INT C MACH LEAR
   Ghadimi S, 2012, SIAM J OPTIMIZ, V22, P1469, DOI 10.1137/110848864
   Hazan E., 2011, P 24 ANN C COMP LEAR
   Hazan E, 2012, OPTIMIZATION FOR MACHINE LEARNING, P287
   Hiriart-Urruty J.-B., 1993, CONVEX ANAL MINIMIZA, VI
   Hiriart-Urruty JB, 1993, CONVEX ANAL MINIMIZA
   Lewis DD, 2004, J MACH LEARN RES, V5, P361
   Lichman M., 2013, UCI MACHINE LEARNING
   Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277
   Optimization G., 2015, GUROBI OPTIMIZER REF
   Owen A. B., 2001, EMPIRICAL LIKELIHOOD
   Shalev-Shwartz S., 2016, P 32 INT C MACH LEAR
   Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN
   Zinkevich M., 2003, P 20 INT C MACH LEAR
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700005
DA 2019-06-15
ER

PT S
AU Nan, F
   Wang, J
   Saligrama, V
AF Nan, Feng
   Wang, Joseph
   Saligrama, Venkatesh
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Pruning Random Forests for Prediction on a Budget
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We propose to prune a random forest (RF) for resource-constrained prediction. We first construct a RF and then prune it to optimize expected feature cost & accuracy. We pose pruning RFs as a novel 0-1 integer program with linear constraints that encourages feature re-use. We establish total unimodularity of the constraint set to prove that the corresponding LP relaxation solves the original integer program. We then exploit connections to combinatorial optimization and develop an efficient primal-dual algorithm, scalable to large datasets. In contrast to our bottom-up approach, which benefits from good RF initialization, conventional methods are top-down acquiring features based on their utility value and is generally intractable, requiring heuristics. Empirically, our pruning algorithm outperforms existing state-of-the-art resource-constrained algorithms.
C1 [Nan, Feng] Boston Univ, Syst Engn, Boston, MA 02215 USA.
   [Wang, Joseph; Saligrama, Venkatesh] Boston Univ, Elect Engn, Boston, MA 02215 USA.
RP Nan, F (reprint author), Boston Univ, Syst Engn, Boston, MA 02215 USA.
EM fnan@bu.edu; joewang@bu.edu; srv@bu.edu
FU NSF [CCF: 1320566, CNS: 1330008, CCF: 1527618]; DHS
   [2013-ST-061-ED0001]; ONR [50202168]; US AF [FA8650-14-C-1728]
FX We thank Dr Kilian Weinberger for helpful discussions and Dr David
   Castanon for the insights on the primal dual algorithm. This material is
   based upon work supported in part by NSF Grants CCF: 1320566, CNS:
   1330008, CCF: 1527618, DHS 2013-ST-061-ED0001, ONR Grant 50202168 and US
   AF contract FA8650-14-C-1728.
CR Benbouzid Djalel, 2014, THESIS
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Breiman L., 1984, CLASSIFICATION REGRE
   Chakaravarthy VT, 2011, ACM T ALGORITHMS, V7, DOI 10.1145/1921659.1921661
   Chapelle O., 2011, P YAH LEARN RANK CHA
   Frank A., 2010, UCI MACHINE LEARNING
   Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451
   Gao  T., 2011, ADV NEURAL INFORM PR
   Gurobi Optimization Inc, 2015, GUROBI OPTIMIZER REF
   Kulkarni V. Y., 2012, INT C DAT SCI ENG IC
   Kusner M., 2014, AAAI
   Lazebnik S., 2006, IEEE CVPR
   Li L., 2010, NIPS
   Li XB, 2001, INFORMS J COMPUT, V13, P332, DOI 10.1287/ijoc.13.4.332.9732
   Nan F., 2015, P 32 INT C MACH LEAR
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Sherali HD, 2009, INFORMS J COMPUT, V21, P49, DOI 10.1287/ijoc.1080.0278
   Trapeznikov K., 2013, P 16 INT C ART INT S, P581
   Wang Joseph, 2015, ADV NEURAL INFORM PR
   Xu Z., 2013, P 30 INT C MACH LEAR
   Xu Z. E., 2012, P INT C MACH LEARN I
   Yi Zhang, 2005, WORKING PAPER
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701107
DA 2019-06-15
ER

PT S
AU Natarajan, N
   Jain, P
AF Natarajan, Nagarajan
   Jain, Prateek
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Regret Bounds for Non-decomposable Metrics with Missing Labels
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We consider the problem of recommending relevant labels (items) for a given data point (user). In particular, we are interested in the practically important setting where the evaluation is with respect to non-decomposable (over labels) performance metrics like the F-1 measure, and training data has missing labels. To this end, we propose a generic framework that given a performance metric 41, can devise a regularized objective function and a threshold such that all the values in the predicted score vector above and only above the threshold are selected to be positive. We show that the regret or generalization error in the given metric W is bounded ultimately by estimation error of certain underlying parameters. In particular, we derive regret bounds under three popular settings: a) collaborative filtering, b) multilahel classification, and c) PU (positive-unlabeled) learning. For each of the above problems, we can obtain precise non-asymptotic regret bound which is small even when a large fraction of labels is missing. Our empirical results on synthetic and benchmark datasets demonstrate that by explicitly modeling for missing labels and optimizing the desired performance metric, our algorithm indeed achieves significantly better performance (like F-1 score) when compared to methods that do not model missing label information carefully.
C1 [Natarajan, Nagarajan; Jain, Prateek] Microsoft Res, Bengaluru, India.
RP Natarajan, N (reprint author), Microsoft Res, Bengaluru, India.
EM t-nanata@microsoft.com; prajain@microsoft.com
CR Agarwal S, 2014, J MACH LEARN RES, V15, P1653
   Bhatia K., 2015, ADV NEURAL INFORM PR, P721
   Bhatia K., 2015, ADV NEURAL INFORM PR, P730
   Cai T, 2013, J MACH LEARN RES, V14, P3619
   Davenport MA, 2014, INF INFERENCE, V3, P189, DOI 10.1093/imaiai/iau006
   Dembczynski Krzysztof, 2012, P 29 INT C MACH LEAR, V2012
   Gao W, 2013, ARTIF INTELL, V199, P22, DOI 10.1016/j.artint.2013.03.001
   Hsieh C. J., 2015, P 32 INT C MACH LEAR, P2445
   JAIN P, 2013, ARXIV13060626
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Kotlowski Wojciech, 2015, ARXIV150407272
   Koyejo Oluwasanmi O, 2015, ADV NEURAL INFORM PR, P3303
   Lafond J., 2015, ARXIV150206919
   Nagarajan N., 2014, ADV NEURAL INFORM PR, P2744
   Prabhu Y., 2014, P 20 ACM SIGKDD INT, P263
   Reid MD, 2010, J MACH LEARN RES, V11, P2387
   Vershynin  Roman, 2010, ARXIV10113027
   Yu H., 2014, P 31 INT C MACH LEAR, P593
   Yun H., 2014, NIPS, P2582
   Zhong Kai, 2015, INT C ALG LEARN THEO
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701035
DA 2019-06-15
ER

PT S
AU Ndiaye, E
   Fercoq, O
   Gramfort, A
   Salmon, J
AF Ndiaye, Eugene
   Fercoq, Olivier
   Gramfort, Alexandre
   Salmon, Joseph
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI GAP Safe Screening Rules for Sparse-Group Lasso
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID ALGORITHMS; REGRESSION; SELECTION
AB For statistical learning in high dimension, sparse regularizations have proven useful to boost both computational and statistical efficiency. In some contexts, it is natural to handle more refined structures than pure sparsity, such as for instance group sparsity. Sparse-Group Lasso has recently been introduced in the context of linear regression to enforce sparsity both at the feature and at the group level. We propose the first (provably) safe screening rules for Sparse-Group Lasso, i.e., rules that allow to discard early in the solver features/groups that are inactive at optimal solution. Thanks to efficient dual gap computations relying on the geometric properties of epsilon-norm, safe screening rules for Sparse-Group Lasso lead to significant gains in term of computing time for our coordinate descent implementation.
C1 [Ndiaye, Eugene; Fercoq, Olivier; Gramfort, Alexandre; Salmon, Joseph] Univ Paris Saclay, Telecom ParisTech, CNRS, LTCI, F-75013 Paris, France.
RP Ndiaye, E (reprint author), Univ Paris Saclay, Telecom ParisTech, CNRS, LTCI, F-75013 Paris, France.
EM eugene.ndiaye@telecom-paristech.fr; olivier.fercoq@telecom-paristech.fr;
   alexandre.gramfort@telecom-paristech.fr;
   joseph.salmon@telecom-paristech.fr
FU ANR THALAMEEG [ANR-14-NEUC-0002-01]; NIH [R01 MH106174]; ERC Starting
   Grant SLAB [ERC-YStG-676943]; Chair Machine Learning for Big Data at
   Telecom ParisTech
FX this work was supported by the ANR THALAMEEG ANR-14-NEUC-0002-01, the
   NIH R01 MH106174, by ERC Starting Grant SLAB ERC-YStG-676943 and by the
   Chair Machine Learning for Big Data at Telecom ParisTech.
CR Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8
   Bonnefoy A., 2014, EUSIPCO
   Bonnefoy A, 2015, IEEE T SIGNAL PROCES, V63, P5121, DOI 10.1109/TSP.2015.2447503
   Borwein J., 2006, CONVEX ANAL NONLINEA
   Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9
   Burdakov O., 1988, 33 INT WISS K FORTR, P15
   Burdakov O., 2001, TECH REP
   Chatterjee S., 2012, P 2012 SIAM INT C DA, P47
   El Ghaoui L, 2012, PAC J OPTIM, V8, P667
   Fercoq  O., 2015, ICML, V37, P333
   Friedman J, 2007, ANN APPL STAT, V1, P302, DOI 10.1214/07-AOAS131
   Jenatton R, 2011, J MACH LEARN RES, V12, P2297
   Johnson T. B., 2015, ICML, P1171
   Kalnay E, 1996, B AM METEOROL SOC, V77, P437, DOI 10.1175/1520-0477(1996)077<0437:TNYRP>2.0.CO;2
   Ndiaye E, 2015, NIPS, P811
   Qin ZW, 2013, MATH PROGRAM COMPUT, V5, P143, DOI 10.1007/s12532-013-0051-x
   Simon N, 2013, J COMPUT GRAPH STAT, V22, P231, DOI 10.1080/10618600.2012.681250
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Tibshirani R, 2012, J R STAT SOC B, V74, P245, DOI 10.1111/j.1467-9868.2011.01004.x
   Wang J., 2014, ARXIV14104210
   Xiang Z. J., 2011, P ADV NEUR INF PROC, V24, P900
   Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703041
DA 2019-06-15
ER

PT S
AU Neil, D
   Pfeiffer, M
   Liu, SC
AF Neil, Daniel
   Pfeiffer, Michael
   Liu, Shih-Chii
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Phased LSTM: Accelerating Recurrent Network Training for Long or
   Event-based Sequences
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID TIME
AB Recurrent Neural Networks (RNNs) have become the state-of-the-art choice for extracting patterns from temporal sequences. However, current RNN models are ill-suited to process irregularly sampled data triggered by events generated in continuous time by sensors or other neurons. Such data can occur, for example, when the input comes from novel event-driven artificial sensors that generate sparse, asynchronous streams of events or from multiple conventional sensors with different update intervals. In this work, we introduce the Phased LSTM model, which extends the LSTM unit by adding a new time gate. This gate is controlled by a parametrized oscillation with a frequency range that produces updates of the memory cell only during a small percentage of the cycle. Even with the sparse updates imposed by the oscillation, the Phased LSTM network achieves faster convergence than regular LSTMs on tasks which require learning of long sequences. The model naturally integrates inputs from sensors of arbitrary sampling rates, thereby opening new areas of investigation for processing asynchronous sensory events that carry timing information. It also greatly improves the performance of LSTMs in standard RNN applications, and does so with an order-of-magnitude fewer computes at runtime.
C1 [Neil, Daniel] Univ Zurich, Inst Neuroinformat, CH-8057 Zurich, Switzerland.
   Swiss Fed Inst Technol, CH-8057 Zurich, Switzerland.
RP Neil, D (reprint author), Univ Zurich, Inst Neuroinformat, CH-8057 Zurich, Switzerland.
EM dneil@ini.uzh.ch; pfeiffer@ini.uzh.ch; shih@ini.uzh.ch
CR Bahdanau D., 2014, ARXIV14090473
   Bergstra J, 2010, P PYTH SCI COMP C SC, P3
   Buzsaki G., 2006, RHYTHMS BRAIN
   Cauwenberghs G, 1996, IEEE T NEURAL NETWOR, V7, P346, DOI 10.1109/72.485671
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Cohen GK, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00184
   Cooke M, 2006, J ACOUST SOC AM, V120, P2421, DOI 10.1121/1.2229005
   Dieleman S, 2015, LASAGNE 1 RELEASE
   FUNAHASHI K, 1993, NEURAL NETWORKS, V6, P801, DOI 10.1016/S0893-6080(05)80125-X
   Gers FA, 2000, IEEE IJCNN, P189, DOI 10.1109/IJCNN.2000.861302
   Graves A, 2013, ARXIV13080850
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Hannun A., 2014, ARXIV14125567
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Itseez, 2015, OP SOURC COMP VIS LI
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Koutnik J., 2014, ARXIV14023511
   Mikolov T, 2010, NTFRSPEECH, V2, P3
   Neil D., 2016, IEEE INT S CIRC SYST
   Nessler B, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003037
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Orchard G., 2015, ARXIV150707629
   Pearlmutter BA, 1989, NEURAL COMPUT, V1, P263, DOI 10.1162/neco.1989.1.2.263
   Posch C, 2014, P IEEE, V102, P1470, DOI 10.1109/JPROC.2014.2346153
   Semeniuta S., 2016, ARXIV160305118
   Spinoulas L, 2015, IEEE COMPUT SOC CONF
   Wand M., 2016, ARXIV160108188
   Xu K., 2015, INT C MACH LEARN
NR 29
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702057
DA 2019-06-15
ER

PT S
AU Newling, J
   Fleuret, F
AF Newling, James
   Fleuret, Francois
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Nested Mini-Batch K-Means
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB A new algorithm is proposed which accelerates the mini-batch k-means algorithm of Sculley (2010) by using the distance bounding approach of Elkan (2003). We argue that, when incorporating distance bounds into a mini-batch algorithm, already used data should preferentially be reused. To this end we propose using nested mini-batches, whereby data in a mini-batch at iteration t is automatically reused at iteration t + 1.
   Using nested mini-batches presents two difficulties. The first is that unbalanced use of data can bias estimates, which we resolve by ensuring that each data sample contributes exactly once to centroids. The second is in choosing mini-batch sizes, which we address by balancing premature fine-tuning of centroids with redundancy induced slow-down. Experiments show that the resulting nmbatch algorithm is very effective, often arriving within 1% of the empirical minimum 100x earlier than the standard mini-batch algorithm.
C1 [Newling, James; Fleuret, Francois] Idiap Res Inst, Martigny, Switzerland.
   [Newling, James; Fleuret, Francois] Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
RP Newling, J (reprint author), Idiap Res Inst, Martigny, Switzerland.; Newling, J (reprint author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
EM james.newling@idiap.ch; francois.fleuret@idiap.ch
FU Hasler Foundation [13018 MASH2]
FX James Newling was funded by the Hasler Foundation under the grant 13018
   MASH2.
CR AGARWAL P., 2005, COMBINATORIAL COMPUT, V52, P1
   Bottou L., 1995, Advances in Neural Information Processing Systems 7, P585
   Celebi ME, 2013, EXPERT SYST APPL, V40, P200, DOI 10.1016/j.eswa.2012.07.021
   Coates A, 2011, P 14 INT C ART INT S, P215
   Ding Yufei, 2015, P 32 INT C MACH LEAR, P579
   Drake J, 2013, FASTER K MEANS CLUST
   Elkan C., 2003, ICML, V3, P147
   Hamerly G., 2010, P 2010 SIAM INT C DA, P130, DOI DOI 10.1137/1.9781611972801.12
   Kanungo T, 2002, IEEE T PATTERN ANAL, V24, P881, DOI 10.1109/TPAMI.2002.1017616
   Lewis DD, 2004, J MACH LEARN RES, V5, P361
   Loosli G., 2007, LARGE SCALE KERNEL M, p[301, 6]
   Mairal J., 2009, P 26 ANN INT C MACH, P689, DOI DOI 10.1145/1553374.1553463
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Pelleg D., 1999, P 5 ACM SIGKDD INT C, P277, DOI DOI 10.1145/312129.312248
   Phillips S, 2002, LECT NOTES COMPUTER, V2409
   Sculley D., 2010, P INT C WORLD WID WE, P1177, DOI [10.1145/1772690.1772862, DOI 10.1145/1772690.1772862]
   WANG J, 2012, COMPUTER VISION PATT, P3037
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704006
DA 2019-06-15
ER

PT S
AU Neykov, M
   Wang, ZR
   Liu, H
AF Neykov, Matey
   Wang, Zhaoran
   Liu, Han
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Agnostic Estimation for Misspecified Phase Retrieval Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID REGRESSION
AB The goal of noisy high-dimensional phase retrieval is to estimate an s -sparse parameter beta* is an element of R-d from n realizations of the model Y = (X(sic)beta*)(2) + epsilon. Based on this model, we propose a significant semi-parametric generalization called misspecified phase retrieval (MPR), in which Y = f(X(sic)beta*, epsilon) with unknown f and Cov(Y; (X>beta*)(2)) > 0. For example, MPR encompasses Y = h((X(sic)vertical bar beta*vertical bar) + epsilon with increasing h as a special case. Despite the generality of the MPR model, it eludes the reach of most existing semi-parametric estimators. In this paper, we propose an estimation procedure, which consists of solving a cascade of two convex programs and provably recovers the direction of beta*. Our theory is backed up by thorough numerical results.
C1 [Neykov, Matey; Wang, Zhaoran; Liu, Han] Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA.
RP Neykov, M (reprint author), Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA.
EM mneykov@princeton.edu; zhaoran@princeton.edu; hanliu@princeton.edu
CR Adamczak R, 2015, PROBAB THEORY REL, V162, P531, DOI 10.1007/s00440-014-0579-3
   Berthet Q., 2013, C LEARN THEOR
   Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620
   Boufounos P. T., 2008, ANN C INF SCI SYST
   Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9
   Cai T. T., 2015, ARXIV150603382
   Candes EJ, 2015, APPL COMPUT HARMON A, V39, P277, DOI 10.1016/j.acha.2014.09.004
   Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924
   Candes EJ, 2013, COMMUN PUR APPL MATH, V66, P1241, DOI 10.1002/cpa.21432
   Chen Y., 2013, ARXIV13127006
   Cook R. D., 2005, J AM STAT ASS, V100
   d'Aspremont A, 2007, SIAM REV, V49, P434, DOI 10.1137/050645506
   Ganti R., 2015, ARXIV150608910
   Gao C., 2014, ARXIV14098565
   Genzel M., 2016, ARXIV160203436
   Han F., 2015, ARXIV150907158
   Horowitz JL, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-92870-8_1
   Laurent B, 2000, ANN STAT, V28, P1302
   Lecue G., 2013, ARXIV13115024
   LI KC, 1992, J AM STAT ASSOC, V87, P1025, DOI 10.2307/2290640
   LI KC, 1991, J AM STAT ASSOC, V86, P316, DOI 10.2307/2290563
   LI KC, 1989, ANN STAT, V17, P1009, DOI 10.1214/aos/1176347254
   McCullagh P., 1989, GEN LINEAR MODELS
   Neykov Matey, 2016, J MACHINE LEARNING R, V17, P1
   Peng H, 2011, J STAT PLAN INFER, V141, P1362, DOI 10.1016/j.jspi.2010.10.003
   Plan Y., 2015, IEEE T INFORM THEORY
   Radchenko P, 2015, J MULTIVARIATE ANAL, V139, P266, DOI 10.1016/j.jmva.2015.02.007
   Raskutti G, 2010, J MACH LEARN RES, V11, P2241
   STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632
   Thrampoulidis C., 2015, ARXIV150602181
   Vershynin  Roman, 2010, ARXIV10113027
   Wang Z., 2015, ARXIV151208861
   Xia YC, 1999, J AM STAT ASSOC, V94, P1275, DOI 10.2307/2669941
   Nguyen X, 2012, IEEE INT SYMP INFO
   Yang Zhuoran, 2015, ARXIV151104514
   Yi X., 2015, ADV NEURAL INFORM PR
   Yuan XT, 2013, J MACH LEARN RES, V14, P899
NR 37
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700058
DA 2019-06-15
ER

PT S
AU Neyshabur, B
   Wu, YH
   Salakhutdinov, R
   Srebro, N
AF Neyshabur, Behnam
   Wu, Yuhuai
   Salakhutdinov, Ruslan
   Srebro, Nathan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Path-Normalized Optimization of Recurrent Neural Networks with ReLU
   Activations
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We investigate the parameter-space geometry of recurrent neural networks (RNNs), and develop an adaptation of path-SGD optimization method, attuned to this geometry, that can learn plain RNNs with ReLU activations. On several datasets that require capturing long-term dependency structure, we show that path-SGD can significantly improve trainability of ReLU RNNs compared to RNNs trained with SGD, even with various recently suggested initialization schemes.
C1 [Neyshabur, Behnam; Srebro, Nathan] Toyota Technol Inst, Chicago, IL 60637 USA.
   [Wu, Yuhuai] Univ Toronto, Toronto, ON, Canada.
   [Salakhutdinov, Ruslan] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
RP Neyshabur, B (reprint author), Toyota Technol Inst, Chicago, IL 60637 USA.
EM bneyshabur@ttic.edu; ywu@cs.toronto.edu; rsalakhu@cs.cmu.edu;
   nati@ttic.edu
FU NSF RI/AF [1302662]; Intel ICRI-CI; ONR [N000141310721]; ADeLAIDE grant
   [FA8750-16C-0130-001]
FX This research was supported in part by NSF RI/AF grant 1302662, an Intel
   ICRI-CI award, ONR Grant N000141310721, and ADeLAIDE grant
   FA8750-16C-0130-001. We thank Saizheng Zhang for sharing a base code for
   RNNs.
CR Arjovsky Martin, 2015, ARXIV151106464
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Graves A., 2014, ICML, P1764, DOI DOI 10.1145/1143844.1143891
   Han S., 2016, P INT C LEARN REPR
   Hochreiter  S, 1997, NEURAL COMPUTATION, V9
   Hochreiter S., 1998, INT J UNCERTAINTY FU, V06
   Kingma D. P., 2015, P INT C LEARN REPR
   Kiros  Ryan, 2015, T ASS COMPUTATIONAL
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Krueger  David, 2016, P INT C LEARN REPR
   Le Q. V., 2015, ARXIV150400941
   Marcus M.P., 1993, COMPUTATIONAL LINGUI, V19, P313, DOI DOI 10.1080/07494460903404410
   Mikolov  T., 2012, SUBWORD LANGUAGE MOD
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Neyshabur  Behnam, 2015, ADV NEURAL INFORM PR
   Neyshabur  Behnam, 2015, P 28 C LEARN THEOR C
   Neyshabur  Behnam, 2016, INT C LEARN REPR
   Ollivier  Yann, 2015, INFORM INFERENCE
   Pachitariu M., 2013, ARXIV13015650
   Saxe  A.M., 2014, INT C LEARN REPR
   Talathi Sachin S., 2014, INT C LEARN REPR WOR
   Zhang Saizheng, 2016, ARXIV160208210
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701071
DA 2019-06-15
ER

PT S
AU Ng, YC
   Chilinski, P
   Silva, R
AF Ng, Yin Cheng
   Chilinski, Pawel
   Silva, Ricardo
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Scaling Factorial Hidden Markov Models: Stochastic Variational Inference
   without Messages
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID ALGORITHM
AB Factorial Hidden Markov Models (FHMMs) are powerful models for sequential data but they do not scale well with long sequences. We propose a scalable inference and learning algorithm for FHMMs that draws on ideas from the stochastic variational inference, neural network and copula literatures. Unlike existing approaches, the proposed algorithm requires no message passing procedure among latent variables and can be distributed to a network of computers to speed up learning. Our experiments corroborate that the proposed algorithm does not introduce further approximation bias compared to the proven structured mean-field algorithm, and achieves better performance with long sequences and large FHMMs.
C1 [Ng, Yin Cheng; Silva, Ricardo] UCL, Dept Stat Sci, London, England.
   [Chilinski, Pawel] UCL, Dept Comp Sci, London, England.
RP Ng, YC (reprint author), UCL, Dept Stat Sci, London, England.
EM y.ng.12@ucl.ac.uk; ucabchi@ucl.ac.uk; r.silva@ucl.ac.uk
CR Archer E., 2015, ARXIV151107367
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Elidan  G., 2013, LECT NOTES STAT, P39
   Fan Kai, 2016, UNIFYING VARIATIONAL
   Foti Nicholas, 2014, ADV NEURAL INFORM PR, P3599
   Gao X, 2011, STAT SINICA, V21, P165
   Gershman Samuel J, 2014, P 36 ANN C COGN SCI
   Ghahramani Z, 1997, MACH LEARN, V29, P245, DOI 10.1023/A:1007425814087
   Gopalan PK, 2013, P NATL ACAD SCI USA, V110, P14534, DOI 10.1073/pnas.1221839110
   Han Shaobo, 2015, ARXIV150605860
   HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Johnson Matthew J, 2016, ARXIV160306277
   Kingma D.P., 2013, ARXIV13126114
   LAURITZEN SL, 1988, J ROY STAT SOC B MET, V50, P157
   Lichman M., 2013, UCI MACHINE LEARNING
   Maclaurin Dougal, 2015, AUTOGRAD REVERSE MOD
   Nelsen R. B., 2013, INTRO COPULAS, V139
   Rezende D. J, 2014, ARXIV14014082
   Stuhlmuller A., 2013, ADV NEURAL INFORM PR, P3048
   Teh Y. W., 2007, ADV NEURAL INFORM PR, V19
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P2
   Tran Dustin, 2015, ADV NEURAL INFORM PR, P3550
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704058
DA 2019-06-15
ER

PT S
AU Cuong, V
   Xu, H
AF Nguyen Viet Cuong
   Xu, Huan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Adaptive Maximization of Pointwise Submodular Functions With Budget
   Constraint
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We study the worst-case adaptive optimization problem with budget constraint that is useful for modeling various practical applications in artificial intelligence and machine learning. We investigate the near-optimality of greedy algorithms for this problem with both modular and non-modular cost functions. In both cases, we prove that two simple greedy algorithms are not near-optimal but the best between them is near-optimal if the utility function satisfies pointwise submodularity and pointwise cost-sensitive submodularity respectively. This implies a combined algorithm that is near-optimal with respect to the optimal algorithm that uses half of the budget. We discuss applications of our theoretical results and also report experiments comparing the greedy algorithms on the active learning problem.
C1 [Nguyen Viet Cuong] Univ Cambridge, Dept Engn, Cambridge, England.
   [Xu, Huan] Georgia Inst Technol, Stewart Sch Ind & Syst Engn, Atlanta, GA 30332 USA.
RP Cuong, V (reprint author), Univ Cambridge, Dept Engn, Cambridge, England.
EM vcn22@cam.ac.uk; huan.xu@isye.gatech.edu
FU Agency for Science, Technology and Research (A*STAR) of Singapore
   through SERC PSF [R266000101305]
FX This work was done when both authors were at the National University of
   Singapore. The authors were partially supported by the Agency for
   Science, Technology and Research (A*STAR) of Singapore through SERC PSF
   Grant R266000101305.
CR Asadpour Arash, 2008, INTERNET NETWORK EC
   Cuong Nguyen Viet, 2013, NIPS
   Cuong Nguyen Viet, 2016, AAAI
   Cuong Nguyen Viet, 2014, UAI
   Dean Brian C., 2004, FOCS
   Golovin  Daniel, 2011, JAIR
   Gotovos Alkis, 2015, IJCAI
   Guillory Andrew, 2012, THESIS
   Guillory Andrew, 2011, ICML
   Guillory Andrew, 2011, NIPS
   Guillory Andrew, 2010, ICML
   Javdani Shervin, 2014, AISTATS
   Joachims T, 1996, DTIC DOCUMENT
   Khuller S, 1999, INFORM PROCESS LETT, V70, P39, DOI 10.1016/S0020-0190(99)00031-9
   Krause A, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1989734.1989736
   Krause Andreas, 2007, AAAI
   Krause Andreas, 2007, ICML
   Kusner Matt J., 2014, NIPS WORKSH DISCR CO
   Leskovec  J., 2007, KDD
   McCallum A. K., 1998, ICML
   Nemhauser G. L., 1978, Mathematics of Operations Research, V3, P177, DOI 10.1287/moor.3.3.177
   Sviridenko M, 2004, OPER RES LETT, V32, P41, DOI 10.1016/S0167-6377(03)00062-2
   Wei Kai, 2015, ICML
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704050
DA 2019-06-15
ER

PT S
AU Niepert, M
AF Niepert, Mathias
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Discriminative Gaifman Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We present discriminative Gaifman models, a novel family of relational machine learning models. Gaifman models learn feature representations bottom up from representations of locally connected and bounded-size regions of knowledge bases (KBs). Considering local and bounded-size neighborhoods of knowledge bases renders logical inference and learning tractable, mitigates the problem of over-fitting, and facilitates weight sharing. Gaifman models sample neighborhoods of knowledge bases so as to make the learned relational models more robust to missing objects and relations which is a common situation in open-world KBs. We present the core ideas of Gaifman models and apply them to large-scale relational learning problems. We also discuss the ways in which Gaifman models relate to some existing relational machine learning approaches.
C1 [Niepert, Mathias] NEC Labs Europe, Heidelberg, Germany.
RP Niepert, M (reprint author), NEC Labs Europe, Heidelberg, Germany.
EM mathias.niepert@neclabs.eu
CR Berant J, 2011, 49 ANN M ASS COMP LI, V1, P610
   Bollacker K., 2008, P 2008 ACM SIGMOD IN, P1247, DOI DOI 10.1145/1376616.1376746
   Bordes A., 2012, P INT C ART INT STAT, P127
   Bordes A., 2011, AAAI C ART INT
   Bordes A., 2013, ADV NEURAL INFORM PR, P2787
   Carlson A., 2010, 24 AAAI C ART INT
   Ceylan I. I., 2016, P 15 INT C PRINC KNO
   Dries A, 2015, LECT NOTES ARTIF INT, V9286, P312, DOI 10.1007/978-3-319-23461-8_37
   Gaifman H., 1982, STUDIES LOGIC FDN MA, P105
   Gardner M, 2015, P 2015 C EMP METH NA, P1488
   Hoffart J, 2013, ARTIF INTELL, V194, P28, DOI 10.1016/j.artint.2012.06.001
   Jenatton  R., 2012, ADV NEURAL INFORM PR, P3167
   Ji G, 2016, P 30 AAAI C ART INT, P985
   Kersting K, 2012, FRONT ARTIF INTEL AP, V242, P33, DOI 10.3233/978-1-61499-098-7-33
   Lao  N., 2011, P C EMP METH NAT LAN, P529
   Libkin Leonid, 2004, TEXT THEORET COMP S
   Lin Y., 2015, P 2015 C EMP METH NA, P705
   Lin YC, 2015, ADV SOC SCI EDUC HUM, V39, P2181
   Milch B. C., 2006, THESIS
   Nickel M., 2011, P 28 INT C MACH LEAR, P809
   Nickel M, 2016, P IEEE, V104, P11, DOI 10.1109/JPROC.2015.2483592
   Richardson M, 2006, MACH LEARN, V62, P107, DOI 10.1007/s10994-006-5833-1
   Riedel  S., 2013, HLT NAACL
   Rocktaschel T., 2015, C N AM CHAPT ACL NAA
   Schoenmackers S., 2010, P 2010 C EMP METH NA, P1088
   Socher R., 2013, ADV NEURAL INFORM PR, P926
   Trouillon  T., 2016, P 33 INT C MACH LEAR, P2071
   Van den Broeck G., 2013, LIFTED INFERENCE LEA
   Vardi M, 1982, P 14 ACM S THEOR COM, P137, DOI DOI 10.1145/800070.802186
   Yang B., 2015, INT C LEARN REPR
   Yates A., 2007, ASS COMPUTATIONAL LI
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700062
DA 2019-06-15
ER

PT S
AU Niu, G
   du Plessis, MC
   Sakai, T
   Ma, Y
   Sugiyama, M
AF Niu, Gang
   du Plessis, Marthinus C.
   Sakai, Tomoya
   Ma, Yao
   Sugiyama, Masashi
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Theoretical Comparisons of Positive-Unlabeled Learning against
   Positive-Negative Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB In PU learning, a binary classifier is trained from positive (P) and unlabeled (U) data without negative (N) data. Although N data is missing, it sometimes outperforms PN learning (i.e., ordinary supervised learning). Hitherto, neither theoretical nor experimental analysis has been given to explain this phenomenon. In this paper, we theoretically compare PU (and NU) learning against PN learning based on the upper bounds on estimation errors. We find simple conditions when PU and NU learning are likely to outperform PN learning, and we prove that, in terms of the upper bounds, either PU or NU learning (depending on the class-prior probability and the sizes of P and N data) given infinite U data will improve on PN learning. Our theoretical findings well agree with the experimental results on artificial and benchmark data even when the experimental setup does not match the theoretical assumptions exactly.
C1 [Niu, Gang; du Plessis, Marthinus C.; Sakai, Tomoya; Sugiyama, Masashi] Univ Tokyo, Tokyo, Japan.
   [Sugiyama, Masashi] RIKEN, Wako, Saitama, Japan.
   [Ma, Yao] Boston Univ, Boston, MA 02215 USA.
RP Niu, G (reprint author), Univ Tokyo, Tokyo, Japan.
EM gang@ms.k.u-tokyo.ac.jp; christo@ms.k.u-tokyo.ac.jp;
   sakai@ms.k.u-tokyo.ac.jp; yao@ms.k.u-tokyo.ac.jp; sugi@k.u-tokyo.ac.jp
FU JST CREST program; Microsoft Research Asia; JSPS KAKENHI [15J09111]
FX GN was supported by the JST CREST program and Microsoft Research Asia.
   MCdP, YM, and MS were supported by the JST CREST program. TS was
   supported by JSPS KAKENHI 15J09111.
CR Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907
   Blanchard G, 2010, J MACH LEARN RES, V11, P2973
   Bousquet O, 2004, LECT NOTES ARTIF INT, V3176, P169
   Chapelle O., 2006, SEMISUPERVISED LEARN
   Chapelle O., 2009, WWW
   Chung K.-L., 1968, COURSE PROBABILITY T
   Collobert R., 2006, ICML
   Craswell N., 2008, WSDM
   Denis F., 1998, ALT
   du Plessis M. C., 2012, ICML
   du Plessis M. C., 2015, ACML
   du Plessis  M.C., 2014, NIPS
   du Plessis M. C., 2015, ICML
   Dupret G., 2008, SIGIR
   Elkan  C., 2008, KDD
   Iyer A., 2014, ICML
   Koltchinskii V, 2001, IEEE T INFORM THEORY, V47, P1902, DOI 10.1109/18.930926
   Ledoux M., 1991, PROBABILITY BANACH S
   Letouzey F., 2000, ALT
   McDiarmid C., 1989, SURVEYS COMBINATORIC, V141, P148
   Meir R., 2003, J MACHINE LEARNING R, V4, P839
   Mohri M., 2012, FDN MACHINE LEARNING
   Quinonero-Candela J, 2009, NEURAL INF PROCESS S, pXI
   Ramaswamy H. G., 2016, ICML
   Ratsch G, 2001, MACH LEARN, V42, P287, DOI 10.1023/A:1007618119488
   Saerens M, 2002, NEURAL COMPUT, V14, P21, DOI 10.1162/089976602753284446
   Scholkopf B., 2001, LEARNING KERNELS
   Scott  C., 2009, AISTATS
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Ward G, 2009, BIOMETRICS, V65, P554, DOI 10.1111/j.1541-0420.2008.01116.x
   Yuille A. L., 2001, NIPS
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702101
DA 2019-06-15
ER

PT S
AU Nock, R
   Menon, AK
   Ong, CS
AF Nock, Richard
   Menon, Aditya Krishna
   Ong, Cheng Soon
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI A scaled Bregman theorem with applications
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Bregman divergences play a central role in the design and analysis of a range of machine learning algorithms through a handful of popular theorems. We present a new theorem which shows that "Bregman distortions" (employing a potentially non-convex generator) may be exactly re-written as a scaled Bregman divergence computed over transformed data. This property can be viewed from the standpoints of geometry (a scaled isometry with adaptive metrics) or convex optimization (relating generalized perspective transforms). Admissible distortions include geodesic distances on curved manifolds and projections or gauge-normalisation.
   Our theorem allows one to leverage to the wealth and convenience of Bregman divergences when analysing algorithms relying on the aforementioned Bregman distortions. We illustrate this with three novel applications of our theorem: a reduction from multi-class density ratio to class-probability estimation, a new adaptive projection free yet norm-enforcing dual norm mirror descent algorithm, and a reduction from clustering on flat manifolds to clustering on curved manifolds. Experiments on each of these domains validate the analyses and suggest that the scaled Bregman theorem might be a worthy addition to the popular handful of Bregman divergence properties that have been pervasive in machine learning.
C1 [Nock, Richard; Menon, Aditya Krishna; Ong, Cheng Soon] Data61, Canberra, ACT, Australia.
   [Nock, Richard; Menon, Aditya Krishna; Ong, Cheng Soon] Australian Natl Univ, Canberra, ACT, Australia.
   [Nock, Richard] Univ Sydney, Sydney, NSW, Australia.
RP Nock, R (reprint author), Data61, Canberra, ACT, Australia.; Nock, R (reprint author), Australian Natl Univ, Canberra, ACT, Australia.; Nock, R (reprint author), Univ Sydney, Sydney, NSW, Australia.
EM richard.nock@data61.csiro.au; aditya.menon@data61.csiro.au;
   chengsoon.ong@data61.csiro.au
CR Amari S., 2000, METHODS INFORM GEOME
   Arthur D., 2007, 19 SODA
   Banerjee A, 2005, J MACH LEARN RES, V6, P1705
   Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6
   Boissonnat JD, 2010, DISCRETE COMPUT GEOM, V44, P281, DOI 10.1007/s00454-010-9256-1
   Boyd S., 2004, CONVEX OPTIMIZATION
   Buja A, 2005, LOSS FUNCTIONS UNPUB
   Buss SR, 2001, ACM T GRAPHIC, V20, P95, DOI 10.1145/502122.502124
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Collins M., 2002, MLJ
   Dacorogna B, 2008, J CONVEX ANAL, V15, P271
   Dhillon IS, 2007, SIAM J MATRIX ANAL A, V29, P1120, DOI 10.1137/060649021
   Dhillon IS, 2001, MACH LEARN, V42, P143, DOI 10.1023/A:1007612920971
   Duchi J., 2008, P 25 INT C MACH LEAR, P272, DOI DOI 10.1145/1390156.1390191
   Endo Yasunori, 2015, Modeling Decisions for Artificial Intelligence. 12th International Conference, MDAI 2015. Proceedings: 9321, P103, DOI 10.1007/978-3-319-23240-9_9
   GALPERIN GA, 1993, COMMUN MATH PHYS, V154, P63, DOI 10.1007/BF02096832
   Hazan E., 2012, ICML 12, P521
   Hernandez-Lobato M., 2016, 33RD ICML
   Jaggi M., 2013, 30 ICML
   Kivinen J, 2006, IEEE T SIGNAL PROCES, V54, P1782, DOI 10.1109/TSP.2006.872551
   Kuang D, 2015, J GLOBAL OPTIM, V62, P545, DOI 10.1007/s10898-014-0247-2
   Marechal P, 2005, J OPTIMIZ THEORY APP, V126, P175, DOI 10.1007/s10957-005-2667-0
   Marechal P., 2005, J OPTIMIZATION THEOR, V126, P375
   Menon A.-K., 2016, ICML
   Nock R., 2008, ECML
   Nock R, 2016, IEEE T INFORM THEORY, V62, P527, DOI 10.1109/TIT.2015.2448072
   Nock R, 2009, IEEE T PATTERN ANAL, V31, P2048, DOI 10.1109/TPAMI.2008.225
   Reid MD, 2011, J MACH LEARN RES, V12, P731
   Reid MD, 2010, J MACH LEARN RES, V11, P2387
   Rong G., 2010, 14 ACM SPM
   Schwander O., 2013, MATRIX INFORM GEOMET, P403
   Shalev-Shwartz S., 2007, P 24 INT C MACH LEAR, P807, DOI DOI 10.1145/1273496.1273598
   Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4
   Straub J, 2014, PROC CVPR IEEE, P3770, DOI 10.1109/CVPR.2014.488
   Sugiyama M, 2012, ANN I STAT MATH, V64, P1009, DOI 10.1007/s10463-011-0343-8
   Vidal R, 2011, IEEE SIGNAL PROC MAG, V28, P52, DOI 10.1109/MSP.2010.939739
   Williamson R.-C., 2014, COMPOSITE MULT UNPUB
   Zhang Hui, 2016, CORR
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701033
DA 2019-06-15
ER

PT S
AU Nock, R
AF Nock, Richard
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI On Regularizing Rademacher Observation Losses
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB It has recently been shown that supervised learning linear classifiers with two of the most popular losses, the logistic and square loss, is equivalent to optimizing an equivalent loss over sufficient statistics about the class: Rademacher observations (rados). It has also been shown that learning over rados brings solutions to two prominent problems for which the state of the art of learning from examples can be comparatively inferior and in fact less convenient: (i) protecting and learning from private examples, (ii) learning from distributed datasets without entity resolution. Bis repetita placent: the two proofs of equivalence are different and rely on specific properties of the corresponding losses, so whether these can be unified and generalized inevitably comes to mind. This is our first contribution: we show how they can be fit into the same theory for the equivalence between example and rado losses. As a second contribution, we show that the generalization unveils a surprising new connection to regularized learning, and in particular a sufficient condition under which regularizing the loss over examples is equivalent to regularizing the rados (i.e. the data) in the equivalent rado loss, in such a way that an efficient algorithm for one regularized rado loss may be as efficient when changing the regularizer. This is our third contribution: we give a formal boosting algorithm for the regularized exponential rado-loss which boost with any of the ridge, lasso, SLOPE, l(infinity), or elastic net regularizer, using the same master routine for all. Because the regularized exponential rado-loss is the equivalent of the regularized logistic loss over examples we obtain the first efficient proxy to the minimization of the regularized logistic loss over examples using such a wide spectrum of regularizers. Experiments with a readily available code display that regularization significantly improves rado-based learning and compares favourably with example-based learning.
C1 [Nock, Richard] Australian Natl Univ, Data61, Canberra, ACT, Australia.
   [Nock, Richard] Univ Sydney, Sydney, NSW, Australia.
RP Nock, R (reprint author), Australian Natl Univ, Data61, Canberra, ACT, Australia.; Nock, R (reprint author), Univ Sydney, Sydney, NSW, Australia.
EM richard.nock@data61.csiro.au
CR Bach F, 2011, MACH LEARN, V4, P1, DOI DOI 10.1561/2200000015
   BACHE K., 2013, UCI MACHINE LEARNING
   Bogdan M, 2015, ANN APPL STAT
   Duchi J., 2009, ADV NEURAL INFORM PR, V22, P495
   Gentile C, 1998, NIPS, V11, P225
   Kearns M, 1999, J COMPUT SYST SCI, V58, P109, DOI 10.1006/jcss.1997.1543
   Menon A., 2015, NIPS 28
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Nock R., 2008, NIPS 21, P1201
   Nock R., 2015, P 32 ICML, P948
   Patrini G., 2016, 26 IJCAI
   Reid Mark D., 2015, 28 COLT, P1501
   Schapire RE, 2003, LECT NOTES STAT, V171, P149
   Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901
   Su W., 2015, CORR
   Telgarsky M, 2012, J MACH LEARN RES, V13, P561
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Xi Y., 2009, P 12 INT C ART INT S, V5, P615
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704096
DA 2019-06-15
ER

PT S
AU Nogueira, R
   Cho, K
AF Nogueira, Rodrigo
   Cho, Kyunghyun
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI End-to-End Goal-Driven Web Navigation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We propose a goal-driven web navigation as a benchmark task for evaluating an agent with abilities to understand natural language and plan on partially observed environments. In this challenging task, an agent navigates through a website, which is represented as a graph consisting of web pages as nodes and hyperlinks as directed edges, to find a web page in which a query appears. The agent is required to have sophisticated high-level reasoning based on natural languages and efficient sequential decision-making capability to succeed. We release a software tool, called WebNav, that automatically transforms a website into this goal-driven web navigation task, and as an example, we make WikiNav, a dataset constructed from the English Wikipedia. We extensively evaluate different variants of neural net based artificial agents on WikiNav and observe that the proposed goal-driven web navigation well reflects the advances in models, making it a suitable benchmark for evaluating future progress. Furthermore, we extend the WikiNav with question-answer pairs from Jeopardy! and test the proposed agent based on recurrent neural networks against strong inverted index based search engines. The artificial agents trained on WikiNav outperforms the engined based approaches, demonstrating the capability of the proposed goal-driven navigation as a good proxy for measuring the progress in real-world tasks such as focused crawling and question-answering.
C1 [Nogueira, Rodrigo] NYU, Tandon Sch Engn, New York, NY 10003 USA.
   [Cho, Kyunghyun] NYU, Courant Inst Math Sci, New York, NY 10003 USA.
RP Nogueira, R (reprint author), NYU, Tandon Sch Engn, New York, NY 10003 USA.
EM rodrigonogueira@nyu.edu; kyunghyun.cho@nyu.edu
CR Alvarez M, 2007, P 3 INT WORKSH DAT E, P18
   Bahdanau D, 2014, ICLR
   Chakrabarti S, 1999, COMPUT NETW, V31, P1623, DOI 10.1016/S1389-1286(99)00052-3
   He Ji, 2015, ARXIV151104636
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Koutnik J, 2014, GECCO'14: PROCEEDINGS OF THE 2014 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P541, DOI 10.1145/2576768.2598358
   Mikolov T., 2013, COMPUTING RES REPOSI, V1301, P3781, DOI DOI 10.1109/TNN.2003.820440]
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Narasimhan  K., 2015, ARXIV150608941
   Risi Sebastian, 2014, ARXIV14107326
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   West R., 2012, P 21 INT C WORLD WID, P619
   West R., 2012, ICWSM
   West R, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1598
NR 14
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700028
DA 2019-06-15
ER

PT S
AU Nokland, A
AF Nokland, Arild
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Direct Feedback Alignment Provides Learning in Deep Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Artificial neural networks are most commonly trained with the back-propagation algorithm, where the gradient for learning is provided by back-propagating the error, layer by layer, from the output layer to the hidden layers. A recently discovered method called feedback-alignment shows that the weights used for propagating the error backward don't have to be symmetric with the weights used for propagation the activation forward. In fact, random feedback weights work evenly well, because the network learns how to make the feedback useful. In this work, the feedback alignment principle is used for training hidden layers more independently from the rest of the network, and from a zero initial condition. The error is propagated through fixed random feedback connections directly from the output layer to each hidden layer. This simple method is able to achieve zero training error even in convolutional networks and very deep networks, completely without error back-propagation. The method is a step towards biologically plausible machine learning because the error signal is almost local, and no symmetric or reciprocal weights are required. Experiments show that the test performance on MNIST and CIFAR is almost as good as those obtained with back-propagation for fully connected networks. If combined with dropout, the method achieves 1.45% error on the permutation invariant MNIST task.
EM arild.nokland@gmail.com
CR Bengio Yoshua, 2015, CORR
   Gilbert CD, 2013, NAT REV NEUROSCI, V14, P350, DOI 10.1038/nrn3476
   Goodfellow I. J., 2014, CORR
   Hinton G. E., 1983, P IEEE C COMP VIS PA
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Lee DH, 2015, LECT NOTES ARTIF INT, V9284, P498, DOI 10.1007/978-3-319-23528-8_31
   Liao Qianli, 2015, CORR
   Lillicrap Timothy P., 2014, CORR
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Salakhutdinov R., 2009, ARTIF INTELL, V5, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Saxe Andrew M., 2013, CORR
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sussillo David, 2014, CORR
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P5
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Xie XH, 2003, NEURAL COMPUT, V15, P441, DOI 10.1162/089976603762552988
NR 16
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703077
DA 2019-06-15
ER

PT S
AU Norouzi, M
   Bengio, S
   Chen, ZF
   Jaitly, N
   Schuster, M
   Wu, YH
   Schuurmans, D
AF Norouzi, Mohammad
   Bengio, Samy
   Chen, Zhifeng
   Jaitly, Navdeep
   Schuster, Mike
   Wu, Yonghui
   Schuurmans, Dale
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Reward Augmented Maximum Likelihood for Neural Structured Prediction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB A key problem in structured output prediction is direct optimization of the task reward function that matters for test evaluation. This paper presents a simple and computationally efficient approach to incorporate task reward into a maximum likelihood framework. By establishing a link between the log-likelihood and expected reward objectives, we show that an optimal regularized expected reward is achieved when the conditional distribution of the outputs given the inputs is proportional to their exponentiated scaled rewards. Accordingly, we present a framework to smooth the predictive probability of the outputs using their corresponding rewards. We optimize the conditional log-probability of augmented outputs that are sampled proportionally to their exponentiated scaled rewards. Experiments on neural sequence to sequence models for speech recognition and machine translation show notable improvements over a maximum likelihood baseline by using reward augmented maximum likelihood (RML), where the rewards are defined as the negative edit distance between the outputs and the ground truth labels.
C1 [Norouzi, Mohammad; Bengio, Samy; Chen, Zhifeng; Jaitly, Navdeep; Schuster, Mike; Wu, Yonghui; Schuurmans, Dale] Google Brain, Mountain View, CA 94039 USA.
RP Norouzi, M (reprint author), Google Brain, Mountain View, CA 94039 USA.
EM mnorouzi@google.com; bengio@google.com; zhifengc@google.com;
   ndjaitly@google.com; schuster@google.com; yonghu@google.com;
   schuurmans@google.com
CR Andor D., 2016, ARXIV160306042
   Bandanau D., 2015, ICLR
   Banerjee A., 2005, JMLR
   Bengio S., 2015, NIPS
   Chan W., 2016, ICASSP
   Chorowski J., 2014, ARXIV14121602
   Chorowski J. K., 2015, NIPS
   Daume H., 2009, MACH LEARN J
   Degris T., 2012, ACC
   Domke J., 2012, AISTATS
   Gimpel K., 2010, NAACL
   Hinton G., 2015, ARXIV150302531
   Hochreiter S., 1997, NEURAL COMPUTATION
   Kappen H. J., 2012, MACH LEARN J
   Kim B., 2013, NIPS
   Kumar A., 2016, ICML
   Lafferty J. O., 2001, ICML
   Levine S., 2013, ICML
   Levine S., 2013, NIPS
   Lopez-Paz D., 2016, ICLR
   Luong M., 2015, EMNLP
   Luong M.-T., 2015, ACL
   Peters J., 2010, AAAI
   Povey D., 2011, ASRU
   Ranzato Marc Aurelio, 2016, ICLR
   Shen S., 2016, ACL
   Silver D., 2016, NATURE
   Stoyanov V., 2011, AISTATS
   Sutskever  I., 2014, NIPS
   Sutton R. S., 2000, NIPS
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Taskar B., 2004, MAX MARGIN MARKOV NE
   Todorov E., 2006, LINEARLY SOLVABLE MA
   VAN HASSELT H., 2015, ARXIV150906461
   Vlassis N., 2009, AUTONOMOUS ROBOTS
   Volkovs M., 2011, ARXIV11071805V1
   Williams R. J., 1992, MACH LEARN J
   Williams R. J., 1991, CONNECTION SCI
   Wiseman S., 2016, ARXIV160602960
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704071
DA 2019-06-15
ER

PT S
AU Norouzi-Fard, A
   Bazzi, A
   El Halabi, M
   Bogunovic, I
   Hsieh, YP
   Cevher, V
AF Norouzi-Fard, Ashkan
   Bazzi, Abbas
   El Halabi, Marwa
   Bogunovic, Ilija
   Hsieh, Ya-Ping
   Cevher, Volkan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI An Efficient Streaming Algorithm for the Submodular Cover Problem
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We initiate the study of the classical Submodular Cover (SC) problem in the data streaming model which we refer to as the Streaming Submodular Cover (SSC). We show that any single pass streaming algorithm using sublinear memory in the size of the stream will fail to provide any non-trivial approximation guarantees for SSC. Hence, we consider a relaxed version of SSC, where we only seek to find a partial cover. We design the first Efficient bicriteria Submodular Cover Streaming (ESC-Streaming) algorithm for this problem, and provide theoretical guarantees for its performance supported by numerical evidence. Our algorithm finds solutions that are competitive with the near-optimal offline greedy algorithm despite requiring only a single pass over the data stream. In our numerical experiments, we evaluate the performance of ESC-Streaming on active set selection and large-scale graph cover problems.
C1 [Norouzi-Fard, Ashkan; Bazzi, Abbas] Ecole Polytech Fed Lausanne, Theory Computat Lab 2, THL2, Lausanne, Switzerland.
   [El Halabi, Marwa; Bogunovic, Ilija; Hsieh, Ya-Ping; Cevher, Volkan] Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst, LIONS, Lausanne, Switzerland.
RP Norouzi-Fard, A (reprint author), Ecole Polytech Fed Lausanne, Theory Computat Lab 2, THL2, Lausanne, Switzerland.
EM ashkan.norouzifard@epfl.ch; abbas.bazzi@epfl.ch; marwa.elhalabi@epfl.ch;
   ilija.bogunovic@epfl.ch; ya-ping.hsieh@epfl.ch; volkan.cevher@epfl.ch
FU European Commission under ERC Future Proof [SNF 200021-146750, SNF
   CRSII2-147633]; NCCR Marvel; ERC Starting Grant [335288-OptApprox]
FX We would like to thank Michael Kapralov and Ola Svensson for useful
   discussions. This work was supported in part by the European Commission
   under ERC Future Proof, SNF 200021-146750, SNF CRSII2-147633, NCCR
   Marvel, and ERC Starting Grant 335288-OptApprox.
CR Assadi Sepehr, 2016, ARXIV160305715
   Badanidiyuru A, 2014, P 20 ACM SIGKDD INT, P671
   Boldi P., 2011, P 20 INT C WORLD WID, P587, DOI DOI 10.1145/1963405.1963488
   Boldi P, 2014, WWW'14 COMPANION: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P227, DOI 10.1145/2567948.2577304
   Boldi Paolo, P 13 INT WORLD WID W, P595
   Chakrabarti A, 2008, ACM S THEORY COMPUT, P641
   Chakrabarti Amit, 2015, ARXIV150704645
   Chekuri C, 2015, LECT NOTES COMPUT SC, V9134, P318, DOI 10.1007/978-3-662-47672-7_26
   Demaine ED, 2014, LECT NOTES COMPUT SC, V8784, P484, DOI 10.1007/978-3-662-45174-8_33
   Dinur I., 2014, P 46 ANN ACM S THEOR, P624, DOI DOI 10.1145/2591796.2591884
   Emek Y, 2014, LECT NOTES COMPUT SC, V8572, P453
   Feige U, 1998, J ACM, V45, P634, DOI 10.1145/285055.285059
   Fortunato S, 2010, PHYS REP, V486, P75, DOI 10.1016/j.physrep.2009.11.002
   Indyk Piotr, 2015, ARXIV150900118
   Kempe D., 2003, P 9 ACM SIGKDD INT C, P137, DOI DOI 10.1145/956750.956769
   Kim G, 2011, IEEE I CONF COMP VIS, P169, DOI 10.1109/ICCV.2011.6126239
   Krause A., 2012, TRACTABILITY PRACTIC, V3, P19
   Kumar  Ravi, 2015, ACM T PARALLEL COMPU, V2, P14
   Minoux M., 1978, Proceedings of the 8th IFIP Conference on Optimization Techniques, P234
   Mirzasoleiman Baharan, 2015, ADV NEURAL INFORM PR, P2863
   Nemhauser G. L., 1978, Mathematics of Operations Research, V3, P177, DOI 10.1287/moor.3.3.177
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Rupp M, 2015, INT J QUANTUM CHEM, V115, P1058, DOI 10.1002/qua.24954
   Saha B., 2009, SDM, P697
   Seeger  Matthias, 2004, TECHNICAL REPORT
   Tschiatschek S., 2014, ADV NEURAL INFORM PR, P1413
   WOLSEY LA, 1982, COMBINATORICA, V2, P385, DOI 10.1007/BF02579435
   Yang J., 2012, P ACM SIGKDD WORKSH, V3, P1, DOI DOI 10.1145/2350190.2350193
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701032
DA 2019-06-15
ER

PT S
AU Nowozin, S
   Cseke, B
   Tomioka, R
AF Nowozin, Sebastian
   Cseke, Botond
   Tomioka, Ryota
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI f-GAN: Training Generative Neural Samplers using Variational Divergence
   Minimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID INFORMATION; RISK
AB Generative neural samplers are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution defined by the network weights. These models are expressive and allow efficient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization. The generative-adversarial training method allows to train such models through the use of an auxiliary discriminative neural network. We show that the generative-adversarial approach is a special case of an existing more general variational divergence estimation approach. We show that any f-divergence can be used for training generative neural samplers. We discuss the benefits of various choices of divergence functions on training complexity and the quality of the obtained generative models.
C1 [Nowozin, Sebastian; Cseke, Botond; Tomioka, Ryota] Microsoft Res, Machine Intelligence & Percept Grp, Redmond, WA 98052 USA.
RP Nowozin, S (reprint author), Microsoft Res, Machine Intelligence & Percept Grp, Redmond, WA 98052 USA.
EM Sebastian.Nowozin@microsoft.com; Botond.Cseke@microsoft.com;
   ryoto@microsoft.com
CR ALI SM, 1966, J ROY STAT SOC B, V28, P131
   Bishop C. M., 1994, TECHNICAL REPORT
   Bishop CM, 1998, NEURAL COMPUT, V10, P215, DOI 10.1162/089976698300017953
   Clevert D.A., 2015, ARXIV151107289
   Csiszar I., 2004, FDN TRENDS COMMUNICA, V1, P417
   Dauphin Y. N., 2014, ADV NEURAL INFORM PR, P2933
   Dziugaite G. K., 2015, UAI, P258
   Gauthier J., 2014, CLASS PROJECT STANFO, V2014
   Gneiting T, 2007, J AM STAT ASSOC, V102, P359, DOI 10.1198/016214506000001437
   Goodfellow I., 2014, ADV NEURAL INFORM PR, P2672
   Goodfellow I. J., 2015, INT C LEARN REPR ICL
   Graves A, 2013, ARXIV13080850
   Gretton A., 2007, NIPS, P585
   Gutmann Michael, 2010, P INT C ART INT STAT, P297
   Hiriart-Urruty J., 2012, FUNDAMENTALS CONVEX
   Huszar Ferenc, 2015, ARXIV151105101
   Kingma D. P., 2013, ARXIV14020030
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Larochelle H., 2011, AISTATS
   Li Y., 2015, ICML
   Liese F, 2006, IEEE T INFORM THEORY, V52, P4394, DOI 10.1109/TIT.2006.881731
   MACKAY DJC, 1995, NUCL INSTRUM METH A, V354, P73, DOI 10.1016/0168-9002(94)00931-7
   Makhzani A., 2015, ARXIV151105644
   Minka T. P, 2005, TECHNICAL REPORT
   Nguyen XL, 2010, IEEE T INFORM THEORY, V56, P5847, DOI 10.1109/TIT.2010.2068870
   Nielsen F, 2014, IEEE SIGNAL PROC LET, V21, P10, DOI 10.1109/LSP.2013.2288355
   Radford A., 2015, ARXIV151106434
   Reid MD, 2011, J MACH LEARN RES, V12, P731
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   Salimans T., 2016, NIPS
   Sohl-Dickstein J., 2015, P 32 INT C MACH LEAR, P2256
   Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517
   Theis L., 2015, ARXIV151101844
   Uria Benigno, 2013, ADV NEURAL INFORM PR, P2175
   Yu F., 2015, ARXIV150603365
NR 35
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700030
DA 2019-06-15
ER

PT S
AU Oglic, D
   Gartner, T
AF Oglic, Dino
   Gartner, Thomas
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Greedy Feature Construction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We present an effective method for supervised feature construction. The main goal of the approach is to construct a feature representation for which a set of linear hypotheses is of sufficient capacity - large enough to contain a satisfactory solution to the considered problem and small enough to allow good generalization from a small number of training examples. We achieve this goal with a greedy procedure that constructs features by empirically fitting squared error residuals. The proposed constructive procedure is consistent and can output a rich set of features. The effectiveness of the approach is evaluated empirically by fitting a linear ridge regression model in the constructed feature space and our empirical results indicate a superior performance of our approach over competing methods.
C1 [Oglic, Dino] Univ Bonn, Inst Informat 3, Bonn, Germany.
   [Oglic, Dino; Gartner, Thomas] Univ Nottingham, Sch Comp Sci, Nottingham, England.
RP Oglic, D (reprint author), Univ Bonn, Inst Informat 3, Bonn, Germany.; Oglic, D (reprint author), Univ Nottingham, Sch Comp Sci, Nottingham, England.
EM dino.oglic@uni-bonn.de; thomas.gaertner@nottingham.ac.uk
FU German Science Foundation [GA 1615/1-1]
FX We are grateful for access to the University of Nottingham High
   Performance Computing Facility. A part of this work was also supported
   by the German Science Foundation (grant number GA 1615/1-1).
CR Anthony  M., 2009, NEURAL NETWORK LEARN
   Aronszajn N., 1950, T AM MATH SOC
   Bach Francis R., P 22 INT C MACH LEAR
   Barron A.R., 1993, IEEE T INFORM THEORY, V39, P3
   Baxter Jonathan, 2000, J ARTIFICIAL INTELLI, V12
   Bertinet  A., 2004, REPRODUCING KERNEL H
   Bochner Salomon, 1932, AKAD VERLAGSGESELLSC
   Carl B., 1990, ENTROPY COMPACTNESS
   Cucker F, 2002, B AM MATH SOC, V39, P1
   Dai Bo, ADV NEURAL INFORM PR, V27
   Demsar J, 2006, J MACH LEARN RES, V7, P1
   Donahue Michael J., 1997, CONSTRUCTIVE APPROXI, V13
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Fine Shai, 2002, J MACHINE LEARNING R, V2
   Friedman Jerome H., 2000, ANN STAT, V29
   Gelfand IM, 1963, CALCULUS VARIATIONS
   Genton Marc G., 2002, J MACHINE LEARNING R, V2
   Kakade Sham M., 2011, ADV NEURAL INFORM PR, V24
   Kalai Adam T., 2009, P C LEARN THEOR
   Keerthi Sathiya, 2006, ADV NEURAL INFORM PR, V19
   Kolmogorov Andrey N., 1959, USPEHI MATEMATICHESK, V14
   Kulis Brian, 2006, P 23 INT C MACH LEAR
   Mason Llew, 1999, ADV LARGE MARGIN CLA
   Mayer Sebastian, 2015, CONSTRUCTIVE APPROXI, V42
   Mitchell T. M., 1997, MACHINE LEARNING
   Rahimi Ali, ADV NEURAL INFORM PR, V20
   Rudin W., 1991, INT SERIES PURE APPL
   WELCH BL, 1947, BIOMETRIKA, V34, P28, DOI 10.2307/2332510
   WILCOXON F, 1945, BIOMETRICS BULL, V1, P80, DOI 10.2307/3001968
   Yang Z, 2015, P 18 INT C ART INT S
   Zinkevich Martin A., 2010, ADV NEURAL INFORM PR, V23
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704081
DA 2019-06-15
ER

PT S
AU Oh, TH
   Matsushita, Y
   Kweon, IS
   Wipf, D
AF Oh, Tae-Hyun
   Matsushita, Yasuyuki
   Kweon, In So
   Wipf, David
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI A Pseudo-Bayesian Algorithm for Robust PCA
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Commonly used in many applications, robust PCA represents an algorithmic attempt to reduce the sensitivity of classical PCA to outliers. The basic idea is to learn a decomposition of some data matrix of interest into low rank and sparse components, the latter representing unwanted outliers. Although the resulting problem is typically NP-hard, convex relaxations provide a computationally-expedient alternative with theoretical support. However, in practical regimes performance guarantees break down and a variety of non-convex alternatives, including Bayesian-inspired models, have been proposed to boost estimation quality. Unfortunately though, without additional a priori knowledge none of these methods can significantly expand the critical operational range such that exact principal subspace recovery is possible. Into this mix we propose a novel pseudo-Bayesian algorithm that explicitly compensates for design weaknesses in many existing non-convex approaches leading to state-of-the-art performance with a sound analytical foundation.
C1 [Oh, Tae-Hyun; Kweon, In So] Korea Adv Inst Sci & Technol, Elect Engn, Daejeon, South Korea.
   [Matsushita, Yasuyuki] Osaka Univ, Multimedia Engn, Osaka, Japan.
   [Wipf, David] Microsoft Res, Beijing, Peoples R China.
RP Oh, TH (reprint author), Korea Adv Inst Sci & Technol, Elect Engn, Daejeon, South Korea.
EM thoh.kaist.ac.kr@gmail.com; yasumat@ist.osaka-u.ac.jp;
   iskweon@kaist.ac.kr; davidwip@microsoft.com
FU NRF of Korea grant - Korea government, MSIP [2010-0028680]; JSPS KAKENHI
   Grant [JP16H01732]
FX This work was done while the first author was an intern at Microsoft
   Research, Beijing. The first and third authors were supported by the NRF
   of Korea grant funded by the Korea government, MSIP (No. 2010-0028680).
   The second author was partly supported by JSPS KAKENHI Grant Number
   JP16H01732.
CR Babacan S. D., 2012, IEEE T SIGNAL PROCES
   Bishop C. M., 2006, PATTERN RECOGNITION
   Boyd S., 2011, FDN TRENDS MACHINE L
   Candes  E., 2009, FDN COMPUTATIONAL MA
   Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Chandrasekaran V., 2011, SIAM J OPTIM
   Chartrand R., 2012, IEEE T SIGNAL PROCES
   Ding  X., 2011, IEEE T IMAGE PROCESS
   Elhamifar  E., 2013, IEEE T PATTERN ANAL
   Fan J., 2001, J AM STAT ASS
   Ghorbanzadeh M, 2013, IEEE ICC
   Hunter D. R., 2004, AM STAT
   Ji H., 2010, IEEE C COMP VIS PATT
   Jordan M. I., 1999, MACH LEARN
   Lakshminarayanan B., 2011, AISTATS
   Lin Z., 2010, ARXIV10095055
   Lu CW, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2487868
   Mohan K., 2012, J MACH LEARN RES
   Murphy K.P., 1999, UAI
   Murphy KP, 2012, MACHINE LEARNING PRO
   Oh T.-H., 2015, ARXIV151202188
   Oh T.-H., 2015, IEEE T PATTERN ANAL
   Oh T.-H., 2016, IEEE T PATTERN ANAL
   Palmer J. A., 2003, TECH REP
   Parker J. T., 2013, ARXIV13102632
   Peng Y., 2012, IEEE T PATTERN ANAL
   Shen HY, 2011, IEEE IC COMP COM NET
   Tron R., 2007, IEEE C COMP VIS PATT
   Vidal R., 2011, IEEE SIGNAL PROCESS
   Wang N., 2013, IEEE INT C COMP VIS
   Wipf D., 2012, UAI
   Wipf D., 2011, IEEE T INFORM THEORY
   Wu L., 2010, AS C COMP VIS
   Xin B., 2015, INT C MACH LEARN
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703071
DA 2019-06-15
ER

PT S
AU Onken, A
   Panzeri, S
AF Onken, Arno
   Panzeri, Stefano
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Mixed vine copulas as joint models of spike counts and local field
   potentials
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID DISCRETE; CONSTRUCTIONS
AB Concurrent measurements of neural activity at multiple scales, sometimes performed with multimodal techniques, become increasingly important for studying brain function. However, statistical methods for their concurrent analysis are currently lacking. Here we introduce such techniques in a framework based on vine copulas with mixed margins to construct multivariate stochastic models. These models can describe detailed mixed interactions between discrete variables such as neural spike counts, and continuous variables such as local field potentials. We propose efficient methods for likelihood calculation, inference, sampling and mutual information estimation within this framework. We test our methods on simulated data and demonstrate applicability on mixed data generated by a biologically realistic neural network. Our methods hold the promise to considerably improve statistical analysis of neural data recorded simultaneously at different scales.
C1 [Onken, Arno; Panzeri, Stefano] Ist Italiano Tecnol, I-38068 Rovereto, TN, Italy.
RP Onken, A (reprint author), Ist Italiano Tecnol, I-38068 Rovereto, TN, Italy.
EM arno.onken@iit.it; stefano.panzeri@iit.it
FU European Commission [659227]
FX This work was supported by the European Commission's Horizon 2020
   Programme (H2020-MSCA-IF-2014) under grant agreement number 659227
   ("STOMMAC").
CR Aas K, 2009, INSUR MATH ECON, V44, P182, DOI 10.1016/j.insmatheco.2007.02.001
   Acar EF, 2012, J MULTIVARIATE ANAL, V110, P74, DOI 10.1016/j.jmva.2012.02.001
   AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705
   Byrd RH, 1999, SIAM J OPTIMIZ, V9, P877, DOI 10.1137/S1052623497325107
   Carlson D. E., 2014, ADV NEURAL INFORM PR, P2060
   Cover T. M., 2006, ELEMENTS INFORM THEO
   de Leon AR, 2011, STAT MED, V30, P175, DOI 10.1002/sim.4087
   Ince RAA, 2013, J NEUROSCI, V33, P18277, DOI 10.1523/JNEUROSCI.2631-13.2013
   Jaworski P., 2013, LECT NOTES STAT P, V213
   Jenison RL, 2004, NEURAL COMPUT, V16, P665, DOI 10.1162/089976604322860659
   Joe H, 1996, 166 U BRIT COL DEP S
   Kelly RC, 2010, J COMPUT NEUROSCI, V29, P567, DOI 10.1007/s10827-009-0208-9
   Nelsen Roger B., 2006, INTRO COPULAS
   Onken A, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000577
   Panagiotelis A, 2012, J AM STAT ASSOC, V107, P1063, DOI 10.1080/01621459.2012.682850
   Panzeri S, 2015, TRENDS COGN SCI, V19, P162, DOI 10.1016/j.tics.2015.01.002
   Racine J, 2015, EMPIR ECON, V48, P37, DOI 10.1007/s00181-015-0913-3
   Robert C. P., 2004, MONTE CARLO STAT MET
   Sacerdote L, 2012, BRAIN RES, V1434, P243, DOI 10.1016/j.brainres.2011.08.064
   SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x
   Sklar A, 1959, PUBL I STAT U PARIS, V8, P229, DOI DOI 10.12691/IJEFM-3-2-3
   Smith M., 2010, J AM STAT ASS, V105
   Smith MS, 2012, J AM STAT ASSOC, V107, P290, DOI 10.1080/01621459.2011.644501
   Song PXK, 2009, BIOMETRICS, V65, P60, DOI 10.1111/j.1541-0420.2008.01058.x
   Tomsett RJ, 2015, BRAIN STRUCT FUNCT, V220, P2333, DOI 10.1007/s00429-014-0793-x
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700033
DA 2019-06-15
ER

PT S
AU Orabona, F
   Pal, D
AF Orabona, Francesco
   Pal, David
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Coin Betting and Parameter-Free Online Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB In the recent years, a number of parameter-free algorithms have been developed for online linear optimization over Hilbert spaces and for learning with expert advice. These algorithms achieve optimal regret bounds that depend on the unknown competitors, without having to tune the learning rates with oracle choices.
   We present a new intuitive framework to design parameter-free algorithms for both online linear optimization over Hilbert spaces and for learning with expert advice, based on reductions to betting on outcomes of adversarial coins. We instantiate it using a betting algorithm based on the Krichevsky-Trofimov estimator. The resulting algorithms are simple, with no parameters to be tuned, and they improve or match previous results in terms of regret guarantee and per-round complexity.
C1 [Orabona, Francesco] SUNY Stony Brook, Stony Brook, NY 11794 USA.
   [Pal, David] Yahoo Res, New York, NY USA.
RP Orabona, F (reprint author), SUNY Stony Brook, Stony Brook, NY 11794 USA.
EM francesco@orabona.com; dpal@yahoo-inc.com
CR Artin E., 1964, GAMMA FUNCTION
   Batir N, 2008, ARCH MATH, V91, P554, DOI 10.1007/s00013-008-2856-9
   Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   CesaBianchi N, 1997, J ACM, V44, P427, DOI 10.1145/258128.258179
   Chaudhuri K., 2009, ADV NEURAL INFORM PR, P297
   Chen Ch.-P., 2005, GEN MATH, V13, P65
   Chernov A., 2010, P 26 C UNC ART INT
   Cover T. M., 2006, ELEMENTS INFORM THEO
   Foster D. J., 2015, ADV NEURAL INFORM PR, V28, P3375
   Foster D. J., 2016, COMMUNICATION
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Hoorfar A., 2008, J INEQUAL PURE APPL, V9
   KELLY JL, 1956, IRE T INFORM THEOR, V2, P185, DOI 10.1109/TIT.1956.1056803
   Koolen W. M., 2015, COLT, V40, P1155
   KRICHEVSKY RE, 1981, IEEE T INFORM THEORY, V27, P199, DOI 10.1109/TIT.1981.1056331
   LITTLESTONE N, 1994, INFORM COMPUT, V108, P212, DOI 10.1006/inco.1994.1009
   Luo H., 2014, ADV NEURAL INFORM PR, V27, P1368
   Luo Haipeng, 2015, P 28 C LEARN THEOR, P1286
   McAllester D., 2013, ARXIV13072118
   Mcmahan Brendan, 2012, ADV NEURAL INFORM PR, P2402
   McMahan Brendan, 2013, ADV NEURAL INFORM PR, V26, P2724
   McMahan H. B., 2014, P 27 C LEARN THEOR C, P1020
   Orabona Francesco, 2014, ADV NEURAL INFORM PR, P1116
   Orabona Francesco, 2013, ADV NEURAL INFORM PR, P1806
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Vovk V, 1998, J COMPUT SYST SCI, V56, P153, DOI 10.1006/jcss.1997.1556
   Whittaker ET, 1962, COURSE MODERN ANAL
   WILLEMS FMJ, 1995, IEEE T INFORM THEORY, V41, P653, DOI 10.1109/18.382012
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701016
DA 2019-06-15
ER

PT S
AU Ortega, PA
   Stocker, AA
AF Ortega, Pedro A.
   Stocker, Alan A.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Human Decision-Making under Limited Time
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID REGRET THEORY
AB Subjective expected utility theory assumes that decision-makers possess unlimited computational resources to reason about their choices; however, virtually all decisions in everyday life are made under resource constraints i.e. decision-makers are bounded in their rationality. Here we experimentally tested the predictions made by a formalization of bounded rationality based on ideas from statistical mechanics and information-theory. We systematically tested human subjects in their ability to solve combinatorial puzzles under different time limitations. We found that our bounded-rational model accounts well for the data. The decomposition of the fitted model parameter into the subjects' expected utility function and resource parameter provide interesting insight into the subjects' information capacity limits Our results confirm that humans gradually fall back on their learned prior choice patterns when confronted with increasing resource limitations.
C1 [Ortega, Pedro A.; Stocker, Alan A.] Univ Penn, Dept Psychol, Philadelphia, PA 19104 USA.
RP Ortega, PA (reprint author), Univ Penn, Dept Psychol, Philadelphia, PA 19104 USA.
EM ope@seas.upenn.edu; astocker@sas.upenn.edu
FU Office of Naval Research [N000141110744]; University of Pennsylvania
FX This work was supported by the Office of Naval Research (Grant
   N000141110744) and the University of Pennsylvania.
CR Banerjee A, 2005, J MACH LEARN RES, V6, P1705
   BELL DE, 1982, OPER RES, V30, P961, DOI 10.1287/opre.30.5.961
   Bleichrodt H, 2015, ECON J, V125, P493, DOI 10.1111/ecoj.12200
   Fishburn P. C., 1982, FDN EXPECTED UTILITY
   Friedman JW, 2005, GAME ECON BEHAV, V51, P296, DOI 10.1016/j.geb.2003.08.004
   Gigerenzer G, 2001, BOUNDED RATIONALITY
   Lieder F., 2014, ADV NEURAL INFORM PR, V27, P2870
   LOOMES G, 1982, ECON J, V92, P805, DOI 10.2307/2232669
   Ortega P. A., 2013, P ROYAL SOC A, V469, P2153, DOI 1098/rspa.2012.0683
   Rubinstein Ariel, 1998, MODELING BOUNDED RAT
   Savage L. J, 1954, FDN STAT
   Shenhav A, 2013, NEURON, V79, P217, DOI 10.1016/j.neuron.2013.07.007
   Simon H, 1984, MODELS BOUNDED RATIO
   Srivastava N., 2012, ADV NEURAL INFORM PR
   Srivastava N., 2014, ADV NEURAL INFORM PR
   Tishby N, 2011, PERCEPTION REASON AC
   Torrance G W, 1989, Int J Technol Assess Health Care, V5, P559
   Ziebart B. D., 2008, AAAI, V8, P1433
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701106
DA 2019-06-15
ER

PT S
AU Osband, I
   Blundell, C
   Pritzel, A
   Van Roy, B
AF Osband, Ian
   Blundell, Charles
   Pritzel, Alexander
   Van Roy, Benjamin
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Deep Exploration via Bootstrapped DQN
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Efficient exploration remains a major challenge for reinforcement learning (RL). Common dithering strategies for exploration, such as epsilon-greedy, do not carry out temporally-extended (or deep) exploration; this can lead to exponentially larger data requirements. However, most algorithms for statistically efficient RL are not computationally tractable in complex environments. Randomized value functions offer a promising approach to efficient exploration with generalization, but existing algorithms are not compatible with nonlinearly parameterized value functions. As a first step towards addressing such contexts we develop bootstrapped DQN. We demonstrate that bootstrapped DQN can combine deep exploration with deep neural networks for exponentially faster learning than any dithering strategy. In the Arcade Learning Environment bootstrapped DQN substantially improves learning speed and cumulative performance across most games.
C1 [Osband, Ian; Van Roy, Benjamin] Stanford Univ, Stanford, CA 94305 USA.
   [Osband, Ian; Blundell, Charles; Pritzel, Alexander] Google DeepMind, London, England.
RP Osband, I (reprint author), Stanford Univ, Stanford, CA 94305 USA.; Osband, I (reprint author), Google DeepMind, London, England.
EM iosband@google.com; cblundell@google.com; apritzel@google.com;
   bvr@stanford.edu
CR Bellemare M. G., 2012, ARXIV12074708
   BICKEL PJ, 1981, ANN STAT, V9, P1196, DOI 10.1214/aos/1176345637
   Blundell C., 2015, ICML
   Dann  Christoph, 2015, ADV NEURAL INFORM PR, P2800
   Efron B., 1994, INTRO BOOTSTRAP
   Efron B., 1982, JACKKNIFE BOOTSTRAP, V38
   Gal Y, 2015, ARXIV150602142
   Guez  A., 2012, ADV NEURAL INFORM PR, P1025
   Jaksch T, 2010, J MACH LEARN RES, V11, P1563
   Kakade S. M., 2003, THESIS
   LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Osband I., 2013, ADV NEURAL INFORM PR, P3003
   Osband Ian, 2014, ADV NEURAL INFORM PR, P1466
   Osband  Ian, 2015, ARXIV150700300
   Osband Ian, 2014, ARXIV14020635
   Owen AB, 2012, ANN APPL STAT, V6, P895, DOI 10.1214/12-AOAS547
   Schaul T, 2015, ARXIV151105952
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Stadie BC, 2015, ARXIV150700814
   STRENS M, 2000, P 17 INT C MACH LEAR, P943
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   TESAURO G, 1995, COMMUN ACM, V38, P58, DOI 10.1145/203330.203343
   Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.2307/2332286
   VAN HASSELT H., 2015, ARXIV150906461
   Wang Z., 2015, ARXIV151106581
   Wen Z, 2013, ADV NEURAL INFORM PR, V26, P3021
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704026
DA 2019-06-15
ER

PT S
AU Ostrovsky, D
   Harchaoui, Z
   Juditsky, A
   Nemirovski, A
AF Ostrovsky, Dmitry
   Harchaoui, Zaid
   Juditsky, Anatoli
   Nemirovski, Arkadi
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Structure-Blind Signal Recovery
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We consider the problem of recovering a signal observed in Gaussian noise. If the set of signals is convex and compact, and can be specified beforehand, one can use classical linear estimators that achieve a risk within a constant factor of the minimax risk. However, when the set is unspecified, designing an estimator that is blind to the hidden structure of the signal remains a challenging problem. We propose a new family of estimators to recover signals observed in Gaussian noise. Instead of specifying the set where the signal lives, we assume the existence of a well-performing linear estimator. Proposed estimators enjoy exact oracle inequalities and can be efficiently computed through convex optimization. We present several numerical illustrations that show the potential of the approach.
C1 [Ostrovsky, Dmitry; Juditsky, Anatoli] Univ Grenoble Alpes, LJK, 700 Ave Cent,38401 Domaine Univ, St Martin Dheres, France.
   [Harchaoui, Zaid] Univ Washington, Seattle, WA 98195 USA.
   [Nemirovski, Arkadi] Georgia Inst Technol, Atlanta, GA 30332 USA.
RP Ostrovsky, D (reprint author), Univ Grenoble Alpes, LJK, 700 Ave Cent,38401 Domaine Univ, St Martin Dheres, France.
EM dmitry.ostrovsky@imag.fr; zaid.harchaoui@imag.fr;
   anatoli.juditsky@imag.fr; arkadi.nemirovski@imag.fr
RI Nemirovski, Arkadi S/A-8375-2009
OI Nemirovski, Arkadi S/0000-0002-5001-7420
FU LabEx PERSYVAL-Lab [ANR-11-LABX-0025]; project Titan (CNRS-Mastodons);
   project Macaron [ANR-14-CE23-0003-01]; MSR-Inria joint centre; program
   "Learning in Machines and Brains" (CIFAR); NSF [CMMI-1262063,
   CCF-1523768]
FX We would like to thank Arnak Dalalyan and Gabriel Peyre for fruitful
   discussions. DO, AJ, ZH were supported by the LabEx PERSYVAL-Lab
   (ANR-11-LABX-0025) and the project Titan (CNRS-Mastodons). ZH was also
   supported by the project Macaron (ANR-14-CE23-0003-01), the MSR-Inria
   joint centre, and the program "Learning in Machines and Brains" (CIFAR).
   Research of AN was supported by NSF grants CMMI-1262063, CCF-1523768.
CR Bhaskar BN, 2013, IEEE T SIGNAL PROCES, V61, P5987, DOI 10.1109/TSP.2013.2273443
   DONOHO DL, 1992, ANN STAT, V20, P944, DOI 10.1214/aos/1176348665
   DONOHO DL, 1994, ANN STAT, V22, P238, DOI 10.1214/aos/1176325367
   Harchaoui Z., 2015, P 28 C LEARN THEOR C, P929
   Haykin S., 1991, ADAPTIVE FILTER THEO
   IBRAGIMOV IA, 1985, THEOR PROBAB APPL+, V29, P18
   IBRAGIMOV IA, 1988, THEOR PROBAB APPL+, V32, P30, DOI 10.1137/1132002
   Juditsky A, 2010, APPL COMPUT HARMON A, V29, P354, DOI 10.1016/j.acha.2010.01.003
   Juditsky A, 2009, APPL COMPUT HARMON A, V27, P157, DOI 10.1016/j.acha.2009.02.001
   Juditsky AB, 2009, ANN STAT, V37, P2278, DOI 10.1214/08-AOS654
   Kailath T, 2000, PR H INF SY, pXIX
   Lepski O, 2014, ANN STAT, V42, P1, DOI 10.1214/13-AOS1152
   Mallat S., 1999, WAVELET TOUR SIGNAL
   Nesterov Y, 2013, ACTA NUMER, V22, P509, DOI 10.1017/S096249291300007X
   Ostrovsky D., 2016, ARXIV160705712V2
   Tsybakov A. B., 2008, INTRO NONPARAMETRIC
   Wasserman L, 2006, ALL NONPARAMETRIC ST
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700027
DA 2019-06-15
ER

PT S
AU Pan, XH
   Lam, M
   Tu, S
   Papailiopoulos, D
   Zhang, C
   Jordan, MI
   Ramchandran, K
   Re, C
   Recht, B
AF Pan, Xinghao
   Lam, Maximilian
   Tu, Stephen
   Papailiopoulos, Dimitris
   Zhang, Ce
   Jordan, Michael I.
   Ramchandran, Kannan
   Re, Chris
   Recht, Benjamin
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI CYCLADES: Conflict-free Asynchronous Machine Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We present CYCLADES, a general framework for parallelizing stochastic optimization algorithms in a shared memory setting. CYCLADES is asynchronous during model updates, and requires no memory locking mechanisms, similar to HOGWILD!-type algorithms. Unlike HOGWILD!, CYCLADES introduces no conflicts during parallel execution, and offers a black-box analysis for provable speedups across a large family of algorithms. Due to its inherent cache locality and conflict-free nature, our multi-core implementation of CYCLADES consistently outperforms HOGWILD!-type algorithms on sufficiently sparse datasets, leading to up to 40% speedup gains compared to HOGWILD!, and up to 5x gains over asynchronous implementations of variance reduction algorithms.
C1 [Pan, Xinghao; Lam, Maximilian; Tu, Stephen; Papailiopoulos, Dimitris; Jordan, Michael I.; Ramchandran, Kannan; Recht, Benjamin] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
   [Zhang, Ce; Re, Chris] Stanford Univ, Dept Comp Sci, Palo Alto, CA 94304 USA.
   [Jordan, Michael I.; Recht, Benjamin] Univ Calif Berkeley, Dept Stat, Berkeley, CA USA.
RP Pan, XH (reprint author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
CR Agarwal A., 2011, P ADV NEUR INF PROC, P873
   Arora Sanjeev, 2015, ARXIV150203520
   Bertsekas D. P., 1989, PARALLEL DISTRIBUTED, V23
   Chilimbi T., 2014, USENIX OSDI
   De Sa C., 2015, ARXIV150606438
   Dean J., 2012, NIPS
   Defazio A., 2014, ADV NEURAL INFORM PR, P1646
   Duchi J., 2013, ADV NEURAL INFORM PR, P2832
   Gemulla R., 2011, P 17 ACM SIGKDD INT, P69, DOI [10.1145/2020408.2020426, DOI 10.1145/2020408.2020426]
   Jin C., 2015, ARXIV151008896
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Krivelevich M., 2016, ELECTRON J COMB, V23, P1
   Liu J, 2015, SIAM J OPTIMIZ, V25, P351, DOI 10.1137/140961134
   Low Y, 2014, ARXIV14082041
   Ma Justin, 2009, P 26 ANN INT C MACH, P681, DOI DOI 10.1145/1553374.1553462
   Mania Horia, 2015, ARXIV150706970
   Niu F., 2011, ADV NEURAL INFORM PR, V24, P693
   Pan X., 2015, ADV NEURAL INFORM PR, P82
   Pan X., 2014, NIPS 27
   Pan X., 2013, NIPS 26
   Reddi S. J., 2015, ARXIV150606840
   Richtarik P., 2012, ARXIV12120873
   TSITSIKLIS JN, 1986, IEEE T AUTOMAT CONTR, V31, P803, DOI 10.1109/TAC.1986.1104412
   Zhang C, 2014, PROC VLDB ENDOW, V7, P1283, DOI 10.14778/2732977.2733001
   Zhuang Yong, 2013, P 7 ACM C REC SYST, P249, DOI DOI 10.1145/2507157.2507164
   Zinkevich M., 2009, ADV NEURAL INFORM PR, P2331
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973705018
DA 2019-06-15
ER

PT S
AU Papa, G
   Clemencon, S
   Bellet, A
AF Papa, Guillaume
   Clemencon, Stephan
   Bellet, Aurelien
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI On Graph Reconstruction via Empirical Risk Minimization: Fast Learning
   Rates and Scalability
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID U-STATISTICS; RANKING
AB The problem of predicting connections between a set of data points finds many applications, in systems biology and social network analysis among others. This paper focuses on the graph reconstruction problem, where the prediction rule is obtained by minimizing the average error over all n(n - 1)/2 possible pairs of the n nodes of a training graph. Our first contribution is to derive learning rates of order O-P (log n/n) for this problem, significantly improving upon the slow rates of order O-P (1/root n) established in the seminal work of Biau and Bleakley (2006). Strikingly, these fast rates are universal, in contrast to similar results known for other statistical learning problems (e.g., classification, density level set estimation, ranking, clustering) which require strong assumptions on the distribution of the data. Motivated by applications to large graphs, our second contribution deals with the computational complexity of graph reconstruction. Specifically, we investigate to which extent the learning rates can be preserved when replacing the empirical reconstruction risk by a computationally cheaper Monte-Carlo version, obtained by sampling with replacement B << n(2) pairs of nodes. Finally, we illustrate our theoretical results by numerical experiments on synthetic and real graphs.
C1 [Papa, Guillaume; Clemencon, Stephan] Univ Paris Saclay, Telecom ParisTech, CNRS, LTCI, F-75013 Paris, France.
   [Bellet, Aurelien] INRIA, F-59650 Villeneuve Dascq, France.
RP Papa, G (reprint author), Univ Paris Saclay, Telecom ParisTech, CNRS, LTCI, F-75013 Paris, France.
EM guillaume.papa@telecom-paristech.fr;
   stephan.clemencon@telecom-paristech.fr; aurelien.bellet@inria.fr
FU chair "Machine Learning for Big Data" of Telecom ParisTech; CPER
   Nord-Pas de Calais/FEDER DATA Advanced data science and technologies
   2015-2020
FX This work was partially supported by the chair "Machine Learning for Big
   Data" of Telecom ParisTech and by a grant from CPER Nord-Pas de
   Calais/FEDER DATA Advanced data science and technologies 2015-2020.
CR Agarwal S, 2014, J MACH LEARN RES, V15, P1653
   Antos A, 2005, IEEE T INFORM THEORY, V51, P4013, DOI 10.1109/TIT.2005.856976
   ARCONES MA, 1994, STOCH PROC APPL, V52, P17, DOI 10.1016/0304-4149(94)90098-1
   Bellet A., 2015, METRIC LEARNING
   Biau G, 2006, STATIST RISK MODEL, V24, P209, DOI 10.1524/stnd.2006.24.2.209
   Boucheron S, 2005, ANN PROBAB, V33, P514, DOI 10.1214/009117904000000856
   Clemencon S., 2011, ICML
   Clemencon S, 2008, ANN STAT, V36, P844, DOI 10.1214/009052607000000910
   Clemencon S, 2014, J MULTIVARIATE ANAL, V124, P42, DOI 10.1016/j.jmva.2013.10.001
   Clemencon S, 2010, CONSTR APPROX, V32, P619, DOI 10.1007/s00365-010-9084-9
   Cukierski W., 2011, IJCNN
   de la Pena V., 1999, DECOUPLING DEPENDENC
   HOEFFDING W, 1948, ANN MATH STAT, V19, P293, DOI 10.1214/aoms/1177730196
   HORVITZ DG, 1952, J AM STAT ASSOC, V47, P663, DOI 10.2307/2280784
   Jansen R, 2003, SCIENCE, V302, P449, DOI 10.1126/science.1087361
   JANSON S, 1991, PROBAB THEORY REL, V90, P341, DOI 10.1007/BF01193750
   Kanehisa M, 2001, Pharmacogenomics, V2, P373, DOI 10.1517/14622416.2.4.373
   Lee AJ, 1990, U STAT THEORY PRACTI
   Liben-Nowell D., 2003, CIKM
   Lichtenwalter Ryan N., 2010, KDD
   Mammen E, 1999, ANN STAT, V27, P1808
   Massart P., 2006, ANN STAT, V34
   Mattick JS, 2005, SCIENCE, V307, P856, DOI 10.1126/science.1103737
   Rigollet P, 2009, BERNOULLI, V15, P1154, DOI 10.3150/09-BEJ184
   Shaw B., 2011, NIPS
   Spielman D., 2005, LECT NOTES IPCO SUMM
   Tsybakov AB, 2004, ANN STAT, V32, P135
   Vert J. - P., 2004, ADV NEURAL INFORM PR, P1433
   Vert JP, 2007, BMC BIOINFORMATICS, V8, DOI 10.1186/1471-2105-8-S10-S8
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973705002
DA 2019-06-15
ER

PT S
AU Papaxanthos, L
   Llinares-Lopez, F
   Bodenham, D
   Borgwardt, K
AF Papaxanthos, Laetitia
   Llinares-Lopez, Felipe
   Bodenham, Dean
   Borgwardt, Karsten
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Finding significant combinations of features in the presence of
   categorical covariates
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID ASSOCIATION
AB In high-dimensional settings, where the number of features p is much larger than the number of samples n, methods that systematically examine arbitrary combinations of features have only recently begun to be explored. However, none of the current methods is able to assess the association between feature combinations and a target variable while conditioning on a categorical covariate. As a result, many false discoveries might occur due to unaccounted confounding effects.
   We propose the Fast Automatic Conditional Search (FACS) algorithm, a significant discriminative itemset mining method which conditions on categorical covariates and only scales as O(k log k), where k is the number of states of the categorical covariate. Based on the Cochran-Mantel-Haenszel Test, FACS demonstrates superior speed and statistical power on simulated and real-world datasets compared to the state of the art, opening the door to numerous applications in biomedicine.
C1 [Papaxanthos, Laetitia; Llinares-Lopez, Felipe; Bodenham, Dean; Borgwardt, Karsten] Swiss Fed Inst Technol, Machine Learning & Computat Biol Lab, D BSSE, Zurich, Switzerland.
RP Papaxanthos, L (reprint author), Swiss Fed Inst Technol, Machine Learning & Computat Biol Lab, D BSSE, Zurich, Switzerland.
FU SNSF Starting Grant 'Significant Pattern Mining'; Marie Curie ITN
   MLPM2012 [316861]
FX This work was funded in part by the SNSF Starting Grant 'Significant
   Pattern Mining' (KB) and the Marie Curie ITN MLPM2012, Grant No. 316861
   (KB, FLL).
CR Atwell S, 2010, NATURE, V465, P627, DOI 10.1038/nature08800
   Azencott CA, 2013, BIOINFORMATICS, V29, P171, DOI 10.1093/bioinformatics/btt238
   Bonferroni CE, 1936, PUBBLICAZIONI R I SU, V8, P3
   Devlin B, 1999, BIOMETRICS, V55, P997, DOI 10.1111/j.0006-341X.1999.00997.x
   DUNN OJ, 1959, ANN MATH STAT, V30, P192, DOI 10.1214/aoms/1177706374
   Fisher RA, 1922, J R STAT SOC, V85, P87, DOI 10.2307/2340521
   Llinares-Lopez F., 2015, P 21 ACM SIGKDD INT, P725
   Llinares-Lopez F, 2015, BIOINFORMATICS, V31, P240, DOI 10.1093/bioinformatics/btv263
   MANTEL N, 1959, J NATL CANCER I, V22, P719
   Minato Shin-ichi, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8725, P422, DOI 10.1007/978-3-662-44851-9_27
   Pearson K, 1900, PHILOS MAG, V50, P157, DOI 10.1080/14786440009463897
   Price AL, 2006, NAT GENET, V38, P904, DOI 10.1038/ng1847
   Sugiyama M., 2015, SIAM DATA MINING SDM
   TARONE RE, 1990, BIOMETRICS, V46, P515, DOI 10.2307/2531456
   Terada A, 2013, P NATL ACAD SCI USA, V110, P12996, DOI 10.1073/pnas.1302233110
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Vilhjalmsson BJ, 2013, NAT REV GENET, V14, P1, DOI 10.1038/nrg3382
   Webb G.I., 2006, P 12 ACM SIGKDD INT, P434, DOI [10.1145/1150402.1150451, DOI 10.1145/1150402.1150451]
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702092
DA 2019-06-15
ER

PT S
AU Patel, AB
   Nguyen, T
   Baraniuk, RG
AF Patel, Ankit B.
   Tan Nguyen
   Baraniuk, Richard G.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI A Probabilistic Framework for Deep Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We develop a probabilistic framework for deep learning based on the Deep Rendering Mixture Model (DRMM), a new generative probabilistic model that explicitly capture variations in data due to latent task nuisance variables. We demonstrate that max-sum inference in the DRMM yields an algorithm that exactly reproduces the operations in deep convolutional neural networks (DCNs), providing a first principles derivation. Our framework provides new insights into the successes and shortcomings of DCNs as well as a principled route to their improvement. DRMM training via the Expectation-Maximization (EM) algorithm is a powerful alternative to DCN back-propagation, and initial training results are promising. Classification based on the DRMM and other variants outperforms DCNs in supervised digit classification, training 2-3. faster while achieving similar accuracy. Moreover, the DRMM is applicable to semi-supervised and unsupervised learning tasks, achieving results that are state-of-the-art in several categories on the MNIST benchmark and comparable to state of the art on the CIFAR10 benchmark.
C1 [Patel, Ankit B.] Rice Univ, Baylor Coll Med, Houston, TX 77251 USA.
   [Tan Nguyen; Baraniuk, Richard G.] Rice Univ, Houston, TX 77251 USA.
RP Patel, AB (reprint author), Rice Univ, Baylor Coll Med, Houston, TX 77251 USA.
EM ankitp@bcm.edu; mn15@rice.edu; richb@rice.edu
FU IARPA via DoI/IBC [D16PC00003]; NSF [CCF-1527501]; AFOSR
   [FA9550-14-1-0088]; ARO [W911NF-15-1-0316]; ONR [N00014-12-1-0579]; NSF;
   NSF IGERT Training Grant [DGE-1250104]
FX Thanks to Xaq Pitkow and Ben Poole for helpful feedback. ABP and RGB
   were supported by IARPA via DoI/IBC contract D16PC00003. <SUP>1</SUP>
   RGB was supported by NSF CCF-1527501, AFOSR FA9550-14-1-0088, ARO
   W911NF-15-1-0316, and ONR N00014-12-1-0579. TN was supported by an NSF
   Graduate Reseach Fellowship and NSF IGERT Training Grant (DGE-1250104).
CR Anselmi F., 2013, MAGIC MAT THEORY DEE
   Arora S., 2013, ARXIV13106343
   Bishop C. M., 2006, PATTERN RECOGNITION, V4
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230
   Goodfellow I.J., 2013, ARXIV13024389
   Kingma D. P., 2014, ADV NEURAL INFORM PR, V4, P3581
   Kingma D.P., 2013, ARXIV13126114
   Lasserre J, 2007, BAYESIAN STAT, V8, P3
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee D. - H., 2013, WORKSH CHALL REPR LE, V3
   Maaloe Lars, 2016, ARXIV160205473
   Makhzani A., 2015, ADV NEURAL INFORM PR, P2773
   Montufar G. F., 2014, ADV NEURAL INFORM PR, P2924
   Ng AY, 2002, ADV NEUR IN, V14, P841
   Patel A. B., 2015, ARXIV150400641
   Rasmus A, 2015, ADV NEURAL INFORM PR, P3532
   Rifai S., 2011, ADV NEURAL INFORM PR, P2294
   Rifai S., 2011, ICML, P833
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salimans T., 2016, ARXIV160603498
   Sheikh AS, 2014, J MACH LEARN RES, V15, P2653
   Simonyan K., 2013, 13126034 ARXIV
   Soatto S., 2016, INT C LEARN REPR
   Springenberg J. T., 2014, 14126806 ARXIV
   Springenberg Jost Tobias, 2015, ARXIV151106390
   Tang Y., 2012, ARXIV12064635
   Taylor G., 2016, ARXIV160502026
   van den Oord A., 2014, ADV NEURAL INFORM PR, P3518
   Vapnik V. N., 1998, STAT LEARNING THEORY, V1
   Zhao J., 2016, ARXIV150602351
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701088
DA 2019-06-15
ER

PT S
AU Pazis, J
   Parr, R
   How, JP
AF Pazis, Jason
   Parr, Ronald
   How, Jonathan P.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Improving PAC Exploration Using the Median of Means
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We present the first application of the median of means in a PAC exploration algorithm for MDPs. Using the median of means allows us to significantly reduce the dependence of our bounds on the range of values that the value function can take, while introducing a dependence on the (potentially much smaller) variance of the Bellman operator. Additionally, our algorithm is the first algorithm with PAC bounds that can be applied to MDPs with unbounded rewards.
C1 [Pazis, Jason] MIT, Lab Informat & Decis Syst, Cambridge, MA 02139 USA.
   [Parr, Ronald] Duke Univ, Dept Comp Sci, Durham, NC 27708 USA.
   [How, Jonathan P.] MIT, Dept Aeronaut & Astronaut, Aerosp Controls Lab, Cambridge, MA 02139 USA.
RP Pazis, J (reprint author), MIT, Lab Informat & Decis Syst, Cambridge, MA 02139 USA.
EM jpazis@mit.edu; parr@cs.duke.edu; jhow@mit.edu
FU ONR MURI Grant [N000141110688]; National Science Foundation
   [IIS-1218931]; Boeing Company
FX We would like to thank Emma Brunskill, Tor Lattimore, and Christoph Dann
   for spotting an error in an earlier version of this paper, as well as
   the anonymous reviewers for helpful comments and suggestions. This
   material is based upon work supported in part by The Boeing Company, by
   ONR MURI Grant N000141110688, and by the National Science Foundation
   under Grant No. IIS-1218931. Opinions, findings, conclusions or
   recommendations herein are those of the authors and not necessarily
   those of the NSF.
CR Alon N, 1999, J COMPUT SYST SCI, V58, P137, DOI 10.1006/jcss.1997.1545
   Bartlett Peter L, 2009, P 25 C UNC ART INT U, P35
   Dann Christoph, 2015, ADV NEURAL INFORM PR
   Guo Z., 2015, AAAI C ART INT, P2624
   HARRISON JM, 1972, ANN MATH STAT, V43, P636, DOI 10.1214/aoms/1177692643
   Jaksch T, 2010, J MACH LEARN RES, V11, P1563
   Lattimore Tor, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P320, DOI 10.1007/978-3-642-34106-9_26
   Maillard  O.-A., 2014, ADV NEURAL INFORM PR, P1835
   McDiarmid C., 1989, SURVEYS COMBINATORIC, V141, P148
   Osband Ian, 2014, ADV NEURAL INFORM PR, P1466
   Pazis J., 2013, P 27 AAAI C ART INT, P774
   Pazis Jason, 2016, AAAI C ART INT
   Puterman M. L., 1994, MARKOV DECISION PROC
   Strehl AL, 2009, J MACH LEARN RES, V10, P2413
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Szita  I., 2010, INT C MACH LEARN ICM, P1031
NR 16
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704101
DA 2019-06-15
ER

PT S
AU Pentina, A
   Urner, R
AF Pentina, Anastasia
   Urner, Ruth
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Lifelong Learning with Weighted Majority Votes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Better understanding of the potential benefits of information transfer and representation learning is an important step towards the goal of building intelligent systems that are able to persist in the world and learn over time. In this work, we consider a setting where the learner encounters a stream of tasks but is able to retain only limited information from each encountered task, such as a learned predictor. In contrast to most previous works analyzing this scenario, we do not make any distributional assumptions on the task generating process. Instead, we formulate a complexity measure that captures the diversity of the observed tasks. We provide a lifelong learning algorithm with error guarantees for every observed task (rather than on average). We show sample complexity reductions in comparison to solving every task in isolation in terms of our task complexity measure. Further, our algorithmic framework can naturally be viewed as learning a representation from encountered tasks with a neural network.
C1 [Pentina, Anastasia] IST Austria, Klosterneuburg, Austria.
   [Urner, Ruth] Max Planck Inst Intelligent Syst, Tubingen, Germany.
RP Pentina, A (reprint author), IST Austria, Klosterneuburg, Austria.
EM apentina@ist.ac.at; rurner@tuebingen.mpg.de
FU European Research Council under the European Union [308036]
FX This work was in parts funded by the European Research Council under the
   European Union's Seventh Framework Programme (FP7/2007-2013)/ERC grant
   agreement no 308036.
CR Abernethy J., 2007, WORKSH COMP LEARN TH
   Argyriou A., 2008, MACHINE LEARNING
   Balcan  Maria-Florina, 2015, WORKSH COMP LEARN TH
   Baxter Jonathan, 2000, J ARTIFICIAL INTELLI, V12
   Ben- David S., 2010, MACHINE LEARNING
   Ben-David S., 2003, EXPLOITING TASK RELA
   Boucheron Stephane, 2005, ESAIM-PROBAB STAT, V9, P323, DOI DOI 10.1051/ps:2005018
   Crammer K., 2012, C NEUR INF PROC SYST
   Eaton E., 2013, INT C MACH LEARN
   Evgeniou A., 2007, C NEUR INF PROC SYST
   Freund Y., 1996, INT C MACH LEARN
   Germain P, 2016, INT C MACH LEARN
   Kifer D., 2004, INT C VER LARG DAT B
   Kolter JZ, 2007, J MACH LEARN RES, V8, P2755
   Kumar A., 2012, INT C MACH LEARN
   Kuzborskij I., 2013, INT C MACH LEARN
   Mansour Y., 2009, WORKSH COMP LEARN TH
   Maurer A., 2013, INT C MACH LEARN
   Maurer A, 2009, MACH LEARN, V75, P327, DOI 10.1007/s10994-009-5109-7
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pentina A., 2014, INT C MACH LEARN
   Pentina A., 2015, ALGORITHMIC LEARNING
   Pontil M., 2013, WORKSH COMP LEARN TH
   Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN
   THRUN S, 1995, ROBOT AUTON SYST, V15, P25, DOI 10.1016/0921-8890(95)00004-Y
   Thrun S., 1998, LEARNING LEARN
   VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700059
DA 2019-06-15
ER

PT S
AU Perrot, M
   Courty, N
   Flamary, R
   Habrard, A
AF Perrot, Michael
   Courty, Nicolas
   Flamary, Remi
   Habrard, Amaury
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Mapping Estimation for Discrete Optimal Transport
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We are interested in the computation of the transport map of an Optimal Transport problem. Most of the computational approaches of Optimal Transport use the Kantorovich relaxation of the problem to learn a probabilistic coupling gamma but do not address the problem of learning the underlying transport map T linked to the original Monge problem. Consequently, it lowers the potential usage of such methods in contexts where out-of-samples computations are mandatory. In this paper we propose a new way to jointly learn the coupling and an approximation of the transport map. We use a jointly convex formulation which can be efficiently optimized. Additionally, jointly learning the coupling and the transport map allows to smooth the result of the Optimal Transport and generalize it to out-of-samples examples. Empirically, we show the interest and the relevance of our method in two tasks: domain adaptation and image editing.
C1 [Perrot, Michael; Habrard, Amaury] Univ Lyon, UJM St Etienne, CNRS, Lab Hubert Curien UMR 5516, F-42023 St Etienne, France.
   [Courty, Nicolas] Univ Bretagne Sud, IRISA, UMR 6074, CNRS, Lorient, France.
   [Flamary, Remi] Univ Cote dAzur, UMR 7293, CNRS, Lagrange,OCA, Nice, France.
RP Perrot, M (reprint author), Univ Lyon, UJM St Etienne, CNRS, Lab Hubert Curien UMR 5516, F-42023 St Etienne, France.
EM michael.perrot@univ-st-etienne.fr; courty@univ-ubs.fr;
   remi.flamary@unice.fr; amaury.habrard@univ-st-etienne.fr
FU french ANR project LIVES [ANR-15-CE23-0026-03]
FX This work was supported in part by the french ANR project LIVES
   ANR-15-CE23-0026-03.
CR Benamou J.-D., 2014, J COMPUTATIONAL PHYS, V260
   Benamou J.-D., 2015, SISC
   Bruzzone L., 2010, IEEE PAML, V32
   Canas G., 2012, NIPS
   Courty N., 2014, ECML PKDD
   Cuturi M., 2013, NIPS
   Cuturi M., 2016, SIIMS
   Cuturi M., 2014, ICML
   Deng F., 2012, ACCV CHAPTER COLOR A
   Donahue J., 2014, ICML
   Fernando B., 2013, ICCV
   Ferradans S., 2014, SIIMS
   Frank M., 1956, NRL, V3
   Frogner C., 2015, NIPS
   Gong B., 2012, CVPR
   Jaggi  M., 2013, ICML
   Kantorovich L., 1942, C R DOKLADY ACAD SCI, V37
   McCann R., 1997, ADV MATH, V128
   Mueller J., 2015, NIPS
   Perez P., 2003, ACM T GRAPHICS, V22
   Perrot M., 2015, NIPS
   Reich S., 2013, SISC
   Seguy V., 2015, NIPS
   Solomon J., 2014, ICML
   Solomon J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766963
   Tseng P., 2001, J OPTIMIZATION THEOR, V109
   Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5
   Zhong E., 2010, ECML PKDD
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702059
DA 2019-06-15
ER

PT S
AU Petrik, M
   Ghavamzadeh, M
   Chow, Y
AF Petrik, Marek
   Ghavamzadeh, Mohammad
   Chow, Yinlam
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Safe Policy Improvement by Minimizing Robust Baseline Regret
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB An important problem in sequential decision-making under uncertainty is to use limited data to compute a safe policy, which is guaranteed to outperform a given baseline strategy. In this paper, we develop and analyze a new model-based approach that computes a safe policy, given an inaccurate model of the system's dynamics and guarantees on the accuracy of this model. The new robust method uses this model to directly minimize the (negative) regret w.r.t. the baseline policy. Contrary to existing approaches, minimizing the regret allows one to improve the baseline policy in states with accurate dynamics and to seamlessly fall back to the baseline policy, otherwise. We show that our formulation is NP-hard and propose a simple approximate algorithm. Our empirical results on several domains further show that even the simple approximate algorithm can outperform standard approaches.
C1 [Petrik, Marek] Univ New Hampshire, Durham, NH 03824 USA.
   [Ghavamzadeh, Mohammad] Adobe Res, Lille, France.
   [Ghavamzadeh, Mohammad] INRIA Lille, Lille, France.
   [Chow, Yinlam] Stanford Univ, Stanford, CA 94305 USA.
RP Petrik, M (reprint author), Univ New Hampshire, Durham, NH 03824 USA.
EM mpetrik@cs.unh.edu; ghavamza@adobe.com; ychow@stanford.edu
CR Ahmed A., 2013, ARAB J GEOSCI, P1
   Hansen TD, 2013, J ACM, V60, DOI 10.1145/2432622.2432623
   Iyengar GN, 2005, MATH OPER RES, V30, P257, DOI 10.1287/moor.1040.0129
   Kakade  S., 2002, ICML, P267
   LeTallec  Yann, 2007, THESIS
   Nilim A, 2005, OPER RES, V53, P780, DOI 10.1287/opre.1050.0216
   Petrik M., 2015, UNCERTAINTY ARTIFICI, P692
   Petrik  Marek, 2014, NEURAL INFORM PROCES
   Pirotta M., 2013, P 30 INT C MACH LEAR
   Thomas P., 2015, P 29 C ART INT
   Thomas Philip S., 2015, P 32 INT C MACH LEAR, P2380
   Weissman T., 2003, TECH REP
   Wiesemann W, 2013, MATH OPER RES, V38, P153, DOI 10.1287/moor.1120.0566
   Xu H, 2009, IEEE DECIS CONTR P, P3606, DOI 10.1109/CDC.2009.5400796
NR 14
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702041
DA 2019-06-15
ER

PT S
AU Ping, W
   Liu, Q
   Ihler, A
AF Ping, Wei
   Liu, Qiang
   Ihler, Alexander
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning Infinite RBMs with Frank-Wolfe
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB In this work, we propose an infinite restricted Boltzmann machine (RBM), whose maximum likelihood estimation (MLE) corresponds to a constrained convex optimization. We consider the Frank-Wolfe algorithm to solve the program, which provides a sparse solution that can be interpreted as inserting a hidden unit at each iteration, so that the optimization process takes the form of a sequence of finite models of increasing complexity. As a side benefit, this can be used to easily and efficiently identify an appropriate number of hidden units during the optimization. The resulting model can also be used as an initialization for typical state-of-the-art RBM training algorithms such as contrastive divergence, leading to models with consistently higher test likelihood than random initialization.
C1 [Ping, Wei; Ihler, Alexander] UC Irvine, Comp Sci, Irvine, CA 92697 USA.
   [Liu, Qiang] Dartmouth Coll, Comp Sci, Hanover, NH 03755 USA.
RP Ping, W (reprint author), UC Irvine, Comp Sci, Irvine, CA 92697 USA.
EM wping@ics.uci.edu; qliu@cs.dartmouth.edu; ihler@ics.uci.edu
RI Ping, Wei/O-4470-2019
FU NSF [IIS-1254071, CCF-1331915]; United States Air Force under the DARPA
   PPAML program [FA8750-14-C-0011]
FX This work is sponsored in part by NSF grants IIS-1254071 and
   CCF-1331915. It is also funded in part by the United States Air Force
   under Contract No. FA8750-14-C-0011 under the DARPA PPAML program.
CR Aslan O., 2013, NIPS
   Bach F., 2014, ARXIV14128690
   Belanger D., 2013, NIPS WORKSH GREED OP
   Bengio Y., 2005, NIPS
   Beygelzimer A., 2015, NIPS
   Bradley D. M., 2009, UAI
   Clarkson KL, 2010, ACM T ALGORITHMS, V6, DOI 10.1145/1824777.1824783
   Cote M.-A., 2015, NEURAL COMPUTATION
   Frank Marguerite, 1956, NAVAL RES LOGISTICS
   Friedman Jerome, 2001, ANN STAT
   Guelat J., 1986, MATH PROGRAMMING
   Hinton G., 2010, UTML TR
   Hinton G. E., 2002, NEURAL COMPUTATION
   Hinton G. E., 2006, NEURAL COMPUTATION
   Jaggi  M., 2013, ICML
   Krishnan R. G., 2015, NIPS
   Krizhevsky A., 2010, AISTATS
   Lacoste-Julien S., 2013, ICML
   Lecun Y., 1998, P IEEE
   Likas A., 2003, PATTERN RECOGNITION
   Marlin B. M., 2010, AISTATS
   Nair V., 2010, ICML
   Nalisnick E., 2015, ARXIV151105392
   Nowozin S., 2008, ICML
   Orbanz P., 2011, ENCY MACHINE LEARNIN
   Salakhutdinov R., 2008, ICML
   Salakhutdinov R., 2007, ICML
   Salakhutdinov R., 2009, AISTATS
   Smolensky P., 1986, TECHNICAL REPORT
   Taylor G. W., 2006, NIPS
   Tieleman T., 2008, ICML
   Verbeek J. J., 2003, NEURAL COMPUTATION
   Welling M., 2002, NIPS
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702089
DA 2019-06-15
ER

PT S
AU Poole, B
   Lahiri, S
   Raghu, M
   Sohl-Dickstein, J
   Ganguli, S
AF Poole, Ben
   Lahiri, Subhaneil
   Raghu, Maithra
   Sohl-Dickstein, Jascha
   Ganguli, Surya
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Exponential expressivity in deep neural networks through transient chaos
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We combine Riemannian geometry with the mean field theory of high dimensional chaos to study the nature of signal propagation in generic, deep neural networks with random weights. Our results reveal an order-to-chaos expressivity phase transition, with networks in the chaotic phase computing nonlinear functions whose global curvature grows exponentially with depth but not width. We prove this generic class of deep random functions cannot be efficiently computed by any shallow network, going beyond prior work restricted to the analysis of single functions. Moreover, we formalize and quantitatively demonstrate the long conjectured idea that deep networks can disentangle highly curved manifolds in input space into flat manifolds in hidden space. Our theoretical analysis of the expressive power of deep networks broadly applies to arbitrary nonlinearities, and provides a quantitative underpinning for previously abstract notions about the geometry of deep functions.
C1 [Poole, Ben; Lahiri, Subhaneil; Ganguli, Surya] Stanford Univ, Stanford, CA 94305 USA.
   [Raghu, Maithra; Sohl-Dickstein, Jascha] Google Brain, Mountain View, CA USA.
RP Poole, B (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM benpoole@stanford.edu; sulahiri@stanford.edu; maithra@google.com;
   jaschasd@google.com; sganguli@stanford.edu
CR Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bianchini M, 2014, IEEE T NEUR NET LEAR, V25, P1553, DOI 10.1109/TNNLS.2013.2293637
   Delalleau O., 2011, ADV NEURAL INFORM PR, P666
   DiCarlo JJ, 2007, TRENDS COGN SCI, V11, P333, DOI 10.1016/j.tics.2007.06.010
   Eldan R., 2015, ARXIV151203965
   Hannun A., 2014, ARXIV14125567
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee J M, 2006, RIEMANNIAN MANIFOLDS, V176, DOI 10.1007/b98852
   Martens J., 2013, ADV NEURAL INFORM PR, P2877
   McIntosh Lane T., 2016, ADV NEURAL INFORM PR
   Mhaskar H., 2016, ARXIV160300988
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Montufar G. F., 2014, ADV NEURAL INFORM PR, P2924
   Piech C., 2015, ADV NEURAL INFORM PR, P505
   Raghu M., 2016, ARXIV160605336
   SOMPOLINSKY H, 1988, PHYS REV LETT, V61, P259, DOI 10.1103/PhysRevLett.61.259
   Telgarsky M, 2015, ARXIV150908101
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702069
DA 2019-06-15
ER

PT S
AU Pu, YC
   Gan, Z
   Henao, R
   Yuan, X
   Li, CY
   Stevens, A
   Carin, L
AF Pu, Yunchen
   Gan, Zhe
   Henao, Ricardo
   Yuan, Xin
   Li, Chunyuan
   Stevens, Andrew
   Carin, Lawrence
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Variational Autoencoder for Deep Learning of Images, Labels and Captions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB A novel variational autoencoder is developed to model images, as well as associated labels or captions. The Deep Generative Deconvolutional Network (DGDN) is used as a decoder of the latent image features, and a deep Convolutional Neural Network (CNN) is used as an image encoder; the CNN is used to approximate a distribution for the latent DGDN features/code. The latent code is also linked to generative models for labels (Bayesian support vector machine) or captions (recurrent neural network). When predicting a label/caption for a new image at test, averaging is performed across the distribution of latent codes; this is computationally efficient as a consequence of the learned CNN-based encoder. Since the framework is capable of modeling the image in the presence/absence of associated labels/captions, a new semi-supervised setting is manifested for CNN learning with images; the framework even allows unsupervised CNN learning, based on images alone.
C1 [Pu, Yunchen; Gan, Zhe; Henao, Ricardo; Li, Chunyuan; Stevens, Andrew; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA.
   [Yuan, Xin] Nokia Bell Labs, Murray Hill, NJ USA.
RP Pu, YC (reprint author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA.
EM yp42@duke.edu; zg27@duke.edu; r.henao@duke.edu; xyuan@bell-labs.com;
   cl319@duke.edu; ajs104@duke.edu; lcarin@duke.edu
FU ARO; DARPA; DOE; NGA; ONR; NSF
FX This research was supported in part by ARO, DARPA, DOE, NGA, ONR and
   NSF. The Titan X used in this work was donated by the NVIDIA
   Corporation.
CR Banerjee S., 2005, ACL WORKSH
   Bastien F., 2012, NIPS WORKSH
   Cho  K., 2014, EMNLP
   Dosovitskiy Alexey, 2015, CVPR
   Fang H, 2015, CVPR
   Griffin G., 2007, CALTECH 256 OBJECT C
   Henao R., 2014, NIPS
   Hochreiter S., 1997, NEURAL COMPUTATION
   Hodosh M., 2013, J ARTIFICIAL INTELLI
   Kingma D. P., 2015, ICLR
   Kingma D. P., 2014, NIPS
   Kingma Diederik P, 2014, ICLR
   Krizhevsky A., 2009, TECH REP
   Krizhevsky A., 2012, NIPS
   Kulkarni Tejas D, 2015, NIPS
   LeCun Y., 1989, NEURAL COMPUTATION
   Li C., 2015, NIPS
   Li F., 2007, COMPUTER VISION IMAG
   Lin T.-Y., 2014, ECCV
   Mnih A., 2014, ICML
   Papineni K., 2002, T ASS COMPUTATIONAL
   Polson N. G., 2011, BAYES ANAL
   Pu Y., 2015, ICLR WORKSH
   Pu Y., 2016, AISTATS
   Rasmus A., 2015, NIPS
   Simonyan Karen, 2015, ICLR
   Sutskever  I., 2014, NIPS
   Szegedy C., 2015, CVPR
   Vapnik VN, 1995, NATURE STAT LEARNING
   Vedantam Ramakrishna, 2015, CVPR
   Vinyals O, 2015, CVPR
   Wu Q., 2016, CVPR
   Xu K, 2015, ICML
   Young P., 2014, T ASS COMPUTATIONAL
NR 34
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704052
DA 2019-06-15
ER

PT S
AU Rabusseau, G
   Kadri, H
AF Rabusseau, Guillaume
   Kadri, Hachem
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Low-Rank Regression with Tensor Responses
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB This paper proposes an efficient algorithm (HOLRR) to handle regression tasks where the outputs have a tensor structure. We formulate the regression problem as the minimization of a least square criterion under a multilinear rank constraint, a difficult non convex problem. HOLRR computes efficiently an approximate solution of this problem, with solid theoretical guarantees. A kernel extension is also presented. Experiments on synthetic and real data show that HOLRR computes accurate solutions while being computationally very competitive.
C1 [Rabusseau, Guillaume; Kadri, Hachem] Aix Marseille Univ, CNRS, LIF, Marseille, France.
RP Rabusseau, G (reprint author), Aix Marseille Univ, CNRS, LIF, Marseille, France.
EM guillaume.rabusseau@lif.univ.mrs.fr; hachem.kadri@lif.univ.mrs.fr
FU ANR JCJC program MAD [ANR- 14-CE27-0002]
FX We thank Francois Denis and the reviewers for their helpful comments and
   suggestions. This work was partially supported by ANR JCJC program MAD
   (ANR- 14-CE27-0002).
CR ANDERSON TW, 1951, ANN MATH STAT, V22, P327, DOI 10.1214/aoms/1177729580
   Bahadori M. T., 2014, NIPS
   BALDI P, 1989, NEURAL NETWORKS, V2, P53, DOI 10.1016/0893-6080(89)90014-2
   Cichocki A, 2009, NONNEGATIVE MATRIX T
   De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1253, DOI 10.1137/S0895479896305696
   Foygel R., 2012, NIPS
   Gandy S, 2011, INVERSE PROBL, V27, DOI 10.1088/0266-5611/27/2/025010
   Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806
   Hillar CJ, 2013, J ACM, V60, DOI 10.1145/2512329
   Izenman A. J., 1975, Journal of Multivariate Analysis, V5, P248, DOI 10.1016/0047-259X(75)90042-1
   Izenman AJ, 2008, SPRINGER TEXTS STAT, P1, DOI 10.1007/978-0-387-78189-1_1
   Kar Purushottam, 2012, AISTATS
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Long X., 2012, UBICOMP
   Lozano A. C., 2009, KDD
   Lu H., 2013, MULTILINEAR SUBSPACE
   Mohri M., 2012, FDN MACHINE LEARNING
   Mukherjee Ashin, 2011, Stat Anal Data Min, V4, P612, DOI 10.1002/sam.10138
   Nickel Maximilian, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference (ECML PKDD 2013). Proceedings: LNCS 8189, P272, DOI 10.1007/978-3-642-40991-2_18
   Rahimi A., 2007, NIPS
   Reinsel G. C., 1998, LECT NOTES STAT
   Romera-paredes B., 2013, ICML
   Signoretto M., 2013, ARXIV13104977
   Signoretto M., 2013, MACH LEARN, P1
   Srebro N., 2004, NIPS
   WARREN HE, 1968, T AM MATH SOC, V133, P167, DOI 10.2307/1994937
   Wimalawarne K., 2014, NIPS
   Zhao Q., 2013, ICASSP
   Zhao QB, 2013, IEEE T PATTERN ANAL, V35, P1660, DOI 10.1109/TPAMI.2012.254
   Zhou H, 2013, J AM STAT ASSOC, V108, P540, DOI 10.1080/01621459.2013.776499
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702049
DA 2019-06-15
ER

PT S
AU Rae, JW
   Hunt, JJ
   Harley, T
   Danihelka, I
   Senior, A
   Wayne, G
   Graves, A
   Lillicrap, TP
AF Rae, Jack W.
   Hunt, Jonathan J.
   Harley, Tim
   Danihelka, Ivo
   Senior, Andrew
   Wayne, Greg
   Graves, Alex
   Lillicrap, Timothy P.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Neural networks augmented with external memory have the ability to learn algorithmic solutions to complex tasks. These models appear promising for applications such as language modeling and machine translation. However, they scale poorly in both space and time as the amount of memory grows - limiting their applicability to real-world domains. Here, we present an end-to-end differentiable memory access scheme, which we call Sparse Access Memory (SAM), that retains the representational power of the original approaches whilst training efficiently with very large memories. We show that SAM achieves asymptotic lower bounds in space and time complexity, and find that an implementation runs 1,000 x faster and with 3,000 x less physical memory than non-sparse models. SAM learns with comparable data efficiency to existing models on a range of synthetic tasks and one-shot Omniglot character recognition, and can scale to tasks requiring 100,000s of time steps and memories. As well, we show how our approach can be adapted for models that maintain temporal associations between memories, as with the recently introduced Differentiable Neural Computer.
C1 [Rae, Jack W.; Hunt, Jonathan J.; Harley, Tim; Danihelka, Ivo; Senior, Andrew; Wayne, Greg; Graves, Alex; Lillicrap, Timothy P.] Google DeepMind, London, England.
RP Rae, JW (reprint author), Google DeepMind, London, England.
EM jwrae@gmail.com; jjhunt@gmail.com; tharley@gmail.com;
   danihelka@gmail.com; andrewsenior@gmail.com; gregwayne@gmail.com;
   gravesa@gmail.com; countzero@gmail.com
CR Arya S, 1998, J ACM, V45, P891, DOI 10.1145/293347.293348
   Bahdanau D., 2014, ARXIV14090473
   Bawa M., 2005, P 14 INT C WORLD WID, P651, DOI DOI 10.1145/1060745.1060840
   Brady TF, 2008, P NATL ACAD SCI USA, V105, P14325, DOI 10.1073/pnas.0803390105
   Collobert  R., 2011, BIGLEARN
   Graves A., 2014, ARXIV14105401
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Graves Alex, 2016, NATURE
   Guennebaud G., 2010, EIGEN V3
   Hill F., 2015, ARXIV151102301
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   Lakshminarayanan B., 2014, ADV NEURAL INFORM PR, P3140
   Motwani R, 2007, SIAM J DISCRETE MATH, V21, P930, DOI 10.1137/050646858
   Muja Marius, 2014, PATTERN ANAL MACHINE, V36
   Santoro A., 2016, INT C MACH LEARN
   Sukhbaatar S., 2015, ADV NEURAL INFORM PR, P2431
   Sutskever  I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P2
   Weston J, 2015, ARXIV150205698
   Weston J., 2014, ARXIV14103916
   Zaremba Wojciech, 2015, ARXIV150500521
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702045
DA 2019-06-15
ER

PT S
AU Ragain, S
   Ugander, J
AF Ragain, Stephen
   Ugander, Johan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Pairwise Choice Markov Chains
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID MODELS; ELIMINATION; CONTEXT; AXIOM
AB As datasets capturing human choices grow in richness and scale-particularly in online domains-there is an increasing need for choice models that escape traditional choice-theoretic axioms such as regularity, stochastic transitivity, and Luce's choice axiom. In this work we introduce the Pairwise Choice Markov Chain (PCMC) model of discrete choice, an inferentially tractable model that does not assume any of the above axioms while still satisfying the foundational axiom of uniform expansion, a considerably weaker assumption than Luce's choice axiom. We show that the PCMC model significantly outperforms both the Multinomial Logit (MNL) model and a mixed MNL (MMNL) model in prediction tasks on both synthetic and empirical datasets known to exhibit violations of Luce's axiom. Our analysis also synthesizes several recent observations connecting the Multinomial Logit model and Markov chains; the PCMC model retains the Multinomial Logit model as a special case.
C1 [Ragain, Stephen; Ugander, Johan] Stanford Univ, Management Sci & Engn, Stanford, CA 94305 USA.
RP Ragain, S (reprint author), Stanford Univ, Management Sci & Engn, Stanford, CA 94305 USA.
EM sragain@stanford.edu; jugander@stanford.edu
FU David Morgenthaler II Faculty Fellowship; Dantzig-Lieberman Operations
   Research Fellowship
FX This work was supported in part by a David Morgenthaler II Faculty
   Fellowship and a Dantzig-Lieberman Operations Research Fellowship.
CR ADAMS E, 1958, PSYCHOMETRIKA, V23, P355, DOI 10.1007/BF02289784
   Benson Austin R., 2016, WWW
   Blanchet J., 2013, EC
   Block H. D., 1960, CONTRIBUTIONS PROBAB, P97, DOI DOI 10.1007/978-94-010-9276-0_8
   BORSCHSUPAN A, 1990, J ECONOMETRICS, V43, P373, DOI 10.1016/0304-4076(90)90126-E
   BOYD JH, 1980, TRANSPORT RES A-POL, V14, P367, DOI 10.1016/0191-2607(80)90055-2
   BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.2307/2334029
   Chen S., 2016, WSDM
   Debreu G., 1960, AM EC REV
   FALMAGNE JC, 1978, J MATH PSYCHOL, V18, P52, DOI 10.1016/0022-2496(78)90048-2
   HARARY F, 1966, AM MATH MON, V73, P231, DOI 10.2307/2315334
   HUBER J, 1982, J CONSUM RES, V9, P90, DOI 10.1086/208899
   Ieong S., 2012, ICML
   Kleinberg J., 2015, EC, P511
   Kohli R, 2015, OPER RES, V63, P512, DOI 10.1287/opre.2015.1373
   Koppelman F. S, 2006, SELF INSTRUCTING COU, V31
   Kumar R., 2015, P 8 ACM INT C WEB SE, P359
   Luce R. D., 1959, INDIVIDUAL CHOICE BE
   LUCE RD, 1977, J MATH PSYCHOL, V15, P215, DOI 10.1016/0022-2496(77)90032-3
   MANSKI CF, 1977, THEOR DECIS, V8, P229, DOI 10.1007/BF00133443
   Maystre L., 2015, NIPS
   McFadden D, 2000, J APPL ECONOM, V15, P447, DOI 10.1002/1099-1255(200009/10)15:5<447::AID-JAE570>3.0.CO;2-1
   MCFADDEN D, 1980, J BUS, V53, pS13, DOI 10.1086/296093
   Negahban S., 2015, ARXIV12091688V4
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   SIMONSON I, 1992, J MARKETING RES, V29, P281, DOI 10.2307/3172740
   Thurstone LL, 1927, PSYCHOL REV, V34, P273, DOI 10.1037/h0070288
   Trueblood JS, 2013, PSYCHOL SCI, V24, P901, DOI 10.1177/0956797612464241
   TVERSKY A, 1972, PSYCHOL REV, V79, P281, DOI 10.1037/h0032955
   YELLOTT JI, 1977, J MATH PSYCHOL, V15, P109, DOI 10.1016/0022-2496(77)90026-8
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702034
DA 2019-06-15
ER

PT S
AU Rainforth, T
   Le, TA
   van de Meent, JW
   Osborne, MA
   Wood, F
AF Rainforth, Tom
   Tuan Anh Le
   van de Meent, Jan-Willem
   Osborne, Michael A.
   Wood, Frank
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Bayesian Optimization for Probabilistic Programs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We present the first general purpose framework for marginal maximum a posteriori estimation of probabilistic program variables. By using a series of code transformations, the evidence of any probabilistic program, and therefore of any graphical model, can be optimized with respect to an arbitrary subset of its sampled variables. To carry out this optimization, we develop the first Bayesian optimization package to directly exploit the source code of its target, leading to innovations in problem-independent hyperpriors, unbounded optimization, and implicit constraint satisfaction; delivering significant performance improvements over prominent existing packages. We present applications of our method to a number of tasks including engineering design and parameter optimization.
C1 [Rainforth, Tom; Tuan Anh Le; Osborne, Michael A.; Wood, Frank] Univ Oxford, Dept Engn Sci, Oxford, England.
   [van de Meent, Jan-Willem] Northeastern Univ, Coll Comp & Informat Sci, Boston, MA 02115 USA.
RP Rainforth, T (reprint author), Univ Oxford, Dept Engn Sci, Oxford, England.
EM twgr@robots.ox.ac.uk; tuananh@robots.ox.ac.uk;
   j.vandemeent@northeastern.edu; mosb@robots.ox.ac.uk;
   fwood@robots.ox.ac.uk
FU BP industrial grant; Google studentship [DF6700]; DARPA PPAML through
   the U.S. AFRL [FA8750-14-2-0006, 61160290-111668]
FX Tom Rainforth is supported by a BP industrial grant. Tuan Anh Le is
   supported by a Google studentship, project code DF6700. Frank Wood is
   supported under DARPA PPAML through the U.S. AFRL under Cooperative
   Agreement FA8750-14-2-0006, Sub Award number 61160290-111668.
CR Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x
   Berard J, 2014, ELECTRON J PROBAB, V19, P1, DOI 10.1214/EJP.v19-3428
   Bergstra J. S., 2011, ADV NEURAL INFORM PR, V2011, P2546
   Carpenter B., 2015, J STAT SOFTWARE
   Csillery K, 2010, TRENDS ECOL EVOL, V25, P410, DOI 10.1016/j.tree.2010.04.001
   Duane S., 1987, PHYS LETT B
   Eggensperger K., 2013, NIPS WORKSH BAYES OP, P1
   Gardner J.R., 2014, ICML, P937
   Goodman N., 2008, P 24 C UNC ART INT, V8, P220
   Goodman N. D., 2014, DESIGN IMPLEMENTATIO
   Gutmann MU, 2016, J MACH LEARN RES, V17
   Hernandez-Lobato J M, 2014, ADV NEURAL INFORM PR, P918
   HERNANDEZLOBATO D, 2016, JMLR, V17
   Hutter Frank, 2011, Learning and Intelligent Optimization. 5th International Conference, LION 5. Selected Papers, P507, DOI 10.1007/978-3-642-25566-3_40
   Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147
   Mansinghka V, 2014, ARXIV14040099
   Minka T., 2010, INFER NET 2 4
   Murphy KP, 2012, MACHINE LEARNING PRO
   Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028
   OSBORNE M. A., 2009, 3 INT C LEARN INT OP, P1
   Paige B., 2014, P ADV NEUR INF PROC, V27, P3410
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Shahriari B., 2016, AISTATS
   Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218
   Snoek J., 2015, ICML
   Snoek J., 2012, ADV NEURAL INFORM PR, V2012, P2951
   van de Meent Jan-Willem, 2016, AISTATS, P1195
   Wingate D., 2011, P 43 INT C ART INT S, P770
   Wood F., 2014, AISTATS, P2
   Xie C., 2012, PHYS TEACHER, V50
   Zinkov R., 2016, ARXIV160301882
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703057
DA 2019-06-15
ER

PT S
AU Raju, RV
   Pitkow, X
AF Raju, Rajkumar V.
   Pitkow, Xaq
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Inference by Reparameterization in Neural Population Codes
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID BAYESIAN-INFERENCE; INFORMATION
AB Behavioral experiments on humans and animals suggest that the brain performs probabilistic inference to interpret its environment. Here we present a new general-purpose, biologically-plausible neural implementation of approximate inference. The neural network represents uncertainty using Probabilistic Population Codes (PPCs), which are distributed neural representations that naturally encode probability distributions, and support marginalization and evidence integration in a biologically-plausible manner. By connecting multiple PPCs together as a probabilistic graphical model, we represent multivariate probability distributions. Approximate inference in graphical models can be accomplished by message-passing algorithms that disseminate local information throughout the graph. An attractive and often accurate example of such an algorithm is Loopy Belief Propagation (LBP), which uses local marginalization and evidence integration operations to perform approximate inference efficiently even for complex models. Unfortunately, a subtle feature of LBP renders it neurally implausible. However, LBP can be elegantly reformulated as a sequence of Tree-based Reparameterizations (TRP) of the graphical model. We re-express the TRP updates as a nonlinear dynamical system with both fast and slow timescales, and show that this produces a neurally plausible solution. By combining all of these ideas, we show that a network of PPCs can represent multivariate probability distributions and implement the TRP updates to perform probabilistic inference. Simulations with Gaussian graphical models demonstrate that the neural network inference quality is comparable to the direct evaluation of LBP and robust to noise, and thus provides a promising mechanism for general probabilistic inference in the population codes of the brain.
C1 [Raju, Rajkumar V.] Rice Univ, Dept ECE, Houston, TX 77005 USA.
   [Pitkow, Xaq] Rice Univ, Baylor Coll Med, Dept Neurosci, Dept ECE, Houston, TX 77005 USA.
RP Raju, RV (reprint author), Rice Univ, Dept ECE, Houston, TX 77005 USA.
EM rv12@rice.edu; xaq@rice.edu
FU McNair Foundation; NSF CAREER Award [IOS-1552868]; Intelligence Advanced
   Research Projects Activity (IARPA) via Department of Interior/Interior
   Business Center (DoI/IBC) [D16PC00003]
FX XP and RR were supported in part by a grant from the McNair Foundation,
   NSF CAREER Award IOS-1552868, and by the Intelligence Advanced Research
   Projects Activity (IARPA) via Department of Interior/Interior Business
   Center (DoI/IBC) contract number D16PC00003.<SUP>1</SUP>
CR Archer E, 2015, STATML151107367 ARXI
   Beck Jeff, 2012, ADV NEURAL INFORM PR, P3059
   Beck JM, 2011, J NEUROSCI, V31, P15310, DOI 10.1523/JNEUROSCI.1706-11.2011
   Beck JM, 2008, NEURON, V60, P1142, DOI 10.1016/j.neuron.2008.09.021
   Carandini M, 2012, NAT REV NEUROSCI, V13, P51, DOI 10.1038/nrn3136
   Deneve S, 2014, ADV NEURAL INFORM PR, P2024
   Doya K., 2007, BAYESIAN BRAIN PROBA
   George D, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000532
   Grabska-Barwinska Agnieszka, 2013, ADV NEURAL INFORM PR, P1968
   Graf ABA, 2011, NAT NEUROSCI, V14, P239, DOI 10.1038/nn.2733
   Hayden BY, 2010, J NEUROSCI, V30, P3339, DOI 10.1523/JNEUROSCI.4874-09.2010
   HEEGER DJ, 1992, VISUAL NEUROSCI, V9, P181, DOI 10.1017/S0952523800009640
   Jazayeri M, 2006, NAT NEUROSCI, V9, P690, DOI 10.1038/nn1691
   Knill D. C., 1996, PERCEPTION BAYESIAN
   Lee TS, 2003, J OPT SOC AM A, V20, P1434, DOI 10.1364/JOSAA.20.001434
   Litvak S, 2009, NEURAL COMPUT, V21, P3010, DOI 10.1162/neco.2009.05-08-783
   Ma WJ, 2006, NAT NEUROSCI, V9, P1432, DOI 10.1038/nn1790
   Ott T, 2006, ADV NEURAL INFORM PR, P1057
   Pearl J, 1988, PROBABILISTIC REASON
   Pitkow X, 2015, COSYNE
   Pouget A, 2013, NAT NEUROSCI, V16, P1170, DOI 10.1038/nn.3495
   Rao RP, 2004, ADV NEURAL INFORM PR, P1113
   Raposo D, 2014, NAT NEUROSCI, V17, P1784, DOI 10.1038/nn.3865
   Rigotti M, 2013, NATURE, V497, P585, DOI 10.1038/nature12160
   Rubin DB, 2015, NEURON, V85, P402, DOI 10.1016/j.neuron.2014.12.026
   Steimer A, 2009, NEURAL COMPUT, V21, P2502, DOI 10.1162/neco.2009.08-08-837
   Wainwright MJ, 2003, IEEE T INFORM THEORY, V49, P1120, DOI 10.1109/TIT.2003.810642
   Yedidia J. S., 2003, EXPLORING ARTIF INTE, V8, P236
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704001
DA 2019-06-15
ER

PT S
AU Ramamohan, S
   Rajkumar, A
   Agarwal, S
AF Ramamohan, Siddartha
   Rajkumar, Arun
   Agarwal, Shivani
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Dueling Bandits: Beyond Condorcet Winners to General Tournament
   Solutions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Recent work on deriving O(log T) anytime regret bounds for stochastic dueling bandit problems has considered mostly Condorcet winners, which do not always exist, and more recently, winners defined by the Copeland set, which do always exist. In this work, we consider a broad notion of winners defined by tournament solutions in social choice theory, which include the Copeland set as a special case but also include several other notions of winners such as the top cycle, uncovered set, and Banks set, and which, like the Copeland set, always exist. We develop a family of UCB-style dueling bandit algorithms for such general tournament solutions, and show O(log T) anytime regret bounds for them. Experiments confirm the ability of our algorithms to achieve low regret relative to the target winning set of interest.
C1 [Ramamohan, Siddartha] Indian Inst Sci, Bangalore 560012, Karnataka, India.
   [Rajkumar, Arun] Xerox Res, Bangalore 560103, Karnataka, India.
   [Agarwal, Shivani] Univ Penn, Philadelphia, PA 19104 USA.
RP Ramamohan, S (reprint author), Indian Inst Sci, Bangalore 560012, Karnataka, India.
EM siddartha.yr@csa.iisc.ernet.in; arun_r@csa.iisc.ernet.in;
   ashivani@seas.upenn.edu
FU Google
FX Thanks to the anonymous reviewers for helpful comments and suggestions.
   SR thanks Google for a travel grant to present this work at the
   conference.
CR Ailon Nir, 2014, P 31 INT C MACH LEAR
   Brandt F, 2016, HDB COMPUTATIONAL SO
   Brandt F, 2015, DISCRETE APPL MATH, V187, P41, DOI 10.1016/j.dam.2015.01.041
   Busa- Fekete Robert, 2013, P 30 INT C MACH LEAR
   Dudik Miroslav, 2015, P 28 C LEARN THEOR
   Hudry O, 1999, SOC CHOICE WELFARE, V16, P137, DOI 10.1007/s003550050135
   Jamieson Kevin, 2015, P 18 INT C ART INT S
   Jamieson Kevin, 2011, ADV NEURAL INFORM PR
   Komiyama Junpei, 2015, P 28 C LEARN THEOR
   SHEPSLE KA, 1984, AM J POLIT SCI, V28, P49, DOI 10.2307/2110787
   Urvoy Tanguy, 2013, P 30 INT C MACH LEAR
   Yue YS, 2012, J COMPUT SYST SCI, V78, P1538, DOI 10.1016/j.jcss.2011.12.028
   Yue Yisong, 2011, P 28 INT C MACH LEAR
   Zoghi M, 2015, ADV NEUR IN, V28
   Zoghi Masrour, 2014, P 31 INT C MACH LEAR
   Zoghi Masrour, 2015, P 8 ACM INT C WEB SE
NR 16
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702084
DA 2019-06-15
ER

PT S
AU Ranganath, R
   Altosaar, J
   Tran, D
   Blei, DM
AF Ranganath, Rajesh
   Altosaar, Jaan
   Tran, Dustin
   Blei, David M.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Operator Variational Inference
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Variational inference is an umbrella term for algorithms which cast Bayesian inference as optimization. Classically, variational inference uses the Kullback-Leibler divergence to define the optimization. Though this divergence has been widely used, the resultant posterior approximation can suffer from undesirable statistical properties. To address this, we reexamine variational inference from its roots as an optimization problem. We use operators, or functions of functions, to design variational objectives. As one example, we design a variational objective with a Langevin-Stein operator. We develop a black box algorithm, operator variational inference (opvi), for optimizing any operator objective. Importantly, operators enable us to make explicit the statistical and computational tradeoffs for variational inference. We can characterize different properties of variational objectives, such as objectives that admit data subsampling-allowing inference to scale to massive data-as well as objectives that admit variational programs-a rich class of posterior approximations that does not require a tractable density. We illustrate the benefits of opvi on a mixture model and a generative model of images.
C1 [Ranganath, Rajesh; Altosaar, Jaan] Princeton Univ, Princeton, NJ 08544 USA.
   [Tran, Dustin; Blei, David M.] Columbia Univ, New York, NY 10027 USA.
RP Ranganath, R (reprint author), Princeton Univ, Princeton, NJ 08544 USA.
FU NSF [IIS-1247664]; ONR [N00014-11-1-0651]; DARPA [FA8750-14-2-0009,
   N66001-15-C-4032]; Adobe; NSERC PGS-D; Porter Ogden Jacobus Fellowship;
   Seibel Foundation; Sloan Foundation
FX This work is supported by NSF IIS-1247664, ONR N00014-11-1-0651, DARPA
   FA8750-14-2-0009, DARPA N66001-15-C-4032, Adobe, NSERC PGS-D, Porter
   Ogden Jacobus Fellowship, Seibel Foundation, and the Sloan Foundation.
   The authors would like to thank Dawen Liang, Ben Poole, Stephan Mandt,
   Kevin Murphy, Christian Naesseth, and the anonymous reviews for their
   helpful feedback and comments.
CR Assaraf R., 1999, PHYS REV LET
   Barbour A. D., 1988, J APPL PROBABILITY
   Carpenter B, 2015, ARXIV150907164
   Ghahramani Z, 2001, ADV NEUR IN, V13, P507
   Goodfellow I., 2014, NEURAL INFORM PROCES
   Hernandez-Lobato J. M., 2015, BLACK BOX ALPHA DIVE
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Hyvarinen A, 2005, J MACH LEARN RES, V6, P695
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Kingma D. P., 2014, CORR
   Kingma Diederik P, 2014, ICLR
   Kushner H. J., 1997, STOCHASTIC APPROXIMA
   Li Y., 2016, ARXIV160202311
   Maaloe Lars, 2016, ARXIV160205473
   Minka T. P., 2001, UAI
   Minka Thomas, 2004, TECHNICAL REPORT
   Nielsen F., 2013, ARXIV13093029
   Ranganath R., 2016, INT C MACH LEARN
   Ranganath Rajesh, 2014, AISTATS
   Rezende D. J., 2014, INT C MACH LEARN
   Rezende D. J., 2015, P 31 INT C MACH LEAR
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Salakhutdinov R., 2008, INT C MACH LEARN
   Theis L., 2016, INT C LEARN REPR
   Titsias M., 2014, P 31 INT C MACH LEAR, P1971
   Wingate D., 2013, ARXIV E PRINTS
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700055
DA 2019-06-15
ER

PT S
AU Ratner, A
   De Sa, C
   Wu, S
   Selsam, D
   Re, C
AF Ratner, Alexander
   De Sa, Christopher
   Wu, Sen
   Selsam, Daniel
   Re, Christopher
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Data Programming: Creating Large Training Sets, Quickly
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Large labeled training sets are the critical building blocks of supervised learning methods and are key enablers of deep learning techniques. For some applications, creating labeled training sets is the most time-consuming and expensive part of applying machine learning. We therefore propose a paradigm for the programmatic creation of training sets called data programming in which users express weak supervision strategies or domain heuristics as labeling functions, which are programs that label subsets of the data, but that are noisy and may conflict. We show that by explicitly representing this training set labeling process as a generative model, we can "denoise" the generated training set, and establish theoretically that we can recover the parameters of these generative models in a handful of settings. We then show how to modify a discriminative loss function to make it noise-aware, and demonstrate our method over a range of discriminative models including logistic regression and LSTMs. Experimentally, on the 2014 TAC-KBP Slot Filling challenge, we show that data programming would have led to a new winning score, and also show that applying data programming to an LSTM model leads to a TAC-KBP score almost 6 F1 points over a state-of-the-art LSTM baseline (and into second place in the competition). Additionally, in initial user studies we observed that data programming may be an easier way for non-experts to create machine learning models when training data is limited or unavailable.
C1 [Ratner, Alexander; De Sa, Christopher; Wu, Sen; Selsam, Daniel; Re, Christopher] Stanford Univ, Stanford, CA 94305 USA.
RP Ratner, A (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM ajratner@stanford.edu; cdesa@stanford.edu; senwu@stanford.edu;
   dselsam@stanford.edu; chrismre@stanford.edu
FU DARPA [FA8750-13-2-0039, FA8750-12-2-0335]; NSF [IIS-1247701,
   CCF-1337375, IIS-1353606]; DOE [108845]; ONR [N000141210041,
   N000141310129]; NIH [U54EB020405]; DARPA's SIMPLEX program; Oracle;
   NVIDIA; Huawei; SAP Labs; Sloan Research Fellowship; Moore Foundation;
   American Family Insurance; Google; Toshiba;  [NSFCCF-1111943]
FX Thanks to Theodoros Rekatsinas, Manas Joglekar, Henry Ehrenberg, Jason
   Fries, Percy Liang, the DeepDive and DDLite users and many others for
   their helpful conversations. The authors acknowledge the support of:
   DARPA FA8750-12-2-0335; NSF IIS-1247701; NSFCCF-1111943; DOE 108845; NSF
   CCF-1337375; DARPA FA8750-13-2-0039; NSF IIS-1353606; ONR N000141210041
   and N000141310129; NIH U54EB020405; DARPA's SIMPLEX program; Oracle;
   NVIDIA; Huawei; SAP Labs; Sloan Research Fellowship; Moore Foundation;
   American Family Insurance; Google; and Toshiba. The views and
   conclusions expressed in this material are those of the authors and
   should not be interpreted as necessarily representing the official
   policies or endorsements, either expressed or implied, of DARPA, AFRL,
   NSF, ONR, NIH, or the U.S. Government.
CR Alfonseca E., P ACL
   Angeli G., 2014, TAC KBP, V695
   Balsubramani A., 2015, ADV NEURAL INFORM PR, P1351
   Berend D., 2014, NIPS
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Bootkrajang Jakramate, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P143, DOI 10.1007/978-3-642-33460-3_15
   Craven M, 1999, Proc Int Conf Intell Syst Mol Biol, P77
   Dalvi N. N., 2013, P 22 INT C WORLD WID, P285, DOI DOI 10.1145/2488388.2488414
   Dawid A. P., 1979, APPL STAT, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]
   Dogan R. I., P 2012 WORKSH BIOM N
   Ehrenberg H. R., 2016, HILDA 16 SIGMOD, P13
   Gao H., 2011, TECHNICAL REPORT
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hoffmann R., P ACL
   Joglekar M., DAT ENG ICDE 2015 IE
   Karger D. R., 2011, NIPS, V24, P1953
   Krishna R., 2016, ARXIV160207332
   Krogel MA, 2004, MACH LEARN, V57, P61, DOI 10.1023/B:MACH.0000035472.73496.0c
   LUGOSI G, 1992, PATTERN RECOGN, V25, P79, DOI 10.1016/0031-3203(92)90008-7
   Mallory E. K., 2015, BIOINFORMATICS
   Mintz M., 2009, P JOINT C 47 ANN M A
   Mooney R. J., 2007, ANN M ASS COMP LING, P576
   Natarajan N., ADV NEURAL INFORM PR, V26
   Parisi F, 2014, P NATL ACAD SCI USA, V111, P1253, DOI 10.1073/pnas.1219097111
   Riedel S, 2010, LECT NOTES ARTIF INT, V6323, P148, DOI 10.1007/978-3-642-15939-8_10
   Roth B., 2013, EMNLP, P24
   Roth B., P 22 ACM C KNOWL MAN
   Schapire R. E., 2012, BOOSTING FDN ALGORIT
   Shin J, 2015, PROC VLDB ENDOW, V8, P1310, DOI 10.14778/2809974.2809991
   Surdeanu M., 2014, P TEXT AN C TAC2014
   Takamatsu S., P ACL
   Verga P., 2015, ARXIV151106396
   Zhang Y., 2014, ADV NEURAL INFORM PR, P1260
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704048
DA 2019-06-15
ER

PT S
AU Raziperchikolaei, R
   Carreira-Perpinan, MA
AF Raziperchikolaei, Ramin
   Carreira-Perpinan, Miguel A.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Optimizing Affinity-Based Binary Hashing Using Auxiliary Coordinates
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID DIMENSIONALITY REDUCTION; ALGORITHMS
AB In supervised binary hashing, one wants to learn a function that maps a high-dimensional feature vector to a vector of binary codes, for application to fast image retrieval. This typically results in a difficult optimization problem, nonconvex and nonsmooth, because of the discrete variables involved. Much work has simply relaxed the problem during training, solving a continuous optimization, and truncating the codes a posteriori. This gives reasonable results but is quite suboptimal. Recent work has tried to optimize the objective directly over the binary codes and achieved better results, but the hash function was still learned a posteriori, which remains suboptimal. We propose a general framework for learning hash functions using affinity-based loss functions that uses auxiliary coordinates. This closes the loop and optimizes jointly over the hash functions and the binary codes so that they gradually match each other. The resulting algorithm can be seen as an iterated version of the procedure of optimizing first over the codes and then learning the hash function. Compared to this, our optimization is guaranteed to obtain better hash functions while being not much slower, as demonstrated experimentally in various supervised datasets. In addition, our framework facilitates the design of optimization algorithms for arbitrary types of loss and hash functions.
C1 [Raziperchikolaei, Ramin; Carreira-Perpinan, Miguel A.] Univ Calif Merced, EECS, Merced, CA 95343 USA.
RP Raziperchikolaei, R (reprint author), Univ Calif Merced, EECS, Merced, CA 95343 USA.
EM rraziperchikolaei@ucmerced.edu; mcarreira-perpinan@ucmerced.edu
FU NSF [IIS-1423515]
FX Work supported by NSF award IIS-1423515.
CR Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Carreira-Perpinan M., 2012, ARXIV12125921CSLG
   Carreira-Perpinan M., 2014, AISTATS
   Carreira-Perpinan M., 2015, NIPS
   Carreira-Perpinan M. A, 2010, ICML
   Carreira-PerpiNanan M. A., 2015, CVPR
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Ge T., 2014, ECCV
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Grauman K., 2013, MACHINE LEARNING COM, V411, P49
   Krizhevsky A., 2009, THESIS
   Kulis B., 2009, NIPS
   Kulis B, 2012, IEEE T PATTERN ANAL, V34, P1092, DOI 10.1109/TPAMI.2011.219
   Lin G., 2014, CVPR
   Lin G., 2013, ICCV
   Liu W., 2011, ICML
   Liu W., 2012, CVPR
   Loosli G., 2007, LARGE SCALE KERNEL M, p[301, 6]
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Norouzi M., 2011, ICML
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   van der Maaten L. J. P., 2013, INT C LEARN REPR ICL
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vladymyrov M., 2014, AISTATS
   Vladymyrov M., 2012, ICML
   Weiss Y., 2009, NIPS
   Wu XH, 2012, IEEE SOUTHEASTCON
   Yang Z., 2013, ICML
   Yu S.X., 2003, ICCV
   Zhang D., 2010, SIGIR
   Zhu CY, 1997, ACM T MATH SOFTWARE, V23, P550, DOI 10.1145/279232.279236
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702028
DA 2019-06-15
ER

PT S
AU Reddi, SJ
   Sra, S
   Poczos, B
   Smola, AJ
AF Reddi, Sashank J.
   Sra, Suvrit
   Poczos, Barnabas
   Smola, Alexander J.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Proximal Stochastic Methods for Nonsmooth Nonconvex Finite-Sum
   Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We analyze stochastic algorithms for optimizing nonconvex, nonsmooth finite-sum problems, where the nonsmooth part is convex. Surprisingly, unlike the smooth case, our knowledge of this fundamental problem is very limited. For example, it is not known whether the proximal stochastic gradient method with constant minibatch converges to a stationary point. To tackle this issue, we develop fast stochastic algorithms that provably converge to a stationary point for constant minibatches. Furthermore, using a variant of these algorithms, we obtain provably faster convergence than batch proximal gradient descent. Our results are based on the recent variance reduction techniques for convex optimization but with a novel analysis for handling nonconvex and nonsmooth functions. We also prove global linear convergence rate for an interesting subclass of nonsmooth nonconvex functions, which subsumes several recent works.
C1 [Reddi, Sashank J.; Poczos, Barnabas; Smola, Alexander J.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Sra, Suvrit] MIT, Cambridge, MA 02139 USA.
RP Reddi, SJ (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM sjakkamr@cs.cmu.edu; suvrit@mit.edu; bapoczos@cs.cmu.edu; alex@smola.org
FU NSF [IIS-1409802]
FX SS acknowledges support of NSF grant: IIS-1409802.
CR Agarwal A., 2014, ARXIV14100723
   Bach F, 2012, OPTIMIZATION FOR MACHINE LEARNING, P19
   Bottou Leon, 1991, P NEUR, V91, P8
   Defazio A., 2014, ADV NEURAL INFORM PR, P1646
   FUKUSHIMA M, 1981, INT J SYST SCI, V12, P989, DOI 10.1080/00207728108963798
   Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811
   Ghadimi Saeed, 2014, MATH PROGRAM, V155, P267
   Johnson R., 2013, ADV NEURAL INFORM PR, V26, P315
   Karimi H., 2016, JOINT EUR C MACH LEA, P795, DOI DOI 10.1007/978-3-319-46128-1_50
   Lan  Guanghui, 2015, ARXIV150702000
   Li Xingguo, 2016, ICML
   Liu J, 2015, SIAM J OPTIMIZ, V25, P351, DOI 10.1137/140961134
   MINE H, 1981, J OPTIMIZ THEORY APP, V33, P9, DOI 10.1007/BF00935173
   MOREAU JJ, 1962, CR HEBD ACAD SCI, V255, P2897
   Nemirovski A, 1983, PROBLEM COMPLEXITY M
   Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001
   Nesterov Yurii E., 2003, INTRO LECT CONVEX OP
   Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003
   Polyak B. T., 1963, USSR COMP MATH MATH, V3, P864
   Reddi S. J, 2016, INT C MACH LEARN, P314
   Reddi S. J., 2015, NIPS, P2629
   Reddi Sashank J., 2016, 54 ANN ALL C COMM CO
   Reddi Sashank J., 2016, CORR
   ROCKAFELLAR RT, 1976, SIAM J CONTROL, V14, P877, DOI 10.1137/0314056
   Schmidt  M., 2013, ARXIV13092388
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Shalev-Shwartz Shai, 2015, CORR
   Shamir O., 2014, ARXIV14092848
   Sra S., 2012, ADV NEURAL INFORM PR, P530
   Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791
   Zhu Zeyuan Allen, 2015, CORR
NR 31
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700080
DA 2019-06-15
ER

PT S
AU Reed, S
   Akata, Z
   Mohan, S
   Tenka, S
   Schiele, B
   Lee, H
AF Reed, Scott
   Akata, Zeynep
   Mohan, Santosh
   Tenka, Samuel
   Schiele, Bernt
   Lee, Honglak
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning What and Where to Draw
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Generative Adversarial Networks (GANs) have recently demonstrated the capability to synthesize compelling real-world images, such as room interiors, album covers, manga, faces, birds, and flowers. While existing models can synthesize images based on global constraints such as a class label or caption, they do not provide control over pose or object location. We propose a new model, the Generative Adversarial What-Where Network (GAWWN), that synthesizes images given instructions describing what content to draw in which location. We show high-quality 128 x 128 image synthesis on the Caltech-UCSD Birds dataset, conditioned on both informal text descriptions and also object location. Our system exposes control over both the bounding box around the bird and its constituent parts. By modeling the conditional distributions over part locations, our system also enables conditioning on arbitrary subsets of parts (e.g. only the beak and tail), yielding an efficient interface for picking part locations.
C1 [Reed, Scott; Mohan, Santosh; Tenka, Samuel; Lee, Honglak] Univ Michigan, Ann Arbor, MI 48109 USA.
   [Akata, Zeynep; Schiele, Bernt] Max Planck Inst Informat, Saarbrucken, Germany.
   [Reed, Scott] DeepMind, London, England.
RP Reed, S (reprint author), Univ Michigan, Ann Arbor, MI 48109 USA.; Reed, S (reprint author), DeepMind, London, England.
EM reedscot@google.com; akata@mpi-inf.mpg.de; santoshm@umich.edu;
   samtenka@umich.edu; schiele@mpi-inf.mpg.de; honglak@umich.edu
FU NSF CAREER [IIS-1453651]; ONR [N00014-13-1-0762]; Sloan Research
   Fellowship
FX This work was supported in part by NSF CAREER IIS-1453651, ONR
   N00014-13-1-0762, and a Sloan Research Fellowship.
CR Akata Z., 2015, CVPR
   Andriluka M., 2014, CVPR
   Cho K., 2014, SYNTAX SEMANTICS STR
   Denton E. L., 2015, NIPS
   Dosovitskiy Alexey, 2015, CVPR
   Goodfellow I., 2014, NIPS
   Gregor K., 2015, ICML
   Hinton G. E., 2016, NIPS
   Jaderberg M., 2015, NIPS
   Kingma Diederik P, 2014, ICLR
   Kiros R., 2014, ACL
   Kulkarni Tejas D, 2015, NIPS
   Larochelle H., 2011, AISTATS
   Mansimov E., 2016, ICLR
   Oh  J., 2015, NIPS
   Oquab Q., 2016, MODULES SPATIAL TRAN
   Radford A., 2016, ICLR
   Reed S., 2016, ICML
   Reed S., 2016, CVPR
   Reed S. E., 2015, NIPS
   Rezende D., 2016, ICML
   Rezende D. J., 2014, ICML
   Salakhutdinov R., 2009, AISTATS
   Theis L, 2015, NIPS
   van den Oord A., 2016, ICML
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Yang Jimei, 2015, NIPS
NR 27
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700075
DA 2019-06-15
ER

PT S
AU Ren, Y
   Li, JL
   Luo, YC
   Zhu, J
AF Ren, Yong
   Li, Jialian
   Luo, Yucen
   Zhu, Jun
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Conditional Generative Moment-Matching Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Maximum mean discrepancy (MMD) has been successfully applied to learn deep generative models for characterizing a joint distribution of variables via kernel mean embedding. In this paper, we present conditional generative moment-matching networks (CGMMN), which learn a conditional distribution given some input variables based on a conditional maximum mean discrepancy (CMMD) criterion. The learning is performed by stochastic gradient descent with the gradient calculated by back-propagation. We evaluate CGMMN on a wide range of tasks, including predictive modeling, contextual generation, and Bayesian dark knowledge, which distills knowledge from a Bayesian model by learning a relatively small CGMMN student network. Our results demonstrate competitive performance in all the tasks.
C1 [Zhu, Jun] Tsinghua Univ, Dept Comp Sci & Tech, TNList Lab, Beijing, Peoples R China.
   Tsinghua Univ, Ctr Bioinspired Comp Res, State Key Lab Intell Tech & Syst, Beijing, Peoples R China.
RP Zhu, J (reprint author), Tsinghua Univ, Dept Comp Sci & Tech, TNList Lab, Beijing, Peoples R China.
EM renyong15@mails.tsinghua.edu.cn; jl12@mails.tsinghua.edu.cn;
   luoyc15@mails.tsinghua.edu.cn; dcszj@tsinghua.edu.cn
FU National Basic Research Program (973 Program) of China [2013CB329403];
   National NSF of China Projects [61620106010, 61322308, 61332007]; Youth
   Top-notch Talent Support Program; Tencent; Intel
FX The work was supported by the National Basic Research Program (973
   Program) of China (No. 2013CB329403), National NSF of China Projects
   (Nos. 61620106010, 61322308, 61332007), the Youth Top-notch Talent
   Support Program, and the Collaborative Projects with Tencent and Intel.
CR Denton E. L., 2015, NIPS
   Dosovitskiy A., 2015, ARXIV14115928
   Dziugaite G. K., 2015, UAI
   Gens R., 2012, NIPS
   Goodfellow I, 2013, ICML
   Goodfellow I., 2014, NIPS
   Gretton A., 2008, JMLR
   Grunewalder S., 2012, ICML
   He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55
   Hernandez-Lobato J. M., 2015, ICML
   Hofmann T, 2008, ANN STAT, V36, P1171, DOI 10.1214/009053607000000677
   Kallenbery O., 2002, FDN MODERN PROBABILI
   Kingma D. P., 2015, ICLR
   Kingma Diederik P, 2014, ICLR
   Korattikara A., 2015, NIPS
   Lafferty J. O., 2001, ICML
   Lee C. Y., 2015, AISTATS
   Li C., 2015, NIPS
   Li Y., 2015, ICML 2015
   Lin M., 2014, ICLR
   Mirza M., 2014, ARXIV14111784, DOI DOI 10.1029/2009WR008312
   Nair V., 2010, ICML
   NG A., 2001, NIPS
   Poon H., 2011, UAI
   Sermanet P., 2012, ICPR
   Shalev-Shwartz S., 2011, MATH PROGRAMMING B
   Smola A., 2007, INT C ALG LEARN THEO
   Sohn K., 2015, NIPS
   Song L., 2009, ICML
   Srivastava  N., 2012, NIPS
   Vinyals O., 2015, ARXIV14114555V2
   Yan  Xinchen, 2015, ARXIV151200570
   Zeiler M., 2013, ICLR
NR 33
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702002
DA 2019-06-15
ER

PT S
AU Rezende, DJ
   Eslami, SMA
   Mohamed, S
   Battaglia, P
   Jaderberg, M
   Heess, N
AF Rezende, Danilo Jimenez
   Eslami, S. M. Ali
   Mohamed, Shakir
   Battaglia, Peter
   Jaderberg, Max
   Heess, Nicolas
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Unsupervised Learning of 3D Structure from Images
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB A key goal of computer vision is to recover the underlying 3D structure that gives rise to 2D observations of the world. If endowed with 3D understanding, agents can abstract away from the complexity of the rendering process to form stable, disentangled representations of scene elements. In this paper we learn strong deep generative models of 3D structures, and recover these structures from 2D images via probabilistic inference. We demonstrate high-quality samples and report log-likelihoods on several datasets, including ShapeNet [2], and establish the first benchmarks in the literature. We also show how these models and their inference networks can be trained jointly, end-to-end, and directly from 2D images without any use of ground-truth 3D labels. This demonstrates for the first time the feasibility of learning to infer 3D representations of the world in a purely unsupervised manner.
C1 [Rezende, Danilo Jimenez; Eslami, S. M. Ali; Mohamed, Shakir; Battaglia, Peter; Jaderberg, Max; Heess, Nicolas] Google DeepMind, London, England.
RP Rezende, DJ (reprint author), Google DeepMind, London, England.
EM danilor@google.com; aeslami@google.com; shakir@google.com;
   peterbattaglia@google.com; jaderberg@google.com; heess@google.com
CR Burda Y., 2015, 150900519 ARXIV
   Chang A. X., 2015, 151203012 ARXIV
   Choy C. B., 2016, 160400449 ARXIV
   Del Pero L, 2012, PROC CVPR IEEE, P2719, DOI 10.1109/CVPR.2012.6247994
   Dosovitskiy A, 2015, PROC CVPR IEEE, P1538, DOI 10.1109/CVPR.2015.7298761
   Eslami S., 2016, ATTEND INFER REPEAT
   Gregor K., 2015, ICML
   Gregor K., 2016, 160408772 ARXIV
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Jaderberg M, 2015, ADV NEURAL INFORM PR, P2008
   Jiajun W., 2016, 161007584 ARXIV
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kingma Diederik P, 2014, ICLR
   Kulkarni T, 2014, NIPS 2014 ABC WORKSH
   Kulkarni T. D., 2014, ARXIV14071339
   LeCun Yann, MNIST DATABASE HANDW
   Loper MM, 2014, LECT NOTES COMPUT SC, V8695, P154, DOI 10.1007/978-3-319-10584-0_11
   Mansinghka V., 2013, ADV NEURAL INFORM PR, P1520
   Mnih A., 2016, 160206725 ARXIV
   OpenGL Architecture Review Board, 1993, OPENGL REF MAN OFF R
   Rezende D. J., 2016, 160305106 ARXIV
   Rezende D. J., 2014, ICML
   Salakhutdinov R., 2008, P 25 INT C MACH LEAR, P872, DOI DOI 10.1145/1390156.1390266
   Sundareswara R, 2008, J VISION, V8, DOI 10.1167/8.5.12
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Wingate D., 2011, P NIPS 11, P1152
   Wu J., 2015, ADV NEURAL INFORM PR, P127, DOI DOI 10.1007/978-3-319-26532-2_15
   WU ZR, 2015, PROC CVPR IEEE, P1912, DOI DOI 10.1109/CVPR.2015.7298801
   Zhou T., 2016, VIEW SYNTHESIS APPEA
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973705014
DA 2019-06-15
ER

PT S
AU Ritchie, D
   Thomas, A
   Hanrahan, P
   Goodman, ND
AF Ritchie, Daniel
   Thomas, Anna
   Hanrahan, Pat
   Goodman, Noah D.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Neurally-Guided Procedural Models: Amortized Inference for Procedural
   Graphics Programs using Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Probabilistic inference algorithms such as Sequential Monte Carlo (SMC) provide powerful tools for constraining procedural models in computer graphics, but they require many samples to produce desirable results. In this paper, we show how to create procedural models which learn how to satisfy constraints. We augment procedural models with neural networks which control how the model makes random choices based on the output it has generated thus far. We call such models neurally-guided procedural models. As a pre-computation, we train these models to maximize the likelihood of example outputs generated via SMC. They are then used as efficient SMC importance samplers, generating high-quality results with very few samples. We evaluate our method on L-system-like models with image-based constraints. Given a desired quality threshold, neurally-guided models can generate satisfactory results up to 10x faster than unguided models.
C1 [Ritchie, Daniel; Thomas, Anna; Hanrahan, Pat; Goodman, Noah D.] Stanford Univ, Stanford, CA 94305 USA.
RP Ritchie, D (reprint author), Stanford Univ, Stanford, CA 94305 USA.
CR Benes Bedrich, EUROGRAPHICS 2011
   Brooks S, 2011, CH CRC HANDB MOD STA, P1, DOI 10.1201/b10905
   Dang Minh, 2015, SIGGRAPH ASIA
   Goodman N. D., 2014, DESIGN IMPLEMENTATIO
   Gu Shixiang, 2015, NIPS
   Kingma D. P., ICLR 2015
   Kingma Diederik P., ICLR 2014
   Kulkarni T., CVPR 2015
   MacKay DJC, 2002, INFORM THEORY INFERE
   Mech Radomir, 1996, SIGGRAPH
   Mnih Andriy, ICML 2014
   Mnih V., 2014, NIPS
   Muller Pascal, 2006, SIGGRAPH
   Norman K., AISTATS 2014
   Paige B., ICML 2016
   Prusinkiewicz P., 1990, ALGORITHMIC BEAUTY P
   Prusinkiewicz Przemyslaw, SIGGRAPH 1994
   Rezende Danilo Jimenez, ICML 2014
   Ritchie Daniel, EUROGRAPHICS 2015
   Ritchie Daniel, SIGGRAPH 2015
   Stava O, 2014, COMPUT GRAPH FORUM, V33, P118, DOI 10.1111/cgf.12282
   Talton JO, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944851
   Wingate David, NIPS 2012 WORKSH PRO
   Wong Michael T., SIGGRAPH 1998
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702100
DA 2019-06-15
ER

PT S
AU Rogers, R
   Roth, A
   Ullman, J
   Vadhan, S
AF Rogers, Ryan
   Roth, Aaron
   Ullman, Jonathan
   Vadhan, Salil
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Privacy Odometers and Filters: Pay-as-you-Go Composition
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID NOISE
AB In this paper we initiate the study of adaptive composition in differential privacy when the length of the composition, and the privacy parameters themselves can be chosen adaptively, as a function of the outcome of previously run analyses. This case is much more delicate than the setting covered by existing composition theorems, in which the algorithms themselves can be chosen adaptively, but the privacy parameters must be fixed up front. Indeed, it isn't even clear how to define differential privacy in the adaptive parameter setting. We proceed by defining two objects which cover the two main use cases of composition theorems. A privacy filter is a stopping time rule that allows an analyst to halt a computation before his pre-specified privacy budget is exceeded. A privacy odometer allows the analyst to track realized privacy loss as he goes, without needing to pre-specify a privacy budget. We show that unlike the case in which privacy parameters are fixed, in the adaptive parameter setting, these two use cases are distinct. We show that there exist privacy filters with bounds comparable (up to constants) with existing privacy composition theorems. We also give a privacy odometer that nearly matches non-adaptive private composition theorems, but is sometimes worse by a small asymptotic factor. Moreover, we show that this is inherent, and that any valid privacy odometer in the adaptive parameter setting must lose this factor, which shows a formal separation between the filter and odometer use-cases.
C1 [Rogers, Ryan] Univ Penn, Dept Appl Math & Computat Sci, Philadelphia, PA 19104 USA.
   [Roth, Aaron] Univ Penn, Dept Comp & Informat Sci, Philadelphia, PA 19104 USA.
   [Ullman, Jonathan] Northeastern Univ, Coll Comp & Informat Sci, Boston, MA 02115 USA.
   [Vadhan, Salil] Harvard Univ, Ctr Res Computat & Soc, Cambridge, MA 02138 USA.
   [Vadhan, Salil] Harvard Univ, John A Paulson Sch Engn & Appl Sci, Cambridge, MA 02138 USA.
   [Vadhan, Salil] Natl Chiao Tung Univ, Dept Appl Math, Hsinchu, Taiwan.
   [Vadhan, Salil] Natl Chiao Tung Univ, Shing Tung Yau Ctr, Hsinchu, Taiwan.
RP Rogers, R (reprint author), Univ Penn, Dept Appl Math & Computat Sci, Philadelphia, PA 19104 USA.
EM ryrogers@sas.upenn.edu; aaroth@cis.upenn.edu; jullman@ccs.neu.edu;
   salil@seas.harvard.edu
FU NSF CAREER award; NSF [CNS-1513694, CNS-1237235]; Sloan Foundation;
   Simons Investigator Award
FX Supported in part by an NSF CAREER award, NSF grant CNS-1513694, and a
   grant from the Sloan Foundation.; Work done while visiting the
   Department of Applied Mathematics and the Shing-Tung Yau Center at
   National Chiao-Tung University in Taiwan. Also supported by NSF grant
   CNS-1237235, a grant from the Sloan Foundation, and a Simons
   Investigator Award.
CR Bassily  Raef, 2016, P 48 ANN ACM S THEOR
   De La Pena VH, 2004, ANN PROBAB, V32, P1902, DOI 10.1214/009117904000000397
   Dwork C., 2015, P 47 ANN ACM S THEOR, P117
   Dwork C, 2006, LECT NOTES COMPUT SC, V4004, P486
   Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265
   Dwork C, 2010, ANN IEEE SYMP FOUND, P51, DOI 10.1109/FOCS.2010.12
   Ebadi Hamid, 2015, ABS150502642 CORR
   Kairouz P., 2015, P 32 INT C MACH LEAR, P1376
   Kasiviswanathan Shiva Prasad, 2014, J PRIVACY CONFIDENTI, V6, P1
   Ledoux M., 1991, SERIES MODERN SURVEY
   Murtagh J, 2016, LECT NOTES COMPUT SC, V9562, P157, DOI 10.1007/978-3-662-49096-9_7
   van de Geer Sara A, 2002, HOEFFDINGS INEQUALIT
NR 12
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701027
DA 2019-06-15
ER

PT S
AU Rogez, G
   Schmid, C
AF Rogez, Gregory
   Schmid, Cordelia
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI MoCap-guided Data Augmentation for 3D Pose Estimation in the Wild
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB This paper addresses the problem of 3D human pose estimation in the wild. A significant challenge is the lack of training data, i.e., 2D images of humans annotated with 3D poses. Such data is necessary to train state-of-the-art CNN architectures. Here, we propose a solution to generate a large set of photorealistic synthetic images of humans with 3D pose annotations. We introduce an image-based synthesis engine that artificially augments a dataset of real images with 2D human pose annotations using 3D Motion Capture (MoCap) data. Given a candidate 3D pose our algorithm selects for each joint an image whose 2D pose locally matches the projected 3D pose. The selected images are then combined to generate a new synthetic image by stitching local image patches in a kinematically constrained manner. The resulting images are used to train an end-to-end CNN for full-body 3D pose estimation. We cluster the training data into a large number of pose classes and tackle pose estimation as a K-way classification problem. Such an approach is viable only with large training sets such as ours. Our method outperforms the state of the art in terms of 3D pose estimation in controlled environments (Human3.6M) and shows promising results for in-the-wild images (LSP). This demonstrates that CNNs trained on artificial images generalize well to real images.
C1 [Rogez, Gregory; Schmid, Cordelia] Inria Grenoble Rhone Alpes, Lab Jean Kuntzmann, Grenoble, France.
RP Rogez, G (reprint author), Inria Grenoble Rhone Alpes, Lab Jean Kuntzmann, Grenoble, France.
FU European Commission under FP7 Marie Curie IOF grant
   [PIOF-GA-2012-328288]; ERC advanced grant Allegro; NVIDIA
FX This work was supported by the European Commission under FP7 Marie Curie
   IOF grant (PIOF-GA-2012-328288) and partially supported by ERC advanced
   grant Allegro. We acknowledge the support of NVIDIA with the donation of
   the GPUs used for this research. We thank P. Weinzaepfel for his help
   and the anonymous reviewers for their comments and suggestions.
CR Agarwal A, 2006, IEEE T PATTERN ANAL, V28, P44, DOI 10.1109/TPAMI.2006.21
   Akhter I., 2015, CVPR
   Andriluka M., 2014, CVPR
   Bissacco A., 2006, NIPS
   Bo LF, 2010, INT J COMPUT VISION, V87, P28, DOI 10.1007/s11263-008-0204-y
   Bourdev L., 2009, ICCV
   Chen X., 2014, NIPS
   Dosovitskiy A., 2015, ICCV
   Enzweiler M., 2008, CVPR
   Fan X., 2014, ECCV
   Hattori H., 2015, CVPR
   Hornung A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1186644.1186645
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Jaderberg M., 2015, NIPS
   Jaderberg M, 2016, INT J COMPUT VISION, V116, P1, DOI 10.1007/s11263-015-0823-z
   Johnson S., 2010, BMVC
   Johnson S., 2011, CVPR
   Kostrikov I., 2014, BMVC
   Krizhevsky A., 2012, NIPS
   Li S., 2015, ICCV
   Okada R., 2008, ECCV
   Park D., 2015, CVPRW
   Peng Xingchao, 2015, ICCV
   Pishchulin L., 2016, CVPR
   Pishchulin L., 2012, CVPR
   Rogez G., 2015, CVPR
   Rogez G, 2012, INT J COMPUT VISION, V99, P25, DOI 10.1007/s11263-012-0516-9
   Romero J., 2010, ICRA
   Shakhnarovich  G., 2003, ICCV
   Shotton J., 2011, CVPR
   Simo-Serra E., 2013, CVPR
   Simo-Serra E., 2012, CVPR
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Su Hao, 2015, ICCV
   Tekin B., 2016, CVPR
   Tompson J. J., 2014, NIPS
   Toshev A., 2014, CVPR
   Wang C, 2014, CVPR
   Wei S., 2016, CVPR
   Wu Z., 2015, CVPR
   Yang W., 2016, CVPR
   Yasin H., 2016, CVPR
   Zhou F., 2014, ECCV
   Zhou X., 2016, CVPR
   Zuffi S., 2015, CVPR
NR 45
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704087
DA 2019-06-15
ER

PT S
AU Rosenfeld, N
   Globerson, A
AF Rosenfeld, Nir
   Globerson, Amir
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Optimal Tagging with Markov Chain Optimization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Many information systems use tags and keywords to describe and annotate content. These allow for efficient organization and categorization of items, as well as facilitate relevant search queries. As such, the selected set of tags for an item can have a considerable effect on the volume of traffic that eventually reaches an item. In tagging systems where tags are exclusively chosen by an item's owner, who in turn is interested in maximizing traffic, a principled approach for assigning tags can prove valuable. In this paper we introduce the problem of optimal tagging, where the task is to choose a subset of tags for a new item such that the probability of browsing users reaching that item is maximized.
   We formulate the problem by modeling traffic using a Markov chain, and asking how transitions in this chain should be modified to maximize traffic into a certain state of interest. The resulting optimization problem involves maximizing a certain function over subsets, under a cardinality constraint.
   We show that the optimization problem is NP-hard, but has a (1 - 1/e)-approximation via a simple greedy algorithm due to monotonicity and submodularity. Furthermore, the structure of the problem allows for an efficient computation of the greedy step. To demonstrate the effectiveness of our method, we perform experiments on three tagging datasets, and show that the greedy algorithm outperforms other baselines.
C1 [Rosenfeld, Nir] Hebrew Univ Jerusalem, Sch Comp Sci & Engn, Jerusalem, Israel.
   [Globerson, Amir] Tel Aviv Univ, Blavatnik Sch Comp Sci, Tel Aviv, Israel.
RP Rosenfeld, N (reprint author), Hebrew Univ Jerusalem, Sch Comp Sci & Engn, Jerusalem, Israel.
EM nir.rosenfeld@mail.huji.ac.il; gamir@post.tau.ac.il
FU ISF Centers of Excellence [2180/15]; Intel Collaborative Research
   Institute for Computational Intelligence (ICRI-CI)
FX This work was supported by the ISF Centers of Excellence grant 2180/15,
   and by the Intel Collaborative Research Institute for Computational
   Intelligence (ICRI-CI).
CR Avrachenkov K, 2006, STOCH MODELS, V22, P319, DOI 10.1080/15326340600649052
   Brin S, 2012, COMPUT NETW, V56, P3825, DOI 10.1016/j.comnet.2012.10.007
   Cantador I., 2011, P 5 ACM C REC SYST R
   Csaji BC, 2010, LECT NOTES ARTIF INT, V6331, P89, DOI 10.1007/978-3-642-16108-7_11
   Fang Xiaomin, 2015, 29 AAAI C ART INT
   Gionis Aristides, 2013, P SIAM INT C DAT MIN, P387
   Goyal A., 2011, P 20 INT C COMP WORL, P47, DOI DOI 10.1145/1963192.1963217
   Hotho A, 2006, LWA, P111
   Kempe D., 2003, P 9 ACM SIGKDD INT C, P137, DOI DOI 10.1145/956750.956769
   Kim H.N., 2011, P 5 ACM C REC SYST, P45
   Krause A, 2008, J MACH LEARN RES, V9, P235
   Mavroforakis Charalampos, 2015, ARXIV150902533
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Olsen M, 2014, THEOR COMPUT SCI, V518, P96, DOI 10.1016/j.tcs.2013.08.003
   Olsen M, 2010, LECT NOTES COMPUT SC, V6509, P87, DOI 10.1007/978-3-642-17461-2_7
   Olsen M, 2010, LECT NOTES COMPUT SC, V6078, P37, DOI 10.1007/978-3-642-13073-1_5
   REID JK, 1982, MATH PROGRAM, V24, P55, DOI 10.1007/BF01585094
   Sigurbjornsson B, 2008, P 17 INT C WORLD WID, P327, DOI [10.1145/1367497.1367542, DOI 10.1145/1367497.1367542]
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700006
DA 2019-06-15
ER

PT S
AU Roy, A
   Pokutta, S
AF Roy, Aurko
   Pokutta, Sebastian
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Hierarchical Clustering via Spreading Metrics
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We study the cost function for hierarchical clusterings introduced by [16] where hierarchies are treated as first-class objects rather than deriving their cost from projections into flat clusters. It was also shown in [16] that a top-down algorithm returns a hierarchical clustering of cost at most O (alpha(n) log n) times the cost of the optimal hierarchical clustering, where alpha(n) is the approximation ratio of the Sparsest Cut subroutine used. Thus using the best known approximation algorithm for Sparsest Cut due to Arora-Rao-Vazirani, the top-down algorithm returns a hierarchical clustering of cost at most O (log(3/2) n) times the cost of the optimal solution. We improve this by giving an O(log n)-approximation algorithm for this problem. Our main technical ingredients are a combinatorial characterization of ultrametrics induced by this cost function, deriving an Integer Linear Programming (ILP) formulation for this family of ultrametrics, and showing how to iteratively round an LP relaxation of this formulation by using the idea of sphere growing which has been extensively used in the context of graph partitioning. We also prove that our algorithm returns an O(log n)-approximate hierarchical clustering for a generalization of this cost function also studied in [16]. We also give constant factor inapproximability results for this problem.
C1 [Roy, Aurko] Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.
   [Pokutta, Sebastian] Georgia Inst Technol, ISyE, Atlanta, GA 30332 USA.
RP Roy, A (reprint author), Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.
EM aurko@gatech.edu; sebastian.pokutta@isye.gatech.edu
FU NSF CAREER award [CMMI-1452463]; NSF [CMMI-1333789]
FX Research reported in this paper was partially supported by NSF CAREER
   award CMMI-1452463 and NSF grant CMMI-1333789. The authors thank Kunal
   Talwar and Mohit Singh for helpful discussions and anonymous reviewers
   for helping improve the presentation of this paper.
CR Ackerman  M., 2010, COLT, P270
   Ailon N, 2005, ANN IEEE SYMP FOUND, P73, DOI 10.1109/SFCS.2005.36
   Arora S, 2009, J ACM, V56, DOI 10.1145/1502793.1502794
   Balcan MF, 2008, ACM S THEORY COMPUT, P671
   Bartal Y, 2004, LECT NOTES COMPUT SC, V3221, P89
   Charikar M, 2003, ANN IEEE SYMP FOUND, P524, DOI 10.1109/SFCS.2003.1238225
   Charikar Moses, 2016, ARXIV160909548
   Dasgupta S, 2005, J COMPUT SYST SCI, V70, P555, DOI 10.1016/j.jcis.2004.10.006
   Dasgupta S, 2016, STOC'16: PROCEEDINGS OF THE 48TH ANNUAL ACM SIGACT SYMPOSIUM ON THEORY OF COMPUTING, P118, DOI 10.1145/2897518.2897527
   Di Summa M, 2015, DISCRETE APPL MATH, V180, P70, DOI 10.1016/j.dam.2014.07.023
   Even G, 1999, SIAM J COMPUT, V28, P2187, DOI 10.1137/S0097539796308217
   Even G, 2000, J ACM, V47, P585, DOI 10.1145/347476.347478
   Felsenstein J, 2004, INFERRING PHYLOGENIE, V2
   Friedman J., 2001, ELEMENTS STAT LEARNI, V1
   Garg N, 1996, SIAM J COMPUT, V25, P235, DOI 10.1137/S0097539793243016
   Krauthgamer R, 2009, PROCEEDINGS OF THE TWENTIETH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P942
   Leighton T., 1988, 29th Annual Symposium on Foundations of Computer Science (IEEE Cat. No.88CH2652-6), P422, DOI 10.1109/SFCS.1988.21958
   Lichman M., 2013, UCI MACHINE LEARNING
   Meila M, 2001, MACH LEARN, V42, P9, DOI 10.1023/A:1007648401407
NR 19
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702072
DA 2019-06-15
ER

PT S
AU Rubin, TN
   Koyejo, O
   Jones, MN
   Yarkoni, T
AF Rubin, Timothy N.
   Koyejo, Oluwasanmi
   Jones, Michael N.
   Yarkoni, Tal
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Generalized Correspondence-LDA Models (GC-LDA) for Identifying
   Functional Regions in the Brain
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB This paper presents Generalized Correspondence-LDA (GC-LDA), a generalization of the Correspondence-LDA model that allows for variable spatial representations to be associated with topics, and increased flexibility in terms of the strength of the correspondence between data types induced by the model. We present three variants of GC-LDA, each of which associates topics with a different spatial representation, and apply them to a corpus of neuroimaging data. In the context of this dataset, each topic corresponds to a functional brain region, where the region's spatial extent is captured by a probability distribution over neural activity, and the region's cognitive function is captured by a probability distribution over linguistic terms. We illustrate the qualitative improvements offered by GC-LDA in terms of the types of topics extracted with alternative spatial representations, as well as the model's ability to incorporate a-priori knowledge from the neuroimaging literature. We furthermore demonstrate that the novel features of GC-LDA improve predictions for missing data.
C1 [Rubin, Timothy N.] SurveyMonkey, San Mateo, CA 94403 USA.
   [Koyejo, Oluwasanmi] Univ Illinois, Urbana, IL 61801 USA.
   [Jones, Michael N.] Indiana Univ, Bloomington, IN 47405 USA.
   [Yarkoni, Tal] Univ Texas Austin, Austin, TX 78712 USA.
RP Rubin, TN (reprint author), SurveyMonkey, San Mateo, CA 94403 USA.
CR Asuncion Arthur, 2009, P 25 C UNC ART INT, P27
   Blei D.M., 2003, P 26 ANN INT ACM SIG, P127, DOI DOI 10.1145/860435.860460
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Calhoun VD, 2009, NEUROIMAGE, V45, pS163, DOI 10.1016/j.neuroimage.2008.10.057
   Manning JR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0094914
   Owen AM, 2005, HUM BRAIN MAPP, V25, P46, DOI 10.1002/hbm.20131
   Poldrack RA, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002707
   Smith SM, 2009, P NATL ACAD SCI USA, V106, P13040, DOI 10.1073/pnas.0905267106
   Thirion B, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00167
   Van Essen DC, 2013, NEUROIMAGE, V80, P62, DOI 10.1016/j.neuroimage.2013.05.041
   Vigneau M, 2006, NEUROIMAGE, V30, P1414, DOI 10.1016/j.neuroimage.2005.11.002
   Yarkoni T, 2011, NAT METHODS, V8, P665, DOI [10.1038/NMETH.1635, 10.1038/nmeth.1635]
   Yeo BT Thomas, 2014, CEREB CORTEX
NR 13
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702021
DA 2019-06-15
ER

PT S
AU Rudolph, M
   Ruiz, FJR
   Mandt, S
   Blei, DM
AF Rudolph, Maja
   Ruiz, Francisco J. R.
   Mandt, Stephan
   Blei, David M.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Exponential Family Embeddings
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Word embeddings are a powerful approach for capturing semantic similarity among terms in a vocabulary. In this paper, we develop exponential family embeddings, a class of methods that extends the idea of word embeddings to other types of high-dimensional data. As examples, we studied neural data with real-valued observations, count data from a market basket analysis, and ratings data from a movie recommendation system. The main idea is to model each observation conditioned on a set of other observations. This set is called the context, and the way the context is defined is a modeling choice that depends on the problem. In language the context is the surrounding words; in neuroscience the context is close-by neurons; in market basket data the context is other items in the shopping cart. Each type of embedding model defines the context, the exponential family of conditional distributions, and how the latent embedding vectors are shared across data. We infer the embeddings with a scalable algorithm based on stochastic gradient descent. On all three applications-neural activity of zebrafish, users' shopping behavior, and movie ratings-we found exponential family embedding models to be more effective than other types of dimension reduction. They better reconstruct held-out data and find interesting qualitative structure.
C1 [Rudolph, Maja; Ruiz, Francisco J. R.; Mandt, Stephan; Blei, David M.] Columbia Univ, New York, NY 10027 USA.
   [Ruiz, Francisco J. R.] Univ Cambridge, Cambridge, England.
RP Rudolph, M (reprint author), Columbia Univ, New York, NY 10027 USA.
FU EU H2020 programme [706760]; NFS [IIS-1247664]; ONR [N00014-11-1-0651];
   DARPA [FA8750-14-2-0009, N66001-15-C-4032]; Adobe; John Templeton
   Foundation; Sloan Foundation
FX This work is supported by the EU H2020 programme (Marie Sklodowska-Curie
   grant agreement 706760), NFS IIS-1247664, ONR N00014-11-1-0651, DARPA
   FA8750-14-2-0009, DARPA N66001-15-C-4032, Adobe, the John Templeton
   Foundation, and the Sloan Foundation.
CR Ahrens MB, 2013, NAT METHODS, V10, P413, DOI [10.1038/nmeth.2434, 10.1038/NMETH.2434]
   Arnold BC, 2001, STAT SCI, V16, P249
   Bengio Y, 2006, STUD FUZZ SOFT COMP, V194, P137
   Bronnenberg BJ, 2008, MARKET SCI, V27, P745, DOI 10.1287/mksc.1080.0450
   Brown L, 1986, LECT NOTES MONOGRAPH, V9, pi
   Collins Michael, 2001, ADV NEURAL INFORM PR, P617
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Friedrich J, 2015, NIPS WORKSH NEUR SYS
   Gopalan Prem, 2015, UNCERTAINTY ARTIFICI
   Gutmann M., 2010, J MACHINE LEARNING R
   Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872
   Harris ZS, 1954, WORD, V10, P146, DOI 10.1080/00437956.1954.11659520
   Hu Y, 2008, DATA MINING
   Levy O., 2014, ADV NEURAL INFORM PR, V27, P2177, DOI DOI 10.1162/153244303322533223
   McCullagh P, 1989, GEN LINEAR MODELS, V37
   Mikolov T., 2013, P NAACL 2013, P746
   Mikolov T, 2013, ICLR WORKSH P
   Mikolov T., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951
   Mnih A., 2013, ADV NEURAL INFORM PR, P2265
   Mnih A., 2012, P 29 INT C MACH LEAR, P1751
   Neal Radford M., 1990, LEARNING STOCHASTIC
   Pennington J., 2014, EMNLP, P1532, DOI DOI 10.3115/V1/D14-1162
   Ranganath R, 2015, ARTIFICIAL INTELLIGE
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Taddy M, 2015, ANN APPL STAT, V9, P1394, DOI 10.1214/15-AOAS831
   Vilnis Luke, 2015, INT C LEARN REPR
NR 26
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704095
DA 2019-06-15
ER

PT S
AU Ruiz, FJR
   Titsias, MK
   Blei, DM
AF Ruiz, Francisco J. R.
   Titsias, Michalis K.
   Blei, David M.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI The Generalized Reparameterization Gradient
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB The reparameterization gradient has become a widely used method to obtain Monte Carlo gradients to optimize the variational objective. However, this technique does not easily apply to commonly used distributions such as beta or gamma without further approximations, and most practical applications of the reparameterization gradient fit Gaussian distributions. In this paper, we introduce the generalized reparameterization gradient, a method that extends the reparameterization gradient to a wider class of variational distributions. Generalized reparameterizations use invertible transformations of the latent variables which lead to transformed distributions that weakly depend on the variational parameters. This results in new Monte Carlo gradients that combine reparameterization gradients and score function gradients. We demonstrate our approach on variational inference for two complex probabilistic models. The generalized reparameterization is effective: even a single sample from the variational distribution is enough to obtain a low-variance gradient.
C1 [Ruiz, Francisco J. R.] Univ Cambridge, Cambridge, England.
   [Ruiz, Francisco J. R.; Blei, David M.] Columbia Univ, New York, NY 10027 USA.
   [Titsias, Michalis K.] Athens Univ Econ & Business, Athens, Greece.
RP Ruiz, FJR (reprint author), Univ Cambridge, Cambridge, England.; Ruiz, FJR (reprint author), Columbia Univ, New York, NY 10027 USA.
FU EU [706760]; NFS [IIS-1247664]; ONR [N00014-11-1-0651]; DARPA
   [FA8750-14-2-0009, N66001-15-C-4032]; Adobe; John Templeton Foundation;
   Sloan Foundation
FX This project has received funding from the EU H2020 programme (Marie
   Sklodowska-Curie grant agreement 706760), NFS IIS-1247664, ONR
   N00014-11-1-0651, DARPA FA8750-14-2-0009, DARPA N66001-15-C-4032, Adobe,
   the John Templeton Foundation, and the Sloan Foundation. The authors
   would also like to thank Kriste Krstovski, Alp Kuckukelbir, and
   Christian A. Naesseth for helpful comments and discussions.
CR BAYDIN AG, 2015, ARXIV150205767
   BONNET G, 1964, ANNALES TELECOMMUN, V19, P203
   Carbonetto P., 2009, ADV NEURAL INFORM PR
   Casella G, 1996, BIOMETRIKA, V83, P81, DOI 10.1093/biomet/83.1.81
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Fan K., 2015, ADV NEURAL INFORM PR
   Ghahramani Z., 2001, ADV NEURAL INFORM PR
   GLYNN PW, 1990, COMMUN ACM, V33, P75, DOI 10.1145/84537.84552
   Gu S., 2016, INT C LEARN REPR
   HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Kingma Diederik, 2014, INT C LEARN REPR
   Knowles D. A., 2015, ARXIV150901631V1
   Kucukelbir A., 2016, ARXIV160300788
   Kucukelbir A., 2015, ADV NEURAL INFORM PR
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   Mnih Andriy, 2014, INT C MACH LEARN
   NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6
   Paisley  J., 2012, INT C MACH LEARN
   PRICE R, 1958, IRE T INFORM THEOR, V4, P69, DOI 10.1109/TIT.1958.1057444
   Ranganath R, 2015, ARTIFICIAL INTELLIGE
   Ranganath Rajesh, 2014, ARTIFICIAL INTELLIGE
   Rezende D. J., 2014, INT C MACH LEARN
   Ross S., 2002, SIMULATION
   Ruiz F. J. R., 2016, UNCERTAINTY ARTIFICI
   Salimans T, 2013, BAYESIAN ANAL, V8, P837, DOI 10.1214/13-BA858
   Schulman J., 2015, ADV NEURAL INFORM PR
   Tieleman T., 2012, COURSERA NEURAL NETW, V4
   Titsias M. K., 2014, INT C MACH LEARN
   Titsias M. K., 2015, ADV NEURAL INFORM PR
   van de Meent J.-W., 2016, ARTIFICIAL INTELLIGE
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Wingate D., 2013, ARXIV13011299
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702075
DA 2019-06-15
ER

PT S
AU Rustamov, RM
   Guibas, L
AF Rustamov, Raif M.
   Guibas, Leonidas
BE Rish, I
   Langs, G
   Wehbe, L
   Cecchi, G
   Chang, KMK
   Murphy, B
TI Hyperalignment of Multi-subject fMRI Data by Synchronized Projections
SO MACHINE LEARNING AND INTERPRETATION IN NEUROIMAGING, MLINI 2014
SE Lecture Notes in Artificial Intelligence
LA English
DT Proceedings Paper
CT 4th International Workshop on Machine Learning and Interpretation in
   Neuroimaging (MLINI) Held at Conference on Neural Information Processing
   Systems (NIPS)
CY DEC 13, 2014
CL Montreal, CANADA
ID HUMAN CORTICAL ANATOMY; ALIGNMENT
AB Group analysis of fMRI data via multivariate pattern methods requires accurate alignments between neuronal activities of different subjects in order to attain competitive inter-subject classification rates. Hyperalignment, a recent technique pioneered by Haxby and collaborators, aligns the activations of different subjects by mapping them into a common abstract high-dimensional space. While hyperalignment is very successful in terms of classification performance, its "anatomy free" nature excludes the use of potentially helpful information inherent in anatomy. In this paper, we present a novel approach to hyperalignment that allows incorporating anatomical information in a non-trivial way. Experiments demonstrate the effectiveness of our approach over the original hyperalignment and several other natural alternatives.
C1 [Rustamov, Raif M.; Guibas, Leonidas] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
RP Rustamov, RM (reprint author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
EM rustamov@research.att.com
CR Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Churchland PM, 1998, J PHILOS, V95, P5, DOI 10.2307/2564566
   Conroy Bryan R, 2009, Adv Neural Inf Process Syst, V22, P378
   Conroy BR, 2013, NEUROIMAGE, V81, P400, DOI 10.1016/j.neuroimage.2013.05.009
   Fischl B, 1999, HUM BRAIN MAPP, V8, P272, DOI 10.1002/(SICI)1097-0193(1999)8:4<272::AID-HBM10>3.0.CO;2-4
   Haxby JV, 2011, NEURON, V72, P404, DOI 10.1016/j.neuron.2011.08.026
   Lorbert A., 2012, NIPS, V25, P1799
   Sabuncu MR, 2010, CEREB CORTEX, V20, P130, DOI 10.1093/cercor/bhp085
   Singer A, 2012, COMMUN PUR APPL MATH, V65, P1067, DOI 10.1002/cpa.21395
   Singer A, 2011, APPL COMPUT HARMON A, V30, P20, DOI 10.1016/j.acha.2010.02.001
   Talairach J, 1988, COPLANAR STEREOTAXIC
   Wang F., 2013, P INT C COMP VIS ICC
   Xu H, 2012, 2012 IEEE STATISTICAL SIGNAL PROCESSING WORKSHOP (SSP), P229, DOI 10.1109/SSP.2012.6319668
NR 13
TC 0
Z9 0
U1 0
U2 2
PU SPRINGER INTERNATIONAL PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
EI 1611-3349
BN 978-3-319-45173-2; 978-3-319-45174-9
J9 LECT NOTES ARTIF INT
PY 2016
VL 9444
BP 115
EP 121
DI 10.1007/978-3-319-45174-9_12
PG 7
WC Computer Science, Artificial Intelligence; Neuroimaging
SC Computer Science; Neurosciences & Neurology
GA BG4FZ
UT WOS:000388723400012
DA 2019-06-15
ER

PT S
AU Saad, F
   Mansinghka, V
AF Saad, Feras
   Mansinghka, Vikash
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI A Probabilistic Programming Approach To Probabilistic Data Analysis
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Probabilistic techniques are central to data analysis, but different approaches can be challenging to apply, combine, and compare. This paper introduces composable generative population models (CGPMs), a computational abstraction that extends directed graphical models and can be used to describe and compose a broad class of probabilistic data analysis techniques. Examples include discriminative machine learning, hierarchical Bayesian models, multivariate kernel methods, clustering algorithms, and arbitrary probabilistic programs. We demonstrate the integration of CGPMs into BayesDB, a probabilistic programming platform that can express data analysis tasks using a modeling definition language and structured query language. The practical value is illustrated in two ways. First, the paper describes an analysis on a database of Earth satellites, which identifies records that probably violate Kepler's Third Law by composing causal probabilistic programs with non-parametric Bayes in 50 lines of probabilistic code. Second, it reports the lines of code and accuracy of CGPMs compared with baseline solutions from standard machine learning libraries.
C1 [Saad, Feras; Mansinghka, Vikash] MIT, Probabilist Comp Project, Cambridge, MA 02139 USA.
RP Saad, F (reprint author), MIT, Probabilist Comp Project, Cambridge, MA 02139 USA.
EM fsaad@mit.edu; vkm@mit.edu
CR [Anonymous], 2015, UCS SATELLITE DATABA
   Carpenter B., 2016, J STAT SOFTW
   Casella G., 2002, DUXBURY ADV SERIES S
   Davidian M., 1995, NONLINEAR MODELS REP, V62
   Devroye L., 1986, 1986 Winter Simulation Conference Proceedings, P260
   Fink Daniel, 1997, COMPENDIUM CONJUGATE
   Friedman N, 1999, IJCAI-99: PROCEEDINGS OF THE SIXTEENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 & 2, P1300
   Koller D., 2009, PROBABILISTIC GRAPHI
   Mansinghka V., 2015, ARXIV151201272
   Mansinghka V., 2014, CORR
   Mansinghka  Vikash, 2015, ARXIV151205006
   Milch B., 2007, INTRO STAT RELATIONA, P373
   Pfeffer A., 2009, OBJECT ORIENTED PROB, V137
   Saad F., 2016, ARXIV160805347
   Spiegelhalter David J, 1996, BUGS BAYESIAN INFERE
NR 15
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700024
DA 2019-06-15
ER

PT S
AU Saberian, M
   Pereira, JC
   Xu, C
   Yang, J
   Vasconcelos, N
AF Saberian, Mohammad
   Pereira, Jose Costa
   Xu, Can
   Yang, Jian
   Vasconcelos, Nuno
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Large Margin Discriminant Dimensionality Reduction in Prediction Space
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB In this paper we establish a duality between boosting and SVM, and use this to derive a novel discriminant dimensionality reduction algorithm. In particular, using the multiclass formulation of boosting and SVM we note that both use a combination of mapping and linear classification to maximize the multiclass margin. In SVM this is implemented using a pre-defined mapping (induced by the kernel) and optimizing the linear classifiers. In boosting the linear classifiers are pre-defined and the mapping (predictor) is learned through a combination of weak learners. We argue that the intermediate mapping, i.e. boosting predictor, is preserving the discriminant aspects of the data and that by controlling the dimension of this mapping it is possible to obtain discriminant low dimensional representations for the data. We use the aforementioned duality and propose a new method, Large Margin Discriminant Dimensionality Reduction (LADDER) that jointly learns the mapping and the linear classifiers in an efficient manner. This leads to a data-driven mapping which can embed data into any number of dimensions. Experimental results show that this embedding can significantly improve performance on tasks such as hashing and image/scene classification.
C1 [Saberian, Mohammad] Netflix, Los Gatos, CA 95032 USA.
   [Pereira, Jose Costa] INESCTEC, Porto, Portugal.
   [Xu, Can] Google, Mountain View, CA USA.
   [Yang, Jian] Yahoo Res, Washington, DC USA.
   [Vasconcelos, Nuno] Univ Calif San Diego, San Diego, CA USA.
RP Saberian, M (reprint author), Netflix, Los Gatos, CA 95032 USA.
EM esaberian@netflix.com; jose.c.pereira@inesctec.pt; canxu@google.com;
   jianyang@yahoo-inc.com; nvasconcelos@ucsd.edu
CR [Anonymous], 2008, VISUALIZING HIGH DIM
   Bo L., 2012, ISER
   Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628
   Datar M., 2004, P 20 ANN S COMP GEOM, P253, DOI DOI 10.1145/997817.997857
   Dekel O., 2002, ADV NEURAL INFORM PR, P945
   Dollar  P., 2009, P BMVC
   Freund Y., 1997, J COMP SYS SCI
   Gao T., 2011, ICML
   Goldberger J, 2004, P ADV NEUR INF PROC, P513
   Gonen M, 2011, J MACH LEARN RES, V12, P2211
   Gong Y., 2012, IEEE T PATTERN ANAL, V99
   Gong Y., 2014, P ECCV
   He X., 2003, ADV NIPS
   He X., 2005, P IEEE ICCV
   Hsu CW, 2002, IEEE T NEURAL NETWOR, V13, P415, DOI 10.1109/72.991427
   Jia Y., 2014, ARXIV14085093
   Knuth D. H., 1973, ART COMPUTER PROGRAM
   Krizhevsky A., 2009, TECHNICAL REPORT
   Krizhevsky A., 2012, ADV NIPS
   Kulis B., 2009, ADV NEURAL INFORM PR, P1042
   Kulis B, 2012, IEEE T PATTERN ANAL, V34, P1092, DOI 10.1109/TPAMI.2011.219
   Larsson F, 2011, IET COMPUT VIS, V5, P244, DOI 10.1049/iet-cvi.2010.0040
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Mukherjee I., 2010, ADV NIPS
   Nocedal S. J., 1999, NUMERICAL OPTIMIZATI
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Parizi S. N., 2012, P IEEE CVPR
   Pereira JC, 2014, COMPUT VIS IMAGE UND, V124, P123, DOI 10.1016/j.cviu.2014.03.003
   Pereira JC, 2012, PROC CVPR IEEE, P3093, DOI 10.1109/CVPR.2012.6248041
   Perronnin F., 2010, P ECCV
   PUDIL P, 1994, PATTERN RECOGN LETT, V15, P1119, DOI 10.1016/0167-8655(94)90127-9
   Quattoni A., 2009, P IEEE CVPR
   Roweis S., 1998, ADV NIPS
   Saberian M., 2011, ADV NIPS
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
   Sivic J, 2008, PROC CVPR IEEE, P1, DOI 10.1109/CVPRW.2008.4562950
   Sonnenburg S, 2006, J MACH LEARN RES, V7, P1531
   Sugiyama M, 2007, J MACH LEARN RES, V8, P1027
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Vasconcelos N., 2012, P ECCV
   Weinberger K. Q., 2009, ADV NIPS
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Weston J., 1999, 7th European Symposium on Artificial Neural Networks. ESANN'99. Proceedings, P219
   Weston J., 2010, P ECML
   Zhao B, 2013, PROC CVPR IEEE, P3350, DOI 10.1109/CVPR.2013.430
NR 45
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703094
DA 2019-06-15
ER

PT S
AU Sadhanala, V
   Wang, YX
   Tibshirani, RJ
AF Sadhanala, Veeranjaneyulu
   Wang, Yu-Xiang
   Tibshirani, Ryan J.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Total Variation Classes Beyond 1d: Minimax Rates, and the Limitations of
   Linear Smoothers
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID TOTAL VARIATION MINIMIZATION; ALGORITHM; RECOVERY; PATH
AB We consider the problem of estimating a function defined over n locations on a d-dimensional grid (having all side lengths equal to n(1/d)). When the function is constrained to have discrete total variation bounded by C-n, we derive the minimax optimal (squared) l(2) estimation error rate, parametrized by n, C-n. Total variation denoising, also known as the fused lasso, is seen to be rate optimal. Several simpler estimators exist, such as Laplacian smoothing and Laplacian eigenmaps. A natural question is: can these simpler estimators perform just as well? We prove that these estimators, and more broadly all estimators given by linear transformations of the input data, are suboptimal over the class of functions with bounded variation. This extends fundamental findings of Donoho and Johnstone [12] on 1-dimensional total variation spaces to higher dimensions. The implication is that the computationally simpler methods cannot be used for such sophisticated denoising tasks, without sacrificing statistical accuracy. We also derive minimax rates for discrete Sobolev spaces over d-dimensional grids, which are, in some sense, smaller than the total variation function spaces. Indeed, these are small enough spaces that linear estimators can be optimal-and a few well-known ones are, such as Laplacian smoothing and Laplacian eigenmaps, as we show. Lastly, we investigate the adaptivity of the total variation denoiser to these smaller Sobolev function spaces.
C1 [Sadhanala, Veeranjaneyulu; Wang, Yu-Xiang] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.
   [Tibshirani, Ryan J.] Carnegie Mellon Univ, Dept Stat, Pittsburgh, PA 15213 USA.
RP Sadhanala, V (reprint author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.
EM vsadhana@cs.cmu.edu; yuxiangw@cs.cmu.edu; ryantibs@stat.cmu.edu
FU NSF [DMS-1309174, DMS-1554123, BCS-0941518]; Singapore NRF under its
   International Research Centre @ Singapore Funding Initiative; Baidu
   Scholarship
FX We thank Jan-Christian Hutter and Philippe Rigollet, whose paper [16]
   inspired us to think carefully about problem scalings (i.e., radii of TV
   and Sobolev classes) in the first place. YW was supported by NSF Award
   BCS-0941518 to CMU Statistics, a grant by Singapore NRF under its
   International Research Centre @ Singapore Funding Initiative, and a
   Baidu Scholarship. RT was supported by NSF Grants DMS-1309174 and
   DMS-1554123.
CR ACAR R, 1994, INVERSE PROBL, V10, P1217, DOI 10.1088/0266-5611/10/6/003
   Barbero  A., 2014, ARXIV14110589
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Belkin Mikhail, 2002, ADV NEURAL INFORM PR, V15
   Belkin Mikhail, 2005, C LEARN THEOR COLT 0, V18
   Birge L., 2001, J EUR MATH SOC, V3, P203, DOI DOI 10.1007/S100970100031
   Chambolle A, 1997, NUMER MATH, V76, P167, DOI 10.1007/s002110050258
   Chambolle A, 2009, INT J COMPUT VISION, V84, P288, DOI 10.1007/s11263-009-0238-9
   Conte Samuel, 1980, INT SERIES PURE APPL
   Dobson DC, 1996, SIAM J APPL MATH, V56, P1181, DOI 10.1137/S003613999427560X
   DONOHO DL, 1990, ANN STAT, V18, P1416, DOI 10.1214/aos/1176347758
   DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425
   Donoho DL, 1998, ANN STAT, V26, P879
   Gyorfi L, 2002, DISTRIBUTION FREE TH
   Hoefling H, 2010, J COMPUT GRAPH STAT, V19, P984, DOI 10.1198/jcgs.2010.09208
   Hutter Jan-Christian, 2016, C LEARN THEOR COLT 1
   KUNSCH HR, 1994, ANN I STAT MATH, V46, P1, DOI 10.1007/BF00773588
   Mammen E, 1997, ANN STAT, V25, P387
   Needell D, 2013, SIAM J IMAGING SCI, V6, P1035, DOI 10.1137/120868281
   Ng MK, 1999, SIAM J SCI COMPUT, V21, P851, DOI 10.1137/S1064827598341384
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Sharpnack J, 2012, P INT C ART INT STAT, P1028
   Sharpnack James, 2010, ADV NEURAL INFORM PR, V13
   Smola Alexander, 2003, P ANN C LEARN THEOR, V16
   Tibshirani R, 2005, J R STAT SOC B, V67, P91, DOI 10.1111/j.1467-9868.2005.00490.x
   Tibshirani RJ, 2014, ANN STAT, V42, P285, DOI 10.1214/13-AOS1189
   Tibshirani RJ, 2011, ANN STAT, V39, P1335, DOI 10.1214/11-AOS878
   Wang YL, 2008, SIAM J IMAGING SCI, V1, P248, DOI 10.1137/080724265
   Wang Yu-Xiang, 2016, J MACHINE LEARNING R
   Zhu Xiaojin, 2003, INT C MACH LEARN ICM, V20
NR 30
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704094
DA 2019-06-15
ER

PT S
AU Sajjadi, M
   Javanmardi, M
   Tasdizen, T
AF Sajjadi, Mehdi
   Javanmardi, Mehran
   Tasdizen, Tolga
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Regularization With Stochastic Transformations and Perturbations for
   Deep Semi-Supervised Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Effective convolutional neural networks are trained on large sets of labeled data. However, creating large labeled datasets is a very costly and time-consuming task. Semi-supervised learning uses unlabeled data to train a model with higher accuracy when there is a limited set of labeled data available. In this paper, we consider the problem of semi-supervised learning with convolutional neural networks. Techniques such as randomized data augmentation, dropout and random max-pooling provide better generalization and stability for classifiers that are trained using gradient descent. Multiple passes of an individual sample through the network might lead to different predictions due to the non-deterministic behavior of these techniques. We propose an unsupervised loss function that takes advantage of the stochastic nature of these methods and minimizes the difference between the predictions of multiple passes of a training sample through the network. We evaluate the proposed method on several benchmark datasets.
C1 [Sajjadi, Mehdi; Javanmardi, Mehran; Tasdizen, Tolga] Univ Utah, Dept Elect & Comp Engn, Salt Lake City, UT 84112 USA.
RP Sajjadi, M (reprint author), Univ Utah, Dept Elect & Comp Engn, Salt Lake City, UT 84112 USA.
EM mehdi@sci.utah.edu; mehran@sci.utah.edu; tolga@sci.utah.edu
FU NSF [IIS-1149299]
FX This work was supported by NSF IIS-1149299.
CR Agrawal P, 2015, IEEE I CONF COMP VIS, P37, DOI 10.1109/ICCV.2015.13
   Bennett KP, 1999, ADV NEUR IN, V11, P368
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Blum A., 2001, LEARNING LABELED UNL
   Chang Jia-Ren, 2015, ARXIV151102583
   Ciregan D., 2012, PROC CVPR IEEE, P3642, DOI [10.1109/CVPR.2012.6248110, DOI 10.1109/CVPR.2012.6248110]
   de Sa Virginia R, 1994, ADV NEURAL INFORM PR, P112
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Dosovitskiy  A., 2014, ADV NEURAL INFORM PR, P766
   Glorot X., 2010, JLMR P TRACK, P249, DOI DOI 10.1.1/207.2059
   Graham B., 2014, ARXIV14096070
   GRAHAM Benjamin, 2014, ARXIV14126071
   Hinton G. E, 2012, ARXIV12070580
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Jayaraman D., 2016, COMPUTER VISION PATT
   Jayaraman D, 2015, IEEE I CONF COMP VIS, P1413, DOI 10.1109/ICCV.2015.166
   Joachims T, 1999, MACHINE LEARNING, PROCEEDINGS, P200
   Johnson Rie, 2015, Adv Neural Inf Process Syst, V28, P919
   Kavukcuoglu K., 2010, ARXIV10103467
   Krizhevskey A., 2014, CUDA CONVNET
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Le Cun B. B., 1990, ADV NEURAL INFORM PR
   LeCun Y, 2004, PROC CVPR IEEE, P97
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2010, IEEE INT SYMP CIRC S, P253, DOI 10.1109/ISCAS.2010.5537907
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Miller DJ, 1997, ADV NEUR IN, V9, P571
   Netzer Y., 2011, NIPS WORKSH DEEP LEA, V2011, P4
   Rasmus A, 2015, ADV NEURAL INFORM PR, P3532
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sajjadi M., 2016, INT C IM PROC
   Sermanet P., 2013, COMPUT VIS PATTERN R, V1312, P6229, DOI DOI 10.1109/CVPR.2015.7299176.ARXIV:1312.6229
   Simard PY, 1998, LECT NOTES COMPUT SC, V1524, P239
   Sun L, 2014, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2014.336
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Wang XL, 2015, IEEE I CONF COMP VIS, P2794, DOI 10.1109/ICCV.2015.320
   Weston Jason, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P639, DOI 10.1007/978-3-642-35289-8_34
   Zhu X., 2002, TECH REP
   Zhu X., 2003, INT C MACH LEARN, V20, P912
NR 40
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702080
DA 2019-06-15
ER

PT S
AU Salimans, T
   Goodfellow, I
   Zaremba, W
   Cheung, V
   Radford, A
   Chen, X
AF Salimans, Tim
   Goodfellow, Ian
   Zaremba, Wojciech
   Cheung, Vicki
   Radford, Alec
   Chen, Xi
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Improved Techniques for Training GANs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21:3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.
EM tim@openai.com; ian@openai.com; woj@openai.com; vicki@openai.com;
   alec@openai.com; peter@openai.com
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Brown G. W., 1951, ACTIVITY ANAL PRODUC, P374
   Denton Emily, 2015, ARXIV150605751
   Dziugaite G. K., 2015, ARXIV150503906
   Fukumizu K., 2007, ADV NIPS, P489
   Goodfellow I., 2014, NIPS
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Goodfellow Ian J, 2014, ARXIV14126515
   Gretton A, 2005, LECT NOTES ARTIF INT, V3734, P63
   Im D. J., 2016, ARXIV160205110
   Ioffe S., 2015, ARXIV150203167
   Kingma D. P., 2014, NEURAL INFORM PROCES
   Li Y., 2015, CORR
   Maaloe Lars, 2016, ARXIV160205473
   Miyato  T., 2015, ARXIV150700677
   Odena A., 2016, ARXIV160601583
   Radford A., 2015, ARXIV151106434
   Rasmus  Antti, 2015, ADV NEURAL INFORM PR
   Salimans T., 2016, ARXIV160207868
   Smola A, 2007, LECT NOTES ARTIF INT, V4754, P13
   Springenberg Jost Tobias, 2015, ARXIV151106390
   Sutskever Ilya, 2015, ARXIV151106440
   Szegedy C., 2015, ARXIV E PRINTS
   Szegedy C., 2015, ARXIV151200567
   Szegedy C, 2013, ARXIV13126199
   Warde-Farley David, 2016, PERTURBATIONS OPTIMI
   Yoo Donggeun, 2016, ARXIV160307442
   Zhao Junbo, 2015, ARXIV150602351
NR 28
TC 0
Z9 0
U1 5
U2 5
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700089
DA 2019-06-15
ER

PT S
AU Salimans, T
   Kingma, DP
AF Salimans, Tim
   Kingma, Diederik P.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Weight Normalization: A Simple Reparameterization to Accelerate Training
   of Deep Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.
C1 [Salimans, Tim; Kingma, Diederik P.] OpenAI, San Francisco, CA 94110 USA.
RP Salimans, T (reprint author), OpenAI, San Francisco, CA 94110 USA.
EM tim@openai.com; dpkingma@openai.com
CR Amari S, 1997, ADV NEUR IN, V9, P127
   Ba J. L., 2016, ARXIV160706450
   Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912
   Cooijmans T., 2016, ARXIV160309025
   Desjardins G., 2015, ADV NEURAL INFORM PR, P2062
   Glorot X., 2010, JLMR P TRACK, P249, DOI DOI 10.1.1/207.2059
   Goodfellow I, 2016, DEEP LEARNING
   Goodfellow I, 2013, ICML
   Gregor K., 2015, ARXIV150204623
   Grosse R., 2015, P 32 INT C MACH LEAR, P2304
   He  K., 2015, ARXIV151203385
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Huang G, 2016, ARXIV160806993
   Ioffe S., 2015, ICML
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kingma D. P., 2013, P 2 INT C LEARN REPR
   Kingma D. P., 2016, ARXIV160604934
   Krahenbuhl Philipp, 2015, ARXIV151106856
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Lee C.-Y., 2014, DEEP LEARN REPR LEAR
   Lin M., 2014, ICLR C TRACK
   Martens J., 2010, P 27 INT C MACH LEAR, P735, DOI DOI 10.1155/2011/176802
   Martens James, 2015, ARXIV150305671
   Mishkin Dmytro, 2015, ARXIV151106422
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046
   Raiko T., 2012, P INT C ART INT STAT, V22, P924
   Rezende Danilo Jimenez, 2014, P 31 INT C MACH LEAR, V32, P1278
   Salimans T., 2015, ICML
   Springenberg J., 2015, ICLR WORKSH TRACK
   Srebro N, 2005, LECT NOTES COMPUT SC, V3559, P545, DOI 10.1007/11503415_37
   Sutskever I., 2013, P 30 INT C MACH LEAR, V28, P1139
   Zhang SL, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P2635
NR 33
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700078
DA 2019-06-15
ER

PT S
AU Sangnier, M
   Fercoq, O
   d'Alche-Buc, F
AF Sangnier, Maxime
   Fercoq, Olivier
   d'Alche-Buc, Florence
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Joint quantile regression in vector-valued RKHSs
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID CURVES
AB Addressing the will to give a more complete picture than an average relationship provided by standard regression, a novel framework for estimating and predicting simultaneously several conditional quantiles is introduced. The proposed methodology leverages kernel-based multi-task learning to curb the embarrassing phenomenon of quantile crossing, with a one-step estimation procedure and no post-processing. Moreover, this framework comes along with theoretical guarantees and an efficient coordinate descent learning algorithm. Numerical experiments on benchmark and real datasets highlight the enhancements of our approach regarding the prediction error, the crossing occurrences and the training time.
C1 [Sangnier, Maxime; Fercoq, Olivier; d'Alche-Buc, Florence] Univ Paris Saclay, Telecom ParisTech, LTCI, CNRS, F-75013 Paris, France.
RP Sangnier, M (reprint author), Univ Paris Saclay, Telecom ParisTech, LTCI, CNRS, F-75013 Paris, France.
EM maxime.sangnier@telecom-paristech.fr;
   olivier.fercoq@telecom-paristech.fr;
   florence.dalche@telecom-paristech.fr
FU industrial chair "Machine Learning for Big Data"
FX This work was supported by the industrial chair "Machine Learning for
   Big Data".
CR Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036
   Anderson M. S., 2012, CVXOPT PYTHON PACKAG
   Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Bondell HD, 2010, BIOMETRIKA, V97, P825, DOI 10.1093/biomet/asq048
   Brouard C., 2011, P 28 INT C MACH LEAR
   Brouard C, 2016, J MACH LEARN RES, V17
   Chernozhukov V, 2010, ECONOMETRICA, V78, P1093, DOI 10.3982/ECTA7880
   Dette H, 2008, J ROY STAT SOC B, V70, P609, DOI 10.1111/j.1467-9868.2008.00651.x
   Dinuzzo F., 2011, P 28 INT C MACH LEAR
   Fercoq O., 2015, ARXIV150804625MATH
   Minh HQ, 2016, J MACH LEARN RES, V17
   Hallin M, 2016, STAT PROBABIL LETT, V109, P232, DOI 10.1016/j.spl.2015.11.021
   He XM, 1997, AM STAT, V51, P186, DOI 10.2307/2685417
   Kadri H, 2016, J MACH LEARN RES, V17
   KOENKER R, 1978, ECONOMETRICA, V46, P33, DOI 10.2307/1913643
   Koenker R., 2005, QUANTILE REGRESSION
   Li YJ, 2007, J AM STAT ASSOC, V102, P255, DOI 10.1198/016214506000000979
   Liu YF, 2011, J NONPARAMETR STAT, V23, P415, DOI 10.1080/10485252.2010.537336
   Maurer A., 2016, P 27 INT C ALG LEARN
   Micchelli CA, 2005, NEURAL COMPUT, V17, P177, DOI 10.1162/0899766052530802
   Schnabel SK, 2013, ASTA-ADV STAT ANAL, V97, P77, DOI 10.1007/s10182-012-0198-1
   Shalev-Shwartz S, 2013, J MACH LEARN RES, V14, P567
   Sindhwani V., 2013, P 29 C UNC ART INT
   Takeuchi I, 2004, IEEE IJCNN, P401, DOI 10.1109/IJCNN.2004.1379939
   Takeuchi I., 2013, ADV NIPS, P1358
   Takeuchi I, 2006, J MACH LEARN RES, V7, P1231
   Wu YC, 2009, STAT INTERFACE, V2, P299
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701096
DA 2019-06-15
ER

PT S
AU Savin, C
   Tkacik, G
AF Savin, Cristina
   Tkacik, Gasper
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Estimating Nonlinear Neural Response Functions using GP Priors and
   Kronecker Methods
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID GRID CELLS; HIPPOCAMPUS; POSITION; MAPS
AB Jointly characterizing neural responses in terms of several external variables promises novel insights into circuit function, but remains computationally prohibitive in practice. Here we use gaussian process (GP) priors and exploit recent advances in fast GP inference and learning based on Kronecker methods, to efficiently estimate multidimensional nonlinear tuning functions. Our estimator requires considerably less data than traditional methods and further provides principled uncertainty estimates. We apply these tools to hippocampal recordings during open field exploration and use them to characterize the joint dependence of CA1 responses on the position of the animal and several other variables, including the animal's speed, direction of motion, and network oscillations. Our results provide an unprecedentedly detailed quantification of the tuning of hippocampal neurons. The model's generality suggests that our approach can be used to estimate neural response properties in other brain regions.
C1 [Savin, Cristina; Tkacik, Gasper] IST Austria, AT-3400 Klosterneuburg, Austria.
RP Savin, C (reprint author), IST Austria, AT-3400 Klosterneuburg, Austria.
EM csavin@ist.ac.at; tkacik@ist.ac.at
FU People Programme (Marie Curie Actions) of the European Union's Seventh
   Framework Programme (FP7/2007-2013) under REA [291734]
FX We thank Jozsef Csicsvari for kindly sharing the CA1 data. This work was
   supported by the People Programme (Marie Curie Actions) of the European
   Union's Seventh Framework Programme (FP7/2007-2013) under REA grant
   agreement no. 291734.
CR Brun VH, 2008, HIPPOCAMPUS, V18, P1200, DOI 10.1002/hipo.20504
   Dupret D, 2010, NAT NEUROSCI, V13, P995, DOI 10.1038/nn.2599
   Fiser J., 2013, NOT ASSESS IMPORTANC
   Flaxman Seth R, 2015, P 32 INT C MACH LEAR, V37, P607
   Frank LM, 2002, J NEUROSCI, V22, P3817
   Grossmann Allie H, 2016, Small GTPases, P1, DOI 10.1080/21541248.2016.1259710
   Hensman J., 2015, ADV NEURAL INFORM PR
   Huxter JR, 2008, NAT NEUROSCI, V11, P587, DOI 10.1038/nn.2106
   Macke JH, 2011, NEUROIMAGE, V56, P570, DOI 10.1016/j.neuroimage.2010.04.272
   MCNAUGHTON BL, 1983, EXP BRAIN RES, V52, P41
   Moser EI, 2008, ANNU REV NEUROSCI, V31, P69, DOI 10.1146/annurev.neuro.31.061307.090723
   Moser EI, 2014, NAT REV NEUROSCI, V15, P466, DOI 10.1038/nrn3766
   Okun M., 2015, NATURE
   Park M, 2014, NEURAL COMPUT, V26, P1519, DOI 10.1162/NECO_a_00615
   Pillow J. W., 2006, BAYESIAN BRAIN PROBA, P1
   Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140
   Rad KR, 2010, NETWORK-COMP NEURAL, V21, P142, DOI 10.3109/0954898X.2010.532288
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Saatci Y., 2012, THESIS
   Tkacik G, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003408
   Tkacik G, 2013, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2013/03/P03011
   Wilson A., 2013, GAUSSIAN PROCESS KER
NR 22
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701010
DA 2019-06-15
ER

PT S
AU Saxena, S
   Verbeek, J
AF Saxena, Shreyas
   Verbeek, Jakob
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Convolutional Neural Fabrics
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Despite the success of CNNs, selecting the optimal architecture for a given task remains an open problem. Instead of aiming to select a single optimal architecture, we propose a "fabric" that embeds an exponentially large number of architectures. The fabric consists of a 3D trellis that connects response maps at different layers, scales, and channels with a sparse homogeneous local connectivity pattern. The only hyper-parameters of a fabric are the number of channels and layers. While individual architectures can be recovered as paths, the fabric can in addition ensemble all embedded architectures together, sharing their weights where their paths overlap. Parameters can be learned using standard methods based on back-propagation, at a cost that scales linearly in the fabric size. We present benchmark results competitive with the state of the art for image classification on MNIST and CIFAR10, and for semantic segmentation on the Part Labels dataset.
C1 [Saxena, Shreyas; Verbeek, Jakob] INRIA Grenoble, Lab Jean Kuntzmann, Grenoble, France.
RP Saxena, S (reprint author), INRIA Grenoble, Lab Jean Kuntzmann, Grenoble, France.
FU LabEx PERSYVAL-Lab [ANR-11-LABX-0025-01]
FX We would like to thank NVIDIA for the donation of GPUs used in this
   research. This work has been partially supported by the LabEx
   PERSYVAL-Lab (ANR-11-LABX-0025-01).
CR Chang J.-R., 2015, BATCH NORMALIZED MAX
   Chen  T., 2016, ICLR
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Goodfellow I, 2013, ICML
   Graves A., 2007, ICANN
   He K., 2016, ECCV
   Honari S., 2016, CVPR
   Huang G. B., 2007, 0749 U MASS
   Ioffe S., 2015, ICML
   Kae A., 2013, CVPR
   Kalchbrenner N., 2016, ICLR
   Kingma D. P., 2015, ICLR
   Krizhevsky A., 2012, NIPS
   Kulkarni P., 2015, BMVC
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y., 1989, NIPS
   Lee C.-Y., 2016, GEN POOLING FUNCTION
   Lin M., 2014, ICLR
   Liu S, 2015, CVPR
   Long  J., 2015, CVPR
   Mairal J., 2014, NIPS
   Mishkin Dmytro, 2016, ICLR
   Misra  Ishan, 2016, CVPR
   Noh H., 2015, ICCV
   Pfister T., 2015, CVPR
   Ronneberger O, 2015, MED IMAGE COMPUTING
   Simonyan Karen, 2015, ICLR
   Singh S., 2016, NIPS
   Springenberg J. T., 2015, ICLR
   Srivastava N., 2014, JMLR
   Tsogkas S., 2015, DEEP LEARNING SEMANT
   Wan L., 2013, ICML
   Zheng H., 2015, LEARNING HIGH LEVEL
   Zhou Y., 2015, INT S NEUR NETW
NR 34
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702051
DA 2019-06-15
ER

PT S
AU Schein, A
   Zhou, MY
   Wallach, H
AF Schein, Aaron
   Zhou, Mingyuan
   Wallach, Hanna
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Poisson-Gamma Dynamical Systems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We introduce a new dynamical system for sequentially observed multivariate count data. This model is based on the gamma-Poisson construction-a natural choice for count data-and relies on a novel Bayesian nonparametric prior that ties and shrinks the model parameters, thus avoiding overfitting. We present an efficient MCMC inference algorithm that advances recent work on augmentation schemes for inference in negative binomial models. Finally, we demonstrate the model's inductive bias using a variety of real-world data sets, showing that it exhibits superior predictive performance over other models and infers highly interpretable latent structure.
C1 [Schein, Aaron] Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.
   [Zhou, Mingyuan] Univ Texas Austin, McCombs Sch Business, Austin, TX 78712 USA.
   [Wallach, Hanna] Microsoft Res, 641 Ave Amer, New York, NY 10011 USA.
RP Schein, A (reprint author), Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.
EM aschein@cs.umass.edu; mingyuan.zhou@mccombs.utexas.edu;
   hanna@dirichlet.net
FU UMass Amherst CIIR; NSF [SBE-0965436, IIS-1320219]
FX We thank David Belanger, Roy Adams, Kostis Gourgoulias, Ben Marlin, Dan
   Sheldon, and Tim Vieira for many helpful conversations. This work was
   supported in part by the UMass Amherst CIIR and in part by NSF grants
   SBE-0965436 and IIS-1320219. Any opinions, findings, conclusions, or
   recommendations are those of the authors and do not necessarily reflect
   those of the sponsors.
CR Acharya S, 2015, PROCEEDINGS OF THE 18TH INTERNATIONAL CONFERENCE OF THE ASSOCIATION OF PSYCHOLOGY AND PSYCHIATRY FOR ADULTS AND CHILDREN (A.P.P.A.C 2013)
   ADELSON RM, 1966, OPER RES QUART, V17, P73, DOI 10.2307/3007241
   Basbug M. E., 2016, P 33 INT C MACH LEAR
   Blei D. M., 2006, ICML, P113, DOI DOI 10.1145/1143844.1143859
   BULMER MG, 1974, BIOMETRICS, V30, P101, DOI 10.2307/2529621
   Canny J., 2004, Proceedings of Sheffield SIGIR 2004. The Twenty-Seventh Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P122, DOI 10.1145/1008992.1009016
   Cemgil A. T., 2009, COMPUTATIONAL INTELL
   Charlin Laurent, 2015, P 9 ACM C REC SYST, P155
   Corless RM, 1996, ADV COMPUT MATH, V5, P329, DOI 10.1007/BF02124750
   Dunson DB, 2005, BIOSTATISTICS, V6, P11, DOI 10.1093/biostatistics/kxh025
   Durbin J, 2012, TIME SERIES ANAL STA
   FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360
   Ghahramani Z., 1998, ADV NEURAL INFORM PR, V11, P431
   Gopalan P., 2015, P 31 C UNC ART INT
   Haykin S., 2001, KALMAN FILTERING NEU
   Kalman R. E, 1960, J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]
   Kingman J. F. C., 1972, POISSON PROCESSES
   Kleinberg J, 2003, DATA MIN KNOWL DISC, V7, P373, DOI 10.1023/A:1024940629314
   Macke J. H., 2011, ADV NEURAL INF PROCE, V24, P1350
   McCullagh P., 1989, GEN LINEAR MODELS
   Rugh W. J., 1995, LINEAR SYSTEM THEORY
   Zhou M., 2012, P 15 INT C ART INT S
   Zhou M., ARXIV160407464
   Zhou M, 2015, P MACH LEARN RES, P1135
   Zhou  M., 2012, ADV NEURAL INFORM PR, P2546
   Zhou MY, 2016, J MACH LEARN RES, V17
   Zhou MY, 2015, IEEE T PATTERN ANAL, V37, P307, DOI 10.1109/TPAMI.2013.211
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700047
DA 2019-06-15
ER

PT S
AU Schulam, P
   Arora, R
AF Schulam, Peter
   Arora, Raman
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Disease Trajectory Maps
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID MODELS
AB Medical researchers are coming to appreciate that many diseases are in fact complex, heterogeneous syndromes composed of subpopulations that express different variants of a related complication. Longitudinal data extracted from individual electronic health records (EHR) offer an exciting new way to study subtle differences in the way these diseases progress over time. In this paper, we focus on answering two questions that can be asked using these databases of longitudinal EHR data. First, we want to understand whether there are individuals with similar disease trajectories and whether there are a small number of degrees of freedom that account for differences in trajectories across the population. Second, we want to understand how important clinical outcomes are associated with disease trajectories. To answer these questions, we propose the Disease Trajectory Map (DTM), a novel probabilistic model that learns low-dimensional representations of sparse and irregularly sampled longitudinal data. We propose a stochastic variational inference algorithm for learning the DTM that allows the model to scale to large modern medical datasets. To demonstrate the DTM, we analyze data collected on patients with the complex autoimmune disease, scleroderma. We find that DTM learns meaningful representations of disease trajectories and that the representations are significantly associated with important clinical outcomes.
C1 [Schulam, Peter; Arora, Raman] Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21218 USA.
RP Schulam, P (reprint author), Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21218 USA.
EM pschulam@cs.jhu.edu; arora@cs.jhu.edu
FU NSF Graduate Research Fellowship; NSF BIGDATA [IIS-1546482]
FX PS is supported by an NSF Graduate Research Fellowship. RA is supported
   in part by NSF BIGDATA grant IIS-1546482.
CR Allanore Y, 2015, NAT REV DIS PRIMERS, V1, DOI 10.1038/nrdp.2015.2
   Bigelow Jamie L, 2012, J AM STAT ASS
   Brillinger D. R., 2001, TIME SERIES DATA ANA, V36
   Carvalho, 2012, J AM STAT ASS
   Castaldi P. J., 2014, THORAX
   CASTRO PE, 1986, TECHNOMETRICS, V28, P329, DOI 10.2307/1268982
   Craig J., 2008, NAT ED, V1, P184
   Damianou A. C., 2015, JMLR, V2
   Gelman A., 2014, BAYESIAN DATA ANAL, V2
   Hensman J, 2013, ARXIV13096835
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   James GM, 2000, BIOMETRIKA, V87, P587, DOI 10.1093/biomet/87.3.587
   KAISER HF, 1958, PSYCHOMETRIKA, V23, P187, DOI 10.1007/BF02289233
   Keogh E, 2001, SIGMOD REC, V30, P151
   Kleinman KP, 1998, BIOMETRICS, V54, P921, DOI 10.2307/2533846
   Lawrence ND, 2004, ADV NEUR IN, V16, P329
   Levin K, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P410, DOI 10.1109/ASRU.2013.6707765
   Lin J, 2007, DATA MIN KNOWL DISC, V15, P107, DOI 10.1007/s10618-007-0064-z
   Lotvall J, 2011, J ALLERGY CLIN IMMUN, V127, P355, DOI 10.1016/j.jaci.2010.11.037
   MacLehose RF, 2009, STAT SINICA, V19, P611
   Marlin B.M., 2012, P 2 ACM SIGHIT INT H, P389, DOI DOI 10.1145/2110363.2110408
   Ramsay J. O., 2002, APPL FUNCTIONAL DATA
   Ramsay JO, 2006, FUNCTIONAL DATA ANAL
   Rice JA, 2001, BIOMETRICS, V57, P253, DOI 10.1111/j.0006-341X.2001.00253.x
   Saria S., 2015, INT SYS IEEE
   Saria S., 2011, IJCAI, V22
   Schulam P, 2015, ADV NEURAL INFORM PR, P748
   Schulam P., 2015, P 29 AAAI C ART INT, P2956
   Snelson E., 2005, NIPS
   Duong T, 2012, P NATL ACAD SCI USA, V109, P8382, DOI 10.1073/pnas.1117796109
   Tipping ME, 1999, J ROY STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196
   Titsias M., 2009, AISTATS
   Titsias M. K., 2010, AISTATS
   Varadarajan B., 2008, P 46 ANN M ASS COMP, P165, DOI DOI 10.3115/1557690.1557736
   Varga J, 2012, SCLERODERMA: FROM PATHOGENESIS TO COMPREHENSIVE MANAGEMENT, P1, DOI 10.1007/978-1-4419-5774-0
   Verbeke G, 2009, SPRINGER SER STAT, P1
   Watanabe S., 1965, P 4 PRAG C INF THEOR
   Wiggins LD, 2012, J AUTISM DEV DISORD, V42, P191, DOI 10.1007/s10803-011-1230-0
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701034
DA 2019-06-15
ER

PT S
AU Schulz, E
   Tenenbaum, JB
   Duvenaud, D
   Speekenbrink, M
   Gershman, SJ
AF Schulz, Eric
   Tenenbaum, Joshua B.
   Duvenaud, David
   Speekenbrink, Maarten
   Gershman, Samuel J.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Probing the Compositionality of Intuitive Functions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID EXTRAPOLATION
AB How do people learn about complex functional structure? Taking inspiration from other areas of cognitive science, we propose that this is accomplished by harnessing compositionality: complex structure is decomposed into simpler building blocks. We formalize this idea within the framework of Bayesian regression using a grammar over Gaussian process kernels. We show that participants prefer compositional over non-compositional function extrapolations, that samples from the human prior over functions are best described by a compositional model, and that people perceive compositional functions as more predictable than their non-compositional but otherwise similar counterparts. We argue that the compositional nature of intuitive functions is consistent with broad principles of human cognition.
C1 [Schulz, Eric; Speekenbrink, Maarten] UCL, London, England.
   [Tenenbaum, Joshua B.] MIT, Cambridge, MA 02139 USA.
   [Duvenaud, David] Univ Toronto, Toronto, ON, Canada.
   [Gershman, Samuel J.] Harvard Univ, Cambridge, MA 02138 USA.
RP Schulz, E (reprint author), UCL, London, England.
EM e.schulz@cs.ucl.ac.uk; jbt@mit.edu; duvenaud@cs.toronto.edu;
   m.speekenbrink@ucl.ac.uk; gershman@fas.harvard.edu
CR Adams R.P., 2013, ARXIV13024245
   Bott L, 2004, J EXP PSYCHOL LEARN, V30, P38, DOI 10.1037/0278-7393.30.1.38
   Brady TF, 2011, J VISION, V11, DOI 10.1167/11.5.4
   BREHMER B, 1974, ORGAN BEHAV HUM PERF, V11, P1, DOI 10.1016/0030-5073(74)90002-6
   Carroll J., 1963, FUNCTIONAL LEARNING
   DeLosh EL, 1997, J EXP PSYCHOL LEARN, V23, P968, DOI 10.1037/0278-7393.23.4.968
   Duvenaud DK, 2013, P INT C MACH LEARN I, V3, P1166
   Gershman S. J., 2016, VISION RES
   Gershman S. J., 2016, DECISION
   Gershman SJ, 2010, CURR OPIN NEUROBIOL, V20, P251, DOI 10.1016/j.conb.2010.02.008
   Griffiths Thomas L, 2009, ADV NEURAL INFORM PR, P553
   Grosse R., 2012, UNCERTAINTY ARTIFICI
   Kalish ML, 2007, PSYCHON B REV, V14, P288, DOI 10.3758/BF03194066
   Kemp C, 2009, PSYCHOL REV, V116, P20, DOI 10.1037/a0014282
   KOH KH, 1991, J EXP PSYCHOL LEARN, V17, P811, DOI 10.1037/0278-7393.17.5.811
   Lucas CG, 2015, PSYCHON B REV, V22, P1193, DOI 10.3758/s13423-015-0808-5
   McDaniel MA, 2005, PSYCHON B REV, V12, P24, DOI 10.3758/BF03196347
   Parpart P., 2015, P 37 ANN M COGN SCI, P1829
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Sanborn AN, 2010, COGNITIVE PSYCHOL, V60, P63, DOI 10.1016/j.cogpsych.2009.07.001
   Schulz E., 2015, P 37 ANN M COGN SCI, P2116
   Wilson A. G., 2015, ADV NEURAL INFORM PR, V28, P2836
   Zhao JY, 2016, COGNITION, V146, P217, DOI 10.1016/j.cognition.2015.09.018
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700094
DA 2019-06-15
ER

PT S
AU Schuurmans, D
   Zinkevich, M
AF Schuurmans, Dale
   Zinkevich, Martin
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Deep Learning Games
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID GRADIENT
AB We investigate a reduction of supervised learning to game playing that reveals new connections and learning methods. For convex one-layer problems, we demonstrate an equivalence between global minimizers of the training problem and Nash equilibria in a simple game. We then show how the game can be extended to general acyclic neural networks with differentiable convex gates, establishing a bijection between the Nash equilibria and critical (or KKT) points of the deep learning problem. Based on these connections we investigate alternative learning methods, and find that regret matching can achieve competitive training performance while producing sparser models than current deep learning strategies.
C1 [Schuurmans, Dale; Zinkevich, Martin] Google, Edmonton, AB, Canada.
   [Schuurmans, Dale; Zinkevich, Martin] Google Brain, Edmonton, AB, Canada.
RP Schuurmans, D (reprint author), Google, Edmonton, AB, Canada.
EM daes@ualberta.ca; martinz@google.com
CR Balduzzi D., 2016, DEEP ONLINE CONVEX O
   Balduzzi D, 2014, AAMAS'14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P1265
   Bottou Leon, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P421, DOI 10.1007/978-3-642-35289-8_25
   Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339
   Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Duchi J., 2008, P 25 INT C MACH LEAR, P272, DOI DOI 10.1145/1390156.1390191
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Freund Y, 1999, GAME ECON BEHAV, V29, P79, DOI 10.1006/game.1999.0738
   Gordon G., 2006, NIPS, V19
   Gordon G., 2005, CMUCALD05112
   HAJNAL A, 1993, J COMPUT SYST SCI, V46, P129, DOI 10.1016/0022-0000(93)90001-D
   Hart S, 2000, ECONOMETRICA, V68, P1127, DOI 10.1111/1468-0262.00153
   Hoeffgen K., 1995, JCSS, V52, P114
   Johanson Michael, 2012, AAAI, P1371
   Karush W., 1939, THESIS
   Kearns M, 2006, SCIENCE, V313, P824, DOI 10.1126/science.1127207
   Kingma D. P., 2015, 14126980 ARXIV, P1, DOI DOI 10.1145/1830483.1830503
   Kivinen J, 1997, INFORM COMPUT, V132, P1, DOI 10.1006/inco.1996.2612
   Kuhn H., 1951, P 2 BERK S MATH STAT, P481, DOI DOI 10.1007/BF01582292
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee J., 2016, 29 ANN C LEARN THEOR, V49
   LITTLESTONE N, 1994, INFORM COMPUT, V108, P212, DOI 10.1006/inco.1994.1009
   Mairal J, 2015, SIAM J OPTIMIZ, V25, P829, DOI 10.1137/140957639
   Nisan N, 2007, ALGORITHMIC GAME THEORY, P1, DOI 10.1017/CBO9780511800481
   Rakhlin S., 2013, ADV NEURAL INFORM PR, P3066
   Ratliff N., 2006, 22 INT C MACH LEARN
   Ratliff Nathan, 2007, 11 INT C ART INT STA
   Razborov A., 1992, ALGORITHM THEORY SWA
   Shalev-Shwartz S., 2006, NIPS
   Shalev-Shwartz S, 2007, MACH LEARN, V69, P115, DOI 10.1007/s10994-007-5014-x
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4
   Srinivasan N., 2002, ICONIP, V1
   Sutskever I., 2013, P 30 INT C MACH LEAR, V28, P1139
   Syrgkanis V., 2015, ADV NEURAL INFORM PR, V28, P2971
   Tammelin O, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P645
   Zinkevich M., 2003, 20 INT C MACH LEARN
   Zinkevich M., 2007, NIPS
NR 38
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702062
DA 2019-06-15
ER

PT S
AU Scieur, D
   D'Aspremont, A
   Bach, F
AF Scieur, Damien
   D'Aspremont, Alexandre
   Bach, Francis
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Regularized Nonlinear Acceleration
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID OPTIMIZATION; CONVERGENCE; ALGORITHMS
AB We describe a convergence acceleration technique for generic optimization problems. Our scheme computes estimates of the optimum from a nonlinear average of the iterates produced by any optimization method. The weights in this average are computed via a simple and small linear system, whose solution can be updated online. This acceleration scheme runs in parallel to the base algorithm, providing improved estimates of the solution on the fly, while the original optimization method is running. Numerical experiments are detailed on classical classification problems.
C1 [Scieur, Damien; Bach, Francis] Ecole Normale Super, INRIA, Paris, France.
   [Scieur, Damien; D'Aspremont, Alexandre; Bach, Francis] Ecole Normale Super, DI, UMR 8548, Paris, France.
   [D'Aspremont, Alexandre] Ecole Normale Super, CNRS, Paris, France.
RP Scieur, D (reprint author), Ecole Normale Super, INRIA, Paris, France.; Scieur, D (reprint author), Ecole Normale Super, DI, UMR 8548, Paris, France.
EM damien.scieur@inria.fr; aspremon@di.ens.fr; francis.bach@inria.fr
FU European Union [607290 SpaRTaN]; ERC SIPA; chaire Economie des nouvelles
   donnees; fonds AXA pour la recherche
FX The research leading to these results has received funding from the
   European Union's Seventh Framework Programme (FP7-PEOPLE-2013-ITN) under
   grant agreement no 607290 SpaRTaN, as well as support from ERC SIPA and
   the chaire Economie des nouvelles donnees with the data science joint
   research initiative with the fonds AXA pour la recherche.
CR Aitken A C, 1927, P R SOC EDINBURGH, V46, P289
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Ben-Tal A., 2001, MPS SIAM SERIES OPTI
   Brezinski C, 1977, LECT NOTES MATH
   CABAY S, 1976, SIAM J NUMER ANAL, V13, P734, DOI 10.1137/0713060
   Drori Y, 2014, MATH PROGRAM, V145, P451, DOI 10.1007/s10107-013-0653-0
   Durbin J, 1960, REV INT STATIST I, V28, P233, DOI [10.2307/1401322, DOI 10.2307/1401322]
   Eddy R., 1979, INFORM LINKAGE APPL, P387
   Golub Gene H, 1961, NUMER MATH, V3, P157
   Heinig G, 2011, LINEAR ALGEBRA APPL, V435, P1, DOI 10.1016/j.laa.2010.12.001
   Lasserre JB, 2001, SIAM J OPTIMIZ, V11, P796, DOI 10.1137/S1052623400366802
   Lessard L, 2016, SIAM J OPTIMIZ, V26, P57, DOI 10.1137/15M1009597
   Levinson N, 1949, EXTRAPOLATION INTERP
   Lin Hongzhou, 2015, ADV NEURAL INFORM PR, P3366
   Mesina M., 1977, Computer Methods in Applied Mechanics and Engineering, V10, P165, DOI 10.1016/0045-7825(77)90004-4
   NEMIROVSKII AS, 1985, USSR COMP MATH MATH+, V25, P21, DOI 10.1016/0041-5553(85)90100-4
   Nesterov Y., 2000, HIGH PERFORMANCE OPT, P405, DOI DOI 10.1007/978-1-4757-3216-0_17
   Nesterov Y., 1983, SOV MATH DOKL, V27, P372
   Nesterov Y, 2015, MATH PROGRAM, V152, P381, DOI 10.1007/s10107-014-0790-0
   Nesterov Yurii E., 2003, INTRO LECT CONVEX OP
   Parrilo P. A., 2000, THESIS
   Shanks D., 1955, J MATH PHYS, V34, P1, DOI DOI 10.1080/00207167308803075
   SIDI A, 1986, SIAM J NUMER ANAL, V23, P178, DOI 10.1137/0723013
   SMITH DA, 1987, SIAM REV, V29, P199, DOI 10.1137/1029042
   Su W., 2014, ADV NEURAL INFORM PR, V27, P2510
   TYRTYSHNIKOV EE, 1994, NUMER MATH, V67, P261, DOI 10.1007/s002110050027
   Wibisono Andre, 2015, TECHNICAL REPORT
   Wynn P., 1956, MTAC, V10, P91
NR 28
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702014
DA 2019-06-15
ER

PT S
AU Seeger, M
   Salinas, D
   Flunkert, V
AF Seeger, Matthias
   Salinas, David
   Flunkert, Valentin
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Bayesian Intermittent Demand Forecasting for Large Inventories
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID INFERENCE
AB We present a scalable and robust Bayesian method for demand forecasting in the context of a large e-commerce platform, paying special attention to intermittent and bursty target statistics. Inference is approximated by the Newton-Raphson algorithm, reduced to linear-time Kalman smoothing, which allows us to operate on several orders of magnitude larger problems than previous related work. In a study on large real-world sales datasets, our method outperforms competing approaches on fast and medium moving items.
C1 [Seeger, Matthias; Salinas, David; Flunkert, Valentin] Amazon Dev Ctr Germany, Krausenstr 38, D-10115 Berlin, Germany.
RP Seeger, M (reprint author), Amazon Dev Ctr Germany, Krausenstr 38, D-10115 Berlin, Germany.
EM matthis@amazon.de; dsalina@amazon.de; flunkert@amazon.de
CR Barber D, 2011, BAYESIAN TIME SERIES
   Barber D, 2006, J MACH LEARN RES, V7, P2515
   Beal M.J., 2003, THESIS
   Bishop C. M., 2006, PATTERN RECOGNITION
   Box G. E., 2013, TIME SERIES ANAL FOR
   Chapados Nicolas, 2014, P 31 INT C MACH LEAR, P1395
   Durbin J, 2012, TIME SERIES ANAL STA
   Heskes Tom, 2002, UNCERTAINTY ARTIFICI, V18
   Hyndman RJ, 2008, SPRINGER SER STAT, P1, DOI 10.1007/978-3-540-71918-2
   Hyndman RJ, 2008, J STAT SOFTW, V27, P1
   McCullach P., 1983, GEN LINEAR MODELS
   Minka T., 2001, UNCERTAINTY ARTIFICI, V17
   Rue H, 2009, J R STAT SOC B, V71, P319, DOI 10.1111/j.1467-9868.2008.00700.x
   Snyder L.V., 2011, FUNDAMENTALS SUPPLY
   Snyder RD, 2012, INT J FORECASTING, V28, P485, DOI 10.1016/j.ijforecast.2011.03.009
   Zaharia M., 2012, P 9 USENIX C NETW SY, P2, DOI DOI 10.1111/J.1095-8649.2005.00662.X
NR 16
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702060
DA 2019-06-15
ER

PT S
AU Senanayake, R
   Ott, L
   O'Callaghan, S
   Ramos, F
AF Senanayake, Ransalu
   Ott, Lionel
   O'Callaghan, Simon
   Ramos, Fabio
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Spatio-Temporal Hilbert Maps for Continuous Occupancy Representation in
   Dynamic Environments
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We consider the problem of building continuous occupancy representations in dynamic environments for robotics applications. The problem has hardly been discussed previously due to the complexity of patterns in urban environments, which have both spatial and temporal dependencies. We address the problem as learning a kernel classifier on an efficient feature space. The key novelty of our approach is the incorporation of variations in the time domain into the spatial domain. We propose a method to propagate motion uncertainty into the kernel using a hierarchical model. The main benefit of this approach is that it can directly predict the occupancy state of the map in the future from past observations, being a valuable tool for robot trajectory planning under uncertainty. Our approach preserves the main computational benefits of static Hilbert maps - using stochastic gradient descent for fast optimization of model parameters and incremental updates as new data are captured. Experiments conducted in road intersections of an urban environment demonstrated that spatio-temporal Hilbert maps can accurately model changes in the map while outperforming other techniques on various aspects.
C1 [Senanayake, Ransalu; Ott, Lionel; Ramos, Fabio] Univ Sydney, Sydney, NSW, Australia.
   [O'Callaghan, Simon] CSIRO, Data61, Canberra, ACT, Australia.
RP Senanayake, R (reprint author), Univ Sydney, Sydney, NSW, Australia.
EM rsen4557@uni.sydney.edu.au; lionel.ott@sydney.edu.au;
   simon.ocallaghan@data61.csiro.au; fabio.ramos@sydney.edu.au
CR Bottou L., 2008, NEURAL INFORM PROCES
   Doherty K., 2016, IEEE INT C ROB AUT I
   ELFES A, 1987, IEEE T ROBOTIC AUTOM, V3, P249, DOI 10.1109/JRA.1987.1087096
   Fleet D, 2006, HANDBOOK OF MATHEMATICAL MODELS IN COMPUTER VISION, P239
   Girard A., 2002, NEURAL INFORM PROCES
   Hastie T., 2001, SPRINGER SERIES STAT
   Krajnik T., 2014, IEEE INT C ROB AUT I
   Kuhn H. W., 1955, NAVAL RES LOGISTICS
   Lucas B. D., 1981, P 7 INT JOINT C ART, P674, DOI DOI 10.1109/HPDC.2004.1323531
   O'Callaghan S., 2014, P INT S EXP ROB ISER
   O'Callaghan ST, 2012, INT J ROBOT RES, V31, P42, DOI 10.1177/0278364911421039
   Rahimi A., 2009, NEURAL INFORM PROCES
   Rahimi A., 2008, NEURAL INFORM PROCES
   Ramos F., 2015, P ROBOTICS SCI SYSTE
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Smola A, 2007, LECT NOTES ARTIF INT, V4754, P13
   Wackernagel H., 2003, MULTIVARIATE GEOSTAT
NR 17
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704065
DA 2019-06-15
ER

PT S
AU Sener, O
   Song, HO
   Saxena, A
   Savarese, S
AF Sener, Ozan
   Song, Hyun Oh
   Saxena, Ashutosh
   Savarese, Silvio
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning Transferrable Representations for Unsupervised Domain
   Adaptation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Supervised learning with large scale labelled datasets and deep layered models has caused a paradigm shift in diverse areas in learning and recognition. However, this approach still suffers from generalization issues under the presence of a domain shift between the training and the test data distribution. Since unsupervised domain adaptation algorithms directly address this domain shift problem between a labelled source dataset and an unlabelled target dataset, recent papers [11, 33] have shown promising results by fine-tuning the networks with domain adaptation loss functions which try to align the mismatch between the training and testing data distributions.
   Nevertheless, these recent deep learning based domain adaptation approaches still suffer from issues such as high sensitivity to the gradient reversal hyperparameters [11] and overfitting during the fine-tuning stage. In this paper, we propose a unified deep learning framework where the representation, cross domain transformation, and target label inference are all jointly optimized in an end-to-end fashion for unsupervised domain adaptation. Our experiments show that the proposed method significantly outperforms state-of-the-art algorithms in both object recognition and digit classification experiments by a large margin.
C1 [Sener, Ozan; Song, Hyun Oh; Savarese, Silvio] Stanford Univ, Stanford, CA 94305 USA.
   [Saxena, Ashutosh] Brain Things, Redwood City, CA USA.
RP Sener, O (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM ozan@cs.stanford.edu; hsong@cs.stanford.edu; asaxena@cs.stanford.edu;
   ssilvio@cs.stanford.edu
FU Toyota Center grant [1191689-1-UDAWF];  [ONR-N00014-13-1-0761];  [MURI -
   WF911NF-15-1-0479]
FX We acknowledge the support of ONR-N00014-13-1-0761, MURI -
   WF911NF-15-1-0479 and Toyota Center grant 1191689-1-UDAWF.
CR Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Berlind C., 2015, ICML
   Blum A., 2001, ICML
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Chopra S., 2013, ICML W
   Darrell T., 2014, ARXIV14123474
   Deng J., 2009, CVPR
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Fernando B., 2013, ICCV
   Gammerman A., 1998, UAI
   Ganin  Yaroslav, 2015, ICML
   Gong B., 2012, CVPR
   Gong B., 2013, ICML
   Karpathy A., 2014, CVPR
   Khamis S., 2014, BMVC
   Kodirov E., 2015, ICCV
   Krizhevsky A., 2012, NIPS
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun  Y., 1998, MNIST DATABASE HANDW
   Long M., 2015, LEARNING TRANSFERABL
   Netzer Y., 2011, NIPS W
   Palatucci M., 2009, NIPS
   Raina Rajat, 2007, ICML
   Rohrbach  Marcus, 2013, NIPS
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Sindhwani V., 2005, ICML
   Sun B., 2016, AAAI
   Sun B., 2015, BMVC
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Thrun S., 2012, LEARNING LEARN
   Tommasi T., 2013, ICCV
   van der maaten L., 2014, JMLR
   Weinberger K. Q., 2006, NIPS
   Zhu  X., 2003, ICML
   Zhu  X., 2002, LEARNING LABELED UNL
NR 36
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702107
DA 2019-06-15
ER

PT S
AU Shaloudegi, K
   Gyorgy, A
   Szepesvari, C
   Xu, W
AF Shaloudegi, Kiarash
   Gyorgy, Andras
   Szepesvari, Csaba
   Xu, Wilsun
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI SDP Relaxation with Randomized Rounding for Energy Disaggregation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We develop a scalable, computationally efficient method for the task of energy disaggregation for home appliance monitoring. In this problem the goal is to estimate the energy consumption of each appliance over time based on the total energy-consumption signal of a household. The current state of the art is to model the problem as inference in factorial HMMs, and use quadratic programming to find an approximate solution to the resulting quadratic integer program. Here we take a more principled approach, better suited to integer programming problems, and find an approximate optimum by combining convex semidefinite relaxations randomized rounding, as well as a scalable ADMM method that exploits the special structure of the resulting semidefinite program. Simulation results both in synthetic and real-world datasets demonstrate the superiority of our method.
C1 [Shaloudegi, Kiarash; Gyorgy, Andras] Imperial Coll London, London, England.
   [Szepesvari, Csaba; Xu, Wilsun] Univ Alberta, Edmonton, AB, Canada.
RP Shaloudegi, K (reprint author), Imperial Coll London, London, England.
EM k.shaloudegi16@imperial.ac.uk; a.gyorgy@imperial.ac.uk;
   szepesva@ualberta.ca; wxu@ualberta.ca
FU Alberta Innovates Technology Futures through the Alberta Ingenuity
   Centre for Machine Learning; NSERC
FX This work was supported in part by the Alberta Innovates Technology
   Futures through the Alberta Ingenuity Centre for Machine Learning and by
   NSERC. K. is indebted to Pooria Joulani and Mohammad Ajallooeian, whom
   provided much useful technical advise, while all authors are grateful
   for Zico Kolter for sharing his code.
CR Anandkumar A., 2012, ARXIV12030683, V23, P1
   Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016
   Burer S, 2006, SIAM J OPTIMIZ, V16, P726, DOI 10.1137/040609574
   Chang HH, 2012, IEEE T IND APPL, V48, P764, DOI 10.1109/TIA.2011.2180497
   Dong M, 2013, IEEE T SMART GRID, V4, P1421, DOI 10.1109/TSG.2013.2245926
   Dong M, 2012, IEEE T SMART GRID, V3, P787, DOI 10.1109/TSG.2012.2185522
   Egarter D, 2015, IEEE T INSTRUM MEAS, V64, P467, DOI 10.1109/TIM.2014.2344373
   Figueiredo M, 2012, NEUROCOMPUTING, V96, P66, DOI 10.1016/j.neucom.2011.10.037
   Ghahramani Z, 1997, MACH LEARN, V29, P245, DOI 10.1023/A:1007425814087
   Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684
   Guo ZY, 2015, IEEE T POWER SYST, V30, P254, DOI 10.1109/TPWRS.2014.2327041
   Hyungsul Kim, 2011, SDM, V11, P747, DOI DOI 10.1137/1.9781611972818.64
   Kelly J, 2015, BUILDSYS'15 PROCEEDINGS OF THE 2ND ACM INTERNATIONAL CONFERENCE ON EMBEDDED SYSTEMS FOR ENERGY-EFFICIENT BUILT, P55, DOI 10.1145/2821650.2821672
   Koller D., 2009, ADAPTIVE COMPUTATION
   Kolter J.Z., 2010, ADV NEURAL INFORM PR, P1153
   Kolter J.Z., 2012, J MACHINE LEARNING R, P1472
   Kolter J.Z., 2011, WORKSH DAT MIN APPL, P59
   Kontorovich L. A., 2013, JMLR P, P702
   Liang J, 2010, IEEE T POWER DELIVER, V25, P551, DOI 10.1109/TPWRD.2009.2033799
   Lofberg J., 2004, CACSD
   Lovasz L, 1991, SIAM J OPTIMIZ, V1, P166, DOI 10.1137/0801013
   Malick J, 2009, SIAM J OPTIMIZ, V20, P336, DOI 10.1137/070704575
   Mattfeld Carl, 2014, ARXIV14047472
   Nesterov Y., 2004, INTRO LECT CONVEX OP
   Park J., 2015, ARXIV150407672
   Prudenzi A, 2002, 2002 IEEE POWER ENGINEERING SOCIETY WINTER MEETING, VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P941, DOI 10.1109/PESW.2002.985144
   Rennie SJ, 2009, INT CONF ACOUST SPEE, P3845, DOI 10.1109/ICASSP.2009.4960466
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Weiss M, 2012, INT CONF PERVAS COMP, P190, DOI 10.1109/PerCom.2012.6199866
   Wen ZW, 2010, MATH PROGRAM COMPUT, V2, P203, DOI 10.1007/s12532-010-0017-1
   Zhong M., 2014, ADV NEURAL INFORM PR, P3590
   Zia Tehseen, 2011, IECON 2011 - 37th Annual Conference of IEEE Industrial Electronics, P3218, DOI 10.1109/IECON.2011.6119826
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704079
DA 2019-06-15
ER

PT S
AU Shamir, O
AF Shamir, Ohad
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Without-Replacement Sampling for Stochastic Gradient Methods
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Stochastic gradient methods for machine learning and optimization problems are usually analyzed assuming data points are sampled with replacement. In contrast, sampling without replacement is far less understood, yet in practice it is very common, often easier to implement, and usually performs better. In this paper, we provide competitive convergence guarantees for without-replacement sampling under several scenarios, focusing on the natural regime of few passes over the data. Moreover, we describe a useful application of these results in the context of distributed optimization with randomly-partitioned data, yielding a nearly-optimal algorithm for regularized least squares (in terms of both communication complexity and runtime complexity) under broad parameter regimes. Our proof techniques combine ideas from stochastic optimization, adversarial online learning and transductive learning theory, and can potentially be applied to other stochastic optimization and learning problems.
C1 [Shamir, Ohad] Weizmann Inst Sci, Dept Comp Sci & Appl Math, Rehovot, Israel.
RP Shamir, O (reprint author), Weizmann Inst Sci, Dept Comp Sci & Appl Math, Rehovot, Israel.
EM ohad.shamir@weizmann.ac.il
FU FP7 Marie Curie CIG grant; ISF [425/13]; Intel Collaborative Research
   Institute for Computational Intelligence (ICRI-CI)
FX This research is supported in part by an FP7 Marie Curie CIG grant, an
   ISF grant 425/13, and by the Intel Collaborative Research Institute for
   Computational Intelligence (ICRI-CI).
CR Agarwal A, 2011, CORR
   Balcan M., 2012, COLT
   Bottou L., 2012, NEURAL NETWORKS TRIC
   Cotter A., 2011, NIPS
   Dekel O, 2012, J MACH LEARN RES, V13, P165
   El-Yaniv R, 2009, J ARTIF INTELL RES, V35, P193
   Gross D., 2010, ARXIV10012738
   Gurbuzbalaban M., 2015, ARXIV151008560
   Hazan E., 2015, BOOK DRAFT
   Jaggi M., 2014, NIPS
   Johnson R., 2013, NIPS
   Lacoste-Julien  S., 2012, ARXIV12122002
   Lee J. D., 2015, ARXIV150707595
   Nedic A, 2001, APPL OPTIMIZAT, V54, P223
   Rakhlin A., 2011, ARXIV11095647
   Recht B., 2012, COLT
   Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Shamir O., 2014, ALLERTON
   Shamir O., 2014, ICML
   Shamir Ohad, 2013, ICML
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Xiao L, 2010, J MACH LEARN RES, V11, P2543
   Zhang YC, 2013, J MACH LEARN RES, V14, P3321
   Zhang Yuchen, 2015, ICML
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701102
DA 2019-06-15
ER

PT S
AU Sheikh, AS
   Lucke, J
AF Sheikh, Abdul-Saboor
   Luecke, Joerg
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Select-and-Sample for Spike-and-Slab Sparse Coding
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Probabilistic inference serves as a popular model for neural processing. It is still unclear, however, how approximate probabilistic inference can be accurate and scalable to very high-dimensional continuous latent spaces. Especially as typical posteriors for sensory data can be expected to exhibit complex latent dependencies including multiple modes. Here, we study an approach that can efficiently be scaled while maintaining a richly structured posterior approximation under these conditions. As example model we use spike-and-slab sparse coding for V1 processing, and combine latent subspace selection with Gibbs sampling (select-and-sample). Unlike factored variational approaches, the method can maintain large numbers of posterior modes and complex latent dependencies. Unlike pure sampling, the method is scalable to very high-dimensional latent spaces. Among all sparse coding approaches with non-trivial posterior approximations (MAP or ICA-like models), we report the largest-scale results. In applications we firstly verify the approach by showing competitiveness in standard denoising benchmarks. Secondly, we use its scalability to, for the first time, study highly-overcomplete settings for V1 encoding using sophisticated posterior representations. More generally, our study shows that very accurate probabilistic inference for multi-modal posteriors with complex dependencies is tractable, functionally desirable and consistent with models for neural inference.
C1 [Sheikh, Abdul-Saboor] Tech Univ Berlin, Berlin, Germany.
   [Sheikh, Abdul-Saboor; Luecke, Joerg] Carl von Ossietzky Univ Oldenburg, Cluster Excellence Hearing4all, Oldenburg, Germany.
   [Sheikh, Abdul-Saboor] SAP Innovat Ctr Network, Berlin, Germany.
   [Luecke, Joerg] Carl von Ossietzky Univ Oldenburg, Res Ctr Neurosensory Sci, Oldenburg, Germany.
   [Luecke, Joerg] Carl von Ossietzky Univ Oldenburg, Dept Med Phys & Acoust, Oldenburg, Germany.
RP Sheikh, AS (reprint author), Tech Univ Berlin, Berlin, Germany.; Sheikh, AS (reprint author), SAP Innovat Ctr Network, Berlin, Germany.
EM sheikh.abdulsaboor@gmail.com; joerg.luecke@uol.de
FU DFG: Cluster of Excellence EXC 1077/1 (Hearing4all);  [LU 1196/5-1]
FX We thank E. Guiraud for help with Alg. 1 (illustration) and acknowledge
   funding by the DFG: Cluster of Excellence EXC 1077/1 (Hearing4all) and
   grant LU 1196/5-1.
CR Berkes P, 2011, SCIENCE, V331, P83, DOI 10.1126/science.1195870
   Coates A., 2011, P 28 INT C MACH LEAR, P921
   Dai Z., 2013, ADV NEURAL INFORM PR, V26, P243
   Exarchakis Georgios, 2012, Latent Variable Analysis and Signal Separation. Proceedings 10th International Conference, LVA/ICA 2012, P204, DOI 10.1007/978-3-642-28551-6_26
   Garrigues P., 2007, NIPS
   Goodfellow IJ, 2013, IEEE T PATTERN ANAL, V35, P1902, DOI 10.1109/TPAMI.2012.273
   Hinton GE, 1998, NATO ADV SCI I D-BEH, V89, P479
   Ilin A, 2005, NEURAL PROCESS LETT, V22, P183, DOI 10.1007/s11063-005-5265-0
   Jost P., 2006, IEEE INT C AC SPEECH, V5
   Kruger N., 1997, EUR S ANNS
   Li HB, 2009, PROCEEDINGS OF THE FIFTH INTERNATIONAL CONFERENCE ON IMAGE AND GRAPHICS (ICIG 2009), P754, DOI 10.1109/ICIG.2009.101
   Lucke Jorg, 2012, Latent Variable Analysis and Signal Separation. Proceedings 10th International Conference, LVA/ICA 2012, P213, DOI 10.1007/978-3-642-28551-6_27
   Lucke J, 2010, J MACH LEARN RES, V11, P2855
   Mairal J, 2012, FOUND TRENDS COMPUT, V8, DOI 10.1561/0600000058
   Mensch A., 2016, ICML
   Mohamed S., 2012, ICML
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Olshausen BA, 2013, PROC SPIE, V8651, DOI 10.1117/12.2013504
   Patel Ankit B, 2016, ADV NEURAL INFORM PR
   Puertas J, 2010, ADV NEURAL INF PROCE, V23, P1939
   Rezende D. J., 2015, ICML
   Ringach DL, 2002, J NEUROPHYSIOL, V88, P455, DOI 10.1152/jn.00881.2001
   Salimans T., 2015, ICML
   Schnass K, 2015, J MACH LEARN RES, V16, P1211
   Sheikh AS, 2014, J MACH LEARN RES, V15, P2653
   Shelton J., 2011, ADV NEURAL INFORM PR, P2618
   Shelton JA, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0124088
   Titsias M. K., 2011, ADV NEURAL INFORM PR, V24, P2339
   van Hateren JH, 1998, P ROY SOC B-BIOL SCI, V265, P359, DOI 10.1098/rspb.1998.0303
   Zhou M., 2009, ADV NEURAL INFORM PR, V22, P2295
   Zylberberg J, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002250
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702023
DA 2019-06-15
ER

PT S
AU Shen, YY
   Huang, QX
   Srebro, N
   Sanghavi, S
AF Shen, Yanyao
   Huang, Qixing
   Srebro, Nathan
   Sanghavi, Sujay
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Normalized Spectral Map Synchronization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Estimating maps among large collections of objects (e.g., dense correspondences across images and 3D shapes) is a fundamental problem across a wide range of domains. In this paper, we provide theoretical justifications of spectral techniques for the map synchronization problem, i.e., it takes as input a collection of objects and noisy maps estimated between pairs of objects along a connected object graph, and outputs clean maps between all pairs of objects. We show that a simple normalized spectral method (or NormSpecSync) that projects the blocks of the top eigenvectors of a data matrix to the map space, exhibits surprisingly good behavior -NormSpecSync is much more efficient than state-of-the-art convex optimization techniques, yet still admitting similar exact recovery conditions. We demonstrate the usefulness of NormSpecSync on both synthetic and real datasets.
C1 [Shen, Yanyao; Huang, Qixing; Sanghavi, Sujay] UT Austin, Austin, TX 78712 USA.
   [Huang, Qixing; Srebro, Nathan] TTI Chicago, Chicago, IL 60637 USA.
RP Shen, YY (reprint author), UT Austin, Austin, TX 78712 USA.
EM shenyanyao@utexas.edu; huangqx@cs.utexas.edu; nati@ttic.edu;
   sanghavi@mail.utexas.edu
FU [DMS-1700234];  [CCF-1302435];  [CCF-1320175];  [CCF-1564000]; 
   [CNS-0954059];  [IIS-1302662];  [IIS-1546500]
FX We would like to thank the anonymous reviewers for detailed comments on
   how to improve the paper. The authors would like to thank the support of
   DMS-1700234, CCF-1302435, CCF-1320175, CCF-1564000, CNS-0954059,
   IIS-1302662, and IIS-1546500.
CR Nguyen A, 2011, COMPUT GRAPH FORUM, V30, P1481, DOI 10.1111/j.1467-8659.2011.02022.x
   Burkard R., 2009, ASSIGNMENT PROBLEMS
   Caetano TS, 2007, IEEE I CONF COMP VIS, P86
   Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Chen Y., 2015, CORR
   Crandall D., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3001, DOI 10.1109/CVPR.2011.5995626
   Dalal N, 2005, PROC CVPR IEEE, P886
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Huang QX, 2006, ACM T GRAPHIC, V25, P569, DOI 10.1145/1141911.1141925
   Huang QX, 2013, COMPUT GRAPH FORUM, V32, P177, DOI 10.1111/cgf.12184
   Huber D. F., 2002, TECH REP
   KIM V., 2012, ACM T GRAPHIC, V31, P4
   Kim V., 2011, P SIGGRAPH, V30, P4
   Liu C, 2008, LECT NOTES COMPUT SC, V5304, P28, DOI 10.1007/978-3-540-88690-7_3
   Liu HL, 2014, INT J OPT, DOI 10.1155/2014/693807
   Marande W, 2007, SCIENCE, V318, P415, DOI 10.1126/science.1148033
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   Pachauri D., 2014, ADV NEURAL INFORM PR, P541
   Pachauri D., 2013, ADV NEURAL INFORM PR, P1860
   Wang L., 2012, CORR
   Zach C, 2010, PROC CVPR IEEE, P1426, DOI 10.1109/CVPR.2010.5539801
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700092
DA 2019-06-15
ER

PT S
AU Shen, YJ
   Choi, A
   Darwiche, A
AF Shen, Yujia
   Choi, Arthur
   Darwiche, Adnan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Tractable Operations for Arithmetic Circuits of Probabilistic Models
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID BELIEF PROPAGATION; INFERENCE
AB We consider tractable representations of probability distributions and the polytime operations they support. In particular, we consider a recently proposed arithmetic circuit representation, the Probabilistic Sentential Decision Diagram (PSDD). We show that PSDDs support a polytime multiplication operator, while they do not support a polytime operator for summing-out variables. A polytime multiplication operator makes PSDDs suitable for a broader class of applications compared to classes of arithmetic circuits that do not support multiplication. As one example, we show that PSDD multiplication leads to a very simple but effective compilation algorithm for probabilistic graphical models: represent each model factor as a PSDD, and then multiply them.
C1 [Shen, Yujia; Choi, Arthur; Darwiche, Adnan] Univ Calif Los Angeles, Comp Sci Dept, Los Angeles, CA 90095 USA.
RP Shen, YJ (reprint author), Univ Calif Los Angeles, Comp Sci Dept, Los Angeles, CA 90095 USA.
EM yujias@cs.ucla.edu; ychoi@cs.ucla.edu; darwiche@cs.ucla.edu
FU NSF [IIS-1514253]; ONR [N00014-15-1-2339]
FX This work was partially supported by NSF grant #IIS-1514253 and ONR
   grant #N00014-15-1-2339.
CR Acar U. A., 2008, UAI, P1
   BAHAR RI, 1993, 1993 IEEE/ACM INTERNATIONAL CONFERENCE ON COMPUTER-AIDED DESIGN - DIGEST OF TECHNICAL PAPERS, P188, DOI 10.1109/ICCAD.1993.580054
   Bekker J., 2015, NIPS
   Boutilier C, 1996, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P115
   Bova S., 2016, P 30 AAAI C ART INT, P929
   Chan H., 2006, UAI
   Chavira M, 2008, ARTIF INTELL, V172, P772, DOI 10.1016/j.artint.2007.11.002
   Choi Arthur, 2013, Symbolic and Quantitative Approaches to Reasoning with Uncertainty. 12th European Conference, ECSQARU 2013. Proceedings. LNCS 7958, P121, DOI 10.1007/978-3-642-39091-3_11
   Choi Arthur, 2011, New Frontiers in Artificial Intelligence. JSAI-isAI 2010 Workshops LENLS, JURISIN, AMBN, ISS. Revised Selected Papers, P167, DOI 10.1007/978-3-642-25655-4_16
   Choi A., 2016, AAAI
   Choi A., 2015, IJCAI
   Darwiche A, 2009, MODELING AND REASONING WITH BAYESIAN NETWORKS, P1, DOI 10.1017/CBO9780511811357
   Darwiche A, 2001, J ACM, V48, P608, DOI 10.1145/502090.502091
   Darwiche A, 2002, J ARTIF INTELL RES, V17, P229, DOI 10.1613/jair.989
   Darwiche A, 2003, J ACM, V50, P280, DOI 10.1145/765568.765570
   Darwiche A., 2008, RESULTS PROBABILISTI
   Darwiche A., 2011, P INT JOINT C ART IN, P819, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-143
   Darwiche A., 2001, J APPL NONCLASSICAL, V11, P11
   Delcher A. L., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P116
   Felzenszwalb PF, 2006, INT J COMPUT VISION, V70, P41, DOI 10.1007/s11263-006-7899-4
   Friedman N, 1998, NATO ADV SCI I D-BEH, V89, P421
   Gogate V., 2013, UAI
   Herrmann RG, 2013, 2013 BRAZILIAN CONFERENCE ON INTELLIGENT SYSTEMS (BRACIS), P175, DOI 10.1109/BRACIS.2013.37
   Jaeger M, 2004, INT J UNCERTAIN FUZZ, V12, P19, DOI 10.1142/S0218488504002564
   Kisa D., 2014, KR
   Koller D., 2009, PROBABILISTIC GRAPHI
   Larkin D., 2003, AISTATS
   Lowd D., 2008, P 24 C UNC ART INT, P383
   Lowd D., 2013, J MACHINE LEARNING R, P406
   Mateescu R, 2008, J ARTIF INTELL RES, V33, P465, DOI 10.1613/jair.2605
   Oztok U, 2014, LECT NOTES COMPUT SC, V8656, P42, DOI 10.1007/978-3-319-10428-7_7
   Pearl J, 1988, PROBABILISTIC REASON
   Poon H., 2011, P 27 C UNC ART INT, P337
   Rooshenas A., 2014, P 31 INT C MACH LEAR, V32, P710
   Roth D, 1996, ARTIF INTELL, V82, P273, DOI 10.1016/0004-3702(94)00092-1
   Sanner S, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P1384
   SHIMONY SE, 1994, ARTIF INTELL, V68, P399, DOI 10.1016/0004-3702(94)90072-8
   Sieling D., 1993, Parallel Processing Letters, V3, P3, DOI 10.1142/S0129626493000022
   Xue Y., 2012, P 26 AAAI C ART INT, P842
   Yedidia JS, 2005, IEEE T INFORM THEORY, V51, P2282, DOI 10.1109/TIT.2005.850085
NR 40
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702110
DA 2019-06-15
ER

PT S
AU Shishkin, A
   Bezzubtseva, A
   Drutsa, A
   Shishkov, I
   Gladkikh, E
   Gusev, G
   Serdyukov, P
AF Shishkin, Alexander
   Bezzubtseva, Anastasia
   Drutsa, Alexey
   Shishkov, Ilia
   Gladkikh, Ekaterina
   Gusev, Gleb
   Serdyukov, Pavel
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Efficient High-Order Interaction-Aware Feature Selection Based on
   Conditional Mutual Information
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID RELEVANCE
AB This study introduces a novel feature selection approach CMICOT, which is a further evolution of filter methods with sequential forward selection (SFS) whose scoring functions are based on conditional mutual information (MI). We state and study a novel saddle point (max-min) optimization problem to build a scoring function that is able to identify joint interactions between several features. This method fills the gap of MI-based SFS techniques with high-order dependencies. In this high-dimensional case, the estimation of MI has prohibitively high sample complexity. We mitigate this cost using a greedy approximation and binary representatives what makes our technique able to be effectively used. The superiority of our approach is demonstrated by comparison with recently proposed interaction-aware filters and several interaction-agnostic state-of-the-art ones on ten publicly available benchmark datasets.
C1 [Shishkin, Alexander; Bezzubtseva, Anastasia; Drutsa, Alexey; Shishkov, Ilia; Gladkikh, Ekaterina; Gusev, Gleb; Serdyukov, Pavel] Yandex, 16 Leo Tolstoy St, Moscow 119021, Russia.
RP Shishkin, A (reprint author), Yandex, 16 Leo Tolstoy St, Moscow 119021, Russia.
EM sisoid@yandex-team.ru; nstbezz@yandex-team.ru; adrutsa@yandex-team.ru;
   ishfb@yandex-team.ru; kglad@yandex-team.ru; gleb57@yandex-team.ru;
   pavser@yandex-team.ru
CR BATTITI R, 1994, IEEE T NEURAL NETWOR, V5, P537, DOI 10.1109/72.298224
   Brown G, 2012, J MACH LEARN RES, V13, P27
   Chen  Z., 2015, ARXIV150200231
   Cover T. M., 2012, ELEMENTS INFORM THEO
   Dougherty J., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P194
   Fleuret F, 2004, J MACH LEARN RES, V5, P1531
   Friedman Jerome, 2001, ANN STAT
   Guyon I., 2003, Journal of Machine Learning Research, V3, P1157, DOI 10.1162/153244303322753616
   Hand D. J., 2001, MACHINE LEARNING
   Hutter M, 2002, ADV NEUR IN, V14, P399
   Jakulin  A., 2003, ANAL ATTRIBUTE DEPEN
   Lewis D. D., 1992, P WORKSH SPEECH NAT, P212
   Liu Jie, 2012, JMLR Workshop Conf Proc, V22, P712
   Meyer PE, 2008, IEEE J-STSP, V2, P261, DOI 10.1109/JSTSP.2008.923858
   Paninski L, 2003, NEURAL COMPUT, V15, P1191, DOI 10.1162/089976603321780272
   Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159
   Vergara JR, 2014, NEURAL COMPUT APPL, V24, P175, DOI 10.1007/s00521-013-1368-0
   Vinh N. X., 2015, PATTERN RECOGNITION
   Wang G., 2004, P 13 ACM INT C INF K, P342
   WHITNEY AW, 1971, IEEE T COMPUT, VC 20, P1100, DOI 10.1109/T-C.1971.223410
   Yang H, 1999, P INT ICSC S ADV INT, P22
   Yu L, 2004, J MACH LEARN RES, V5, P1205
   Zaffalon  M., 2002, P 18 INT C UNC ART I, P577
   Zeng ZL, 2015, PATTERN RECOGN, V48, P2656, DOI 10.1016/j.patcog.2015.02.025
   Zhao Z, 2009, INTELL DATA ANAL, V13, P207, DOI 10.3233/IDA-2009-0364
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704108
DA 2019-06-15
ER

PT S
AU Shpakova, T
   Bach, F
AF Shpakova, Tatiana
   Bach, Francis
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Parameter Learning for Log-supermodular Distributions
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We consider log-supermodular models on binary variables, which are probabilistic models with negative log-densities which are submodular. These models provide probabilistic interpretations of common combinatorial optimization tasks such as image segmentation. In this paper, we focus primarily on parameter estimation in the models from known upper-bounds on the intractable log-partition function. We show that the bound based on separable optimization on the base polytope of the submodular function is always inferior to a bound based on "perturb-and-MAP" ideas. Then, to learn parameters, given that our approximation of the log-partition function is an expectation (over our own randomization), we use a stochastic subgradient technique to maximize a lower-bound on the log-likelihood. This can also be extended to conditional maximum likelihood. We illustrate our new results in a set of experiments in binary image denoising, where we highlight the flexibility of a probabilistic model to learn with missing data.
C1 [Shpakova, Tatiana; Bach, Francis] Ecole Normale Super Paris, INRIA, Paris, France.
RP Shpakova, T (reprint author), Ecole Normale Super Paris, INRIA, Paris, France.
EM tatiana.shpakova@inria.fr; francis.bach@inria.fr
FU European Union's H2020 Framework Programme under grant MacSeNet
   [H2020-MSCA-ITN-2014]
FX We acknowledge support the European Union's H2020 Framework Programme
   (H2020-MSCA-ITN-2014) under grant agreement no642685 MacSeNet, and thank
   Sesh Kumar, Anastasia Podosinnikova and Anton Osokin for interesting
   discussions related to this work.
CR Bach F., 2015, 151100394 ARXIV
   Bach F, 2013, FOUND TRENDS MACH LE, V6, P145, DOI 10.1561/2200000039
   Borenstein E., 2004, P ECCV
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Djolonga J., 2015, P ICML
   Djolonga J., 2014, ADV NIPS
   FUJISHIGE S., 2005, ANN DISCRETE MATH
   Golovin D, 2011, J ARTIF INTELL RES, V42, P427
   Hazan T., 2012, P ICML
   JERRUM M, 1993, SIAM J COMPUT, V22, P1087, DOI 10.1137/0222066
   Kohli P, 2009, INT J COMPUT VISION, V82, P302, DOI 10.1007/s11263-008-0202-0
   Krause A, 2014, TRACTABILITY, P71
   Lafferty J., 2001, P ICML
   Lin H., 2011, P NAACL HLT
   Nadarajah S, 2005, INT J MATH MATH SCI, V2005, P3169, DOI DOI 10.1155/IJMMS.2005.3169
   Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277
   Papandreou G., 2011, P ICCV
   Szummer M., 2008, P ECCV
   Tarlow D., 2012, P AISTATS
   Taskar B., 2003, MAX MARGIN MARKOV NE
   Tschiatschek S., 2016, P AISTATS
   Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Zhang J., 2015, P ICCV
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703038
DA 2019-06-15
ER

PT S
AU Shpitser, I
AF Shpitser, Ilya
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Consistent Estimation of Functions of Data Missing Non-Monotonically and
   Not at Random
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID CAUSAL INFERENCE
AB Missing records are a perennial problem in analysis of complex data of all types, when the target of inference is some function of the full data law. In simple cases, where data is missing at random or completely at random [15], well-known adjustments exist that result in consistent estimators of target quantities.
   Assumptions underlying these estimators are generally not realistic in practical missing data problems. Unfortunately, consistent estimators in more complex cases where data is missing not at random, and where no ordering on variables induces monotonicity of missingness status are not known in general, with some notable exceptions [13, 18, 16].
   In this paper, we propose a general class of consistent estimators for cases where data is missing not at random, and missingness status is non-monotonic. Our estimators, which are generalized inverse probability weighting estimators, make no assumptions on the underlying full data law, but instead place independence restrictions, and certain other fairly mild assumptions, on the distribution of missingness status conditional on the data.
   The assumptions we place on the distribution of missingness status conditional on the data can be viewed as a version of a conditional Markov random field (MRF) corresponding to a chain graph. Assumptions embedded in our model permit identification from the observed data law, and admit a natural fitting procedure based on the pseudo likelihood approach of [2]. We illustrate our approach with a simple simulation study, and an analysis of risk of premature birth in women in Botswana exposed to highly active anti-retroviral therapy.
C1 [Shpitser, Ilya] Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21218 USA.
RP Shpitser, I (reprint author), Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21218 USA.
EM ilyas@cs.jhu.edu
CR Bang H, 2005, BIOMETRICS, V61, P962, DOI 10.1111/j.1541-0420.2005.00377.x
   BESAG J, 1975, STATISTICIAN, V24, P179, DOI 10.2307/2987782
   DAWID AP, 1979, J ROY STAT SOC B MET, V41, P1
   Eric J., DISCRETE CHOICE MODE
   HORVITZ DG, 1952, J AM STAT ASSOC, V47, P663, DOI 10.2307/2280784
   Lafferty J. D., 2001, P 18 INT C MACH LEAR, V3, P282, DOI DOI 10.1038/NPROT.2006.61
   Lauritzen SL, 1996, GRAPHICAL MODELS
   Mohan K, 2014, ADV NEURAL INFORM PR, V27, P1520
   Mohan K., 2013, ADV NEUTRAL INFORM P, V26, P1277
   Mozeika A, 2014, PHYS REV E, V90, DOI 10.1103/PhysRevE.90.010101
   Newey WK, 1994, HDB ECONOMETRICS, V4, P2111, DOI DOI 10.1016/S1573-4412(05)80005-4
   Pearl J, 1988, PROBABILISTIC REASON
   ROBINS J, 1986, MATH MODELLING, V7, P1393, DOI 10.1016/0270-0255(86)90088-6
   Robins JM, 1997, STAT MED, V16, P21
   RUBIN DB, 1976, BIOMETRIKA, V63, P581, DOI 10.2307/2335739
   Sadinle Mauricio, 2016, WORKING PAPER
   Shpitser  I., 2015, P 31 C UNC ART INT A, P802
   VAN DER LAAN M. J, 2003, SPR S STAT
NR 18
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702044
DA 2019-06-15
ER

PT S
AU Shrivastava, A
AF Shrivastava, Anshumali
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Simple and Efficient Weighted Minwise Hashing
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Weighted minwise hashing (WMH) is one of the fundamental subroutine, required by many celebrated approximation algorithms, commonly adopted in industrial practice for large - scale search and learning. The resource bottleneck with WMH is the computation of multiple (typically a few hundreds to thousands) independent hashes of the data. We propose a simple rejection type sampling scheme based on a carefully designed red-green map, where we show that the number of rejected sample has exactly the same distribution as weighted minwise sampling. The running time of our method, for many practical datasets, is an order of magnitude smaller than existing methods. Experimental evaluations, on real datasets, show that for computing 500 WMH, our proposal can be 60000x faster than the Ioffe's method without losing any accuracy. Our method is also around 100x faster than approximate heuristics capitalizing on the efficient "densified" one permutation hashing schemes [26, 27]. Given the simplicity of our approach and its significant advantages, we hope that it will replace existing implementations in practice.
C1 [Shrivastava, Anshumali] Rice Univ, Dept Comp Sci, Houston, TX 77005 USA.
RP Shrivastava, A (reprint author), Rice Univ, Dept Comp Sci, Houston, TX 77005 USA.
EM anshumali@rice.edu
FU Rice Faculty Initiative Award 2016-17
FX This work is supported by Rice Faculty Initiative Award 2016-17. We
   would like to thank anonymous reviewers, Don Macmillen, and Ryan Moulton
   for feedbacks on the presentation of the paper.
CR Bayardo R. J., 2007, WWW, P131, DOI DOI 10.1145/1242572.1242591
   Broder A. Z., 1998, FUN
   Broder AZ, 1998, COMPRESSION AND COMPLEXITY OF SEQUENCES 1997 - PROCEEDINGS, P21, DOI 10.1109/SEQUEN.1997.666900
   BRODER AZ, 1997, WWW, V29, P1157
   Charikar M. S., 2002, P 34 ANN ACM S THEOR, P380, DOI DOI 10.1145/509907.509965
   Dalal N, 2005, PROC CVPR IEEE, P886
   Datar M., 2004, P 20 ANN S COMP GEOM, P253, DOI DOI 10.1145/997817.997857
   Dwork C., ALGORITHMIC FDN DIFF
   Gollapudi S., 2006, P 15 ACM INT C INF K, P475
   Haeupler  B., 2014, ARXIV14104266
   Indyk P, 2001, J ALGORITHM, V38, P84, DOI 10.1006/jagm.2000.1131
   Ioffe S, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P246, DOI 10.1109/ICDM.2010.80
   Kleinberg J., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P14, DOI 10.1109/SFFCS.1999.814572
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li P., 2011, COMMUN ACM
   Li Ping, 2015, KDD
   Manasse Mark, 2010, MSRTR201073
   Mitzenmacher M, 2008, PROCEEDINGS OF THE NINETEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P746
   Patrascu M, 2010, LECT NOTES COMPUT SC, V6198, P715, DOI 10.1007/978-3-642-14165-2_60
   Philbin J., 2007, P IEEE C COMP VIS PA
   Rahimi A., 2007, ADV NEURAL INFORM PR, P1177
   Rajaraman A., MINING MASSIVE DATAS
   Rasheed Z., MC MINH METAGENOME C
   Sadosky P., 2015, ARXIV151007714
   Shrivastava A., 2014, ICML
   Shrivastava A., 2015, THESIS
   Shrivastava A., 2014, AISTATS, P886
   Shrivastava  Anshumali, 2014, UAI
   Wang J., 1999, D LIB MAGAZINE, V5
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703108
DA 2019-06-15
ER

PT S
AU Silva, R
AF Silva, Ricardo
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Observational-Interventional Priors for Dose-Response Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Controlled interventions provide the most direct source of information for learning causal effects. In particular, a dose-response curve can be learned by varying the treatment level and observing the corresponding outcomes. However, interventions can be expensive and time-consuming. Observational data, where the treatment is not controlled by a known mechanism, is sometimes available. Under some strong assumptions, observational data allows for the estimation of dose-response curves. Estimating such curves nonparametrically is hard: sample sizes for controlled interventions may be small, while in the observational case a large number of measured confounders may need to be marginalized. In this paper, we introduce a hierarchical Gaussian process prior that constructs a distribution over the dose-response curve by learning from observational data, and reshapes the distribution with a nonparametric affine transform learned from controlled interventions. This function composition from different sources is shown to speed-up learning, which we demonstrate with a thorough sensitivity analysis and an application to modeling the effect of therapy on cognitive skills of premature infants.
C1 [Silva, Ricardo] UCL, Dept Stat Sci, London, England.
   [Silva, Ricardo] UCL, Ctr Computat Stat & Machine Learning, London, England.
RP Silva, R (reprint author), UCL, Dept Stat Sci, London, England.; Silva, R (reprint author), UCL, Ctr Computat Stat & Machine Learning, London, England.
EM ricardo@stats.ucl.ac.uk
CR Bareinboim E., 2016, P NATL ACAD SCI
   Bayarri MJ, 2007, TECHNOMETRICS, V49, P138, DOI 10.1198/004017007000000092
   BROOKSGUNN J, 1992, J PEDIATR-US, V120, P350, DOI 10.1016/S0022-3476(05)80896-0
   Damianou A. C., 2013, P AISTATS, V31, P207
   Ernest J, 2015, ELECTRON J STAT, V9, P3155, DOI 10.1214/15-EJS1075
   Gramacy RB, 2008, J AM STAT ASSOC, V103, P1119, DOI 10.1198/016214508000000689
   Hill J, 2013, ANN APPL STAT, V7, P1386, DOI 10.1214/13-AOAS630
   Hill JL, 2011, J COMPUT GRAPH STAT, V20, P217, DOI 10.1198/jcgs.2010.08162
   Hyttinen A, 2013, J MACH LEARN RES, V14, P3041
   Liu F, 2009, BAYESIAN ANAL, V4, P119, DOI 10.1214/09-BA404
   MacKay D, 1994, ASHRAE T, V100, P1053
   MACKAY DJC, 1992, NEURAL COMPUT, V4, P590, DOI 10.1162/neco.1992.4.4.590
   McCandless LC, 2007, STAT MED, V26, P2331, DOI 10.1002/sim.2711
   Morgan SL, 2015, ANAL METHOD SOC RES, P1
   Neal RM, 2003, ANN STAT, V31, P705, DOI 10.1214/aos/1056562461
   Pearl J., 2000, CAUSALITY MODELS REA
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Robins J, 2007, STAT SCI, V22, P544, DOI DOI 10.1214/07-STS227D
   Spirtes P., 2000, CAUSATION PREDICTION
   VanderWeele TJ, 2011, BIOMETRICS, V67, P1406, DOI 10.1111/j.1541-0420.2011.01619.x
NR 20
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700071
DA 2019-06-15
ER

PT S
AU Simon-Gabriel, CJ
   Scibior, A
   Tolstikhin, I
   Scholkopf, B
AF Simon-Gabriel, Carl-Johann
   Scibior, Adam
   Tolstikhin, Ilya
   Schoelkopf, Bernhard
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Consistent Kernel Mean Estimation for Functions of Random Variables
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We provide a theoretical foundation for non-parametric estimation of functions of random variables using kernel mean embeddings. We show that for any continuous function f, consistent estimators of the mean embedding of a random variable X lead to consistent estimators of the mean embedding of f(X). For Matern kernels and sufficiently smooth functions we also provide rates of convergence.
   Our results extend to functions of multiple random variables. If the variables are dependent, we require an estimator of the mean embedding of their joint distribution as a starting point; if they are independent, it is sufficient to have separate estimators of the mean embeddings of their marginal distributions. In either case, our results cover both mean embeddings based on i.i.d. samples as well as "reduced set" expansions in terms of dependent expansion points. The latter serves as a justification for using such expansions to limit memory resources when applying the approach as a basis for probabilistic programming.
C1 [Simon-Gabriel, Carl-Johann; Scibior, Adam; Tolstikhin, Ilya; Schoelkopf, Bernhard] Max Planck Inst Intelligent Syst, Dept Empir Inference, Spemanstr 38, D-72076 Tubingen, Germany.
   [Scibior, Adam] Univ Cambridge, Engn Dept, Cambridge, England.
RP Simon-Gabriel, CJ (reprint author), Max Planck Inst Intelligent Syst, Dept Empir Inference, Spemanstr 38, D-72076 Tubingen, Germany.
EM cjsimon@tuebingen.mpg.de; adam.scibior@tuebingen.mpg.de;
   ilya@tuebingen.mpg.de; bs@tuebingen.mpg.de
FU Google European Fellowship in Causal Inference
FX We thank Krikamol Muandet for providing the code used to generate Figure
   1, Paul Rubenstein, Motonobu Kanagawa and Bharath Sriperumbudur for very
   useful discussions, and our anonymous reviewers for their valuable
   feedback. Carl-Johann Simon-Gabriel is supported by a Google European
   Fellowship in Causal Inference.
CR Adams R. A., 2003, SOBOLEV SPACES
   Bennett C., 1988, PURE APPL MATH
   Berlinet A., 2004, RKHS PROBABILITY STA
   Chen Y., 2010, UAI
   Fukumizu K, 2013, J MACH LEARN RES, V14, P3753
   Gradshteyn I. S., 2007, TABLE INTEGRALS SERI
   Kalos M. H., 2008, MONTE CARLO METHODS
   Kanagawa M., 2016, ARXIV160507254STAT
   Katznelson Y., 2004, INTRO HARMONIC ANAL
   Korzen M, 2014, J STAT SOFTW, V57, P1
   Lacoste-Julien S., 2015, P 18 INT C ART INT S, V38, P544
   Mathai A., 1973, INDIAN J STAT A, P39
   McKinley KS, 2016, ACM SIGPLAN NOTICES, V51, P1, DOI 10.1145/2837614.2843895
   Milios D., 2009, THESIS
   Poisson S. D., 1837, RECHERCHES PROBABILI
   Scholkopf B, 2015, STAT COMPUT, V25, P755, DOI 10.1007/s11222-015-9558-5
   Scholkopf B, 2002, LEARNING KERNELS SUP
   Scovel C., 2014, J COMPLEXITY, V26
   Simon-Gabriel C.J., 2016, TECHNICAL REPORT
   Smola A., 2007, ALT
   Song L., 2009, INT WORKSH INT SYST, P1
   Springer M. D., 1979, ALGEBRA RANDOM VARIA
   Sriperumbudur BK, 2011, J MACH LEARN RES, V12, P2389
   Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517
   Steinwart I., 2008, INFORM SCI STAT
   Steinwart I, 2012, CONSTR APPROX, V35, P363, DOI 10.1007/s00365-012-9153-3
   Tolstikhin I., 2016, ARXIV160204361MATHST
   Wendland H., 2004, SCATTERED DATA APPRO
   Williamson R.C., 1989, THESIS
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704069
DA 2019-06-15
ER

PT S
AU Singh, S
   Hoiem, D
   Forsyth, D
AF Singh, Saurabh
   Hoiem, Derek
   Forsyth, David
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Swapout: Learning an ensemble of deep architectures
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We describe Swapout, a new stochastic training method, that outperforms ResNets of identical network structure yielding impressive results on CIFAR-10 and CIFAR-100. Swapout samples from a rich set of architectures including dropout [20], stochastic depth [7] and residual architectures [5, 6] as special cases. When viewed as a regularization method swapout not only inhibits co-adaptation of units in a layer, similar to dropout, but also across network layers. We conjecture that swapout achieves strong regularization by implicitly tying the parameters across layers. When viewed as an ensemble training method, it samples a much richer set of architectures than existing methods such as dropout or stochastic depth. We propose a parameterization that reveals connections to exiting architectures and suggests a much richer set of architectures to be explored. We show that our formulation suggests an efficient training method and validate our conclusions on CIFAR-10 and CIFAR-100 matching state of the art accuracy. Remarkably, our 32 layer wider model performs similar to a 1001 layer ResNet model.
C1 [Singh, Saurabh; Hoiem, Derek; Forsyth, David] Univ Illinois, Dept Comp Sci, 1304 W Springfield Ave, Urbana, IL 61801 USA.
RP Singh, S (reprint author), Univ Illinois, Dept Comp Sci, 1304 W Springfield Ave, Urbana, IL 61801 USA.
EM ss1@illinois.edu; dhoiem@illinois.edu; daf@illinois.edu
FU ONR MURI [N00014-10-1-0934, N00014-16-1-2007]
FX This work is supported in part by ONR MURI Awards N00014-10-1-0934 and
   N00014-16-1-2007. We would like to thank NVIDIA for donating some of the
   GPUs used in this work.
CR Bengio Y., 2011, P 22 INT C ALG LEARN
   Gal Y., 2015, BAYESIAN CONVOLUTION
   Glorot X., 2011, AISTATS
   Hardt M., 2015, ABS150901240 CORR
   He K., 2015, ABS151203385 CORR
   He Kaiming, 2016, ABS160305027 CORR
   Huang  G., 2016, ABS160309382 CORR
   Ioffe Sergey, 2015, ABS150203167 CORR
   Krizhevsky A., 2012, NIPS
   Krueger D., 2016, ARXIV160601305
   Lecun Y., 1998, P IEEE
   Lee C. Y., 2015, AISTATS
   Lin M, 2013, ABS13124400 CORR
   Nair V., 2010, ICML
   Pinheiro P.H., 2013, ARXIV13062795
   Rahimi A., 2007, NIPS
   Romero A., 2015, ICLR
   Russakovsky  O., 2015, INT J COMPUTER VISIO
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Srivastava N., 2014, J MACHINE LEARNING R
   Srivastava R. K., 2015, NIPS
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Wan L., 2013, P 30 INT C MACH LEAR, P1058
   Zeiler MD, 2013, ARXIV201313013557
NR 24
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701062
DA 2019-06-15
ER

PT S
AU Singh, S
   Du, SS
   Poczos, B
AF Singh, Shashank
   Du, Simon S.
   Poczos, Barnabas
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Efficient Nonparametric Smoothness Estimation
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID INTEGRAL FUNCTIONALS
AB Sobolev quantities (norms, inner products, and distances) of probability density functions are important in the theory of nonparametric statistics, but have rarely been used in practice, due to a lack of practical estimators. They also include, as special cases, L-2 quantities which are used in many applications. We propose and analyze a family of estimators for Sobolev quantities of unknown probability density functions. We bound the finite-sample bias and variance of our estimators, finding that they are generally minimax rate-optimal. Our estimators are significantly more computationally tractable than previous estimators, and exhibit a statistical/computational trade-off allowing them to adapt to computational constraints. We also draw theoretical connections to recent work on fast two-sample testing and empirically validate our estimators on synthetic data.
C1 [Singh, Shashank; Du, Simon S.; Poczos, Barnabas] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
RP Singh, S (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM sss1@andrew.cmu.edu; ssdu@cs.cmu.edu; bapoczos@cs.cmu.edu
FU National Science Foundation Graduate Research Fellowship [DGE-1252522]
FX This material is based upon work supported by a National Science
   Foundation Graduate Research Fellowship to the first author under Grant
   No. DGE-1252522.
CR ANDERSON NH, 1994, J MULTIVARIATE ANAL, V50, P41, DOI 10.1006/jmva.1994.1033
   BICKEL PJ, 1988, SANKHYA SER A, V50, P381
   BIRGE L, 1995, ANN STAT, V23, P11, DOI 10.1214/aos/1176324452
   Chwialkowski K., 2015, P ADV NEUR INF PROC, P1972
   Epps T. W., 1986, J STATISTICAL COMPUT, V26, P177, DOI DOI 10.1080/00949658608810963
   Ginc E, 2008, BERNOULLI, V14, P47, DOI 10.3150/07-BEJ110
   Goria MN, 2005, J NONPARAMETR STAT, V17, P277, DOI 10.1080/104852504200026815
   Gretton A., 2006, ADV NEURAL INFORM PR, P513
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   HALL P, 1987, STAT PROBABIL LETT, V6, P109, DOI 10.1016/0167-7152(87)90083-6
   HEATHCOTE CR, 1972, AUST J STAT, V14, P172, DOI 10.1111/j.1467-842X.1972.tb00355.x
   Hero AO, 2002, IEEE SIGNAL PROC MAG, V19, P85, DOI 10.1109/MSP.2002.1028355
   Ibragimov I., 1978, S ASYMPTOTIC STAT, P41
   Kandasamy K., 2015, ADV NEURAL INFORM PR, P397
   KREISS HO, 1979, SIAM J NUMER ANAL, V16, P421, DOI 10.1137/0716035
   Krishnamurthy A., 2015, AISTATS
   Krishnamurthy Akshay, 2014, P 31 INT C MACH LEAR, P919
   Laurent B, 1996, ANN STAT, V24, P659
   Laurent B., 1992, EFFICIENT ESTIMATION
   Leonenko N, 2008, ANN STAT, V36, P2153, DOI 10.1214/07-AOS539
   Leoni G., 2009, 1 COURSE SOBOLEV SPA, V105
   Moon K., 2014, ADV NEURAL INFORM PR, P2420
   Moon K. R., 2016, ARXIV160106884
   Moon KR, 2014, IEEE INT SYMP INFO, P356, DOI 10.1109/ISIT.2014.6874854
   Pardo L, 2005, STAT INFERENCE BASED
   Poczos B., 2011, INT C ART INT STAT, V15, P609
   Poczos B., 2012, ARXIV12023758
   Poczos B, 2012, PROC CVPR IEEE, P2989, DOI 10.1109/CVPR.2012.6248028
   Principe JC, 2010, INFORM SCI STAT, P1, DOI 10.1007/978-1-4419-1570-2
   Quadrianto N., 2009, ADV NEURAL INFORM PR, P1500
   Ram P., 2009, ADV NEURAL INFORM PR, V23, P1527
   Schweder T., 1975, Scandinavian Journal of Statistics Theory and Applications, V2, P113
   Singh S., 2014, ADV NEURAL INFORM PR, P3032
   Singh S, 2014, INT C MACH LEARN, P333
   Tsybakov A. B., 2008, INTRO NONPARAMETRIC
   Wolsztynski E, 2005, SIGNAL PROCESS, V85, P937, DOI 10.1016/j.sigpro.2004.11.028
   Zaremba W., 2013, ADV NEURAL INFORM PR, P755
   Zhao J, 2015, NEURAL COMPUT, V27, P1345, DOI 10.1162/NECO_a_00732
   ZYGMUND A, 2002, TRIGONOMETRIC SERIES, V1
NR 39
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703005
DA 2019-06-15
ER

PT S
AU Singh, S
   Poczos, B
AF Singh, Shashank
   Poczos, Barnabas
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Finite-Sample Analysis of Fixed-k Nearest Neighbor Density Functional
   Estimators
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID ENTROPY
AB We provide finite-sample analysis of a general framework for using k-nearest neighbor statistics to estimate functionals of a nonparametric continuous probability density, including entropies and divergences. Rather than plugging a consistent density estimate (which requires k -> infinity as the sample size n -> infinity) into the functional of interest, the estimators we consider fix k and perform a bias correction. This is more efficient computationally, and, as we show in certain cases, statistically, leading to faster convergence rates. Our framework unifies several previous estimators, for most of which ours are the first finite sample guarantees.
C1 [Singh, Shashank] Carnegie Mellon Univ, Stat Dept, Pittsburgh, PA 15213 USA.
   [Singh, Shashank; Poczos, Barnabas] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.
RP Singh, S (reprint author), Carnegie Mellon Univ, Stat Dept, Pittsburgh, PA 15213 USA.; Singh, S (reprint author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.
EM sss1@andrew.cmu.edu; bapoczos@cs.cmu.edu
FU National Science Foundation Graduate Research Fellowship [DGE-1252522]
FX This material is based upon work supported by a National Science
   Foundation Graduate Research Fellowship to the first author under Grant
   No. DGE-1252522.
CR Adami C, 2004, PHYS LIFE REV, V1, P3, DOI 10.1016/j.plrev.2004.01.002
   Aghagolzadeh M., 2007, P IEEE INT C IM PROC
   ALEMANY PA, 1994, PHYS REV E, V49, pR956, DOI 10.1103/PhysRevE.49.R956
   Berrett Thomas B, 2016, ARXIV160600304
   Biau Gerard, 2015, LECT NEAREST NEIGHBO, P75
   BIRGE L, 1995, ANN STAT, V23, P11, DOI 10.1214/aos/1176324452
   Chai B., 2009, NIPS
   Chaudhuri K, 2014, IEEE T INFORM THEORY, V60, P7900, DOI 10.1109/TIT.2014.2361055
   Chaudhuri Kamalika, 2014, ADV NEURAL INFORM PR, P3437
   EFRON B, 1981, ANN STAT, V9, P586, DOI 10.1214/aos/1176345462
   Evans D, 2008, P ROY SOC A-MATH PHY, V464, P3175, DOI 10.1098/rspa.2008.0235
   Gao W., 2016, ARXIV160403006
   Goria MN, 2005, J NONPARAMETR STAT, V17, P277, DOI 10.1080/104852504200026815
   Hero A. O., CSPL328
   Hero AO, 2002, IEEE SIGNAL PROC MAG, V19, P85, DOI 10.1109/MSP.2002.1028355
   Hlavackova-Schindler K, 2007, PHYS REP, V441, P1, DOI 10.1016/j.physrep.2006.12.004
   Kandasamy K., 2015, ADV NEURAL INFORM PR, P397
   Kontorovich A., 2015, P 18 INT C ART INT S, P480
   Kozachenko L. F., 1987, PROBL PEREDACHI INF, V23, P9
   KPOTUFE S., 2011, P 28 INT C MACH LEAR, P225
   Krishnamurthy A., 2014, INT C MACH LEARN ICM
   Leonenko N, 2008, ANN STAT, V36, P2153, DOI 10.1214/07-AOS539
   Lewi J., 2007, ADV NEURAL INFORM PR, V19
   LIU H, 2012, NEURAL INFORM PROCES
   LOFTSGAARDEN DO, 1965, ANN MATH STAT, V36, P1049, DOI 10.1214/aoms/1177700079
   Mack YP, 1979, J MULTIVAR ANAL
   Moon K., 2014, ADV NEURAL INFORM PR, P2420
   Moon KR, 2014, IEEE INT SYMP INFO, P356, DOI 10.1109/ISIT.2014.6874854
   Nguyen X., 2010, IEEE T INFORM THEORY
   Oja E., 2003, J MACHINE LEARNING R, V4, P1271
   Oliva J., 2013, INT C MACH LEARN ICM
   Pal D., 2010, P NEURAL INFORM PROC
   Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159
   Perez- Cruz F., 2008, ADV NEURAL INFORM PR, V21
   Poczos B., 2012, 25 IEEE C COMP VIS P
   Poczos B., 2005, ICML
   Poczos B., 2011, INT C ART INT STAT, V15, P609
   Poczos B, 2009, J MACH LEARN RES, V10, P515
   Shan C., 2005, BRIT MACH VIS C BMVC
   Singh S., 2014, INT C MACH LEARN ICM
   Singh S., 2014, NEURAL INFORM PROCES
   Sricharan K, 2013, IEEE T INFORM THEORY, V59, P4374, DOI 10.1109/TIT.2013.2251456
   Sricharan K, 2012, IEEE T INFORM THEORY, V58, P4135, DOI 10.1109/TIT.2012.2195549
   Sricharan K, 2011, 2011 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY PROCEEDINGS (ISIT), P1205, DOI 10.1109/ISIT.2011.6033726
   Szabo Z, 2007, J MACH LEARN RES, V8, P1063
   Szabo Z, 2014, J MACH LEARN RES, V15, P283
   Tsybakov AB, 1996, SCAND J STAT, V23, P75
   Van Hulle MM, 2008, NEURAL COMPUT, V20, P964, DOI 10.1162/neco.2008.10-06-383
   Wang Q, 2009, IEEE T INFORM THEORY, V55, P2392, DOI 10.1109/TIT.2009.2016060
   Wolsztynski E, 2005, SIGNAL PROCESS, V85, P937, DOI 10.1016/j.sigpro.2004.11.028
NR 50
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700087
DA 2019-06-15
ER

PT S
AU Sinha, A
   Duchi, J
AF Sinha, Aman
   Duchi, John
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Learning Kernels with Random Features
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID GENERALIZATION ERROR
AB Randomized features provide a computationally efficient way to approximate kernel machines in machine learning tasks. However, such methods require a user-defined kernel as input. We extend the randomized-feature approach to the task of learning a kernel (via its associated random features). Specifically, we present an efficient optimization problem that learns a kernel in a supervised manner. We prove the consistency of the estimated kernel as well as generalization bounds for the class of estimators induced by the optimized kernel, and we experimentally evaluate our technique on several datasets. Our approach is efficient and highly scalable, and we attain competitive results with a fraction of the training cost of other techniques.
C1 [Sinha, Aman; Duchi, John] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
   [Duchi, John] Stanford Univ, Dept Stat, Stanford, CA 94305 USA.
RP Sinha, A (reprint author), Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
EM amans@stanford.edu; jduchi@stanford.edu
FU Fannie & John Hertz Foundation Fellowship; Stanford Graduate Fellowship
FX This research was supported by a Fannie & John Hertz Foundation
   Fellowship and a Stanford Graduate Fellowship.
CR Alexander Zien, 2007, P 24 INT C MACH LEAR, P1191
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Ben-Hur A, 2005, BIOINFORMATICS, V21, pI38, DOI 10.1093/bioinformatics/bti1016
   Ben-Tal A, 2013, MANAGE SCI, V59, P341, DOI 10.1287/mnsc.1120.1641
   Bertsekas D.P., 1999, NONLINEAR PROGRAMMIN
   Boucheron S., 2013, CONCENTRATION INEQUA
   Cho M., 2013, P NATL ACAD SCI, V110
   Cho Y., 2009, ADV NEURAL INFORM PR, P342, DOI DOI 10.1021/ed028p10
   Cortes C, 2010, P 27 INT C MACH LEAR, P247
   Cortes C, 2012, J MACH LEARN RES, V13, P795
   Cristianini N, 2006, STUD FUZZ SOFT COMP, V194, P205
   Duchi J. C., 2008, P 25 INT C MACH LEAR
   Duvenaud D., 2013, ARXIV13024922
   Girolami M., 2005, P 22 INT C MACH LEAR, V161, P241, DOI DOI 10.1145/1102351.1102382
   Gonen M, 2011, J MACH LEARN RES, V12, P2211
   Hinton GE, 2008, ADV NEURAL INFORM PR, P1249
   Kandola Jaz, 2002, OPTIMIZING KERNEL AL
   Kloft M, 2011, J MACH LEARN RES, V12, P953
   Koltchinskii V, 2002, ANN STAT, V30, P1
   Koltchinskii V, 2005, ANN STAT, V33, P1455, DOI 10.1214/009053605000000228
   Lanckriet GRG, 2004, J MACH LEARN RES, V5, P27
   Le Q., 2013, P 30 INT C MACH LEAR, P244
   Luenberger D. G., 1969, OPTIMIZATION VECTOR
   Qiu SB, 2009, IEEE ACM T COMPUT BI, V6, P190, DOI 10.1109/TCBB.2008.139
   Rahimi A., 2007, ADV NEURAL INFORM PR, V20
   Rahimi A., 2008, ADV NEURAL INFORM PR, V21
   Samson PM, 2000, ANN PROBAB, V28, P416
   Shawe-Taylor John, 2004, KERNEL METHODS PATTE
   Ying Y., 2009, BMC BIOINFORMATICS, V10, P1
NR 29
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701037
DA 2019-06-15
ER

PT S
AU Sinha, A
   Gleich, DF
   Ramani, K
AF Sinha, Ayan
   Gleich, David F.
   Ramani, Karthik
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Deconvolving Feedback Loops in Recommender Systems
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID MODEL
AB Collaborative filtering is a popular technique to infer users' preferences on new content based on the collective information of all users preferences. Recommender systems then use this information to make personalized suggestions to users. When users accept these recommendations it creates a feedback loop in the recommender system, and these loops iteratively influence the collaborative filtering algorithm's predictions over time. We investigate whether it is possible to identify items affected by these feedback loops. We state sufficient assumptions to deconvolve the feedback loops while keeping the inverse solution tractable. We furthermore develop a metric to unravel the recommender system's influence on the entire user-item rating matrix. We use this metric on synthetic and real-world datasets to (1) identify the extent to which the recommender system affects the final rating matrix, (2) rank frequently recommended items, and (3) distinguish whether a user's rated item was recommended or an intrinsic preference. Our results indicate that it is possible to recover the ratings matrix of intrinsic user preferences using a single snapshot of the ratings matrix without any temporal information.
C1 [Sinha, Ayan; Gleich, David F.; Ramani, Karthik] Purdue Univ, W Lafayette, IN 47907 USA.
RP Sinha, A (reprint author), Purdue Univ, W Lafayette, IN 47907 USA.
EM sinhayan@mit.edu; dgleich@purdue.edu; ramani@purdue.edu
FU NSF [CAREER CCF-1149756, IIS-1422918, IIS-1546488]; Center for Science
   of Information STC [CCF-093937]; DARPA SIMPLEX
FX David Gleich would like to acknowledge the support of the NSF via awards
   CAREER CCF-1149756, IIS-1422918, IIS-1546488, and the Center for Science
   of Information STC, CCF-093937, as well as the support of DARPA SIMPLEX.
CR Adomavicius G, 2005, IEEE T KNOWL DATA EN, V17, P734, DOI 10.1109/TKDE.2005.99
   Amatriain X., 2009, P 3 ACM C REC SYST R, P173, DOI DOI 10.1145/1639714.1639744
   Barzel B, 2013, NAT BIOTECHNOL, V31, P720, DOI 10.1038/nbt.2601
   Bennett J., 2007, P KDD CUP WORKSH, P3
   Bottou L, 2013, J MACH LEARN RES, V14, P3207
   Chen L, 2015, USER MODEL USER-ADAP, V25, P99, DOI 10.1007/s11257-015-9155-5
   Cosley Dan, 2003, P SIGCHI C HUM FACT, V5, P585, DOI DOI 10.1145/642611.642713
   Deshpande M, 2004, ACM T INFORM SYST, V22, P143, DOI 10.1145/963770.963776
   Edelman B, 2007, AM ECON REV, V97, P242, DOI 10.1257/aer.97.1.242
   Feizi S, 2013, NAT BIOTECHNOL, V31, P726, DOI 10.1038/nbt.2635
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gleich D.F., 2011, P 17 ACM SIGKDD INT, P60
   Goldberg K, 2001, INFORM RETRIEVAL, V4, P133, DOI 10.1023/A:1011419012209
   Hu YF, 2008, IEEE DATA MINING, P263, DOI 10.1109/ICDM.2008.22
   Jawaheer G., 2010, P 1 INT WORKSH INF H, P47, DOI DOI 10.1145/1869446.1869453
   Lathia N, 2010, SIGIR 2010: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH DEVELOPMENT IN INFORMATION RETRIEVAL, P210
   Li L., 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758
   Li W., 2010, KDD, P27
   Linden G, 2003, IEEE INTERNET COMPUT, V7, P76, DOI 10.1109/MIC.2003.1167344
   March JG, 1991, ORGAN SCI, V2, P71, DOI 10.1287/orsc.2.1.71
   McAuley JJ, 2013, P 22 INT C WORLD WID, P897, DOI DOI 10.1145/2488388.2488466
   Poston RS, 2005, MIS QUART, V29, P221
   Ricci F, 2011, RECOMMENDER SYSTEMS HANDBOOK, P1, DOI 10.1007/978-0-387-85820-3_1
   Salganik MJ, 2006, SCIENCE, V311, P854, DOI 10.1126/science.1121066
   Sarwar B, 2001, P 10 INT C WORLD WID, P285, DOI DOI 10.1145/371920.372071
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702030
DA 2019-06-15
ER

PT S
AU Sohn, K
AF Sohn, Kihyuk
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Improved Deep Metric Learning with Multi-class N-pair Loss Objective
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID SIMILARITY
AB Deep metric learning has gained much popularity in recent years, following the success of deep learning. However, existing frameworks of deep metric learning based on contrastive loss and triplet loss often suffer from slow convergence, partially because they employ only one negative example while not interacting with the other negative classes in each update. In this paper, we propose to address this problem with a new metric learning objective called multi-class N - pair loss. The proposed objective function firstly generalizes triplet loss by allowing joint comparison among more than one negative examples - more specifically, N - 1 negative examples - and secondly reduces the computational burden of evaluating deep embedding vectors via an efficient batch construction strategy using only N pairs of examples, instead of (N + 1) x N. We demonstrate the superiority of our proposed loss to the triplet loss as well as other competing loss functions for a variety of tasks on several visual recognition benchmark, including fine-grained object recognition and verification, image clustering and retrieval, and face verification and identification.
C1 [Sohn, Kihyuk] NEC Labs Amer Inc, Princeton, NJ 08540 USA.
RP Sohn, K (reprint author), NEC Labs Amer Inc, Princeton, NJ 08540 USA.
EM ksohn@nec-labs.com
CR Best-Rowden L, 2014, IEEE T INF FOREN SEC, V9, P2144, DOI 10.1109/TIFS.2014.2359577
   Chechik G, 2010, J MACH LEARN RES, V11, P1109
   Chopra  S., 2005, CVPR
   Cui Y., 2016, CVPR
   Girshick R, 2016, IEEE T PATTERN ANAL, V38, P142, DOI 10.1109/TPAMI.2015.2437384
   Goldberger J., 2004, NIPS
   Hadsell Raia, 2006, CVPR
   Huang G. B., 2008, CVPR WORKSH
   Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jia Y., 2014, ARXIV14085093
   Kingma D. P., 2015, ICLR
   Krause J., 2013, ICCV WORKSH
   Krizhevsky A., 2012, NIPS
   Liu J., 2015, ABS150607310 CORR
   LOWE DG, 1995, NEURAL COMPUT, V7, P72, DOI 10.1162/neco.1995.7.1.72
   Manning C. D., 2008, INTRO INFORM RETRIEV, V1
   Norouzi M., 2012, NIPS
   Parkhi Omkar M, 2015, BMVC
   Schroff F., 2015, CVPR
   Simonyan Karen, 2015, ICLR
   Song H. Oh, 2016, CVPR
   Sun Yi, 2014, NIPS
   Szegedy C., 2015, CVPR
   Taigman Y., 2014, CVPR
   Wah C., 2011, CNST2011001 CALTECH
   Wang J, 2014, CVPR
   Weinberger K. Q., 2005, NIPS
   Weston Jason, 2011, IJCAI, P2764, DOI [10.5591/978-1-57735-516-8/IJCAI11-460, DOI 10.5591/978-1-57735-516-8/IJCAI11-460]
   Xie S., 2015, CVPR
   Xing E. P., 2003, DISTANCE METRIC LEAR
   Yi D., 2014, LEARNING FACE REPRES, V1411, P7923
   Zhang X., 2016, CVPR
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701057
DA 2019-06-15
ER

PT S
AU Sokolov, A
   Kreutzer, J
   Lo, C
   Riezler, S
AF Sokolov, Artem
   Kreutzer, Julia
   Lo, Christopher
   Riezler, Stefan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Stochastic Structured Prediction under Bandit Feedback
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Stochastic structured prediction under bandit feedback follows a learning protocol where on each of a sequence of iterations, the learner receives an input, predicts an output structure, and receives partial feedback in form of a task loss evaluation of the predicted structure. We present applications of this learning scenario to convex and non-convex objectives for structured prediction and analyze them as stochastic first-order methods. We present an experimental evaluation on problems of natural language processing over exponential output spaces, and compare convergence speed across different objectives under the practical criterion of optimal task performance on development data and the optimization-theoretic criterion of minimal squared gradient norm. Best results under both criteria are obtained for a non-convex objective for pairwise preference learning under bandit feedback.
C1 [Sokolov, Artem; Kreutzer, Julia; Lo, Christopher; Riezler, Stefan] Heidelberg Univ, Computat Linguist, Heidelberg, Germany.
   [Riezler, Stefan] Heidelberg Univ, IWR, Heidelberg, Germany.
   [Lo, Christopher] Tufts Univ, Dept Math, Boston, MA 02111 USA.
   [Sokolov, Artem] Amazon Dev Ctr, Berlin, Germany.
RP Sokolov, A (reprint author), Heidelberg Univ, Computat Linguist, Heidelberg, Germany.; Sokolov, A (reprint author), Amazon Dev Ctr, Berlin, Germany.
EM sokolov@cl.uni-heidelberg.de; kreutzer@cl.uni-heidelberg.de;
   chris.aa.lo@gmail.com; riezler@cl.uni-heidelberg.de
FU German research foundation (DFG); Amazon Development Center Germany
FX This research was supported in part by the German research foundation
   (DFG), and in part by a research cooperation grant with the Amazon
   Development Center Germany.
CR Agarwal A., 2010, COLT
   Busa-Fekete R., 2014, ALT
   Chang Kai-Wei, 2015, ICML
   Duchi JC, 2015, IEEE T INFORM THEORY, V61, P2788, DOI 10.1109/TIT.2015.2409256
   Dyer C., 2010, ACL DEMO
   Freund Y., 2003, J MACHINE LEARNING R, V4, P933
   Ghadimi S., 2012, SIAM J OPTIMIZ, V4, P2342
   Herbrich R, 2000, ADV NEUR IN, P115
   Ionides EL, 2008, J COMPUT GRAPH STAT, V17, P295, DOI 10.1198/106186008X320456
   Joachims  T., 2002, KDD
   Langford  John, 2007, NIPS
   Lazaric A, 2012, J COMPUT SYST SCI, V78, P1516, DOI 10.1016/j.jcss.2011.12.027
   Li L., 2010, WWW
   Polyak B. T., 1987, INTRO OPTIMIZATION
   Ranzato Marc Aurelio, 2016, ICLR
   Sha F., 2003, NAACL
   Simianer P., 2012, ACL
   Smith N., 2011, LINGUISTIC STRUCTURE
   Sokolov A., 2016, ACL
   Sokolov A., 2015, MT SUMMIT 15
   Solodov MV, 1998, COMPUT OPTIM APPL, V11, P23, DOI 10.1023/A:1018366000512
   Sutton R. S., 2000, NIPS
   Thurstone LL, 1927, PSYCHOL REV, V34, P273, DOI 10.1037/h0070288
   Yue Y., 2009, ICML
   Yuille Alan, 2012, Frontiers of Electrical and Electronic Engineering in China, V7, P94, DOI 10.1007/s11460-012-0170-6
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700098
DA 2019-06-15
ER

PT S
AU Song, Y
   Zhu, J
   Ren, Y
AF Song, Yang
   Zhu, Jun
   Ren, Yong
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Kernel Bayesian Inference with Posterior Regularization
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We propose a vector-valued regression problem whose solution is equivalent to the reproducing kernel Hilbert space (RKHS) embedding of the Bayesian posterior distribution. This equivalence provides a new understanding of kernel Bayesian inference. Moreover, the optimization problem induces a new regularization for the posterior embedding estimator, which is faster and has comparable performance to the squared regularization in kernel Bayes' rule. This regularization coincides with a former thresholding approach used in kernel POMDPs whose consistency remains to be established. Our theoretical work solves this open problem and provides consistency analysis in regression settings. Based on our optimizational formulation, we propose a flexible Bayesian posterior regularization framework which for the first time enables us to put regularization at the distribution level. We apply this method to nonparametric state-space filtering tasks with extremely nonlinear dynamics and show performance gains over all other baselines.
C1 [Song, Yang] Tsinghua Univ, Dept Phys, Beijing, Peoples R China.
   [Zhu, Jun; Ren, Yong] Tsinghua Univ, TNList Lab, Dept Comp Sci & Tech, Beijing, Peoples R China.
   [Zhu, Jun; Ren, Yong] Tsinghua Univ, Ctr Bioinspired Comp Res, State Key Lab Intell Tech & Syst, Beijing, Peoples R China.
RP Zhu, J (reprint author), Tsinghua Univ, TNList Lab, Dept Comp Sci & Tech, Beijing, Peoples R China.; Zhu, J (reprint author), Tsinghua Univ, Ctr Bioinspired Comp Res, State Key Lab Intell Tech & Syst, Beijing, Peoples R China.
EM yangsong@cs.stanford.edu; dcszj@.tsinghua.edu.cn;
   renyong15@mails.tsinghua.edu.cn
FU National Basic Research Program (973 Program) of China [2013CB329403];
   National NSF of China [61620106010, 61322308, 61332007]; Youth Top-notch
   Talent Support Program; Tsinghua Initiative Scientific Research Program
   [20141080934]
FX We thank all the anonymous reviewers for valuable suggestions. The work
   was supported by the National Basic Research Program (973 Program) of
   China (No. 2013CB329403), National NSF of China Projects (Nos.
   61620106010, 61322308, 61332007), the Youth Top-notch Talent Support
   Program, and Tsinghua Initiative Scientific Research Program (No.
   20141080934).
CR ARONSZAJN N, 1950, T AM MATH SOC, V68, P337
   Berlinet A., 2011, REPRODUCING KERNEL H
   Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]
   De Vito Ernesto, 2005, TECHNICAL REPORT
   Engl H. W., 1996, REGULARIZATION INVER, V375
   Fukumizu K., 2011, ADV NEURAL INFORM PR, V24, P1737
   Ganchev K, 2010, J MACH LEARN RES, V11, P2001
   Grunewalder Steffen, 2012, ARXIV12064655
   Julier SJ, 1997, P SOC PHOTO-OPT INS, V3068, P182, DOI 10.1117/12.280797
   Kalman R. E, 1960, J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]
   Le Song, 2013, IEEE Signal Processing Magazine, V30, P98, DOI 10.1109/MSP.2013.2252713
   Lever G., 2012, P 29 INT C MACH LEAR, P1823
   Micchelli CA, 2005, NEURAL COMPUT, V17, P177, DOI 10.1162/0899766052530802
   Micchelli CA, 2006, J MACH LEARN RES, V7, P2651
   Nishiyama Y., 2012, ARXIV12104887
   Smola A. J., 1998, LEARNING KERNELS
   Smola AJ, 2003, LECT NOTES ARTIF INT, V2777, P144, DOI 10.1007/978-3-540-45167-9_12
   Smola A, 2007, LECT NOTES ARTIF INT, V4754, P13
   Song Le, 2010, P INT C ART INT STAT, P765
   Song Le, 2010, HILBERT SPACE EMBEDD
   Song LD, 2009, PROCEEDINGS OF THE FIBER SOCIETY 2009 SPRING CONFERENCE, VOLS I AND II, P961
   Wan EA, 2000, IEEE 2000 ADAPTIVE SYSTEMS FOR SIGNAL PROCESSING, COMMUNICATIONS, AND CONTROL SYMPOSIUM - PROCEEDINGS, P153, DOI 10.1109/ASSPCC.2000.882463
   WILLIAMS PM, 1980, BRIT J PHILOS SCI, V31, P131, DOI 10.1093/bjps/31.2.131
   Zhu J, 2014, J MACH LEARN RES, V15, P1799
   Zhu J, 2012, J MACH LEARN RES, V13, P2237
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701092
DA 2019-06-15
ER

PT S
AU Song, Z
   Woodruff, DP
   Zhang, H
AF Song, Zhao
   Woodruff, David P.
   Zhang, Huan
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Sublinear Time Orthogonal Tensor Decomposition
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB A recent work (Wang et. al., NIPS 2015) gives the fastest known algorithms for orthogonal tensor decomposition with provable guarantees. Their algorithm is based on computing sketches of the input tensor, which requires reading the entire input. We show in a number of cases one can achieve the same theoretical guarantees in sublinear time, i.e., even without reading most of the input tensor. Instead of using sketches to estimate inner products in tensor decomposition algorithms, we use importance sampling. To achieve sublinear time, we need to know the norms of tensor slices, and we show how to do this in a number of important cases. For symmetric tensors T = Sigma(k)(i=1) lambda(i)u(i)(circle times p) with lambda(i) > 0 for all i, we estimate such norms in sublinear time whenever p is even. For the important case of p = 3 and small values of k, we can also estimate such norms. For asymmetric tensors sublinear time is not possible in general, but we show if the tensor slice norms are just slightly below parallel to T parallel to(F) then sublinear time is again possible. One of the main strengths of our work is empirical - in a number of cases our algorithm is orders of magnitude faster than existing methods with the same accuracy.
C1 [Song, Zhao] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.
   [Woodruff, David P.] IBM Almaden Res Ctr, San Jose, CA USA.
   [Zhang, Huan] Univ Calif Davis, Dept Elect & Comp Engn, Davis, CA 95616 USA.
RP Song, Z (reprint author), Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.
EM zhaos@utexas.edu; dpwoodru@us.ibm.com; ecezhang@ucdavis.edu
FU XDATA DARPA Air Force Research Laboratory [FA8750-12-C-0323]
FX Supported by XDATA DARPA Air Force Research Laboratory contract
   FA8750-12-C-0323.
CR Anandkumar A., 2012, ADV NEURAL INFORM PR, P917
   Anandkumar A, 2014, J MACH LEARN RES, V15, P2773
   BENTLEY JL, 1980, ACM T MATH SOFTWARE, V6, P359, DOI 10.1145/355900.355907
   Bhojanapalli S., 2015, CORR
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Bringmann K, 2012, LECT NOTES COMPUT SC, V7391, P133, DOI 10.1007/978-3-642-31594-7_12
   Chi Wang, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8726, P290, DOI 10.1007/978-3-662-44845-8_19
   Choi J. H., 2014, ADV NEURAL INFORM PR, V27, P1296
   Clarkson KL, 2012, J ACM, V59, DOI 10.1145/2371656.2371658
   Harshman R. A., 1970, FDN PARAFAC PROCEDUR, V16, P84
   Huang F., 2013, CORR
   Kang U, 2012, P 18 ACM SIGKDD INT, P316, DOI DOI 10.1145/2339530.2339583
   Knuth D. E., 1969, ART COMPUTER PROGRAM, V2, P229
   Moitra A., 2014, TENSOR DECOMPOSITION
   MONEMIZADEH M, 2010, SODA, V135, P1143
   Pham N., 2013, P 19 ACM SIGKDD INT, P239
   Phan AH, 2013, IEEE T SIGNAL PROCES, V61, P4834, DOI 10.1109/TSP.2013.2269903
   Tsourakakis C. E., 2010, SDM, P689
   Tung H.-Y. F., 2015, SPECTRAL METHODS HIE
   Walker A. J., 1977, ACM Transactions on Mathematical Software, V3, P253, DOI 10.1145/355744.355749
   Wang Y., 2016, CORR
   Wang Y., 2015, ADV NEURAL INFORM PR, P991
   WANG YS, 2016, COMMUNICATION
NR 23
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973704021
DA 2019-06-15
ER

PT S
AU Song, Z
   Parr, R
   Liao, XJ
   Carin, L
AF Song, Zhao
   Parr, Ronald
   Liao, Xuejun
   Carin, Lawrence
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Linear Feature Encoding for Reinforcement Learning
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Feature construction is of vital importance in reinforcement learning, as the quality of a value function or policy is largely determined by the corresponding features. The recent successes of deep reinforcement learning (RL) only increase the importance of understanding feature construction. Typical deep RL approaches use a linear output layer, which means that deep RL can be interpreted as a feature construction/encoding network followed by linear value function approximation. This paper develops and evaluates a theory of linear feature encoding. We extend theoretical results on feature quality for linear value function approximation from the uncontrolled case to the controlled case. We then develop a supervised linear feature encoding method that is motivated by insights from linear value function approximation theory, as well as empirical successes from deep RL. The resulting encoder is a surprisingly effective method for linear value function approximation using raw images as inputs.
C1 [Song, Zhao; Liao, Xuejun; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.
   [Parr, Ronald] Duke Univ, Dept Comp Sci, Durham, NC 27708 USA.
RP Song, Z (reprint author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.
FU ARO; DARPA; DOE; NGA; ONR; NSF
FX We thank the anonymous reviewers for their helpful comments and
   suggestions. This research was supported in part by ARO, DARPA, DOE,
   NGA, ONR and NSF.
CR Boyd S., 2004, CONVEX OPTIMIZATION
   Bradtke S., 1996, MACHINE LEARNING
   Farahmand A. M., 2012, NIPS
   Geramifard A., 2013, UAI
   Ghavamzadeh M., 2010, NIPS
   Hansen P. C., 1987, BIT NUMERICAL MATH
   Johns J., 2010, NIPS
   Kolter J. Z., 2009, ICML
   Lagoudakis M., 2003, JMLR
   Liang Y., 2016, AAMAS
   Mahadevan S., 2007, JMLR
   Mallat S. G., 1993, IEEE TSP
   Mnih V., 2015, NATURE
   Oh  J., 2015, NIPS
   Painter-wakefield C., 2012, ICML
   Parr R., 2007, ICML
   Parr R., 2008, ICML
   Petrik M., 2010, ICML
   Schoknecht R., 2002, NIPS
   Sutton R., 1988, MACHINE LEARNING
   Sutton R. S., 2008, UAI
   Sutton R. S., 1998, REINFORCEMENT LEARNI
   Tesauro G., 1994, NEURAL COMPUTATION
   Tibshirani R., 1996, JRSSB
   Williams R. J., 1993, TECH REP
   Yu H., 2009, IEEE TAC
NR 26
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973702052
DA 2019-06-15
ER

PT S
AU Soto, V
   Suarez, A
   Martinez-Munoz, G
AF Soto, Victor
   Suarez, Alberto
   Martinez-Munoz, Gonzalo
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI An urn model for majority voting in classification ensembles
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB In this work we analyze the class prediction of parallel randomized ensembles by majority voting as an urn model. For a given test instance, the ensemble can be viewed as an urn of marbles of different colors. A marble represents an individual classifier. Its color represents the class label prediction of the corresponding classifier. The sequential querying of classifiers in the ensemble can be seen as draws without replacement from the urn. An analysis of this classical urn model based on the hypergeometric distribution makes it possible to estimate the confidence on the outcome of majority voting when only a fraction of the individual predictions is known. These estimates can be used to speed up the prediction by the ensemble. Specifically, the aggregation of votes can be halted when the confidence in the final prediction is sufficiently high. If one assumes a uniform prior for the distribution of possible votes the analysis is shown to be equivalent to a previous one based on Dirichlet distributions. The advantage of the current approach is that prior knowledge on the possible vote outcomes can be readily incorporated in a Bayesian framework. We show how incorporating this type of problem-specific knowledge into the statistical analysis of majority voting leads to faster classification by the ensemble and allows us to estimate the expected average speed-up beforehand.
C1 [Soto, Victor] Columbia Univ, Comp Sci Dept, New York, NY 10027 USA.
   [Suarez, Alberto; Martinez-Munoz, Gonzalo] Univ Autonoma Madrid, Comp Sci Dept, Madrid, Spain.
RP Soto, V (reprint author), Columbia Univ, Comp Sci Dept, New York, NY 10027 USA.
EM vsoto@cs.columbia.edu; gonzalo.martinez@uam.es; alberto.suarez@uam.es
RI Martinez-Munoz, Gonzalo/K-7269-2012
OI Martinez-Munoz, Gonzalo/0000-0002-6125-6056
FU Comunidad de Madrid [CASI-CAM-CM S2013/ICE-2845]; Spanish Ministerio de
   Economia y Competitividad [TIN2013-42351-P, TIN2015-70308-REDT]
FX The authors acknowledge financial support from the Comunidad de Madrid
   (project CASI-CAM-CM S2013/ICE-2845), and from the Spanish Ministerio de
   Economia y Competitividad (projects TIN2013-42351-P and
   TIN2015-70308-REDT).
CR Asuncion A., 2007, UCI MACHINE LEARNING
   Basilico J. D., 2011, Proceedings of the 2011 IEEE 11th International Conference on Data Mining (ICDM 2011), P41, DOI 10.1109/ICDM.2011.39
   Benbouzid D., 2012, P 29 INT C MACH LEAR, P951
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Breiman L, 1996, 460 U CAL STAT DEP
   Caruana R., 2006, ICML 2006 P 23 INT C, P161, DOI DOI 10.1145/1143844.1143865
   Caruana Rich, 2004, P 21 INT C MACH LEAR
   Dietterich TG, 2000, MACH LEARN, V40, P139, DOI 10.1023/A:1007607513941
   Dietterich TG, 2000, LECT NOTES COMPUT SC, V1857, P1
   Fan W, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P146
   Fernandez-Delgado M, 2014, J MACH LEARN RES, V15, P3133
   Gao T., 2011, NIPS
   HANSEN LK, 1990, IEEE T PATTERN ANAL, V12, P993, DOI 10.1109/34.58871
   Hernandez-Lobato D, 2009, IEEE T PATTERN ANAL, V31, P364, DOI 10.1109/TPAMI.2008.204
   HO TK, 1994, IEEE T PATTERN ANAL, V16, P66, DOI 10.1109/34.273716
   Margineantu D.D., 1997, P 14 INT C MACH LEAR, V97, P211
   Markatopoulou F, 2015, NEUROCOMPUTING, V150, P501, DOI 10.1016/j.neucom.2014.07.063
   Reyzin L., 2011, INT C MACH LEARN, P529
   Sutton R. S., 2011, 10 INT C AUT AG MULT, V2, P761
   Wang Haixun, 2003, P 9 ACM SIGKDD INT C, P226, DOI DOI 10.1145/956750.956778
   Zhang Y, 2006, J MACH LEARN RES, V7, P1315
NR 21
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700084
DA 2019-06-15
ER

PT S
AU Springenberg, JT
   Klein, A
   Falkner, S
   Hutter, F
AF Springenberg, Jost Tobias
   Klein, Aaron
   Falkner, Stefan
   Hutter, Frank
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Bayesian Optimization with Robust Bayesian Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Bayesian optimization is a prominent method for optimizing expensive-to-evaluate black-box functions that is widely applied to tuning the hyperparameters of machine learning algorithms. Despite its successes, the prototypical Bayesian optimization approach-using Gaussian process models-does not scale well to either many hyperparameters or many function evaluations. Attacking this lack of scalability and flexibility is thus one of the key challenges of the field. We present a general approach for using flexible parametric models (neural networks) for Bayesian optimization, staying as close to a truly Bayesian treatment as possible. We obtain scalability through stochastic gradient Hamiltonian Monte Carlo, whose robustness we improve via a scale adaptation. Experiments including multi-task Bayesian optimization with 21 tasks, parallel optimization of deep neural networks and deep reinforcement learning show the power and flexibility of this approach.
C1 [Springenberg, Jost Tobias; Klein, Aaron; Falkner, Stefan; Hutter, Frank] Univ Freiburg, Dept Comp Sci, Freiburg, Germany.
RP Springenberg, JT (reprint author), Univ Freiburg, Dept Comp Sci, Freiburg, Germany.
EM springj@cs.uni-freiburg.de; kleinaa@cs.uni-freiburg.de;
   sfalkner@cs.uni-freiburg.de; fh@cs.uni-freiburg.de
FU European Commission [H2020-ICT-645403-ROBDREAM]; German Research
   Foundation (DFG) [SPP 1527, HU 1900/3-1, HU 1900/2-1];
   BrainLinks-BrainTools Cluster of Excellence [EXC 1086]
FX This work has partly been supported by the European Commission under
   Grant no. H2020-ICT-645403-ROBDREAM, by the German Research Foundation
   (DFG), under Priority Programme Autonomous Learning (SPP 1527, grant HU
   1900/3-1), under Emmy Noether grant HU 1900/2-1, and under the
   BrainLinks-BrainTools Cluster of Excellence (grant number EXC 1086).
CR Bergstra J., 2011, P NIPS 11
   Blundell C., 2015, P ICML 15
   Brochu E, 2010, CORR
   Chen Changyou, 2016, P AISTATS
   Chen T., 2014, P ICML 14
   Duane S., 1987, PHYS LETT B
   Duchi  J., 2011, J MACHINE LEARNING R
   Eggensperger K., 2013, BAYESOPT 13
   Feurer M., 2015, P AAAI 15
   Gal Y, 2015, ARXIV150602142
   Girolami Mark, 2011, J ROYAL STAT SOC B
   Graves A., 2011, P ICML 11
   Gu S., 2016, P ICML
   He K., 2016, P CVPR 16
   Hernandez-Lobato J. M., 2015, P ICML 15
   Hutter F., 2011, LION 11
   Jones D., 1998, JGO
   Kingma D. P., 2015, P NIPS 15
   Klein A., 2016, CORR
   Korattikara A., 2015, P NIPS 15
   Li Chunyuan, 2016, P AAAI 16
   Lillicrap T. P., 2016, P ICLR
   Ma Y., 2015, P NIPS 15
   Neal R., 1996, THESIS
   Schaul T., 2013, P ICML 13
   Snoek J., 2015, P ICML 15
   Snoek J., 2012, P NIPS 12
   Srinivas N., 2010, P ICML 10
   Swersky K., 2013, P NIPS 13
   Swersky K., 2014, CORR
   TIELEMAN T, 2012, COURSERA NEURAL NETW
   Vanschoren J., 2014, SIGKDD EXPLOR NEWSL
NR 32
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973700081
DA 2019-06-15
ER

PT S
AU Steinhardt, J
   Liang, P
AF Steinhardt, Jacob
   Liang, Percy
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Unsupervised Risk Estimation Using Only Conditional Independence
   Structure
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID CLASSIFICATION
AB We show how to estimate a model's test error from unlabeled data, on distributions very different from the training distribution, while assuming only that certain conditional independencies are preserved between train and test. We do not need to assume that the optimal predictor is the same between train and test, or that the true distribution lies in any parametric family. We can also efficiently compute gradients of the estimated error and hence perform unsupervised discriminative learning. Our technical tool is the method of moments, which allows us to exploit conditional independencies in the absence of a fully-specified model. Our framework encompasses a large family of losses including the log and exponential loss, and extends to structured output settings such as conditional random fields.
C1 [Steinhardt, Jacob; Liang, Percy] Stanford Univ, Stanford, CA 94305 USA.
RP Steinhardt, J (reprint author), Stanford Univ, Stanford, CA 94305 USA.
EM jsteinhardt@cs.stanford.edu; pliang@cs.stanford.edu
FU Fannie & John Hertz Foundation; NSF; Future of Life Institute
FX This research was supported by a Fannie & John Hertz Foundation
   Fellowship, a NSF Graduate Research Fellowship, and a Future of Life
   Institute grant.
CR Anandkumar A., 2012, COLT
   Anandkumar A., 2013, TENSOR DECOMPOSITION
   ANDERSON TW, 1949, ANN MATH STAT, V20, P46, DOI 10.1214/aoms/1177730090
   ANDERSON TW, 1950, ANN MATH STAT, V21, P570, DOI 10.1214/aoms/1177729752
   Ando R. K., 2007, COLT
   Balasubramanian K, 2011, J MACH LEARN RES, V12, P3119
   Balcan MF, 2010, J ACM, V57, DOI 10.1145/1706591.1706599
   Blitzer J., 2011, AISTATS
   Blum A., 1998, COLT
   Bottou L., 2015, COMMUNICATION
   Chaganty A., 2013, ICML
   Chaganty A., 2014, ICML
   Comon P, 2009, J CHEMOMETR, V23, P393, DOI 10.1002/cem.1236
   Cozman F., 2006, SEMISUPERVISED LEARN
   Dawid A. P., 1979, APPL STAT, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]
   De Lathauwer L, 2006, SIAM J MATRIX ANAL A, V28, P642, DOI 10.1137/040608830
   Donmez P, 2010, J MACH LEARN RES, V11, P1323
   Duchi J., 2010, COLT
   EDMONDS J, 1972, J ACM, V19, P248, DOI 10.1145/321694.321699
   Green JBD, 2014, J ANAL METHODS CHEM, DOI 10.1155/2014/810589
   Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806
   Hansen L. P., 1982, ECONOMETRICA
   Hansen L. P., 2014, J POLITICAL EC, V122
   Hsu D., 2012, NIPS
   Jaffe A., 2015, P 18 INT C ART INT S, P407
   Jaffe Ariel, 2016, P ART INT STAT, P351
   Joachims T., 1999, ICML
   Johansson F., 2016, ICML
   Kakade S. M, 2007, COLT
   Khani F., 2016, ACL
   Kuleshov  V., 2015, AISTATS
   Li LH, 2011, MACH LEARN, V82, P399, DOI 10.1007/s10994-010-5225-4
   Li YF, 2015, IEEE T PATTERN ANAL, V37, P175, DOI 10.1109/TPAMI.2014.2299812
   Liang P., 2008, HLT ACL
   Mansour  Yishay, 2009, COLT
   Merialdo B., 1994, Computational Linguistics, V20, P155
   Nigam K., 1998, ASS ADVANCEMENT ARTI
   Platanios E. A., 2015, THESIS
   Powell J. L., 1994, HDB ECONOMETRICS, V4
   Quinonero-Candela J, 2009, NEURAL INF PROCESS S, pXI
   Sargan J. D., 1958, ECONOMETRICA
   SARGAN JD, 1959, J ROY STAT SOC B, V21, P91
   Sculley D., 2015, NIPS, P2494
   Shafer G, 2008, J MACH LEARN RES, V9, P371
   Shai Ben-David, 2006, P 20 ANN C NEUR INF, P137
   Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4
   Steinhardt J., 2016, COLT
   Tomizawa N., 1971, NETWORKS
NR 48
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701058
DA 2019-06-15
ER

PT S
AU Steinhardt, J
   Valiant, G
   Charikar, M
AF Steinhardt, Jacob
   Valiant, Gregory
   Charikar, Moses
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Avoiding Imposters and Delinquents: Adversarial Crowdsourcing and Peer
   Prediction
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB We consider a crowdsourcing model in which n workers are asked to rate the quality of ro, items previously generated by other workers. An unknown set of an workers generate reliable ratings, while the remaining workers may behave arbitrarily and possibly adversarially. The manager of the experiment can also manually evaluate the quality of a small number of items, and wishes to curate together almost all of the high-quality items with at most an 6 fraction of low-quality items. Perhaps surprisingly, we show that this is possible with an amount of work required of the manager, and each worker, that does not scale with n: the dataset can be curated with O (1/beta alpha(3)epsilon(4)) ratings per worker, and O (1/beta epsilon(2)) ratings by the manager, where beta is the fraction of high-quality items. Our results extend to the more general setting of peer prediction, including peer grading in online classrooms.
C1 [Steinhardt, Jacob; Valiant, Gregory; Charikar, Moses] Stanford Univ, Stanford, CA 94305 USA.
RP Steinhardt, J (reprint author), Stanford Univ, Stanford, CA 94305 USA.
FU Fannie & John Hertz Foundation Fellowship; NSF Graduate Research
   Fellowship; Future of Life Institute grant; NSF CAREER award
   [CCF-1351108]; Sloan Foundation Research Fellowship; Okawa Foundation;
   NSF [CCF-1565581, CCF-1617577, CCF-1302518]; Simons Investigator Award
FX JS was supported by a Fannie & John Hertz Foundation Fellowship, an NSF
   Graduate Research Fellowship, and a Future of Life Institute grant. GV
   was supported by NSF CAREER award CCF-1351108, a Sloan Foundation
   Research Fellowship, and a research grant from the Okawa Foundation. MC
   was supported by NSF grants CCF-1565581, CCF-1617577, CCF-1302518 and a
   Simons Investigator Award.
CR Abbe E., 2015, COMMUNITY DETECTION
   Abbe E., 2015, DETECTION STOCHASTIC
   Agarwal N., 2015, MULTISECTION STOCHAS
   Banks J., 2016, INFORM THEORETIC THR
   Cai TT, 2015, ANN STAT, V43, P1027, DOI 10.1214/14-AOS1290
   Chen Y, 2014, INT GEOSCI REMOTE SE, DOI 10.1109/IGARSS.2014.6947442
   Chin P., 2015, C LEARN THEOR COLT
   Christiano P., 2014, PROVABLY MANIPULATIO
   Christiano P., 2016, ROBUST COLLABORATIVE
   Coja-Oghlan A., 2004, AUTOMATA LANGUAGES P
   Coja-Oghlan A, 2007, J ALGORITHMS, V62, P19, DOI 10.1016/j.jalgor.2004.07.003
   Condon A, 2001, RANDOM STRUCT ALGOR, V18, P116, DOI 10.1002/1098-2418(200103)18:2<116::AID-RSA1001>3.0.CO;2-2
   Dasgupta A., 2013, WWW
   Decelle A, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.066106
   DENG J, 2009, PROC CVPR IEEE, P248, DOI DOI 10.1109/CVPR.2009.5206848
   Feige U, 2001, J COMPUT SYST SCI, V63, P639, DOI 10.1006/jcss.2001.1773
   Feige U, 2000, RANDOM STRUCT ALGOR, V16, P195
   Ghosh A., 2011, P 12 ACM C EL COMM, P167, DOI DOI 10.1145/1993574.1993599
   Green JBD, 2014, J ANAL METHODS CHEM, DOI 10.1155/2014/810589
   Guedon O., 2014, COMMUNITY DETECTION
   Harmon A., 2004, NY TIMES
   Holland P. W., 1983, SOCIAL NETWORKS
   Kamble V., 2015, TRUTH SERUMS MASSIVE
   Karger DR, 2014, OPER RES, V62, P1, DOI 10.1287/opre.2013.1235
   Krivelevich M, 2006, SIAM PROC S, P211
   Kulkarni C, 2015, UNDERST INNOV, P131, DOI 10.1007/978-3-319-06823-7_9
   Le C. M., 2015, CONCENTRATION REGULA
   Makarychev K., 2015, LEARNING COMMUNITIES
   Makarychev Konstantin, 2012, P 44 ANN ACM S THEOR, P367, DOI DOI 10.1145/2213977.2214013
   Massoulie L., 2014, STOC
   Mayzlin D., 2012, TECHNICAL REPORT
   Miller N, 2005, MANAGE SCI, V51, P1359, DOI 10.1287/ninsc.1050.0379
   Moitra A., 2015, ROBUST ARE RECONSTRU
   Mossel E., 2015, STOC
   Mossel E., 2013, BELIEF PROPAGATION R
   MOSSEL E., 2012, STOCHASTIC BLOCK MOD
   Mossel E., 2013, PROOF BLOCK MODEL TH
   Piech C., 2013, TUNED MODELS PEER AS
   Priedhorsky R, 2007, GROUP'07: PROCEEDINGS OF THE 2007 INTERNATIONAL ACM CONFERENCE ON SUPPORTING GROUP WORK, P259
   Resnick P, 2007, RECSYS 07: PROCEEDINGS OF THE 2007 ACM CONFERENCE ON RECOMMENDER SYSTEMS, P25
   Shah N., 2015, ICML
   Shah N. B., 2015, ADV NEURAL INFORM PR
   Shnayder V., 2016, STRONG TRUTHFULNESS
   Vuurens J., 2011, ACM SIGIR WORKSH CRO
   Zhou D., 2015, REGULARIZED MINIMAX
NR 45
TC 0
Z9 0
U1 1
U2 1
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703076
DA 2019-06-15
ER

PT S
AU Stoudenmire, EM
   Schwab, DJ
AF Stoudenmire, E. M.
   Schwab, David J.
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Supervised Learning with Tensor Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
ID FEATURE-EXTRACTION; DECOMPOSITIONS; CLASSIFICATION
AB Tensor networks are approximations of high-order tensors which are efficient to work with and have been very successful for physics and mathematics applications. We demonstrate how algorithms for optimizing tensor networks can be adapted to supervised learning tasks by using matrix product states (tensor trains) to parameterize non-linear kernel learning models. For the MNIST data set we obtain less than 1% test set classification error. We discuss an interpretation of the additional structure imparted by the tensor network to the learned model.
C1 [Stoudenmire, E. M.] Perimeter Inst Theoret Phys, Waterloo, ON N2L 2Y5, Canada.
   [Schwab, David J.] Northwestern Univ, Dept Phys, Evanston, IL 60208 USA.
RP Stoudenmire, EM (reprint author), Perimeter Inst Theoret Phys, Waterloo, ON N2L 2Y5, Canada.
CR Anandkumar A, 2014, J MACH LEARN RES, V15, P2773
   Anandkumar A, 2014, J MACH LEARN RES, V15, P2239
   Phan AH, 2010, IEICE NONLINEAR THEO, V1, P37, DOI 10.1587/nolta.1.37
   Bengua JA, 2015, IEEE INT CONGR BIG, P669, DOI 10.1109/BigDataCongress.2015.105
   Bridgeman Jacob C., 2016, ARXIV160303039
   Cesa-Bianchi N., 2015, P 28 C LEARN THEOR, P297
   Christopher J. C., MNIST HANDWRITTEN DI
   Cichocki  A., 2014, ARXIV14073124
   Evenbly G, 2011, J STAT PHYS, V145, P891, DOI 10.1007/s10955-011-0237-4
   Evenbly G, 2009, PHYS REV B, V79, DOI 10.1103/PhysRevB.79.144108
   Holtz S, 2012, SIAM J SCI COMPUT, V34, pA683, DOI 10.1137/100818893
   Muller KR, 2001, IEEE T NEURAL NETWOR, V12, P181, DOI 10.1109/72.914517
   Nielsen M.A., 2015, NEURAL NETWORKS DEEP
   Novikov A., 2016, ARXIV160503795
   Novikov Alexander, 2015, ARXIV150906569
   Oseledets IV, 2011, SIAM J SCI COMPUT, V33, P2295, DOI 10.1137/090752286
   OSTLUND S, 1995, PHYS REV LETT, V75, P3537, DOI 10.1103/PhysRevLett.75.3537
   Schollwock U, 2011, ANN PHYS-NEW YORK, V326, P96, DOI 10.1016/j.aop.2010.09.012
   Stoudenmire EM, 2013, PHYS REV B, V87, DOI 10.1103/PhysRevB.87.155137
   Vapnik V. N., 2000, NATURE STAT LEARNING
   Verstraete F, 2004, PHYS REV LETT, V93, DOI 10.1103/PhysRevLett.93.227205
   Verstraete F., 2004, CONDMAT0407066
   Vidal G, 2007, PHYS REV LETT, V99, DOI 10.1103/PhysRevLett.99.220405
   Waegeman W, 2012, IEEE T FUZZY SYST, V20, P1090, DOI 10.1109/TFUZZ.2012.2194151
   WHITE SR, 1992, PHYS REV LETT, V69, P2863, DOI 10.1103/PhysRevLett.69.2863
NR 25
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973701068
DA 2019-06-15
ER

PT S
AU Subramaniam, A
   Chatterjee, M
   Mittal, A
AF Subramaniam, Arulkumar
   Chatterjee, Moitreya
   Mittal, Anurag
BE Lee, DD
   Sugiyama, M
   Luxburg, UV
   Guyon, I
   Garnett, R
TI Deep Neural Networks with Inexact Matching for Person Re-Identification
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)
SE Advances in Neural Information Processing Systems
LA English
DT Proceedings Paper
CT 30th Conference on Neural Information Processing Systems (NIPS)
CY 2016
CL Barcelona, SPAIN
AB Person Re-Identification is the task of matching images of a person across multiple camera views. Almost all prior approaches address this challenge by attempting to learn the possible transformations that relate the different views of a person from a training corpora. Then, they utilize these transformation patterns for matching a query image to those in a gallery image bank at test time. This necessitates learning good feature representations of the images and having a robust feature matching technique. Deep learning approaches, such as Convolutional Neural Networks (CNN), simultaneously do both and have shown great promise recently.
   In this work, we propose two CNN-based architectures for Person Re-Identification. In the first, given a pair of images, we extract feature maps from these images via multiple stages of convolution and pooling. A novel inexact matching technique then matches pixels in the first representation with those of the second. Furthermore, we search across a wider region in the second representation for matching. Our novel matching technique allows us to tackle the challenges posed by large viewpoint variations, illumination changes or partial occlusions. Our approach shows a promising performance and requires only about half the parameters as a current state-of-the-art technique. Nonetheless, it also suffers from false matches at times. In order to mitigate this issue, we propose a fused architecture that combines our inexact matching pipeline with a state-of-the-art exact matching technique. We observe substantial gains with the fused model over the current state-of-the-art on multiple challenging datasets of varying sizes, with gains of up to about 21%.
C1 [Subramaniam, Arulkumar; Chatterjee, Moitreya; Mittal, Anurag] Indian Inst Technol Madras, Chennai 600036, Tamil Nadu, India.
RP Subramaniam, A (reprint author), Indian Inst Technol Madras, Chennai 600036, Tamil Nadu, India.
EM aruls@cse.iitm.ac.in; metro.smiles@gmail.com; amittal@cse.iitm.ac.in
CR Ahmed E, 2015, PROC CVPR IEEE, P3908, DOI 10.1109/CVPR.2015.7299016
   Chen DP, 2015, PROC CVPR IEEE, P1565, DOI 10.1109/CVPR.2015.7298764
   Chen JX, 2014, INT C PATT RECOG, P1657, DOI 10.1109/ICPR.2014.292
   Chen YC, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3402
   Davis JV, 2007, P 24 INT C MACH LEAR, P209, DOI DOI 10.1145/1273496.1273523
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   Guillaumin M, 2009, IEEE I CONF COMP VIS, P498, DOI 10.1109/ICCV.2009.5459197
   Hirzer M, 2012, 2012 IEEE NINTH INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL-BASED SURVEILLANCE (AVSS), P203, DOI 10.1109/AVSS.2012.55
   Kostinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Li S, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2155
   Li W., 2013, P AS C COMP VIS, V7724, P31, DOI DOI 10.1007/978-3-642-37331-2
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liao SC, 2015, IEEE I CONF COMP VIS, P3685, DOI 10.1109/ICCV.2015.420
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Loy CC, 2013, IEEE IMAGE PROC, P3567, DOI 10.1109/ICIP.2013.6738736
   Loy CC, 2009, PROC CVPR IEEE, P1988, DOI 10.1109/CVPRW.2009.5206827
   Ma LY, 2014, IEEE T IMAGE PROCESS, V23, P3656, DOI 10.1109/TIP.2014.2331755
   Martinel N, 2015, IEEE T IMAGE PROCESS, V24, P5645, DOI 10.1109/TIP.2015.2487048
   Paisitkriangkrai S, 2015, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2015.7298794
   Prosser B., 2010, P BRIT MACH VIS C, V2, P6
   Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16
   Zhao R, 2014, PROC CVPR IEEE, P144, DOI 10.1109/CVPR.2014.26
   Zhao R, 2013, IEEE I CONF COMP VIS, P2528, DOI 10.1109/ICCV.2013.314
   Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460
   Zheng WS, 2013, IEEE T PATTERN ANAL, V35, P653, DOI 10.1109/TPAMI.2012.138
NR 27
TC 0
Z9 0
U1 0
U2 0
PU NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)
PI LA JOLLA
PA 10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA
SN 1049-5258
J9 ADV NEUR IN
PY 2016
VL 29
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BM0PG
UT WOS:000458973703003
DA 2019-06-15
ER

EF